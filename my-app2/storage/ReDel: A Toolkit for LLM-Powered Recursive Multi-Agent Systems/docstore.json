{"docstore/metadata": {"3276443a-96c9-4093-b31e-ba861e1ccbbc": {"doc_hash": "7777eb080323f3db973da0c0d4d6f86658b191262256f75bdb7872f4c3ca65e1"}, "ddca4564-9449-4eaa-a52c-2a36c05891da": {"doc_hash": "d351b285e1a0ebd9f2f74b603bdaa81aa86a23ab10b83d4e24ce0da741a211a0"}, "d2c67aad-3a15-4442-85c9-c211a3c47e27": {"doc_hash": "ebdd4de495b609780bcf628e77f8470f683c597f06732db2d78b7dcc791b2523"}, "c3fc44d9-3306-4f5b-b001-730a4aa0a12b": {"doc_hash": "80b6a2e97942295d0c7c8da3f80e6241757cd17c7b61f7d68f3c5e916f354357"}, "b9d74d65-ea12-442a-bb93-68f6dc3bb833": {"doc_hash": "64e173e68af57d9f4be07233a857bb792de232589322bade6726e0204b291683"}, "a83b266b-c139-4cc4-9040-3c747df49238": {"doc_hash": "81dabc3346d287cd97c8604357e838d5da25bf353de5831f07d5345fd33652ca"}, "73f4a87c-5a28-457a-be95-dc56c1dfb5f5": {"doc_hash": "f38c0a2223e7cf959c7816688a4deb9b9baed2ded05460b27774be4d066d22f6"}, "66664dfe-0201-4717-889f-a05e988b4ff6": {"doc_hash": "c8efae6ef329aa1dcfa77e01534af7723af9315c8fe4a2199be6a6b068e99647"}, "fcbd7754-297d-4864-ab8d-e7e5fbac6d4c": {"doc_hash": "d79fc4d0c22cd320f4b8d93a784e1eb5e920e73f76732ca67c1944176be8294b"}, "f548c2ec-c616-483f-b66f-5a9ec2a97ae8": {"doc_hash": "7777eb080323f3db973da0c0d4d6f86658b191262256f75bdb7872f4c3ca65e1", "ref_doc_id": "3276443a-96c9-4093-b31e-ba861e1ccbbc"}, "562306ef-bd44-414c-bc84-437fd5519413": {"doc_hash": "d351b285e1a0ebd9f2f74b603bdaa81aa86a23ab10b83d4e24ce0da741a211a0", "ref_doc_id": "ddca4564-9449-4eaa-a52c-2a36c05891da"}, "39893a2a-7efa-4fa9-90b1-cfb5168235a2": {"doc_hash": "ebdd4de495b609780bcf628e77f8470f683c597f06732db2d78b7dcc791b2523", "ref_doc_id": "d2c67aad-3a15-4442-85c9-c211a3c47e27"}, "c507417b-2890-41ca-bb10-37fd9420249a": {"doc_hash": "80b6a2e97942295d0c7c8da3f80e6241757cd17c7b61f7d68f3c5e916f354357", "ref_doc_id": "c3fc44d9-3306-4f5b-b001-730a4aa0a12b"}, "6c92bc54-3b43-49d6-aa4e-aaff8df89d9d": {"doc_hash": "64e173e68af57d9f4be07233a857bb792de232589322bade6726e0204b291683", "ref_doc_id": "b9d74d65-ea12-442a-bb93-68f6dc3bb833"}, "7a9584d0-aaf4-4e34-bda5-51e3b9a6ad02": {"doc_hash": "81dabc3346d287cd97c8604357e838d5da25bf353de5831f07d5345fd33652ca", "ref_doc_id": "a83b266b-c139-4cc4-9040-3c747df49238"}, "58f8c810-1701-4ca2-9e46-4b4038e9e5ec": {"doc_hash": "20083911883d5123d7835e6d06c08f90f4ce314b40d9da2b5dbab942cfb0fafe", "ref_doc_id": "73f4a87c-5a28-457a-be95-dc56c1dfb5f5"}, "10ae9ac2-3f72-45ab-b7ac-839c32f79583": {"doc_hash": "c07ac2106fdac6d44607b801061cbc7d64d145939b07fb61aec49026fb7effd2", "ref_doc_id": "73f4a87c-5a28-457a-be95-dc56c1dfb5f5"}, "72313633-ad93-4bc6-ba03-95ce024458b8": {"doc_hash": "c8efae6ef329aa1dcfa77e01534af7723af9315c8fe4a2199be6a6b068e99647", "ref_doc_id": "66664dfe-0201-4717-889f-a05e988b4ff6"}, "8a89fc09-8130-4933-bae5-4421f308f227": {"doc_hash": "d79fc4d0c22cd320f4b8d93a784e1eb5e920e73f76732ca67c1944176be8294b", "ref_doc_id": "fcbd7754-297d-4864-ab8d-e7e5fbac6d4c"}}, "docstore/data": {"f548c2ec-c616-483f-b66f-5a9ec2a97ae8": {"__data__": {"id_": "f548c2ec-c616-483f-b66f-5a9ec2a97ae8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3276443a-96c9-4093-b31e-ba861e1ccbbc", "node_type": "4", "metadata": {}, "hash": "7777eb080323f3db973da0c0d4d6f86658b191262256f75bdb7872f4c3ca65e1", "class_name": "RelatedNodeInfo"}}, "text": "# ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems\n\n# Andrew Zhu, Liam Dugan, Chris Callison-Burch\n\n# University of Pennsylvania\n\n{andrz,ldugan,ccb}@seas.upenn.edu\n\n# Abstract\n\nRecently, there has been increasing interest in using Large Language Models (LLMs) to construct complex multi-agent systems to perform tasks such as compiling literature reviews, drafting consumer reports, and planning vacations. Many tools and libraries exist for helping create such systems, however none support recursive multi-agent systems\u2014where the models themselves flexibly decide when to delegate tasks and how to organize their delegation structure. In this work, we introduce ReDel: a toolkit for recursive multi-agent systems that supports custom tool-use, delegation schemes, event-based logging, and interactive replay in an easy-to-use web interface. We show that, using ReDel, we are able to achieve significant performance gains on agentic benchmarks and easily identify potential areas of improvements through the visualization and debugging tools. Our code, documentation, and PyPI package are open-source1 and free to use under the MIT license.\n\n# 1 Introduction\n\nA multi-agent system uses multiple large language models (LLMs) together to accomplish complex tasks or answer complex questions beyond the capabilities of a single LLM. Often, in such scenarios, each LLM is provided with tools (Parisi et al., 2022; Schick et al., 2023) that it can use to give it additional capabilities, like searching the internet for real-time data or interacting with a web browser. In most cases, these systems are defined manually, with a human responsible for defining a static problem-decomposition graph and defining an agent to handle each subproblem in the graph (Hong et al., 2024; Wu et al., 2023; Zhang et al., 2024; Qiao et al., 2024, inter alia).\n\nIn a recursive multi-agent system, rather than a human defining the layout of multiple agents, a single root agent is given a tool to spawn additional agents. When faced with a complex task, the root agent can decompose the task into smaller sub-tasks, then delegate those tasks to newly-created sub-agents. Each sub-agent can then either complete the task if it is small enough, or recursively decompose and delegate the task further2 (Khot et al., 2023; Lee and Kim, 2023; Prasad et al., 2024).\n\nIn the current landscape of multi-agent systems, the majority of tooling focuses on human-defined static systems, and poorly handles dynamic systems where agents are added to a computation graph at runtime. Furthermore, much of this tooling is unsuitable for academic purposes (Zhu et al., 2023) or hidden behind paywalls and proprietary licenses.\n\nIn this paper, we present ReDel, a fully-featured open-source toolkit for recursive multi-agent systems. ReDel makes it easy to experiment by providing a modular interface for creating tools, different delegation methods, and logs for later analysis. This granular logging and a central event-driven system makes it easy to listen for signals from any-\n\n2This is where the toolkit\u2019s name, ReDel, comes from: it\u2019s short for Recursive Delegation.\n\n# Help me plan a trip to Japan visiting 3 cities.\n\nI'll need to find flights, hotels, trains, and food.\n\nSearch for flights to Japan...\n\nFlightSearch[SFO, NRT]\n\nFind hotels in Tokyo, Osaka, ...\n\nSearch for a hotel in Tokyo...\n\nSearch for a hotel in Osaka...\n\n# Delegation Graph\n\nFigure 1: ReDel allows developers to create systems of recursive agents, inspect each agent\u2019s state, and visualize a system\u2019s delegation graph (right). Recursive agents can be used to solve complex tasks, such as planning a trip to Japan (left).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "562306ef-bd44-414c-bc84-437fd5519413": {"__data__": {"id_": "562306ef-bd44-414c-bc84-437fd5519413", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ddca4564-9449-4eaa-a52c-2a36c05891da", "node_type": "4", "metadata": {}, "hash": "d351b285e1a0ebd9f2f74b603bdaa81aa86a23ab10b83d4e24ce0da741a211a0", "class_name": "RelatedNodeInfo"}}, "text": "# 2 Related Work\n\n# Recursive Multi-Agent Systems.\n\nRecent work on recursive multi-agent systems has been done by Lee and Kim (2023), Khot et al. (2023), and Prasad et al. (2024). These works introduce the method of fine-tuning or few-shot prompting LLMs to decompose complex tasks and using sub-agents to solve each part (often called recursive or hierarchical decomposition). They show that as tasks grow more complex, recursive multi-agent systems\u2019 performance pulls away from single-agent systems. ReDel builds upon the methods introduced in these works by taking advantage of modern models\u2019 native tool use capability (Schick et al., 2023) to decompose and delegate tasks zero-shot (i.e., without human-written examples in prompt) instead of using few-shot prompting or fine-tuning. As a framework, we provide an extensible interface to apply these approaches to additional tasks and domains.\n\n# Multi-Agent System Frameworks.\n\nAlthough there are other LLM-powered multi-agent system frameworks, each have various weaknesses that make them poorly suited for recursive systems and/or academic purposes. In Table 1, we compare LangGraph (Campos et al., 2023), LlamaIndex (Liu et al., 2022), MetaGPT (Hong et al., 2024), and AutoGPT (Significant Gravitas, 2023) to ReDel, our system. Most are built around static multi-agent systems, with only AutoGPT supporting a single level of delegation. Only LangGraph and LlamaIndex allow agents to run in parallel asynchronously, whereas MetaGPT and AutoGPT run one agent at a time in a synchronous fashion. To log events deep within the system, only LlamaIndex provides a rigorous instrumentation suite to developers that allows them to emit events at any point while a system is running. Most do not allow developers to replay a system run from a log, with only LangGraph allowing replays by taking snapshots of each state of the system. Most do not provide a visualization interface, with only AutoGPT providing a simple chat-based UI. Unless one subscribes to a paid service, LangGraph\u2019s replays cannot be viewed visually, and are instead presented as the raw data of each state. Finally, only AutoGPT and MetaGPT are fully open-source, with LangGraph and LlamaIndex utilizing proprietary code to offer more \u201cpremium\u201d features beyond what their open-source libraries offer.\n\nIn comparison, ReDel allows developers to customize their agents\u2019 delegation strategies and build multi-level dynamic systems while providing all of these features out of the box and remaining fully free and open source. It is the only such toolkit to provide first-class support for recursive multi-agent systems with best-in-class support for system visualization and modern LLMs with tool usage.\n\n# 3 System Design\n\nReDel consists of two main parts: a Python package to define recursive delegation systems, log events, and run experiments, and a web interface to quickly and interactively iterate on defined systems or analyze experiment logs. In the following sections, we discuss these components in more detail.\n\n# 3.1 Tool Usage\n\nIn ReDel, a \u201ctool\u201d is a group of functions, written in Python, that is exposed to an agent. The agent may generate requests to call appropriate functions from this tool, which interact with the environment (e.g. searching the Internet).\n\nDevelopers can define tools in any Python file, and a tool\u2019s methods can be implemented by any Python code. ReDel is implemented in pure Python, and method bodies will not be sent to an agent\u2019s\n\n|Feature|ReDel|LangGraph|LlamaIndex|MetaGPT|AutoGPT|\n|---|---|---|---|---|---|\n|Dynamic Systems|\u2714|\u2714|\u2714|\u2716|\u2716|\n|Parallel Agents|\u2714|\u2714|\u2714|\u2716|\u2716|\n|Event-Driven|\u2714|\u2716|\u2714|\u2716|\u2716|\n|Run Replay|\u2714|\u2714|\u2716|\u2716|\u2716|\n|Web Interface|\u2714|\u2716|\u2716|\u2716|\u2716|\n|Fully Open Source|\u2714|\u2716|\u2716|\u2714|\u2714|\n\nTable 1: A feature comparison between ReDel and competing toolkits. ReDel is the only fully open-source toolkit that supports dynamic multi-agent systems with a rich event-driven base and web interface.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39893a2a-7efa-4fa9-90b1-cfb5168235a2": {"__data__": {"id_": "39893a2a-7efa-4fa9-90b1-cfb5168235a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2c67aad-3a15-4442-85c9-c211a3c47e27", "node_type": "4", "metadata": {}, "hash": "ebdd4de495b609780bcf628e77f8470f683c597f06732db2d78b7dcc791b2523", "class_name": "RelatedNodeInfo"}}, "text": "# 3.2 Delegation Schemes\n\nA delegation scheme is the strategy used by an agent to send tasks to sub-agents. In ReDel, delegation schemes are implemented as a special type of tool that an LLM agent (the \u201cparent\u201d) can call with task instructions as an argument. These instructions are sent to a new sub-agent (the \u201cchild\u201d), which can either complete them if they are simple enough, or break them up into smaller parts and recursively delegate again.\n\nTaking inspiration from common process management paradigms found in operating systems, ReDel comes with two delegation schemes:\n\n- DelegateOne: Block parent agent\u2019s execution until child agent returns its result.\n- DelegateWait: Do not block parent agent\u2019s execution. Instead, provide a separate function to retrieve the result of a particular child.\n\nThe DelegateOne scheme is well-suited for LLMs with parallel function calling as it allows ReDel to let a group of spawned child agents run in parallel, and return their results once they all complete. In contrast, the DelegateWait scheme is well-suited for LLMs without parallel function calling, as it lets these models spawn multiple agents before deciding to wait on any one agent\u2019s result.\n\n# 3.3 Events & Logging\n\nReDel operates as an event-driven framework, with comprehensive built-in events and the ability to define custom events. An event can be defined as anything from the creation of a sub-agent to the usage of a particular tool. Whenever ReDel catches an event, it can log it for later analysis.\n\nDevelopers can also implement their own delegation schemes modularly in a fashion similar to defining tools which can enable more complex behaviour like multi-turn delegation. We include an example of how to define a delegation scheme in Appendix A.\n\nFrom our testing, this is a fairly rare occurrence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c507417b-2890-41ca-bb10-37fd9420249a": {"__data__": {"id_": "c507417b-2890-41ca-bb10-37fd9420249a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3fc44d9-3306-4f5b-b001-730a4aa0a12b", "node_type": "4", "metadata": {}, "hash": "80b6a2e97942295d0c7c8da3f80e6241757cd17c7b61f7d68f3c5e916f354357", "class_name": "RelatedNodeInfo"}}, "text": "# RuDel\n\n# Root node message history.\n\n# Computation graph.\n\nClick a node to view its message history in the selected node view.\n\nA running node.\n\nWaiting on children.\n\nA finished node.\n\nThe selected node.\n\nThe root node.\n\nStart a new empty session with the configured ReDel system.\n\nLoad a saved session in the replay viewer.\n\nInteractive sessions you've started appear here.\n\nRead more about ReDel.\n\nStart a new session with the configured ReDel system by sending the first message.\n\nSend new messages to the root node.\n\n# (a) The home page of the ReDel web interface.\n\n# (b) ReDel\u2019s interactive view allows users to quickly iterate on prompts and tool design, and test end-to-end performance.\n\n# Root node message history.\n\n# Computation graph.\n\nClick a node to view its message history in the selected node view.\n\nA running node.\n\nWaiting on children.\n\nA finished node.\n\nThe selected node.\n\nThe root node.\n\nSort saves by edit time, name, or event count.\n\nThe current directory (relative to the save roots).\n\nSearch all save titles for keywords.\n\n# The save's title.\n\nThe number of events in the save.\n\nThe date and time the save was last modified.\n\n# Jump to:\n\n- Previous/next event\n- Previous/next message (selected node)\n- Previous/next message (root)\n\nEvent count.\n\nSeek (click & drag)\n\n# (c) The save browser displays logs found in configured directories on the filesystem. It allows developers to search for and review previous runs of ReDel systems.\n\n# (d) ReDel\u2019s replay view allows developers to replay saved runs of ReDel systems, giving events temporal context when analyzing or debugging a system\u2019s performance.\n\nFigure 5: The four views of the ReDel web interface: Home (a), Interactive (b), Save Browser (c), and Replay (d).\n\nan event, it logs the event to a JSONL file. This file essentially acts as an execution trace for a system run and users can use standard data analysis tools to inspect this trace and debug their runs. Figure 4 shows how a basic Python script can be used to count a system\u2019s token usage post-hoc.\n\nFurthermore, using just the built-in events, ReDel is able to interactively play back any response through our web interface for extra visual debugging aid (see Section 3.4). In Section 4 we show a case study of how this can be used to debug complex query failures. We provide the set of built-in default events in Appendix B and an example of defining a custom event in Figure 3.\n\n# Interactive View.\n\nIn the interactive view (Figure 5b), users can send messages to the root node to interact with the system. While the system is running, the top right panel contains the delegation graph: a visual representation of each agent in the system, their parent and children, and what their current status is: running (green), waiting (yellow), or done (grey). Users can further inspect each node in the delegation graph by clicking it, which displays its full message history in the bottom right panel. ReDel supports streaming, and LLM generations appear in real-time for every agent.\n\n# Web Interface\n\n# Home Page.\n\nThe home page (Figure 5a) is the default view when starting the interface for the first time. Users can transition to the interactive view by sending a message in the chat bar, or use the provided buttons to load a saved replay or read more about ReDel. The sidebar lets users switch between interactive sessions they have started, start new sessions, or load saved replays.\n\n# Save Browser.\n\nThe save browser (Figure 5c) allows users to select replays to view from the list of previous sessions. This allows researchers to run experiments in batches while saving their logs, and use the interface to review the system\u2019s behaviour at a later date. The save list contains all the saves that the ReDel server found in the provided save directories, their titles, number of events, and when they were last edited. Users can search for keywords in a save\u2019s title and can also sort saves by.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c92bc54-3b43-49d6-aa4e-aaff8df89d9d": {"__data__": {"id_": "6c92bc54-3b43-49d6-aa4e-aaff8df89d9d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9d74d65-ea12-442a-bb93-68f6dc3bb833", "node_type": "4", "metadata": {}, "hash": "64e173e68af57d9f4be07233a857bb792de232589322bade6726e0204b291683", "class_name": "RelatedNodeInfo"}}, "text": "# Evaluation & Case Study\n\nTo evaluate ReDel, we compare its performance to a baseline single-agent system and to the published state-of-the-art system on three different benchmarks. We include the logs and source code for all experiments in our code release.\n\n# 4.1 Experimental Setup\n\n# Benchmarks.\n\nTo properly evaluate ReDel we had to choose only datasets that contained sufficiently complex tasks. For our benchmarks we therefore chose the following:\n\n1. FanOutQA: (Zhu et al., 2024) Agents must compile data from many Wikipedia articles to answer complex information-seeking queries.\n2. TravelPlanner: (Xie et al., 2024) Agents must create travel plans using tools to search flights, restaurant, and attraction databases.\n3. WebArena: (Zhou et al., 2024) Agents must do complex web tasks such as adding products to a shopping cart or commenting on GitLab.\n\nDue to cost constraints we limited our evaluation to roughly 100-300 examples from each benchmark (see Appendix C).\n\n# Models.\n\nFor our main two ReDel systems we used GPT-4o (OpenAI, 2024) and GPT-3.5-turbo (OpenAI, 2022) as the underlying models. In all setups, root nodes are not given tool usage capabilities and use the DelegateOne delegation scheme.\n\nFor the two baseline systems, we used the GPT-4o and GPT-3.5-turbo models as-is. All models were given equal access to all tools and no few-shot prompting or fine-tuning was performed.\n\n# 4.2 Results\n\nIn Table 2 we report the results of our evaluation. We see that, across all benchmarks, our recursive delegation system significantly outperforms its corresponding single-agent baseline. We even present an improvement over the previous state of the art systems in both FanOutQA and TravelPlanner.\n\nFurthermore, we see that the gap between ReDel and the baseline system gets larger as the capabilities of the underlying model improves. We believe that this bodes well for the application of such techniques to future, more powerful models.\n\nIn the few cases where ReDel fails, namely H-Micro on TravelPlanner and SR on WebArena, these are attributable to metric failures and unequal comparisons. In the TravelPlanner case, on further inspection, we find that recursive systems tend to make more commonsense inputs for meals (e.g. \u201con the flight\u201d or \u201cpacked lunch\u201d) \u2013 which causes the TravelPlanner evaluation script to give a score of 0 on the Hard Constraint metric. As for the WebArena result, the published SotA SteP model uses few-shot, chain-of-thought prompting, whereas our systems all use zero-shot prompting.\n\n# Table 2: Systems\u2019 performance on FanOutQA, TravelPlanner, and WebArena.\n\n|System| |LooseFanOutQAModel Judge|CS-Micro|TravelPlannerH-Micro|Final SR| |SR (AC)|WebArenaSR (UA)|\n|---|---|---|---|---|---|---|---|---|\n|ReDel (GPT-4o)|0.687|0.494|67.49|9.52|2.78|0.203|0.179|0.643|\n|ReDel (GPT-3.5-turbo)|0.300|0.087|54.58|0|0|0.092|0.066|0.571|\n|Baseline (GPT-4o)|0.650|0.394|50.83|18.81|0|0.162|0.128|0.786|\n|Baseline (GPT-3.5-turbo)|0.275|0.077|48.75|0.24|0|0.085|0.058|0.571|\n|Published SotA|0.580|0.365|61.1|15.2|1.11|0.358|\u2014|\u2014|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a9584d0-aaf4-4e34-bda5-51e3b9a6ad02": {"__data__": {"id_": "7a9584d0-aaf4-4e34-bda5-51e3b9a6ad02", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a83b266b-c139-4cc4-9040-3c747df49238", "node_type": "4", "metadata": {}, "hash": "81dabc3346d287cd97c8604357e838d5da25bf353de5831f07d5345fd33652ca", "class_name": "RelatedNodeInfo"}}, "text": "# Using ReDel for Error Analysis\n\n| | |FOQA|OCTPUC|OCWAUC| | | |\n|---|---|---|---|---|---|---|---|\n| |System|OC|UC| | | | |\n| |RD (4o)|22.7|11.3|41.1|0.5|31.3|44.8|\n| |RD (3.5-t)|40.8|1.1|96.7|0|54.6|17.7|\n\nTable 3: The overcommitment (OC) and undercommitment (UC) rates, in percent, of the two recursive multi-agent systems we tested, by benchmark.\n\nFor our error analysis, we took the saved log files for each benchmark and manually investigated the logs of both the successful runs as well as the failed runs through the replay view of the ReDel web interface. Through this investigation we observed two common failure cases in recursive multi-agent systems. These cases are as follows:\n\n- Overcommitment: The agent attempts to complete an overly-complex task itself.\n- Undercommitment: The agent performs no work and re-delegates the task it was given.\n\nWe find that overcommitment commonly occurs when an agent performs multiple tool calls and fills its context window with retrieved information. This often, but not always, leads to failures due to the retrieved information truncating the original task from its context. In the ReDel web interface, this manifests as an abnormally small delegation graph, often consisting of only two nodes: the root node, and a single child which the root delegates to and which subsequently overcommits.\n\nIn contrast, we find that undercommitment commonly happens when the model incorrectly decides that it does not have the necessary tools to solve the problem and instead assumes that its future child will possess the required tools to solve the problem. In all three benchmarks, this led to failure as agents entered an infinite loop of delegation until they reached a configured depth limit or timed out. In the web interface, this manifests as a line of nodes in the delegation graph (Figure 6).\n\nIn Table 3 we tabulate the over- and undercommitment rates of ReDel with both GPT-4o and GPT-3.5-turbo for each benchmark. We did this heuristically by counting any delegation graph with two or fewer agents as overcommitted and any delegation graph with a chain of three or more agents with exactly zero or one children as undercommitted. We see that as models get stronger they have a stronger propensity to delegate. However, that propensity to delegate may lead to undercommitment.\n\nGiven the prevalence of these two issues, we hypothesize that recursive multi-agent systems may still see further improvements to performance from interventions that target these behaviors. For example, one could fine-tune or prompt agents with domain-specific instructions that detail when the models should delegate and when they should perform tasks on their own.\n\nFigure 6: Recursive systems exhibiting undercommitment produce long chains of agents (blue boxes), as seen in the ReDel delegation graph.\n\nWhile implementing such improvements is beyond the scope of this paper, we believe that this case study helps to demonstrate the strengths of the ReDel system. Using the delegation graph view, it is easy to identify and characterize errors in recursive multi-agent systems and we hope that through ReDel more research can be done to further refine such systems for maximum utility.\n\n# Conclusion\n\nWe present ReDel, a novel toolkit for working with recursive multi-agent systems. ReDel allows academic developers to quickly build, iterate on, and run experiments involving dynamic multi-agent systems. It offers a modular interface to create tools for agents to use, an event framework to instrument experiments for later analysis, and a free and open-source web interface to interact with and explore developer-defined systems. We use ReDel to demonstrate recursive multi-agent systems\u2019 performance on three diverse benchmarks, and we include the full logs of these runs in our demo release for reproducibility and further exploration.\n\nReDel opens the door for a new paradigm of recursive multi-agent systems, and we are excited to see how developers can utilize our system in the future.\n\n4https://datasets.mechanus.zhu.codes/redel-dist.zip", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4080, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58f8c810-1701-4ca2-9e46-4b4038e9e5ec": {"__data__": {"id_": "58f8c810-1701-4ca2-9e46-4b4038e9e5ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73f4a87c-5a28-457a-be95-dc56c1dfb5f5", "node_type": "4", "metadata": {}, "hash": "f38c0a2223e7cf959c7816688a4deb9b9baed2ded05460b27774be4d066d22f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10ae9ac2-3f72-45ab-b7ac-839c32f79583", "node_type": "1", "metadata": {}, "hash": "c07ac2106fdac6d44607b801061cbc7d64d145939b07fb61aec49026fb7effd2", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nNuno Campos, William FH, Vadym Barda, and Harrison Chase. 2023. LangGraph.\n\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and J\u00fcrgen Schmidhuber. 2024. MetaGPT: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations.\n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations.\n\nSoochan Lee and Gunhee Kim. 2023. Recursion of thought: A divide-and-conquer approach to multi-context reasoning with language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 623\u2013658, Toronto, Canada. Association for Computational Linguistics.\n\nJerry Liu, Logan, and Simon Siu. 2022. LlamaIndex.\n\nOpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue.\n\nOpenAI. 2024. Hello GPT-4o.\n\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. TALM: tool augmented language models. Preprint, arXiv:2205.12255.\n\nArchiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2024. ADaPT: As-needed decomposition and planning with language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 4226\u20134252, Mexico City, Mexico. Association for Computational Linguistics.\n\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. 2023. AutoGen: enabling next-gen llm applications via multi-agent conversation. Preprint, arXiv:2308.08155.\n\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. TravelPlanner: A benchmark for real-world planning with language agents. In Forty-first International Conference on Machine Learning.\n\nCeyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, and Yaodong Yang. 2024. Proagent: Building proactive cooperative agents with large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17591\u201317599.\n\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations.\n\nAndrew Zhu, Liam Dugan, Alyssa Hwang, and Chris Callison-Burch. 2023. Kani: A lightweight and highly hackable framework for building language model applications. In Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 65\u201377, Singapore. Association for Computational Linguistics.\n\nAndrew Zhu, Alyssa Hwang, Liam Dugan, and Chris Callison-Burch. 2024. FanOutQA: a multi-hop, multi-document question answering benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), Bangkok, Thailand. Association for Computational Linguistics.\n\nShuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024. AutoAct: automatic agent learning from scratch for qa via self-planning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10ae9ac2-3f72-45ab-b7ac-839c32f79583": {"__data__": {"id_": "10ae9ac2-3f72-45ab-b7ac-839c32f79583", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73f4a87c-5a28-457a-be95-dc56c1dfb5f5", "node_type": "4", "metadata": {}, "hash": "f38c0a2223e7cf959c7816688a4deb9b9baed2ded05460b27774be4d066d22f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58f8c810-1701-4ca2-9e46-4b4038e9e5ec", "node_type": "1", "metadata": {}, "hash": "20083911883d5123d7835e6d06c08f90f4ce314b40d9da2b5dbab942cfb0fafe", "class_name": "RelatedNodeInfo"}}, "text": "Kani: A lightweight and highly hackable framework for building language model applications. In Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 65\u201377, Singapore. Association for Computational Linguistics.\n\nAndrew Zhu, Alyssa Hwang, Liam Dugan, and Chris Callison-Burch. 2024. FanOutQA: a multi-hop, multi-document question answering benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), Bangkok, Thailand. Association for Computational Linguistics.\n\nShuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024. AutoAct: automatic agent learning from scratch for qa via self-planning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), Bangkok, Thailand. Association for Computational Linguistics.\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems.\n\nSignificant Gravitas. 2023. AutoGPT.", "mimetype": "text/plain", "start_char_idx": 2860, "end_char_idx": 4143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72313633-ad93-4bc6-ba03-95ce024458b8": {"__data__": {"id_": "72313633-ad93-4bc6-ba03-95ce024458b8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66664dfe-0201-4717-889f-a05e988b4ff6", "node_type": "4", "metadata": {}, "hash": "c8efae6ef329aa1dcfa77e01534af7723af9315c8fe4a2199be6a6b068e99647", "class_name": "RelatedNodeInfo"}}, "text": "# A Custom Delegation Scheme\n\nThe following annotated code snippet shows how to use the ReDel Python package to define a delegation scheme \u2013 the delegation scheme here is a reproduction of the bundled DelegateOne scheme.\n\nclass DelegateOne(DelegationBase):\n@ai_function()\nasync def delegate(instructions: str):\n\"\"\"(Insert your prompt for the model here.)\"\"\"\n\n# request a new agent instance from the system\nsubagent = await self.create_delegate_kani(instructions)\n\n# set the state of the delegator agent to be waiting on the delegate\nwith self.kani.run_state(RunState.WAITING):\n# buffer the delegate's response as a list of strings, filtering for ASSISTANT messages\n# use full_round_stream so that the app automatically dispatches streaming events\nresult = []\nasync for stream in subagent.full_round_stream(instructions):\nmsg = await stream.message()\nif msg.role == ChatRole.ASSISTANT and msg.content:\nresult.append(msg.content)\n\n# clean up any of the delegate's ephemeral state and return result to caller\nawait subagent.cleanup()\nreturn \"\\n\".join(result)\nFigure 7: Using ReDel to define a custom delegation scheme. Delegation tools are responsible for the lifecycle of any agent they create.\n\n# B Application Events\n\nThe following table lists the built-in default events that will be emitted on every run of a ReDel system. Each event has a type key which is used to determine what kind of event it is, and a timestamp key.\n\n|Event Name|Key|Description|\n|---|---|---|\n|Agent Spawned|kani_spawn|A new agent was spawned. The data attached to the event contains the full state of the agent at the time it was spawned, which includes its ID, relations to other agents, a description of the LLM powering it, the tools it has access to, and any system prompts.|\n|Agent State Change|kani_state_change|The running state of an agent changed (e.g. from RUNNING to WAITING). Contains the ID of the agent and its new state.|\n|Tokens Used|tokens_used|An agent made a call to the language model powering it. Contains the ID of the agent, the number of tokens in the prompt it sent, and the number of tokens in the completion the LLM returned.|\n|Agent Message|kani_message|An agent added a new message to its chat history. Contains the ID of the agent and the message\u2019s role (e.g. USER or ASSISTANT) and content.|\n|Root Message|root_message|Similar to Agent Message, but only fires for messages in the root node. This is fired in addition to an Agent Message event.|\n|Round Complete|round_complete|Fired when the root node completes a full chat round (i.e. there are no running children and it has generated a response to a user query).|\n\nTable 4: A list of events built-in to the ReDel toolkit.\n\n# C Benchmark Comparison\n\nHere, we tabulate each of the benchmarks tested in our experiments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a89fc09-8130-4933-bae5-4421f308f227": {"__data__": {"id_": "8a89fc09-8130-4933-bae5-4421f308f227", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcbd7754-297d-4864-ab8d-e7e5fbac6d4c", "node_type": "4", "metadata": {}, "hash": "d79fc4d0c22cd320f4b8d93a784e1eb5e920e73f76732ca67c1944176be8294b", "class_name": "RelatedNodeInfo"}}, "text": "# Benchmark\n\n|Benchmark|Split|#|Example|Metrics|\n|---|---|---|---|---|\n|FanOutQA|dev|310|What is the total number of employees in the five largest banks in the world?|Loose: The average proportion of reference strings found in the generated answer. Model Judge: Whether the reference answer and generated answer are equivalent, judged by GPT-4 (gpt-4-0613).|\n|TravelPlanner|val|180|Please help me plan a trip from St. Petersburg to Rockford spanning 3 days from March 16th to March 18th, 2022. The travel should be planned for a single person with a budget of $1,700.|CS-Micro: The proportion of elements in a generated travel plan that do not demonstrate a commonsense error (e.g. visiting the same attraction twice). H-Micro: The proportion of elements in a generated travel plan that do not violate a constraint set by the user or a physical constraint (e.g. budget overruns, non-existent restaurants). Final: The proportion of generated travel plans in which there are no exhibited commonsense errors and all constraints are met (i.e., valid travel plans).|\n|WebArena|test|271|Show me the ergonomic chair with the best rating.|SR: Whether the task is successfully completed or correctly marked as unachievable. SR (AC): Whether the task is successfully completed, only among tasks that are achievable. SR (UA): Whether the task is correctly marked as unachievable, only among tasks that are unachievable.|\n\nTable 5: The dataset split, number of queries, and example queries from each of the benchmarks we test.\n\n# D Additional Design Notes\n\n# D.1 Prompts\n\nIn this section, we provide the prompts used for each benchmark. We use zero-shot prompts for each benchmark, and provide the necessary tools as defined in each benchmark\u2019s paper.\n\nPromptFanOutQA\nUSER: {question}\n(Zhu et al., 2024)TravelPlanner\nSYSTEM: Based on the user\u2019s query, make the best travel plan for the user and save it. Do not ask follow-up questions.\nUSER: {question}\n(Xie et al., 2024)WebArena\nSYSTEM: You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks. These tasks will be accomplished through the use of specific functions you can call.\nHere\u2019s the information you\u2019ll have:\nThe user\u2019s objective: This is the task you\u2019re trying to complete.\nThe current web page\u2019s accessibility tree: This is a simplified representation of the webpage, providing key information.\nThe current web page\u2019s URL: This is the page you\u2019re currently navigating.\nThe open tabs: These are the tabs you have open.\nHomepage: If you want to visit other websites, check out the homepage at http://homepage.com. It has a list of websites you can visit.\nUSER: BROWSER STATE: {observation}\nURL: {url}\nOBJECTIVE: {objective}\n(Zhou et al., 2024)\nTable 6: The prompts used for each benchmark in our evaluation.\n\n# D.2 Identical Delegation Prevention\n\nBy default, the delegation schemes bundled in ReDel will prevent an agent from delegating instructions that are the same as the instructions that were given to it. If an agent attempts to do so, the delegation function returns a message instructing the agent to either attempt the task itself or break it into smaller pieces before delegating again. We implemented this as an early mitigation for undercommitment, but some undercommitment still occurs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"3276443a-96c9-4093-b31e-ba861e1ccbbc": {"node_ids": ["f548c2ec-c616-483f-b66f-5a9ec2a97ae8"], "metadata": {}}, "ddca4564-9449-4eaa-a52c-2a36c05891da": {"node_ids": ["562306ef-bd44-414c-bc84-437fd5519413"], "metadata": {}}, "d2c67aad-3a15-4442-85c9-c211a3c47e27": {"node_ids": ["39893a2a-7efa-4fa9-90b1-cfb5168235a2"], "metadata": {}}, "c3fc44d9-3306-4f5b-b001-730a4aa0a12b": {"node_ids": ["c507417b-2890-41ca-bb10-37fd9420249a"], "metadata": {}}, "b9d74d65-ea12-442a-bb93-68f6dc3bb833": {"node_ids": ["6c92bc54-3b43-49d6-aa4e-aaff8df89d9d"], "metadata": {}}, "a83b266b-c139-4cc4-9040-3c747df49238": {"node_ids": ["7a9584d0-aaf4-4e34-bda5-51e3b9a6ad02"], "metadata": {}}, "73f4a87c-5a28-457a-be95-dc56c1dfb5f5": {"node_ids": ["58f8c810-1701-4ca2-9e46-4b4038e9e5ec", "10ae9ac2-3f72-45ab-b7ac-839c32f79583"], "metadata": {}}, "66664dfe-0201-4717-889f-a05e988b4ff6": {"node_ids": ["72313633-ad93-4bc6-ba03-95ce024458b8"], "metadata": {}}, "fcbd7754-297d-4864-ab8d-e7e5fbac6d4c": {"node_ids": ["8a89fc09-8130-4933-bae5-4421f308f227"], "metadata": {}}}}