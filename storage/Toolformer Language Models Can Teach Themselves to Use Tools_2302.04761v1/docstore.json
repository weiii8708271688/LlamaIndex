{"docstore/data": {"f68d545e-c482-4e5a-916a-b9e6c16a9f89": {"__data__": {"id_": "f68d545e-c482-4e5a-916a-b9e6c16a9f89", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}}, "text": "Toolformer: Language Models Can Teach Themselves to Use Tools\nTimo Schick Jane Dwivedi-Yu Roberto Dess\u00ec\u2020Roberta Raileanu\nMaria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom\nMeta AI Research\u2020Universitat Pompeu Fabra\nAbstract\nLanguage models (LMs) exhibit remarkable\nabilities to solve new tasks from just a few\nexamples or textual instructions, especially at\nscale. They also, paradoxically, struggle with\nbasic functionality, such as arithmetic or fac-\ntual lookup, where much simpler and smaller\nmodels excel. In this paper, we show that\nLMs can teach themselves to use external tools\nvia simple APIs and achieve the best of both\nworlds. We introduce Toolformer , a model\ntrained to decide which APIs to call, when to\ncall them, what arguments to pass, and how to\nbest incorporate the results into future token\nprediction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "07629733-1cdf-4951-8385-d355097f1afc": {"__data__": {"id_": "07629733-1cdf-4951-8385-d355097f1afc", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f68d545e-c482-4e5a-916a-b9e6c16a9f89", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "dcc6f43df45bb78937d522cf21501ea5681bb6dab1765b6895728d2a8fae1d97", "class_name": "RelatedNodeInfo"}}, "text": "This is done in a self-supervised\nway, requiring nothing more than a handful of\ndemonstrations for each API. We incorporate\na range of tools, including a calculator, a Q&A\nsystem, a search engine, a translation system,\nand a calendar. Toolformer achieves substan-\ntially improved zero-shot performance across\na variety of downstream tasks, often competi-\ntive with much larger models, without sacri\ufb01c-\ning its core language modeling abilities.\n1 Introduction\nLarge language models achieve impressive zero-\nand few-shot results on a variety of natural lan-\nguage processing tasks (Brown et al., 2020; Chowd-\nhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scal-\ning.", "mimetype": "text/plain", "start_char_idx": 833, "end_char_idx": 1648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "67d86419-c304-4f5f-bf65-b95021e9de5a": {"__data__": {"id_": "67d86419-c304-4f5f-bf65-b95021e9de5a", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07629733-1cdf-4951-8385-d355097f1afc", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b5ceabd2b91db9ea5ba094b85b3dbea2baedc3936921e360de28fc17f686e78", "class_name": "RelatedNodeInfo"}}, "text": "These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), dif\ufb01cul-\nties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to per-\nform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022).\nThe New England Journal of Medicine is a registered \ntrademark of  [QA(\u201cWho is the publisher of The New  \nEngland Journal of Medicine?\u201d) \u2192 Massachusetts  \nMedical Society]  the MMS. \nOut of 1400 participants, 400 (or [Calculator(400 / 1400)  \n\u2192 0.29]  29%) passed the test.", "mimetype": "text/plain", "start_char_idx": 1649, "end_char_idx": 2349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "41e36281-ebb1-409d-872e-2beda113f627": {"__data__": {"id_": "41e36281-ebb1-409d-872e-2beda113f627", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67d86419-c304-4f5f-bf65-b95021e9de5a", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "adbf1bb12573abd0c53b2e0b42746a0eae3b8c70db756ac96b82bfc25e826453", "class_name": "RelatedNodeInfo"}}, "text": "The name derives from \u201cla tortuga\u201d, the Spanish word for \n[MT(\u201ctortuga\u201d) \u2192 turtle]  turtle. \nThe Brown Act is California\u2019s law  [WikiSearch(\u201cBrown  \nAct\u201d) \u2192 The Ralph M. Brown Act is an act of the  \nCalifornia State Legislature that guarantees the public's  \nright to attend and participate in meetings of local  \nlegislative bodies.]  that requires legislative bodies, like \ncity councils, to hold their meetings open to the public. Figure 1: Exemplary predictions of Toolformer. The\nmodel autonomously decides to call different APIs\n(from top to bottom: a question answering system,\na calculator, a machine translation system, and a\nWikipedia search engine) to obtain information that is\nuseful for completing a piece of text.\nA simple way to overcome these limitations of\ntoday\u2019s language models is to give them the abil-\nity to use external tools such as search engines,\ncalculators, or calendars.", "mimetype": "text/plain", "start_char_idx": 2351, "end_char_idx": 3252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "902ae4f9-50dc-4a9a-987a-7e473bd1268b": {"__data__": {"id_": "902ae4f9-50dc-4a9a-987a-7e473bd1268b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41e36281-ebb1-409d-872e-2beda113f627", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c089e51f9a3ba4199d75ad4bf0ec24067b6068f33ad1c3c904bcd3efd663b1b9", "class_name": "RelatedNodeInfo"}}, "text": "However, existing ap-\nproaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-speci\ufb01c settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hinder-\ning a more widespread adoption of tool use in LMs.\nTherefore, we propose Toolformer , a model that\nlearns to use tools in a novel way, which ful\ufb01lls the\nfollowing desiderata:\n\u2022The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations . This is impor-arXiv:2302.04761v1  [cs.CL]  9 Feb 2023", "mimetype": "text/plain", "start_char_idx": 3253, "end_char_idx": 3828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c9e7a1bb-b3dd-4ac2-a91b-1b4ca5c9ef24": {"__data__": {"id_": "c9e7a1bb-b3dd-4ac2-a91b-1b4ca5c9ef24", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}}, "text": "Toolformer: Language Models Can Teach Themselves to Use Tools\nTimo Schick Jane Dwivedi-Yu Roberto Dess\u00ec\u2020Roberta Raileanu\nMaria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom\nMeta AI Research\u2020Universitat Pompeu Fabra\nAbstract\nLanguage models (LMs) exhibit remarkable\nabilities to solve new tasks from just a few\nexamples or textual instructions, especially at\nscale. They also, paradoxically, struggle with\nbasic functionality, such as arithmetic or fac-\ntual lookup, where much simpler and smaller\nmodels excel. In this paper, we show that\nLMs can teach themselves to use external tools\nvia simple APIs and achieve the best of both\nworlds. We introduce Toolformer , a model\ntrained to decide which APIs to call, when to\ncall them, what arguments to pass, and how to\nbest incorporate the results into future token\nprediction. This is done in a self-supervised\nway, requiring nothing more than a handful of\ndemonstrations for each API. We incorporate\na range of tools, including a calculator, a Q&A\nsystem, a search engine, a translation system,\nand a calendar. Toolformer achieves substan-\ntially improved zero-shot performance across\na variety of downstream tasks, often competi-\ntive with much larger models, without sacri\ufb01c-\ning its core language modeling abilities.\n1 Introduction\nLarge language models achieve impressive zero-\nand few-shot results on a variety of natural lan-\nguage processing tasks (Brown et al., 2020; Chowd-\nhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scal-\ning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a080efe4-9814-4ab8-b4c2-14eae2c14a3e": {"__data__": {"id_": "a080efe4-9814-4ab8-b4c2-14eae2c14a3e", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9e7a1bb-b3dd-4ac2-a91b-1b4ca5c9ef24", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "72f6d24c8cd18e9c731447b3a7f1c33b8d9b098d7eabac80f6e5962d3aaeca44", "class_name": "RelatedNodeInfo"}}, "text": "These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), dif\ufb01cul-\nties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to per-\nform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022).\nThe New England Journal of Medicine is a registered \ntrademark of  [QA(\u201cWho is the publisher of The New  \nEngland Journal of Medicine?\u201d) \u2192 Massachusetts  \nMedical Society]  the MMS. \nOut of 1400 participants, 400 (or [Calculator(400 / 1400)  \n\u2192 0.29]  29%) passed the test. \nThe name derives from \u201cla tortuga\u201d, the Spanish word for \n[MT(\u201ctortuga\u201d) \u2192 turtle]  turtle. \nThe Brown Act is California\u2019s law  [WikiSearch(\u201cBrown  \nAct\u201d) \u2192 The Ralph M. Brown Act is an act of the  \nCalifornia State Legislature that guarantees the public's  \nright to attend and participate in meetings of local  \nlegislative bodies.]  that requires legislative bodies, like \ncity councils, to hold their meetings open to the public. Figure 1: Exemplary predictions of Toolformer. The\nmodel autonomously decides to call different APIs\n(from top to bottom: a question answering system,\na calculator, a machine translation system, and a\nWikipedia search engine) to obtain information that is\nuseful for completing a piece of text.\nA simple way to overcome these limitations of\ntoday\u2019s language models is to give them the abil-\nity to use external tools such as search engines,\ncalculators, or calendars.", "mimetype": "text/plain", "start_char_idx": 1649, "end_char_idx": 3252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8237ce55-9fb6-4460-a67b-d57088343ac2": {"__data__": {"id_": "8237ce55-9fb6-4460-a67b-d57088343ac2", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a080efe4-9814-4ab8-b4c2-14eae2c14a3e", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "53889a8c7f2e133ca5c1b7a734b889d4199bcb18f620d5f9a9320467fcd3d8da", "class_name": "RelatedNodeInfo"}}, "text": "However, existing ap-\nproaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-speci\ufb01c settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hinder-\ning a more widespread adoption of tool use in LMs.\nTherefore, we propose Toolformer , a model that\nlearns to use tools in a novel way, which ful\ufb01lls the\nfollowing desiderata:\n\u2022The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations . This is impor-arXiv:2302.04761v1  [cs.CL]  9 Feb 2023", "mimetype": "text/plain", "start_char_idx": 3253, "end_char_idx": 3828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad": {"__data__": {"id_": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "embedding": null, "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "class_name": "RelatedNodeInfo"}}, "text": "Toolformer: Language Models Can Teach Themselves to Use Tools\nTimo Schick Jane Dwivedi-Yu Roberto Dess\u00ec\u2020Roberta Raileanu\nMaria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom\nMeta AI Research\u2020Universitat Pompeu Fabra\nAbstract\nLanguage models (LMs) exhibit remarkable\nabilities to solve new tasks from just a few\nexamples or textual instructions, especially at\nscale. They also, paradoxically, struggle with\nbasic functionality, such as arithmetic or fac-\ntual lookup, where much simpler and smaller\nmodels excel. In this paper, we show that\nLMs can teach themselves to use external tools\nvia simple APIs and achieve the best of both\nworlds. We introduce Toolformer , a model\ntrained to decide which APIs to call, when to\ncall them, what arguments to pass, and how to\nbest incorporate the results into future token\nprediction. This is done in a self-supervised\nway, requiring nothing more than a handful of\ndemonstrations for each API. We incorporate\na range of tools, including a calculator, a Q&A\nsystem, a search engine, a translation system,\nand a calendar. Toolformer achieves substan-\ntially improved zero-shot performance across\na variety of downstream tasks, often competi-\ntive with much larger models, without sacri\ufb01c-\ning its core language modeling abilities.\n1 Introduction\nLarge language models achieve impressive zero-\nand few-shot results on a variety of natural lan-\nguage processing tasks (Brown et al., 2020; Chowd-\nhery et al., 2022, i.a.) and show several emergent\ncapabilities (Wei et al., 2022). However, all of\nthese models have several inherent limitations that\ncan at best be partially addressed by further scal-\ning. These limitations include an inability to access\nup-to-date information on recent events (Komeili\net al., 2022) and the related tendency to hallucinate\nfacts (Maynez et al., 2020; Ji et al., 2022), dif\ufb01cul-\nties in understanding low-resource languages (Lin\net al., 2021), a lack of mathematical skills to per-\nform precise calculations (Patel et al., 2021) and an\nunawareness of the progression of time (Dhingra\net al., 2022).\nThe New England Journal of Medicine is a registered \ntrademark of  [QA(\u201cWho is the publisher of The New  \nEngland Journal of Medicine?\u201d) \u2192 Massachusetts  \nMedical Society]  the MMS. \nOut of 1400 participants, 400 (or [Calculator(400 / 1400)  \n\u2192 0.29]  29%) passed the test. \nThe name derives from \u201cla tortuga\u201d, the Spanish word for \n[MT(\u201ctortuga\u201d) \u2192 turtle]  turtle. \nThe Brown Act is California\u2019s law  [WikiSearch(\u201cBrown  \nAct\u201d) \u2192 The Ralph M. Brown Act is an act of the  \nCalifornia State Legislature that guarantees the public's  \nright to attend and participate in meetings of local  \nlegislative bodies.]  that requires legislative bodies, like \ncity councils, to hold their meetings open to the public. Figure 1: Exemplary predictions of Toolformer. The\nmodel autonomously decides to call different APIs\n(from top to bottom: a question answering system,\na calculator, a machine translation system, and a\nWikipedia search engine) to obtain information that is\nuseful for completing a piece of text.\nA simple way to overcome these limitations of\ntoday\u2019s language models is to give them the abil-\nity to use external tools such as search engines,\ncalculators, or calendars. However, existing ap-\nproaches either rely on large amounts of human\nannotations (Komeili et al., 2022; Thoppilan et al.,\n2022) or limit tool use to task-speci\ufb01c settings only\n(e.g., Gao et al., 2022; Parisi et al., 2022), hinder-\ning a more widespread adoption of tool use in LMs.\nTherefore, we propose Toolformer , a model that\nlearns to use tools in a novel way, which ful\ufb01lls the\nfollowing desiderata:\n\u2022The use of tools should be learned in a\nself-supervised way without requiring large\namounts of human annotations . This is impor-arXiv:2302.04761v1  [cs.CL]  9 Feb 2023", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5ef68241-0ade-4ed3-9c11-3cf0daa96848": {"__data__": {"id_": "5ef68241-0ade-4ed3-9c11-3cf0daa96848", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}}, "text": "x1: i-1  = Pittsburgh is \n             also known as \n   xi: n = the Steel City x* = Pittsburgh is \n        also known as \n        [QA(What \u2026?  \n        \u2192 Steel City)]  \n        the Steel City. ci1 = What other name is \n         Pittsburgh known by? \nci2 = Which country is \n         Pittsburgh in?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8589b113-0f51-41a6-a845-52c91baf0ace": {"__data__": {"id_": "8589b113-0f51-41a6-a845-52c91baf0ace", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ef68241-0ade-4ed3-9c11-3cf0daa96848", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "93c6a50e38dbf4694d920020fab7b12719de559dc62638a4a50e6e8ab18ce226", "class_name": "RelatedNodeInfo"}}, "text": "ci2 = Which country is \n         Pittsburgh in? ri1 = Steel City \nri2 = United States Li( ci1 \u2192 Steel City )\n < min( Li( ci1 \u2192 \u03b5), Li(\u03b5))\nLi( ci2 \u2192 United States )\n > min( Li( ci2 \u2192 \u03b5), Li(\u03b5))1 \nSample API Calls 2 \nExecute API Calls 3 \nFilter API Calls LM Dataset LM Dataset \nwith API Calls Figure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we \ufb01rst\nsample a position iand corresponding API call candidates c1\ni,c2\ni,...,ck\ni. We then execute these API calls and\n\ufb01lter out all calls which do not reduce the loss Liover the next tokens. All remaining API calls are interleaved\nwith the original text, resulting in a new text x\u2217.", "mimetype": "text/plain", "start_char_idx": 251, "end_char_idx": 930, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f3db2899-4098-49b2-b583-8563ba3a202c": {"__data__": {"id_": "f3db2899-4098-49b2-b583-8563ba3a202c", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8589b113-0f51-41a6-a845-52c91baf0ace", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d8a133bfaf96f444205b75f70fe8426aa1ab898fdb2ea6a7615e160ca253699b", "class_name": "RelatedNodeInfo"}}, "text": "tant not only because of the costs associated\nwith such annotations, but also because what\nhumans \ufb01nd useful may be different from\nwhat a model \ufb01nds useful.\n\u2022The LM should not lose any of its generality\nand should be able to decide for itself when\nandhow to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspeci\ufb01c tasks.\nOur approach for achieving these goals is based\non the recent idea of using large LMs with in-\ncontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Sch\u00fctze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls.", "mimetype": "text/plain", "start_char_idx": 931, "end_char_idx": 1726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "50d10b41-b7f2-4af7-b035-75042552f2c6": {"__data__": {"id_": "50d10b41-b7f2-4af7-b035-75042552f2c6", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3db2899-4098-49b2-b583-8563ba3a202c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "fe7d08e56aeae53e9301c7cccbcafd688075d1af46101f0f1bf434e0bef8a11b", "class_name": "RelatedNodeInfo"}}, "text": "We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\n\ufb01netune the LM itself on the API calls that it con-\nsiders useful. As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a va-\nriety of tools, and to choose for themselves which\ntool to use when and how.\nAs our approach is agnostic of the dataset be-\ning used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the \ufb01rst place.\nThis ensures that the model does not lose any\nof its generality and language modeling abilities.", "mimetype": "text/plain", "start_char_idx": 1727, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2f2e7161-8da6-4cde-ad3a-c94df2bf01ad": {"__data__": {"id_": "2f2e7161-8da6-4cde-ad3a-c94df2bf01ad", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50d10b41-b7f2-4af7-b035-75042552f2c6", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3b44d67014f106fbe1dd957e65fa7fd5819934076325ef8eb05a9c9ccf25bf44", "class_name": "RelatedNodeInfo"}}, "text": "This ensures that the model does not lose any\nof its generality and language modeling abilities.\nWe conduct experiments on a variety of differ-\nent downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komat-\nsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) andseveral other baselines on various tasks.\n2 Approach\nOur aim is to equip a language model Mwith the\nability to use different tools by means of API calls.\nWe require that inputs and outputs for each API\ncan be represented as text sequences. This allows\nseamless insertion of API calls into any given text,\nusing special tokens to mark the start and end of\neach such call.", "mimetype": "text/plain", "start_char_idx": 2250, "end_char_idx": 3056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "93973990-d406-4646-aeb3-2a788d56b9b5": {"__data__": {"id_": "93973990-d406-4646-aeb3-2a788d56b9b5", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f2e7161-8da6-4cde-ad3a-c94df2bf01ad", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "4f0fa375a646f5d20e2752aea6dbd3908fa341549a2d80bcd14ebce3c4582c3c", "class_name": "RelatedNodeInfo"}}, "text": "We represent each API call as a tuple c= (ac,ic)\nwhereacis the name of the API and icis the cor-\nresponding input. Given an API call cwith a cor-\nresponding result r, we denote the linearized se-\nquences of the API call not including and including\nits result, respectively, as:\ne(c) =<API>ac(ic) </API>\ne(c,r) =<API>ac(ic)\u2192r</API>\nwhere \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d are special\ntokens.1Some examples of linearized API calls\ninserted into text sequences are shown in Figure 1.\nGiven a datasetC={x1,..., x|C|}of plain\ntexts, we \ufb01rst convert this dataset into a dataset\nC\u2217augmented with API calls.", "mimetype": "text/plain", "start_char_idx": 3057, "end_char_idx": 3651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "977aef4a-99de-4a09-81e1-a66275c5a139": {"__data__": {"id_": "977aef4a-99de-4a09-81e1-a66275c5a139", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}}, "text": "x1: i-1  = Pittsburgh is \n             also known as \n   xi: n = the Steel City x* = Pittsburgh is \n        also known as \n        [QA(What \u2026?  \n        \u2192 Steel City)]  \n        the Steel City. ci1 = What other name is \n         Pittsburgh known by? \nci2 = Which country is \n         Pittsburgh in? ri1 = Steel City \nri2 = United States Li( ci1 \u2192 Steel City )\n < min( Li( ci1 \u2192 \u03b5), Li(\u03b5))\nLi( ci2 \u2192 United States )\n > min( Li( ci2 \u2192 \u03b5), Li(\u03b5))1 \nSample API Calls 2 \nExecute API Calls 3 \nFilter API Calls LM Dataset LM Dataset \nwith API Calls Figure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we \ufb01rst\nsample a position iand corresponding API call candidates c1\ni,c2\ni,...,ck\ni. We then execute these API calls and\n\ufb01lter out all calls which do not reduce the loss Liover the next tokens. All remaining API calls are interleaved\nwith the original text, resulting in a new text x\u2217.\ntant not only because of the costs associated\nwith such annotations, but also because what\nhumans \ufb01nd useful may be different from\nwhat a model \ufb01nds useful.\n\u2022The LM should not lose any of its generality\nand should be able to decide for itself when\nandhow to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspeci\ufb01c tasks.\nOur approach for achieving these goals is based\non the recent idea of using large LMs with in-\ncontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Sch\u00fctze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "099523d7-cc92-4292-8d64-e6361875b5f2": {"__data__": {"id_": "099523d7-cc92-4292-8d64-e6361875b5f2", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "977aef4a-99de-4a09-81e1-a66275c5a139", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "e63c37237b8dd879ffbdba46b21c54ff5a1bb725d5313e2a41aa2102c497476a", "class_name": "RelatedNodeInfo"}}, "text": "We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\n\ufb01netune the LM itself on the API calls that it con-\nsiders useful. As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a va-\nriety of tools, and to choose for themselves which\ntool to use when and how.\nAs our approach is agnostic of the dataset be-\ning used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the \ufb01rst place.\nThis ensures that the model does not lose any\nof its generality and language modeling abilities.\nWe conduct experiments on a variety of differ-\nent downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komat-\nsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) andseveral other baselines on various tasks.\n2 Approach\nOur aim is to equip a language model Mwith the\nability to use different tools by means of API calls.\nWe require that inputs and outputs for each API\ncan be represented as text sequences. This allows\nseamless insertion of API calls into any given text,\nusing special tokens to mark the start and end of\neach such call.\nWe represent each API call as a tuple c= (ac,ic)\nwhereacis the name of the API and icis the cor-\nresponding input. Given an API call cwith a cor-\nresponding result r, we denote the linearized se-\nquences of the API call not including and including\nits result, respectively, as:\ne(c) =<API>ac(ic) </API>\ne(c,r) =<API>ac(ic)\u2192r</API>\nwhere \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d are special\ntokens.1Some examples of linearized API calls\ninserted into text sequences are shown in Figure 1.", "mimetype": "text/plain", "start_char_idx": 1727, "end_char_idx": 3532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bb23a500-9b12-4d9c-ad23-5b0663f57562": {"__data__": {"id_": "bb23a500-9b12-4d9c-ad23-5b0663f57562", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "099523d7-cc92-4292-8d64-e6361875b5f2", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "eda7c493fad4ef5eaf862f4c94dafc389b741020bf9aab6277531a19964ba443", "class_name": "RelatedNodeInfo"}}, "text": "Given a datasetC={x1,..., x|C|}of plain\ntexts, we \ufb01rst convert this dataset into a dataset\nC\u2217augmented with API calls.", "mimetype": "text/plain", "start_char_idx": 3533, "end_char_idx": 3651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd": {"__data__": {"id_": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22f394d8-1d10-43a1-aaca-3a52ee53aaea", "node_type": "1", "metadata": {}, "hash": "d6f283e5d1ed3fef7f67eda5b42cc05db04c8330393126ac14ca04cce8cab72f", "class_name": "RelatedNodeInfo"}}, "text": "x1: i-1  = Pittsburgh is \n             also known as \n   xi: n = the Steel City x* = Pittsburgh is \n        also known as \n        [QA(What \u2026?  \n        \u2192 Steel City)]  \n        the Steel City. ci1 = What other name is \n         Pittsburgh known by? \nci2 = Which country is \n         Pittsburgh in? ri1 = Steel City \nri2 = United States Li( ci1 \u2192 Steel City )\n < min( Li( ci1 \u2192 \u03b5), Li(\u03b5))\nLi( ci2 \u2192 United States )\n > min( Li( ci2 \u2192 \u03b5), Li(\u03b5))1 \nSample API Calls 2 \nExecute API Calls 3 \nFilter API Calls LM Dataset LM Dataset \nwith API Calls Figure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we \ufb01rst\nsample a position iand corresponding API call candidates c1\ni,c2\ni,...,ck\ni. We then execute these API calls and\n\ufb01lter out all calls which do not reduce the loss Liover the next tokens. All remaining API calls are interleaved\nwith the original text, resulting in a new text x\u2217.\ntant not only because of the costs associated\nwith such annotations, but also because what\nhumans \ufb01nd useful may be different from\nwhat a model \ufb01nds useful.\n\u2022The LM should not lose any of its generality\nand should be able to decide for itself when\nandhow to use which tool. In contrast to\nexisting approaches, this enables a much more\ncomprehensive use of tools that is not tied to\nspeci\ufb01c tasks.\nOur approach for achieving these goals is based\non the recent idea of using large LMs with in-\ncontext learning (Brown et al., 2020) to generate\nentire datasets from scratch (Schick and Sch\u00fctze,\n2021b; Honovich et al., 2022; Wang et al., 2022):\nGiven just a handful of human-written examples\nof how an API can be used, we let a LM annotate\na huge language modeling dataset with potential\nAPI calls. We then use a self-supervised loss to\ndetermine which of these API calls actually help\nthe model in predicting future tokens. Finally, we\n\ufb01netune the LM itself on the API calls that it con-\nsiders useful. As illustrated in Figure 1, through\nthis simple approach, LMs can learn to control a va-\nriety of tools, and to choose for themselves which\ntool to use when and how.\nAs our approach is agnostic of the dataset be-\ning used, we can apply it to the exact same dataset\nthat was used to pretrain a model in the \ufb01rst place.\nThis ensures that the model does not lose any\nof its generality and language modeling abilities.\nWe conduct experiments on a variety of differ-\nent downstream tasks, demonstrating that after\nlearning to use tools, Toolformer, which is based\non a pretrained GPT-J model (Wang and Komat-\nsuzaki, 2021) with 6.7B parameters, achieves much\nstronger zero-shot results, clearly outperforming a\nmuch larger GPT-3 model (Brown et al., 2020) andseveral other baselines on various tasks.\n2 Approach\nOur aim is to equip a language model Mwith the\nability to use different tools by means of API calls.\nWe require that inputs and outputs for each API\ncan be represented as text sequences. This allows\nseamless insertion of API calls into any given text,\nusing special tokens to mark the start and end of\neach such call.\nWe represent each API call as a tuple c= (ac,ic)\nwhereacis the name of the API and icis the cor-\nresponding input. Given an API call cwith a cor-\nresponding result r, we denote the linearized se-\nquences of the API call not including and including\nits result, respectively, as:\ne(c) =<API>ac(ic) </API>\ne(c,r) =<API>ac(ic)\u2192r</API>\nwhere \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d are special\ntokens.1Some examples of linearized API calls\ninserted into text sequences are shown in Figure 1.\nGiven a datasetC={x1,..., x|C|}of plain\ntexts, we \ufb01rst convert this dataset into a dataset\nC\u2217augmented with API calls.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6875ac13-4653-4014-8e95-42ee7d3888ec": {"__data__": {"id_": "6875ac13-4653-4014-8e95-42ee7d3888ec", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}}, "text": "We represent each API call as a tuple c= (ac,ic)\nwhereacis the name of the API and icis the cor-\nresponding input. Given an API call cwith a cor-\nresponding result r, we denote the linearized se-\nquences of the API call not including and including\nits result, respectively, as:\ne(c) =<API>ac(ic) </API>\ne(c,r) =<API>ac(ic)\u2192r</API>\nwhere \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d are special\ntokens.1Some examples of linearized API calls\ninserted into text sequences are shown in Figure 1.\nGiven a datasetC={x1,..., x|C|}of plain\ntexts, we \ufb01rst convert this dataset into a dataset\nC\u2217augmented with API calls. This is done in three\nsteps, illustrated in Figure 2: First, we exploit the\nin-context learning ability of Mto sample a large\nnumber of potential API calls.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "22f394d8-1d10-43a1-aaca-3a52ee53aaea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0f43e3b6-9c7d-4038-a451-5b795042e009": {"__data__": {"id_": "0f43e3b6-9c7d-4038-a451-5b795042e009", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6875ac13-4653-4014-8e95-42ee7d3888ec", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "13fb1969f804e905fd308a7278a84568e786e677ec28a1f9bf47639f14a672a5", "class_name": "RelatedNodeInfo"}}, "text": "We then execute\nthese API calls and \ufb01nally check whether the ob-\ntained responses are helpful for predicting future\ntokens; this is used as a \ufb01ltering criterion. After\n\ufb01ltering, we merge API calls for different tools,\nresulting in the augmented dataset C\u2217, and \ufb01netune\n1In practice, we use the token sequences \u201c [\u201d, \u201c]\u201d and\n\u201c->\u201d to represent \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d, respec-\ntively. This enables our approach to work without modifying\nthe existing LM\u2019s vocabulary. For reasons of readability, we\nstill refer to them as \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d through-\nout this section.", "mimetype": "text/plain", "start_char_idx": 752, "end_char_idx": 1331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "22f394d8-1d10-43a1-aaca-3a52ee53aaea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "34b6bed5-921a-4e1e-aaf7-5ca50f098761": {"__data__": {"id_": "34b6bed5-921a-4e1e-aaf7-5ca50f098761", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}}, "text": "We represent each API call as a tuple c= (ac,ic)\nwhereacis the name of the API and icis the cor-\nresponding input. Given an API call cwith a cor-\nresponding result r, we denote the linearized se-\nquences of the API call not including and including\nits result, respectively, as:\ne(c) =<API>ac(ic) </API>\ne(c,r) =<API>ac(ic)\u2192r</API>\nwhere \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d are special\ntokens.1Some examples of linearized API calls\ninserted into text sequences are shown in Figure 1.\nGiven a datasetC={x1,..., x|C|}of plain\ntexts, we \ufb01rst convert this dataset into a dataset\nC\u2217augmented with API calls. This is done in three\nsteps, illustrated in Figure 2: First, we exploit the\nin-context learning ability of Mto sample a large\nnumber of potential API calls. We then execute\nthese API calls and \ufb01nally check whether the ob-\ntained responses are helpful for predicting future\ntokens; this is used as a \ufb01ltering criterion. After\n\ufb01ltering, we merge API calls for different tools,\nresulting in the augmented dataset C\u2217, and \ufb01netune\n1In practice, we use the token sequences \u201c [\u201d, \u201c]\u201d and\n\u201c->\u201d to represent \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d, respec-\ntively. This enables our approach to work without modifying\nthe existing LM\u2019s vocabulary. For reasons of readability, we\nstill refer to them as \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d through-\nout this section.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "22f394d8-1d10-43a1-aaca-3a52ee53aaea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "22f394d8-1d10-43a1-aaca-3a52ee53aaea": {"__data__": {"id_": "22f394d8-1d10-43a1-aaca-3a52ee53aaea", "embedding": null, "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "514b23e7586ac51d41f29a50e6069c06a15f127e93d1416e3ffe98d9064e178c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "14393ae97bb69e350ea1b5a2b265b8090b194596f0c908b0c712e1fb6006c0cf", "class_name": "RelatedNodeInfo"}}, "text": "We represent each API call as a tuple c= (ac,ic)\nwhereacis the name of the API and icis the cor-\nresponding input. Given an API call cwith a cor-\nresponding result r, we denote the linearized se-\nquences of the API call not including and including\nits result, respectively, as:\ne(c) =<API>ac(ic) </API>\ne(c,r) =<API>ac(ic)\u2192r</API>\nwhere \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d are special\ntokens.1Some examples of linearized API calls\ninserted into text sequences are shown in Figure 1.\nGiven a datasetC={x1,..., x|C|}of plain\ntexts, we \ufb01rst convert this dataset into a dataset\nC\u2217augmented with API calls. This is done in three\nsteps, illustrated in Figure 2: First, we exploit the\nin-context learning ability of Mto sample a large\nnumber of potential API calls. We then execute\nthese API calls and \ufb01nally check whether the ob-\ntained responses are helpful for predicting future\ntokens; this is used as a \ufb01ltering criterion. After\n\ufb01ltering, we merge API calls for different tools,\nresulting in the augmented dataset C\u2217, and \ufb01netune\n1In practice, we use the token sequences \u201c [\u201d, \u201c]\u201d and\n\u201c->\u201d to represent \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d, respec-\ntively. This enables our approach to work without modifying\nthe existing LM\u2019s vocabulary. For reasons of readability, we\nstill refer to them as \u201c <API> \u201d, \u201c</API> \u201d and \u201c\u2192\u201d through-\nout this section.", "mimetype": "text/plain", "start_char_idx": 3057, "end_char_idx": 4388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "22f394d8-1d10-43a1-aaca-3a52ee53aaea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7847fadf-8f6a-4ebc-884f-f4a4a228c14d": {"__data__": {"id_": "7847fadf-8f6a-4ebc-884f-f4a4a228c14d", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}}, "text": "Your task is to add calls to a Question Answering API to a \npiece of text. The questions should help you get \ninformation required to complete the text. You can call the \nAPI by writing \"[QA(question)]\" where \"question\" is the \nquestion you want to ask. Here are some examples of API \ncalls: \nInput:  Joe Biden was born in Scranton, Pennsylvania. \nOutput:  Joe Biden was born in  [QA(\"Where was Joe  \nBiden born?\")]  Scranton, [QA(\"In which state is  \nScranton?\")]  Pennsylvania. \nInput:  Coca-Cola, or Coke, is a carbonated soft drink \nmanufactured by the Coca-Cola Company. \nOutput: Coca-Cola, or [QA(\"What other name is  \nCoca-Cola known by?\")]  Coke, is a carbonated soft drink \nmanufactured by [QA(\"Who manufactures Coca-Cola?\")]  \nthe Coca-Cola Company.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4d69f0ae-68c7-45a0-9e82-afa38d368859": {"__data__": {"id_": "4d69f0ae-68c7-45a0-9e82-afa38d368859", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7847fadf-8f6a-4ebc-884f-f4a4a228c14d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "debf2a14ae2937ab024d7d8a994610a73ddb3acff225045b0707a3f80418c54a", "class_name": "RelatedNodeInfo"}}, "text": "the Coca-Cola Company. \nInput:  x \nOutput: Figure 3: An exemplary prompt P(x)used to generate\nAPI calls for the question answering tool.\nMitself on this dataset. Each of these steps is\ndescribed in more detail below.\nSampling API Calls For each API, we write a\npromptP(x)that encourages the LM to anno-\ntate an example x=x1,...,x nwith API calls.\nAn example of such a prompt for a question an-\nswering tool is shown in Figure 3; all prompts\nused are shown in Appendix A.2. Let pM(zn+1|\nz1,z n)be the probability that Massigns to\ntokenzn+1as a continuation for the sequence\nz1,z n. We \ufb01rst sample up to kcandidate posi-\ntions for doing API calls by computing, for each\ni\u2208{1,n}, the probability\npi=pM(<API>|P(x),", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2b9dc6cf-4ddf-4909-95cf-6c1b1cfe4364": {"__data__": {"id_": "2b9dc6cf-4ddf-4909-95cf-6c1b1cfe4364", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d69f0ae-68c7-45a0-9e82-afa38d368859", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "db232f822895c2e68004ebec97b864c03ee1dabd2c7db8e4750ff39a3902d07e", "class_name": "RelatedNodeInfo"}}, "text": "n}, the probability\npi=pM(<API>|P(x),x1:i\u22121)\nthatMassigns to starting an API call at position\ni. Given a sampling threshold \u03c4s, we keep all po-\nsitionsI={i|pi>\u03c4s}; if there are more than k\nsuch positions, we only keep the top k.\nFor each position i\u2208I, we then obtain up to m\nAPI callsc1\ni,cm\niby sampling from Mgiven the\nsequence [P(x),x1,x i\u22121,<API> ]as a pre\ufb01x\nand</API> as an end-of-sequence token.2\n2We discard all examples where Mdoes not generate the\n</API> token.Executing API Calls As a next step, we execute\nall API calls generated by Mto obtain the corre-\nsponding results.", "mimetype": "text/plain", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "78af0c97-03ab-4eaf-a74b-5e68db0bc36d": {"__data__": {"id_": "78af0c97-03ab-4eaf-a74b-5e68db0bc36d", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b9dc6cf-4ddf-4909-95cf-6c1b1cfe4364", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "6361fbf864bfa5a16d45b30f3ea64f87c2a81a97df7e5f38dd6fe1604dea2316", "class_name": "RelatedNodeInfo"}}, "text": "we execute\nall API calls generated by Mto obtain the corre-\nsponding results. How this is done depends entirely\non the API itself \u2013 for example, it can involve call-\ning another neural network, executing a Python\nscript or using a retrieval system to perform search\nover a large corpus. The response for each API call\ncineeds to be a single text sequence ri.\nFiltering API Calls Letibe the position of the\nAPI callciin the sequence x=x1,...,x n, and let\nribe the response from the API.", "mimetype": "text/plain", "start_char_idx": 1936, "end_char_idx": 2421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e6cdec83-53d9-4eae-9a5f-f9580755b587": {"__data__": {"id_": "e6cdec83-53d9-4eae-9a5f-f9580755b587", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78af0c97-03ab-4eaf-a74b-5e68db0bc36d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "967fe5f5423bddc764bcafb518a41b56b10782ee5daaa554328a9f22a4192887", "class_name": "RelatedNodeInfo"}}, "text": "Further, given a\nsequence (wi|i\u2208N)ofweights , let\nLi(z) =\u2212n\u2211\nj=iwj\u2212i\u00b7logpM(xj|z,x1:j\u22121)\nbe the weighted cross entropy loss for Mover the\ntokensxi,...,x nif the model is pre\ufb01xed with z.\nWe compare two different instantiations of this loss:\nL+\ni=Li(e(ci,ri))\nL\u2212\ni= min (Li(\u03b5),Li(e(ci,\u03b5)))\nwhere\u03b5denotes an empty sequence. The former is\nthe weighted loss over all tokens xi,...,x nif the\nAPI call and its result are given to Mas a pre\ufb01x;3\nthe latter is the minimum of the losses obtained\nfrom (i) doing no API call at all and (ii) doing an\nAPI call, but not providing the response.", "mimetype": "text/plain", "start_char_idx": 2422, "end_char_idx": 3000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6a3e6d03-96b5-4165-9cad-3463da6ea73f": {"__data__": {"id_": "6a3e6d03-96b5-4165-9cad-3463da6ea73f", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6cdec83-53d9-4eae-9a5f-f9580755b587", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d520514812640524f004c7f7fb18697b43c97392c458b5fb843d751d78e51879", "class_name": "RelatedNodeInfo"}}, "text": "Intuitively,\nan API call is helpful to Mif providing it with both\nthe input andthe output of this call makes it easier\nfor the model to predict future tokens, compared to\nnot receiving the API call at all, or receiving only\nits input.", "mimetype": "text/plain", "start_char_idx": 3001, "end_char_idx": 3235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2edc18d3-cb56-4a31-87cd-ee8879699554": {"__data__": {"id_": "2edc18d3-cb56-4a31-87cd-ee8879699554", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}}, "text": "Your task is to add calls to a Question Answering API to a \npiece of text. The questions should help you get \ninformation required to complete the text. You can call the \nAPI by writing \"[QA(question)]\" where \"question\" is the \nquestion you want to ask. Here are some examples of API \ncalls: \nInput:  Joe Biden was born in Scranton, Pennsylvania. \nOutput:  Joe Biden was born in  [QA(\"Where was Joe  \nBiden born?\")]  Scranton, [QA(\"In which state is  \nScranton?\")]  Pennsylvania. \nInput:  Coca-Cola, or Coke, is a carbonated soft drink \nmanufactured by the Coca-Cola Company. \nOutput: Coca-Cola, or [QA(\"What other name is  \nCoca-Cola known by?\")]  Coke, is a carbonated soft drink \nmanufactured by [QA(\"Who manufactures Coca-Cola?\")]  \nthe Coca-Cola Company. \nInput:  x \nOutput: Figure 3: An exemplary prompt P(x)used to generate\nAPI calls for the question answering tool.\nMitself on this dataset. Each of these steps is\ndescribed in more detail below.\nSampling API Calls For each API, we write a\npromptP(x)that encourages the LM to anno-\ntate an example x=x1,...,x nwith API calls.\nAn example of such a prompt for a question an-\nswering tool is shown in Figure 3; all prompts\nused are shown in Appendix A.2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "57d072df-0fcb-452a-a3ba-5e6cce39cec2": {"__data__": {"id_": "57d072df-0fcb-452a-a3ba-5e6cce39cec2", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2edc18d3-cb56-4a31-87cd-ee8879699554", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "f5b3a99bceb6607133f52c5a41e2299c3ff5e3a0403ceb237b9ddf9af6cee622", "class_name": "RelatedNodeInfo"}}, "text": "Let pM(zn+1|\nz1,...,z n)be the probability that Massigns to\ntokenzn+1as a continuation for the sequence\nz1,...,z n. We \ufb01rst sample up to kcandidate posi-\ntions for doing API calls by computing, for each\ni\u2208{1,...,n}, the probability\npi=pM(<API>|P(x),x1:i\u22121)\nthatMassigns to starting an API call at position\ni. Given a sampling threshold \u03c4s, we keep all po-\nsitionsI={i|pi>\u03c4s}; if there are more than k\nsuch positions, we only keep the top k.\nFor each position i\u2208I, we then obtain up to m\nAPI callsc1\ni,...,cm\niby sampling from Mgiven the\nsequence [P(x),x1,...,x i\u22121,<API> ]as a pre\ufb01x\nand</API> as an end-of-sequence token.2\n2We discard all examples where Mdoes not generate the\n</API> token.Executing API Calls As a next step, we execute\nall API calls generated by Mto obtain the corre-\nsponding results. How this is done depends entirely\non the API itself \u2013 for example, it can involve call-\ning another neural network, executing a Python\nscript or using a retrieval system to perform search\nover a large corpus. The response for each API call\ncineeds to be a single text sequence ri.\nFiltering API Calls Letibe the position of the\nAPI callciin the sequence x=x1,...,x n, and let\nribe the response from the API.", "mimetype": "text/plain", "start_char_idx": 1210, "end_char_idx": 2421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "94b5363c-adb9-48a2-ad23-4d5b827a0474": {"__data__": {"id_": "94b5363c-adb9-48a2-ad23-4d5b827a0474", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57d072df-0fcb-452a-a3ba-5e6cce39cec2", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "2248ee9a02d5454adfadbe6f7332a44ac585c59e219c079e69cc1545ceb28fbc", "class_name": "RelatedNodeInfo"}}, "text": "Further, given a\nsequence (wi|i\u2208N)ofweights , let\nLi(z) =\u2212n\u2211\nj=iwj\u2212i\u00b7logpM(xj|z,x1:j\u22121)\nbe the weighted cross entropy loss for Mover the\ntokensxi,...,x nif the model is pre\ufb01xed with z.\nWe compare two different instantiations of this loss:\nL+\ni=Li(e(ci,ri))\nL\u2212\ni= min (Li(\u03b5),Li(e(ci,\u03b5)))\nwhere\u03b5denotes an empty sequence. The former is\nthe weighted loss over all tokens xi,...,x nif the\nAPI call and its result are given to Mas a pre\ufb01x;3\nthe latter is the minimum of the losses obtained\nfrom (i) doing no API call at all and (ii) doing an\nAPI call, but not providing the response. Intuitively,\nan API call is helpful to Mif providing it with both\nthe input andthe output of this call makes it easier\nfor the model to predict future tokens, compared to\nnot receiving the API call at all, or receiving only\nits input.", "mimetype": "text/plain", "start_char_idx": 2422, "end_char_idx": 3235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb": {"__data__": {"id_": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a85a5425-c7a6-4b7d-b54e-4dfb4c459381", "node_type": "1", "metadata": {}, "hash": "817913521c2e4503554fd2516fd93e046606d879d43fcbb4e41464fad02aded7", "class_name": "RelatedNodeInfo"}}, "text": "Your task is to add calls to a Question Answering API to a \npiece of text. The questions should help you get \ninformation required to complete the text. You can call the \nAPI by writing \"[QA(question)]\" where \"question\" is the \nquestion you want to ask. Here are some examples of API \ncalls: \nInput:  Joe Biden was born in Scranton, Pennsylvania. \nOutput:  Joe Biden was born in  [QA(\"Where was Joe  \nBiden born?\")]  Scranton, [QA(\"In which state is  \nScranton?\")]  Pennsylvania. \nInput:  Coca-Cola, or Coke, is a carbonated soft drink \nmanufactured by the Coca-Cola Company. \nOutput: Coca-Cola, or [QA(\"What other name is  \nCoca-Cola known by?\")]  Coke, is a carbonated soft drink \nmanufactured by [QA(\"Who manufactures Coca-Cola?\")]  \nthe Coca-Cola Company. \nInput:  x \nOutput: Figure 3: An exemplary prompt P(x)used to generate\nAPI calls for the question answering tool.\nMitself on this dataset. Each of these steps is\ndescribed in more detail below.\nSampling API Calls For each API, we write a\npromptP(x)that encourages the LM to anno-\ntate an example x=x1,...,x nwith API calls.\nAn example of such a prompt for a question an-\nswering tool is shown in Figure 3; all prompts\nused are shown in Appendix A.2. Let pM(zn+1|\nz1,...,z n)be the probability that Massigns to\ntokenzn+1as a continuation for the sequence\nz1,...,z n. We \ufb01rst sample up to kcandidate posi-\ntions for doing API calls by computing, for each\ni\u2208{1,...,n}, the probability\npi=pM(<API>|P(x),x1:i\u22121)\nthatMassigns to starting an API call at position\ni. Given a sampling threshold \u03c4s, we keep all po-\nsitionsI={i|pi>\u03c4s}; if there are more than k\nsuch positions, we only keep the top k.\nFor each position i\u2208I, we then obtain up to m\nAPI callsc1\ni,...,cm\niby sampling from Mgiven the\nsequence [P(x),x1,...,x i\u22121,<API> ]as a pre\ufb01x\nand</API> as an end-of-sequence token.2\n2We discard all examples where Mdoes not generate the\n</API> token.Executing API Calls As a next step, we execute\nall API calls generated by Mto obtain the corre-\nsponding results. How this is done depends entirely\non the API itself \u2013 for example, it can involve call-\ning another neural network, executing a Python\nscript or using a retrieval system to perform search\nover a large corpus. The response for each API call\ncineeds to be a single text sequence ri.\nFiltering API Calls Letibe the position of the\nAPI callciin the sequence x=x1,...,x n, and let\nribe the response from the API. Further, given a\nsequence (wi|i\u2208N)ofweights , let\nLi(z) =\u2212n\u2211\nj=iwj\u2212i\u00b7logpM(xj|z,x1:j\u22121)\nbe the weighted cross entropy loss for Mover the\ntokensxi,...,x nif the model is pre\ufb01xed with z.\nWe compare two different instantiations of this loss:\nL+\ni=Li(e(ci,ri))\nL\u2212\ni= min (Li(\u03b5),Li(e(ci,\u03b5)))\nwhere\u03b5denotes an empty sequence. The former is\nthe weighted loss over all tokens xi,...,x nif the\nAPI call and its result are given to Mas a pre\ufb01x;3\nthe latter is the minimum of the losses obtained\nfrom (i) doing no API call at all and (ii) doing an\nAPI call, but not providing the response. Intuitively,\nan API call is helpful to Mif providing it with both\nthe input andthe output of this call makes it easier\nfor the model to predict future tokens, compared to\nnot receiving the API call at all, or receiving only\nits input.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "129d0d0c-2cec-4683-9586-7b963b83ff6a": {"__data__": {"id_": "129d0d0c-2cec-4683-9586-7b963b83ff6a", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}}, "text": "The former is\nthe weighted loss over all tokens xi,...,x nif the\nAPI call and its result are given to Mas a pre\ufb01x;3\nthe latter is the minimum of the losses obtained\nfrom (i) doing no API call at all and (ii) doing an\nAPI call, but not providing the response. Intuitively,\nan API call is helpful to Mif providing it with both\nthe input andthe output of this call makes it easier\nfor the model to predict future tokens, compared to\nnot receiving the API call at all, or receiving only\nits input. Given a \ufb01ltering threshold \u03c4f, we thus\nonly keep API calls for which\nL\u2212\ni\u2212L+\ni\u2265\u03c4f\nholds, i.e., adding the API call and its result reduces\nthe loss by at least \u03c4f, compared to not doing any\nAPI call or obtaining no result from it.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a85a5425-c7a6-4b7d-b54e-4dfb4c459381", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3576df63-cd81-4595-ab7c-31d3b85c2968": {"__data__": {"id_": "3576df63-cd81-4595-ab7c-31d3b85c2968", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "129d0d0c-2cec-4683-9586-7b963b83ff6a", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "84556f0d6d1c5d8609ee060b4651fe7a84030a922c31c8405dcc5b762674123b", "class_name": "RelatedNodeInfo"}}, "text": "Model Finetuning After sampling and \ufb01ltering\ncalls for all APIs, we \ufb01nally merge the remaining\nAPI calls and interleave them with the original\ninputs. That is, for an input text x=x1,...,x n\nwith a corresponding API call and result (ci,ri)at\npositioni, we construct the new sequence x\u2217=\n3We provide e(ci,ri)as a pre\ufb01x instead of inserting it at\npositionibecauseMis not yet \ufb01netuned on any examples\ncontaining API calls, so inserting it in the middle of xwould\ninterrupt the \ufb02ow and not align with patterns in the pretraining\ncorpus, thus hurting perplexity.", "mimetype": "text/plain", "start_char_idx": 724, "end_char_idx": 1281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a85a5425-c7a6-4b7d-b54e-4dfb4c459381", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4a263539-eef7-4f63-97ef-1ed9520ca575": {"__data__": {"id_": "4a263539-eef7-4f63-97ef-1ed9520ca575", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}}, "text": "The former is\nthe weighted loss over all tokens xi,...,x nif the\nAPI call and its result are given to Mas a pre\ufb01x;3\nthe latter is the minimum of the losses obtained\nfrom (i) doing no API call at all and (ii) doing an\nAPI call, but not providing the response. Intuitively,\nan API call is helpful to Mif providing it with both\nthe input andthe output of this call makes it easier\nfor the model to predict future tokens, compared to\nnot receiving the API call at all, or receiving only\nits input. Given a \ufb01ltering threshold \u03c4f, we thus\nonly keep API calls for which\nL\u2212\ni\u2212L+\ni\u2265\u03c4f\nholds, i.e., adding the API call and its result reduces\nthe loss by at least \u03c4f, compared to not doing any\nAPI call or obtaining no result from it.\nModel Finetuning After sampling and \ufb01ltering\ncalls for all APIs, we \ufb01nally merge the remaining\nAPI calls and interleave them with the original\ninputs. That is, for an input text x=x1,...,x n\nwith a corresponding API call and result (ci,ri)at\npositioni, we construct the new sequence x\u2217=\n3We provide e(ci,ri)as a pre\ufb01x instead of inserting it at\npositionibecauseMis not yet \ufb01netuned on any examples\ncontaining API calls, so inserting it in the middle of xwould\ninterrupt the \ufb02ow and not align with patterns in the pretraining\ncorpus, thus hurting perplexity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a85a5425-c7a6-4b7d-b54e-4dfb4c459381", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a85a5425-c7a6-4b7d-b54e-4dfb4c459381": {"__data__": {"id_": "a85a5425-c7a6-4b7d-b54e-4dfb4c459381", "embedding": null, "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "509897da-3c37-4679-aaec-1ef93cb6ccff", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "32f34c6026348c67a4bf70104d7766bf3d3ec4735c13ef0ac7d477a7112461ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c47b0e55adf3dae08781a24c998439f1790e0edeba62d47d789e39e8b2790141", "class_name": "RelatedNodeInfo"}}, "text": "The former is\nthe weighted loss over all tokens xi,...,x nif the\nAPI call and its result are given to Mas a pre\ufb01x;3\nthe latter is the minimum of the losses obtained\nfrom (i) doing no API call at all and (ii) doing an\nAPI call, but not providing the response. Intuitively,\nan API call is helpful to Mif providing it with both\nthe input andthe output of this call makes it easier\nfor the model to predict future tokens, compared to\nnot receiving the API call at all, or receiving only\nits input. Given a \ufb01ltering threshold \u03c4f, we thus\nonly keep API calls for which\nL\u2212\ni\u2212L+\ni\u2265\u03c4f\nholds, i.e., adding the API call and its result reduces\nthe loss by at least \u03c4f, compared to not doing any\nAPI call or obtaining no result from it.\nModel Finetuning After sampling and \ufb01ltering\ncalls for all APIs, we \ufb01nally merge the remaining\nAPI calls and interleave them with the original\ninputs. That is, for an input text x=x1,...,x n\nwith a corresponding API call and result (ci,ri)at\npositioni, we construct the new sequence x\u2217=\n3We provide e(ci,ri)as a pre\ufb01x instead of inserting it at\npositionibecauseMis not yet \ufb01netuned on any examples\ncontaining API calls, so inserting it in the middle of xwould\ninterrupt the \ufb02ow and not align with patterns in the pretraining\ncorpus, thus hurting perplexity.", "mimetype": "text/plain", "start_char_idx": 2742, "end_char_idx": 4023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a85a5425-c7a6-4b7d-b54e-4dfb4c459381", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2593dd6c-afbc-4d63-ba3c-e3294e33b9fa": {"__data__": {"id_": "2593dd6c-afbc-4d63-ba3c-e3294e33b9fa", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}}, "text": "x1:i\u22121,e(ci,ri),xi:n; we proceed analogously for\ntexts with multiple API calls. Doing this for all x\u2208\nCresults in the new dataset C\u2217augmented with API\ncalls. We use this new dataset to \ufb01netune M, using\na standard language modeling objective. Crucially,\napart from inserted API calls the augmented dataset\nC\u2217contains the exact same texts as C, the original\ndataset. As a consequence, \ufb01netuning MonC\u2217\nexposes it to the same content as \ufb01netuning on C.\nMoreover, as API calls are inserted in exactly those\npositions and with exactly those inputs that help\nMpredict future tokens, \ufb01netuning on C\u2217enables\nthe language model to decide when and how to use\nwhich tool, based purely on its own feedback.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c2990faf-c06f-41ad-add2-163ff7cf180d": {"__data__": {"id_": "c2990faf-c06f-41ad-add2-163ff7cf180d", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2593dd6c-afbc-4d63-ba3c-e3294e33b9fa", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d6eca8d3374dd021d9444ed8f7a392646b6ec1f7041cf3ce3d8e0edc4164bf8a", "class_name": "RelatedNodeInfo"}}, "text": "Inference When generating text with Mafter\n\ufb01netuning with our approach, we perform regular\ndecoding until Mproduces the \u201c\u2192\u201d token, indicat-\ning that it next expects the response for an API call.\nAt this point, we interrupt the decoding process,\ncall the appropriate API to get a response, and con-\ntinue the decoding process after inserting both the\nresponse and the </API> token.\n3 Tools\nWe explore a variety of tools to address different\nshortcomings of regular LMs. The only constraints\nwe impose on these tools is that (i) both their inputs\nand outputs can be represented as text sequences,\nand (ii) we can obtain a few demonstrations of\ntheir intended use. Concretely, we explore the fol-\nlowing \ufb01ve tools: a question answering system, a\nWikipedia search engine, a calculator, a calendar,\nand a machine translation system. Some examples\nof potential calls and return strings for the APIs\nassociated with each of these tools are shown in\nTable 1.", "mimetype": "text/plain", "start_char_idx": 694, "end_char_idx": 1644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "761989e8-0cfd-4b0b-8939-63d8cf81e5a3": {"__data__": {"id_": "761989e8-0cfd-4b0b-8939-63d8cf81e5a3", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2990faf-c06f-41ad-add2-163ff7cf180d", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e095dceee63621051dc306a62134cfe74c36d0954bba36628ebb121a9d578c9", "class_name": "RelatedNodeInfo"}}, "text": "We brie\ufb02y discuss all tools below; further\ndetails can be found in Appendix A.\nQuestion Answering Our \ufb01rst tool is a question\nanswering system based on another LM that can an-\nswer simple factoid questions. Speci\ufb01cally, we use\nAtlas (Izacard et al., 2022), a retrieval-augmented\nLM \ufb01netuned on Natural Questions (Kwiatkowski\net al., 2019).\nCalculator As a second tool, we use a calculator\nthat can perform simple numeric calculations; we\nonly support the four basic arithmetic operations.\nResults are always rounded to two decimal places.\nWikipedia Search Our third tool is a search en-\ngine that, given a search term, returns short textsnippets from Wikipedia. Compared to our ques-\ntion answering tool, this search enables a model\nto get more comprehensive information on a sub-\nject, but requires it to extract the relevant parts by\nitself.", "mimetype": "text/plain", "start_char_idx": 1645, "end_char_idx": 2488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7ec242e1-7b00-485a-aa65-f90dc1c3d3ab": {"__data__": {"id_": "7ec242e1-7b00-485a-aa65-f90dc1c3d3ab", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "761989e8-0cfd-4b0b-8939-63d8cf81e5a3", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b60f12ba2f285d94e78c9afe52c4cf1b8c7c294802cc34289a3948d001b22811", "class_name": "RelatedNodeInfo"}}, "text": "As our search engine, we use a BM25 re-\ntriever (Robertson et al., 1995; Baeza-Yates et al.,\n1999) that indexes the Wikipedia dump from KILT\n(Petroni et al., 2021).\nMachine Translation System Our fourth tool is\na machine translation system based on a LM that\ncan translate a phrase from any language into En-\nglish. More concretely, we use the 600M parameter\nNLLB (Costa-juss\u00e0 et al., 2022) as our multilingual\nmachine translation model that works for 200 lan-\nguages (including low-resource ones). The source\nlanguage is automatically detected using the fast-\nTextclassi\ufb01er (Joulin et al., 2016), while the target\nlanguage is always set to English.\nCalendar Our \ufb01nal tool is a calendar API that,\nwhen queried, returns the current date without tak-\ning any input. This provides temporal context for\npredictions that require some awareness of time.", "mimetype": "text/plain", "start_char_idx": 2489, "end_char_idx": 3336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "61715869-a72a-428b-b0eb-99e08b4ba441": {"__data__": {"id_": "61715869-a72a-428b-b0eb-99e08b4ba441", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ec242e1-7b00-485a-aa65-f90dc1c3d3ab", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1214790120cbdc4fab39767c49c6f42fc6d8457e7780a4ea199636c0034509b5", "class_name": "RelatedNodeInfo"}}, "text": "This provides temporal context for\npredictions that require some awareness of time.\n4 Experiments\nWe investigate whether our approach enables a\nmodel to use tools without any further supervision\nand to decide for itself when and how to call which\nof the available tools. To test this, we select a vari-\nety of downstream tasks where we assume at least\none of the considered tools to be useful, and evalu-\nate performance in zero-shot settings (Section 4.2).\nBeyond that, we also ensure that our approach does\nnot hurt the model\u2019s core language modeling abili-\nties; we verify this by looking at perplexity on two\nlanguage modeling datasets (Section 4.3).", "mimetype": "text/plain", "start_char_idx": 3253, "end_char_idx": 3907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ef00f5f8-9ca0-4f0d-a12c-5ea1c399a65b": {"__data__": {"id_": "ef00f5f8-9ca0-4f0d-a12c-5ea1c399a65b", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}}, "text": "x1:i\u22121,e(ci,ri),xi:n; we proceed analogously for\ntexts with multiple API calls. Doing this for all x\u2208\nCresults in the new dataset C\u2217augmented with API\ncalls. We use this new dataset to \ufb01netune M, using\na standard language modeling objective. Crucially,\napart from inserted API calls the augmented dataset\nC\u2217contains the exact same texts as C, the original\ndataset. As a consequence, \ufb01netuning MonC\u2217\nexposes it to the same content as \ufb01netuning on C.\nMoreover, as API calls are inserted in exactly those\npositions and with exactly those inputs that help\nMpredict future tokens, \ufb01netuning on C\u2217enables\nthe language model to decide when and how to use\nwhich tool, based purely on its own feedback.\nInference When generating text with Mafter\n\ufb01netuning with our approach, we perform regular\ndecoding until Mproduces the \u201c\u2192\u201d token, indicat-\ning that it next expects the response for an API call.\nAt this point, we interrupt the decoding process,\ncall the appropriate API to get a response, and con-\ntinue the decoding process after inserting both the\nresponse and the </API> token.\n3 Tools\nWe explore a variety of tools to address different\nshortcomings of regular LMs. The only constraints\nwe impose on these tools is that (i) both their inputs\nand outputs can be represented as text sequences,\nand (ii) we can obtain a few demonstrations of\ntheir intended use. Concretely, we explore the fol-\nlowing \ufb01ve tools: a question answering system, a\nWikipedia search engine, a calculator, a calendar,\nand a machine translation system. Some examples\nof potential calls and return strings for the APIs\nassociated with each of these tools are shown in\nTable 1. We brie\ufb02y discuss all tools below; further\ndetails can be found in Appendix A.\nQuestion Answering Our \ufb01rst tool is a question\nanswering system based on another LM that can an-\nswer simple factoid questions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1c9ad27a-bed9-42e3-aa8c-b67b552469da": {"__data__": {"id_": "1c9ad27a-bed9-42e3-aa8c-b67b552469da", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef00f5f8-9ca0-4f0d-a12c-5ea1c399a65b", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "40b547062322046c78bb640516207b0de7c81444ffef6470cae16823c08add67", "class_name": "RelatedNodeInfo"}}, "text": "Speci\ufb01cally, we use\nAtlas (Izacard et al., 2022), a retrieval-augmented\nLM \ufb01netuned on Natural Questions (Kwiatkowski\net al., 2019).\nCalculator As a second tool, we use a calculator\nthat can perform simple numeric calculations; we\nonly support the four basic arithmetic operations.\nResults are always rounded to two decimal places.\nWikipedia Search Our third tool is a search en-\ngine that, given a search term, returns short textsnippets from Wikipedia. Compared to our ques-\ntion answering tool, this search enables a model\nto get more comprehensive information on a sub-\nject, but requires it to extract the relevant parts by\nitself. As our search engine, we use a BM25 re-\ntriever (Robertson et al., 1995; Baeza-Yates et al.,\n1999) that indexes the Wikipedia dump from KILT\n(Petroni et al., 2021).\nMachine Translation System Our fourth tool is\na machine translation system based on a LM that\ncan translate a phrase from any language into En-\nglish. More concretely, we use the 600M parameter\nNLLB (Costa-juss\u00e0 et al., 2022) as our multilingual\nmachine translation model that works for 200 lan-\nguages (including low-resource ones). The source\nlanguage is automatically detected using the fast-\nTextclassi\ufb01er (Joulin et al., 2016), while the target\nlanguage is always set to English.\nCalendar Our \ufb01nal tool is a calendar API that,\nwhen queried, returns the current date without tak-\ning any input. This provides temporal context for\npredictions that require some awareness of time.\n4 Experiments\nWe investigate whether our approach enables a\nmodel to use tools without any further supervision\nand to decide for itself when and how to call which\nof the available tools. To test this, we select a vari-\nety of downstream tasks where we assume at least\none of the considered tools to be useful, and evalu-\nate performance in zero-shot settings (Section 4.2).", "mimetype": "text/plain", "start_char_idx": 1852, "end_char_idx": 3710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "80771438-6266-4cac-9ba7-9a73dcce8afb": {"__data__": {"id_": "80771438-6266-4cac-9ba7-9a73dcce8afb", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c9ad27a-bed9-42e3-aa8c-b67b552469da", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "52227a2443ab7fe49b5257fd98e6d09a2a2ddd65ac65c12bd4bee57e8a43ac2f", "class_name": "RelatedNodeInfo"}}, "text": "Beyond that, we also ensure that our approach does\nnot hurt the model\u2019s core language modeling abili-\nties; we verify this by looking at perplexity on two\nlanguage modeling datasets (Section 4.3).", "mimetype": "text/plain", "start_char_idx": 3711, "end_char_idx": 3907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bf8a0989-52de-4c7a-9846-6093baf1a5e2": {"__data__": {"id_": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b5db312-c332-4b4d-96f2-bf58abaf6547", "node_type": "1", "metadata": {}, "hash": "a5a8071f1a70bd976d28bdce057e29a73c82265a67d537ce32ba1c74e354c76b", "class_name": "RelatedNodeInfo"}}, "text": "x1:i\u22121,e(ci,ri),xi:n; we proceed analogously for\ntexts with multiple API calls. Doing this for all x\u2208\nCresults in the new dataset C\u2217augmented with API\ncalls. We use this new dataset to \ufb01netune M, using\na standard language modeling objective. Crucially,\napart from inserted API calls the augmented dataset\nC\u2217contains the exact same texts as C, the original\ndataset. As a consequence, \ufb01netuning MonC\u2217\nexposes it to the same content as \ufb01netuning on C.\nMoreover, as API calls are inserted in exactly those\npositions and with exactly those inputs that help\nMpredict future tokens, \ufb01netuning on C\u2217enables\nthe language model to decide when and how to use\nwhich tool, based purely on its own feedback.\nInference When generating text with Mafter\n\ufb01netuning with our approach, we perform regular\ndecoding until Mproduces the \u201c\u2192\u201d token, indicat-\ning that it next expects the response for an API call.\nAt this point, we interrupt the decoding process,\ncall the appropriate API to get a response, and con-\ntinue the decoding process after inserting both the\nresponse and the </API> token.\n3 Tools\nWe explore a variety of tools to address different\nshortcomings of regular LMs. The only constraints\nwe impose on these tools is that (i) both their inputs\nand outputs can be represented as text sequences,\nand (ii) we can obtain a few demonstrations of\ntheir intended use. Concretely, we explore the fol-\nlowing \ufb01ve tools: a question answering system, a\nWikipedia search engine, a calculator, a calendar,\nand a machine translation system. Some examples\nof potential calls and return strings for the APIs\nassociated with each of these tools are shown in\nTable 1. We brie\ufb02y discuss all tools below; further\ndetails can be found in Appendix A.\nQuestion Answering Our \ufb01rst tool is a question\nanswering system based on another LM that can an-\nswer simple factoid questions. Speci\ufb01cally, we use\nAtlas (Izacard et al., 2022), a retrieval-augmented\nLM \ufb01netuned on Natural Questions (Kwiatkowski\net al., 2019).\nCalculator As a second tool, we use a calculator\nthat can perform simple numeric calculations; we\nonly support the four basic arithmetic operations.\nResults are always rounded to two decimal places.\nWikipedia Search Our third tool is a search en-\ngine that, given a search term, returns short textsnippets from Wikipedia. Compared to our ques-\ntion answering tool, this search enables a model\nto get more comprehensive information on a sub-\nject, but requires it to extract the relevant parts by\nitself. As our search engine, we use a BM25 re-\ntriever (Robertson et al., 1995; Baeza-Yates et al.,\n1999) that indexes the Wikipedia dump from KILT\n(Petroni et al., 2021).\nMachine Translation System Our fourth tool is\na machine translation system based on a LM that\ncan translate a phrase from any language into En-\nglish. More concretely, we use the 600M parameter\nNLLB (Costa-juss\u00e0 et al., 2022) as our multilingual\nmachine translation model that works for 200 lan-\nguages (including low-resource ones). The source\nlanguage is automatically detected using the fast-\nTextclassi\ufb01er (Joulin et al., 2016), while the target\nlanguage is always set to English.\nCalendar Our \ufb01nal tool is a calendar API that,\nwhen queried, returns the current date without tak-\ning any input. This provides temporal context for\npredictions that require some awareness of time.\n4 Experiments\nWe investigate whether our approach enables a\nmodel to use tools without any further supervision\nand to decide for itself when and how to call which\nof the available tools. To test this, we select a vari-\nety of downstream tasks where we assume at least\none of the considered tools to be useful, and evalu-\nate performance in zero-shot settings (Section 4.2).\nBeyond that, we also ensure that our approach does\nnot hurt the model\u2019s core language modeling abili-\nties; we verify this by looking at perplexity on two\nlanguage modeling datasets (Section 4.3).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "60ddf73f-8559-43c6-8e99-f698534cebfb": {"__data__": {"id_": "60ddf73f-8559-43c6-8e99-f698534cebfb", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}}, "text": "Calendar Our \ufb01nal tool is a calendar API that,\nwhen queried, returns the current date without tak-\ning any input. This provides temporal context for\npredictions that require some awareness of time.\n4 Experiments\nWe investigate whether our approach enables a\nmodel to use tools without any further supervision\nand to decide for itself when and how to call which\nof the available tools. To test this, we select a vari-\nety of downstream tasks where we assume at least\none of the considered tools to be useful, and evalu-\nate performance in zero-shot settings (Section 4.2).\nBeyond that, we also ensure that our approach does\nnot hurt the model\u2019s core language modeling abili-\nties; we verify this by looking at perplexity on two\nlanguage modeling datasets (Section 4.3). Finally,\nwe investigate how the ability to learn using tools\nis affected by model size (Section 4.4).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5b5db312-c332-4b4d-96f2-bf58abaf6547", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0b2eeaaa-4f2d-4ade-a4cb-54e2f0638357": {"__data__": {"id_": "0b2eeaaa-4f2d-4ade-a4cb-54e2f0638357", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60ddf73f-8559-43c6-8e99-f698534cebfb", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "47844d314045ab876e4fc69577206d9f37b2051f778e5fe57e7fc18e120c2273", "class_name": "RelatedNodeInfo"}}, "text": "4.1 Experimental Setup\nDataset Generation Throughout all of our ex-\nperiments, we use a subset of CCNet (Wenzek et al.,\n2020) as our language modeling dataset Cand GPT-\nJ (Wang and Komatsuzaki, 2021) as our language\nmodelM. To reduce the computational cost of\nannotatingCwith API calls, we de\ufb01ne heuristics\nfor some APIs to get a subset of Cfor which API\ncalls are more likely to be helpful than for an av-\nerage text. For example, we only consider texts\nfor the calculator tool if they contain at least three\nnumbers. Details of the heuristics used are given in", "mimetype": "text/plain", "start_char_idx": 871, "end_char_idx": 1433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5b5db312-c332-4b4d-96f2-bf58abaf6547", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e2c92540-a919-42bf-8bab-e6d6c44d4218": {"__data__": {"id_": "e2c92540-a919-42bf-8bab-e6d6c44d4218", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}}, "text": "Calendar Our \ufb01nal tool is a calendar API that,\nwhen queried, returns the current date without tak-\ning any input. This provides temporal context for\npredictions that require some awareness of time.\n4 Experiments\nWe investigate whether our approach enables a\nmodel to use tools without any further supervision\nand to decide for itself when and how to call which\nof the available tools. To test this, we select a vari-\nety of downstream tasks where we assume at least\none of the considered tools to be useful, and evalu-\nate performance in zero-shot settings (Section 4.2).\nBeyond that, we also ensure that our approach does\nnot hurt the model\u2019s core language modeling abili-\nties; we verify this by looking at perplexity on two\nlanguage modeling datasets (Section 4.3). Finally,\nwe investigate how the ability to learn using tools\nis affected by model size (Section 4.4).\n4.1 Experimental Setup\nDataset Generation Throughout all of our ex-\nperiments, we use a subset of CCNet (Wenzek et al.,\n2020) as our language modeling dataset Cand GPT-\nJ (Wang and Komatsuzaki, 2021) as our language\nmodelM. To reduce the computational cost of\nannotatingCwith API calls, we de\ufb01ne heuristics\nfor some APIs to get a subset of Cfor which API\ncalls are more likely to be helpful than for an av-\nerage text. For example, we only consider texts\nfor the calculator tool if they contain at least three\nnumbers. Details of the heuristics used are given in", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5b5db312-c332-4b4d-96f2-bf58abaf6547", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5b5db312-c332-4b4d-96f2-bf58abaf6547": {"__data__": {"id_": "5b5db312-c332-4b4d-96f2-bf58abaf6547", "embedding": null, "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c825aa7816b61655dad04c3744f63db8f5d81325ccbd9503ba5f951520725470", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "6b0c90fae712d97b01fed2b79bc61fe6e947fa02685b98079de9e07e6e7fc783", "class_name": "RelatedNodeInfo"}}, "text": "Calendar Our \ufb01nal tool is a calendar API that,\nwhen queried, returns the current date without tak-\ning any input. This provides temporal context for\npredictions that require some awareness of time.\n4 Experiments\nWe investigate whether our approach enables a\nmodel to use tools without any further supervision\nand to decide for itself when and how to call which\nof the available tools. To test this, we select a vari-\nety of downstream tasks where we assume at least\none of the considered tools to be useful, and evalu-\nate performance in zero-shot settings (Section 4.2).\nBeyond that, we also ensure that our approach does\nnot hurt the model\u2019s core language modeling abili-\nties; we verify this by looking at perplexity on two\nlanguage modeling datasets (Section 4.3). Finally,\nwe investigate how the ability to learn using tools\nis affected by model size (Section 4.4).\n4.1 Experimental Setup\nDataset Generation Throughout all of our ex-\nperiments, we use a subset of CCNet (Wenzek et al.,\n2020) as our language modeling dataset Cand GPT-\nJ (Wang and Komatsuzaki, 2021) as our language\nmodelM. To reduce the computational cost of\nannotatingCwith API calls, we de\ufb01ne heuristics\nfor some APIs to get a subset of Cfor which API\ncalls are more likely to be helpful than for an av-\nerage text. For example, we only consider texts\nfor the calculator tool if they contain at least three\nnumbers. Details of the heuristics used are given in", "mimetype": "text/plain", "start_char_idx": 3139, "end_char_idx": 4572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5b5db312-c332-4b4d-96f2-bf58abaf6547", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "be4ee63a-e796-47b1-aa18-2b85313f38f9": {"__data__": {"id_": "be4ee63a-e796-47b1-aa18-2b85313f38f9", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}}, "text": "API Name Example Input Example Output\nQuestion Answering Where was the Knights\nof Columbus founded?New Haven, Connecticut\nWikipedia Search Fishing Reel Types Spin \ufb01shing > Spin \ufb01shing is distinguished between \ufb02y \ufb01shing and bait\ncast \ufb01shing by the type of rod and reel used. There are two types of reels\nused when spin \ufb01shing, the open faced reel and the closed faced reel.\nCalculator 27 + 4 * 2 35\nCalendar \u03b5 Today is Monday, January 30, 2023.\nMachine Translation s\u00fbret\u00e9 nucl\u00e9aire nuclear safety\nTable 1: Examples of inputs and outputs for all APIs used.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "337acb79-b374-4c66-aa4e-63fa7c9e8d88": {"__data__": {"id_": "337acb79-b374-4c66-aa4e-63fa7c9e8d88", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be4ee63a-e796-47b1-aa18-2b85313f38f9", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "6d24cfd76c614cb4e11bbc1b2eae2484ae2bb391c824e1e709b6e404fe8b1b35", "class_name": "RelatedNodeInfo"}}, "text": "Number of Examples\nAPI \u03c4f= 0.5\u03c4f= 1.0\u03c4f= 2.0\nQuestion Answering 51,987 18,526 5,135\nWikipedia Search 207,241 60,974 13,944\nCalculator 3,680 994 138\nCalendar 61,811 20,587 3,007\nMachine Translation 3,156 1,034 229\nTable 2: Number of examples with API calls in C\u2217for\ndifferent values of our \ufb01ltering threshold \u03c4f.", "mimetype": "text/plain", "start_char_idx": 555, "end_char_idx": 866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "995764a7-21bd-4b20-b213-9f3c95a8eca8": {"__data__": {"id_": "995764a7-21bd-4b20-b213-9f3c95a8eca8", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "337acb79-b374-4c66-aa4e-63fa7c9e8d88", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c5508d09fcea32d1be25f78e77f63b12bc35d9f4778e6b589b695f7f34c93648", "class_name": "RelatedNodeInfo"}}, "text": "Appendix A. For obtaining C\u2217fromC, we perform\nall steps described in Section 2 and additionally\n\ufb01lter out all examples for which all API calls were\neliminated in the \ufb01ltering step.4For the weighting\nfunction, we use\nwt=\u02dcwt\u2211\ns\u2208N\u02dcwswith \u02dcwt= max(0,1\u22120.2\u00b7t)\nto make sure that API calls happen close to where\nthe information provided by the API is actually\nhelpful for the model. The thresholds \u03c4sand\u03c4fare\nchosen individually for each tool to ensure a suf\ufb01-\nciently larger number of examples; see Appendix A\nfor details. Table 2 shows relevant statistics of our\n\ufb01nal dataset augmented with API calls.\nModel Finetuning We \ufb01netune MonC\u2217using\na batch size of 128 and a learning rate of 1\u00b710\u22125\nwith linear warmup for the \ufb01rst 10% of training.", "mimetype": "text/plain", "start_char_idx": 867, "end_char_idx": 1601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6a8e3c9f-e1e9-4295-a76d-d33970774e95": {"__data__": {"id_": "6a8e3c9f-e1e9-4295-a76d-d33970774e95", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "995764a7-21bd-4b20-b213-9f3c95a8eca8", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "008a86ceabfe70525f3a9a472afaeebb7351d3187d951444b79192056cd8009b", "class_name": "RelatedNodeInfo"}}, "text": "Details of our \ufb01netuning procedure are given in\nAppendix B.\nBaseline Models Throughout the remainder of\nthis section, we mainly compare the following mod-\nels:\n4While this \ufb01ltering alters the distribution of training exam-\nples, we assume that the remaining examples are close enough\nto the original distribution so that M\u2019s language modeling\nabilities remain unaffected. This assumption is empirically\nvalidated in Section 4.3.\u2022GPT-J : A regular GPT-J model without any\n\ufb01netuning.\n\u2022GPT-J + CC : GPT-J \ufb01netuned on C, our sub-\nset of CCNet without any API calls.\n\u2022Toolformer : GPT-J \ufb01netuned on C\u2217, our sub-\nset of CCNet augmented with API calls.", "mimetype": "text/plain", "start_char_idx": 1602, "end_char_idx": 2247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7d425980-fabf-4504-b327-179f8aad34ad": {"__data__": {"id_": "7d425980-fabf-4504-b327-179f8aad34ad", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a8e3c9f-e1e9-4295-a76d-d33970774e95", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "228b6f4bd1edbbc1b785aa486abf6099b2915ac051d900cf123aa2d5bd171695", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Toolformer (disabled) : The same model as\nToolformer, but API calls are disabled during\ndecoding.5\nFor most tasks, we additionally compare to OPT\n(66B) (Zhang et al., 2022) and GPT-36(175B)\n(Brown et al., 2020), two models that are about\n10 and 25 times larger than our other baseline mod-\nels, respectively.\n4.2 Downstream Tasks\nWe evaluate all models on a variety of downstream\ntasks. In all cases, we consider a prompted zero-\nshot setup \u2013 i.e., models are instructed to solve\neach task in natural language, but we do not pro-\nvide any in-context examples. This is in contrast\nto prior work on tool use (e.g., Gao et al., 2022;\nParisi et al., 2022), where models are provided\nwith dataset-speci\ufb01c examples of how a tool can be\nused to solve a concrete task.", "mimetype": "text/plain", "start_char_idx": 2248, "end_char_idx": 3009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "50400486-13fd-4062-a0aa-87b26584b149": {"__data__": {"id_": "50400486-13fd-4062-a0aa-87b26584b149", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d425980-fabf-4504-b327-179f8aad34ad", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "e1da473799edd4d64ce1be398cc7b8fa992669b6045d26ef29ea5a5dadba750f", "class_name": "RelatedNodeInfo"}}, "text": "We choose the more\nchallenging zero-shot setup as we are interested\nin seeing whether Toolformer works in precisely\nthose cases where a user does not specify in ad-\nvance which tools should be used in which way for\nsolving a speci\ufb01c problem.\nWe use standard greedy decoding, but with one\nmodi\ufb01cation for Toolformer: We let the model start\nan API call not just when <API> is the most likely\n5This is achieved by manually setting the probability of\nthe<API> token to 0.", "mimetype": "text/plain", "start_char_idx": 3010, "end_char_idx": 3477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2fa245c0-7d2e-42c5-9a38-55eadeb7b48a": {"__data__": {"id_": "2fa245c0-7d2e-42c5-9a38-55eadeb7b48a", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}}, "text": "API Name Example Input Example Output\nQuestion Answering Where was the Knights\nof Columbus founded?New Haven, Connecticut\nWikipedia Search Fishing Reel Types Spin \ufb01shing > Spin \ufb01shing is distinguished between \ufb02y \ufb01shing and bait\ncast \ufb01shing by the type of rod and reel used. There are two types of reels\nused when spin \ufb01shing, the open faced reel and the closed faced reel.\nCalculator 27 + 4 * 2 35\nCalendar \u03b5 Today is Monday, January 30, 2023.\nMachine Translation s\u00fbret\u00e9 nucl\u00e9aire nuclear safety\nTable 1: Examples of inputs and outputs for all APIs used.\nNumber of Examples\nAPI \u03c4f= 0.5\u03c4f= 1.0\u03c4f= 2.0\nQuestion Answering 51,987 18,526 5,135\nWikipedia Search 207,241 60,974 13,944\nCalculator 3,680 994 138\nCalendar 61,811 20,587 3,007\nMachine Translation 3,156 1,034 229\nTable 2: Number of examples with API calls in C\u2217for\ndifferent values of our \ufb01ltering threshold \u03c4f.\nAppendix A. For obtaining C\u2217fromC, we perform\nall steps described in Section 2 and additionally\n\ufb01lter out all examples for which all API calls were\neliminated in the \ufb01ltering step.4For the weighting\nfunction, we use\nwt=\u02dcwt\u2211\ns\u2208N\u02dcwswith \u02dcwt= max(0,1\u22120.2\u00b7t)\nto make sure that API calls happen close to where\nthe information provided by the API is actually\nhelpful for the model. The thresholds \u03c4sand\u03c4fare\nchosen individually for each tool to ensure a suf\ufb01-\nciently larger number of examples; see Appendix A\nfor details. Table 2 shows relevant statistics of our\n\ufb01nal dataset augmented with API calls.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ddd3fe98-9660-421d-ae1c-923b8f3bdfaf": {"__data__": {"id_": "ddd3fe98-9660-421d-ae1c-923b8f3bdfaf", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fa245c0-7d2e-42c5-9a38-55eadeb7b48a", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bbae2c2f4f91b5719dbdea7884c69f92097d3b830c5228dd0a4a38cd5de5950", "class_name": "RelatedNodeInfo"}}, "text": "Table 2 shows relevant statistics of our\n\ufb01nal dataset augmented with API calls.\nModel Finetuning We \ufb01netune MonC\u2217using\na batch size of 128 and a learning rate of 1\u00b710\u22125\nwith linear warmup for the \ufb01rst 10% of training.\nDetails of our \ufb01netuning procedure are given in\nAppendix B.\nBaseline Models Throughout the remainder of\nthis section, we mainly compare the following mod-\nels:\n4While this \ufb01ltering alters the distribution of training exam-\nples, we assume that the remaining examples are close enough\nto the original distribution so that M\u2019s language modeling\nabilities remain unaffected. This assumption is empirically\nvalidated in Section 4.3.\u2022GPT-J : A regular GPT-J model without any\n\ufb01netuning.\n\u2022GPT-J + CC : GPT-J \ufb01netuned on C, our sub-\nset of CCNet without any API calls.\n\u2022Toolformer : GPT-J \ufb01netuned on C\u2217, our sub-\nset of CCNet augmented with API calls.\n\u2022Toolformer (disabled) : The same model as\nToolformer, but API calls are disabled during\ndecoding.5\nFor most tasks, we additionally compare to OPT\n(66B) (Zhang et al., 2022) and GPT-36(175B)\n(Brown et al., 2020), two models that are about\n10 and 25 times larger than our other baseline mod-\nels, respectively.\n4.2 Downstream Tasks\nWe evaluate all models on a variety of downstream\ntasks. In all cases, we consider a prompted zero-\nshot setup \u2013 i.e., models are instructed to solve\neach task in natural language, but we do not pro-\nvide any in-context examples. This is in contrast\nto prior work on tool use (e.g., Gao et al., 2022;\nParisi et al., 2022), where models are provided\nwith dataset-speci\ufb01c examples of how a tool can be\nused to solve a concrete task.", "mimetype": "text/plain", "start_char_idx": 1384, "end_char_idx": 3009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0847deed-64a5-4f45-a3b9-aee7ab6d68e3": {"__data__": {"id_": "0847deed-64a5-4f45-a3b9-aee7ab6d68e3", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddd3fe98-9660-421d-ae1c-923b8f3bdfaf", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b5ff90dcf4fcde700270a16f1f7869dd562c8d9c461dbcd659d1a452b8ed548b", "class_name": "RelatedNodeInfo"}}, "text": "We choose the more\nchallenging zero-shot setup as we are interested\nin seeing whether Toolformer works in precisely\nthose cases where a user does not specify in ad-\nvance which tools should be used in which way for\nsolving a speci\ufb01c problem.\nWe use standard greedy decoding, but with one\nmodi\ufb01cation for Toolformer: We let the model start\nan API call not just when <API> is the most likely\n5This is achieved by manually setting the probability of\nthe<API> token to 0.", "mimetype": "text/plain", "start_char_idx": 3010, "end_char_idx": 3477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f3460b8e-fc3d-4014-ae2d-e0d1524d9946": {"__data__": {"id_": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "332430a4-7156-4128-acd4-5e75f80349ca", "node_type": "1", "metadata": {}, "hash": "9233700f9058fd0e9f447f2aea9a5098138c367ae50e7fedc0a31d2f83a5b5af", "class_name": "RelatedNodeInfo"}}, "text": "API Name Example Input Example Output\nQuestion Answering Where was the Knights\nof Columbus founded?New Haven, Connecticut\nWikipedia Search Fishing Reel Types Spin \ufb01shing > Spin \ufb01shing is distinguished between \ufb02y \ufb01shing and bait\ncast \ufb01shing by the type of rod and reel used. There are two types of reels\nused when spin \ufb01shing, the open faced reel and the closed faced reel.\nCalculator 27 + 4 * 2 35\nCalendar \u03b5 Today is Monday, January 30, 2023.\nMachine Translation s\u00fbret\u00e9 nucl\u00e9aire nuclear safety\nTable 1: Examples of inputs and outputs for all APIs used.\nNumber of Examples\nAPI \u03c4f= 0.5\u03c4f= 1.0\u03c4f= 2.0\nQuestion Answering 51,987 18,526 5,135\nWikipedia Search 207,241 60,974 13,944\nCalculator 3,680 994 138\nCalendar 61,811 20,587 3,007\nMachine Translation 3,156 1,034 229\nTable 2: Number of examples with API calls in C\u2217for\ndifferent values of our \ufb01ltering threshold \u03c4f.\nAppendix A. For obtaining C\u2217fromC, we perform\nall steps described in Section 2 and additionally\n\ufb01lter out all examples for which all API calls were\neliminated in the \ufb01ltering step.4For the weighting\nfunction, we use\nwt=\u02dcwt\u2211\ns\u2208N\u02dcwswith \u02dcwt= max(0,1\u22120.2\u00b7t)\nto make sure that API calls happen close to where\nthe information provided by the API is actually\nhelpful for the model. The thresholds \u03c4sand\u03c4fare\nchosen individually for each tool to ensure a suf\ufb01-\nciently larger number of examples; see Appendix A\nfor details. Table 2 shows relevant statistics of our\n\ufb01nal dataset augmented with API calls.\nModel Finetuning We \ufb01netune MonC\u2217using\na batch size of 128 and a learning rate of 1\u00b710\u22125\nwith linear warmup for the \ufb01rst 10% of training.\nDetails of our \ufb01netuning procedure are given in\nAppendix B.\nBaseline Models Throughout the remainder of\nthis section, we mainly compare the following mod-\nels:\n4While this \ufb01ltering alters the distribution of training exam-\nples, we assume that the remaining examples are close enough\nto the original distribution so that M\u2019s language modeling\nabilities remain unaffected. This assumption is empirically\nvalidated in Section 4.3.\u2022GPT-J : A regular GPT-J model without any\n\ufb01netuning.\n\u2022GPT-J + CC : GPT-J \ufb01netuned on C, our sub-\nset of CCNet without any API calls.\n\u2022Toolformer : GPT-J \ufb01netuned on C\u2217, our sub-\nset of CCNet augmented with API calls.\n\u2022Toolformer (disabled) : The same model as\nToolformer, but API calls are disabled during\ndecoding.5\nFor most tasks, we additionally compare to OPT\n(66B) (Zhang et al., 2022) and GPT-36(175B)\n(Brown et al., 2020), two models that are about\n10 and 25 times larger than our other baseline mod-\nels, respectively.\n4.2 Downstream Tasks\nWe evaluate all models on a variety of downstream\ntasks. In all cases, we consider a prompted zero-\nshot setup \u2013 i.e., models are instructed to solve\neach task in natural language, but we do not pro-\nvide any in-context examples. This is in contrast\nto prior work on tool use (e.g., Gao et al., 2022;\nParisi et al., 2022), where models are provided\nwith dataset-speci\ufb01c examples of how a tool can be\nused to solve a concrete task. We choose the more\nchallenging zero-shot setup as we are interested\nin seeing whether Toolformer works in precisely\nthose cases where a user does not specify in ad-\nvance which tools should be used in which way for\nsolving a speci\ufb01c problem.\nWe use standard greedy decoding, but with one\nmodi\ufb01cation for Toolformer: We let the model start\nan API call not just when <API> is the most likely\n5This is achieved by manually setting the probability of\nthe<API> token to 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6d65ecac-3a61-4f0b-9085-b99766b5c037": {"__data__": {"id_": "6d65ecac-3a61-4f0b-9085-b99766b5c037", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}}, "text": "This is in contrast\nto prior work on tool use (e.g., Gao et al., 2022;\nParisi et al., 2022), where models are provided\nwith dataset-speci\ufb01c examples of how a tool can be\nused to solve a concrete task. We choose the more\nchallenging zero-shot setup as we are interested\nin seeing whether Toolformer works in precisely\nthose cases where a user does not specify in ad-\nvance which tools should be used in which way for\nsolving a speci\ufb01c problem.\nWe use standard greedy decoding, but with one\nmodi\ufb01cation for Toolformer: We let the model start\nan API call not just when <API> is the most likely\n5This is achieved by manually setting the probability of\nthe<API> token to 0.\n6We use the original davinci variant that is not \ufb01netuned\non any instructions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "332430a4-7156-4128-acd4-5e75f80349ca", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f2d1ed81-4f9b-4976-bb7c-c442c1fe8d46": {"__data__": {"id_": "f2d1ed81-4f9b-4976-bb7c-c442c1fe8d46", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}}, "text": "This is in contrast\nto prior work on tool use (e.g., Gao et al., 2022;\nParisi et al., 2022), where models are provided\nwith dataset-speci\ufb01c examples of how a tool can be\nused to solve a concrete task. We choose the more\nchallenging zero-shot setup as we are interested\nin seeing whether Toolformer works in precisely\nthose cases where a user does not specify in ad-\nvance which tools should be used in which way for\nsolving a speci\ufb01c problem.\nWe use standard greedy decoding, but with one\nmodi\ufb01cation for Toolformer: We let the model start\nan API call not just when <API> is the most likely\n5This is achieved by manually setting the probability of\nthe<API> token to 0.\n6We use the original davinci variant that is not \ufb01netuned\non any instructions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "332430a4-7156-4128-acd4-5e75f80349ca", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "332430a4-7156-4128-acd4-5e75f80349ca": {"__data__": {"id_": "332430a4-7156-4128-acd4-5e75f80349ca", "embedding": null, "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d7f236-75ea-4f82-a8ab-3678082838cd", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bc095f13c5fc7a0e6c26c44350fbd1a2061ae8d4270984170693cda0fb7b75c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "42ef70ae579b270e489fc930ba168597b6ce26f4ffd4f0a5a486340cd5b4c7d0", "class_name": "RelatedNodeInfo"}}, "text": "This is in contrast\nto prior work on tool use (e.g., Gao et al., 2022;\nParisi et al., 2022), where models are provided\nwith dataset-speci\ufb01c examples of how a tool can be\nused to solve a concrete task. We choose the more\nchallenging zero-shot setup as we are interested\nin seeing whether Toolformer works in precisely\nthose cases where a user does not specify in ad-\nvance which tools should be used in which way for\nsolving a speci\ufb01c problem.\nWe use standard greedy decoding, but with one\nmodi\ufb01cation for Toolformer: We let the model start\nan API call not just when <API> is the most likely\n5This is achieved by manually setting the probability of\nthe<API> token to 0.\n6We use the original davinci variant that is not \ufb01netuned\non any instructions.", "mimetype": "text/plain", "start_char_idx": 2809, "end_char_idx": 3556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "332430a4-7156-4128-acd4-5e75f80349ca", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c99dc81c-e321-4622-9cba-e740091c4d8b": {"__data__": {"id_": "c99dc81c-e321-4622-9cba-e740091c4d8b", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}}, "text": "token, but whenever it is one of the kmost likely\ntokens. For k= 1, this corresponds to regular\ngreedy decoding; we instead use k= 10 to in-\ncrease the disposition of our model to make use of\nthe APIs that it has access to. At the same time,\nwe only at most one API call per input to make\nsure the model does not get stuck in a loop where\nit constantly calls APIs without producing any ac-\ntual output. The effect of these modi\ufb01cations is\nexplored in Section 5.\n4.2.1 LAMA\nWe evaluate our models on the SQuAD, Google-\nRE and T-REx subsets of the LAMA benchmark\n(Petroni et al., 2019). For each of these subsets, the\ntask is to complete a short statement with a miss-\ning fact (e.g., a date or a place).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "302430ba-129e-4c83-a3d8-978c9203d6b9": {"__data__": {"id_": "302430ba-129e-4c83-a3d8-978c9203d6b9", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c99dc81c-e321-4622-9cba-e740091c4d8b", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8c43b3937165200fca8e5451451f817e5fcbfbdcd5275b2432d0efbd997e6e6f", "class_name": "RelatedNodeInfo"}}, "text": "As LAMA was\noriginally designed to evaluate masked language\nmodels (e.g., Devlin et al., 2019), we \ufb01lter out ex-\namples where the mask token is not the \ufb01nal token,\nso that the remaining examples can be processed\nin a left-to-right fashion. To account for different\ntokenizations and added complexity from not in-\nforming the model that a single word is required,\nwe use a slightly more lenient evaluation criterion\nthan exact match and simply check whether the\ncorrect word is within the \ufb01rst \ufb01ve words predicted\nby the model. As LAMA is based on statements\nobtained directly from Wikipedia, we prevent Tool-\nformer from using the Wikipedia Search API to\navoid giving it an unfair advantage.\nResults for all models can be seen in Table 3.\nAll GPT-J models without tool use achieve similar\nperformance.", "mimetype": "text/plain", "start_char_idx": 703, "end_char_idx": 1504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3d65f2c3-efa3-44a9-9482-a99f34211d69": {"__data__": {"id_": "3d65f2c3-efa3-44a9-9482-a99f34211d69", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "302430ba-129e-4c83-a3d8-978c9203d6b9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "7c0639c02ae810e36e16820ed313b76a03a4446e8b3cc76398c04c1200491191", "class_name": "RelatedNodeInfo"}}, "text": "All GPT-J models without tool use achieve similar\nperformance. Crucially, Toolformer clearly outper-\nforms these baseline models, improving upon the\nbest baseline by 11.7, 5.2 and 18.6 points, respec-\ntively. It also clearly outperforms OPT (66B) and\nGPT-3 (175B), despite both models being much\nlarger. This is achieved because the model inde-\npendently decides to ask the question answering\ntool for the required information in almost all cases\n(98.1%); for only very few examples, it uses a dif-\nferent tool (0.7%) or no tool at all (1.2%).\n4.2.2 Math Datasets\nWe test mathematical reasoning abilities on ASDiv\n(Miao et al., 2020), SV AMP (Patel et al., 2021) and\nthe MAWPS benchmark (Koncel-Kedziorski et al.,\n2016).", "mimetype": "text/plain", "start_char_idx": 1442, "end_char_idx": 2162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f2d71a6b-4eca-42bc-9ff3-38dfb02ad177": {"__data__": {"id_": "f2d71a6b-4eca-42bc-9ff3-38dfb02ad177", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d65f2c3-efa3-44a9-9482-a99f34211d69", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a1d478e8112340ffe73081ad3ac5956d4ee2226f98725978aff8d32f32f3521a", "class_name": "RelatedNodeInfo"}}, "text": "We again account for the fact that we test\nall models in a zero-shot setup by using a more\nlenient evaluation criterion: As the required output\nis always a number, we simply check for the \ufb01rstModel SQuAD Google-RE T-REx\nGPT-J 17.8 4.9 31.9\nGPT-J + CC 19.2 5.6 33.2\nToolformer (disabled) 22.1 6.3 34.9\nToolformer 33.8 11.5 53.5\nOPT (66B) 21.6 2.9 30.1\nGPT-3 (175B) 26.8 7.0 39.8\nTable 3: Results on subsets of LAMA. Toolformer uses\nthe question answering tool for most examples, clearly\noutperforming all baselines of the same size and achiev-\ning results competitive with GPT-3 (175B).", "mimetype": "text/plain", "start_char_idx": 2163, "end_char_idx": 2748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a0d45704-53ac-496a-8d6d-1976876b6164": {"__data__": {"id_": "a0d45704-53ac-496a-8d6d-1976876b6164", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2d71a6b-4eca-42bc-9ff3-38dfb02ad177", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "affc29f70037084d0bd7fdc09de3613df42fcd9c61067fedcdeaf3905ad1c633", "class_name": "RelatedNodeInfo"}}, "text": "Model ASDiv SVAMP MAWPS\nGPT-J 7.5 5.2 9.9\nGPT-J + CC 9.6 5.0 9.3\nToolformer (disabled) 14.8 6.3 15.0\nToolformer 40.4 29.4 44.0\nOPT (66B) 6.0 4.9 7.9\nGPT-3 (175B) 14.0 10.0 19.8\nTable 4: Results for various benchmarks requiring\nmathematical reasoning. Toolformer makes use of the\ncalculator tool for most examples, clearly outperform-\ning even OPT (66B) and GPT-3 (175B).\nnumber predicted by the model.7\nTable 4 shows results for all benchmarks.", "mimetype": "text/plain", "start_char_idx": 2749, "end_char_idx": 3193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e751c095-4fa8-4db4-8364-eba8859af65f": {"__data__": {"id_": "e751c095-4fa8-4db4-8364-eba8859af65f", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}}, "text": "token, but whenever it is one of the kmost likely\ntokens. For k= 1, this corresponds to regular\ngreedy decoding; we instead use k= 10 to in-\ncrease the disposition of our model to make use of\nthe APIs that it has access to. At the same time,\nwe only at most one API call per input to make\nsure the model does not get stuck in a loop where\nit constantly calls APIs without producing any ac-\ntual output. The effect of these modi\ufb01cations is\nexplored in Section 5.\n4.2.1 LAMA\nWe evaluate our models on the SQuAD, Google-\nRE and T-REx subsets of the LAMA benchmark\n(Petroni et al., 2019). For each of these subsets, the\ntask is to complete a short statement with a miss-\ning fact (e.g., a date or a place). As LAMA was\noriginally designed to evaluate masked language\nmodels (e.g., Devlin et al., 2019), we \ufb01lter out ex-\namples where the mask token is not the \ufb01nal token,\nso that the remaining examples can be processed\nin a left-to-right fashion. To account for different\ntokenizations and added complexity from not in-\nforming the model that a single word is required,\nwe use a slightly more lenient evaluation criterion\nthan exact match and simply check whether the\ncorrect word is within the \ufb01rst \ufb01ve words predicted\nby the model. As LAMA is based on statements\nobtained directly from Wikipedia, we prevent Tool-\nformer from using the Wikipedia Search API to\navoid giving it an unfair advantage.\nResults for all models can be seen in Table 3.\nAll GPT-J models without tool use achieve similar\nperformance. Crucially, Toolformer clearly outper-\nforms these baseline models, improving upon the\nbest baseline by 11.7, 5.2 and 18.6 points, respec-\ntively. It also clearly outperforms OPT (66B) and\nGPT-3 (175B), despite both models being much\nlarger.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "795e905c-295f-424a-ba98-5c96747b8b6d": {"__data__": {"id_": "795e905c-295f-424a-ba98-5c96747b8b6d", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e751c095-4fa8-4db4-8364-eba8859af65f", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "7101998647e75be18e3b0984bc049b83bdbc08237474a65d3286f71e497e2a1a", "class_name": "RelatedNodeInfo"}}, "text": "This is achieved because the model inde-\npendently decides to ask the question answering\ntool for the required information in almost all cases\n(98.1%); for only very few examples, it uses a dif-\nferent tool (0.7%) or no tool at all (1.2%).\n4.2.2 Math Datasets\nWe test mathematical reasoning abilities on ASDiv\n(Miao et al., 2020), SV AMP (Patel et al., 2021) and\nthe MAWPS benchmark (Koncel-Kedziorski et al.,\n2016). We again account for the fact that we test\nall models in a zero-shot setup by using a more\nlenient evaluation criterion: As the required output\nis always a number, we simply check for the \ufb01rstModel SQuAD Google-RE T-REx\nGPT-J 17.8 4.9 31.9\nGPT-J + CC 19.2 5.6 33.2\nToolformer (disabled) 22.1 6.3 34.9\nToolformer 33.8 11.5 53.5\nOPT (66B) 21.6 2.9 30.1\nGPT-3 (175B) 26.8 7.0 39.8\nTable 3: Results on subsets of LAMA. Toolformer uses\nthe question answering tool for most examples, clearly\noutperforming all baselines of the same size and achiev-\ning results competitive with GPT-3 (175B).\nModel ASDiv SVAMP MAWPS\nGPT-J 7.5 5.2 9.9\nGPT-J + CC 9.6 5.0 9.3\nToolformer (disabled) 14.8 6.3 15.0\nToolformer 40.4 29.4 44.0\nOPT (66B) 6.0 4.9 7.9\nGPT-3 (175B) 14.0 10.0 19.8\nTable 4: Results for various benchmarks requiring\nmathematical reasoning.", "mimetype": "text/plain", "start_char_idx": 1746, "end_char_idx": 2999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b5f24363-c544-4756-8b55-7354a4f4e2e0": {"__data__": {"id_": "b5f24363-c544-4756-8b55-7354a4f4e2e0", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "795e905c-295f-424a-ba98-5c96747b8b6d", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "853e3f0dc75789735041b9d6a481ebed9aff8f129cd1f06421430a317a5b00f1", "class_name": "RelatedNodeInfo"}}, "text": "Toolformer makes use of the\ncalculator tool for most examples, clearly outperform-\ning even OPT (66B) and GPT-3 (175B).\nnumber predicted by the model.7\nTable 4 shows results for all benchmarks.", "mimetype": "text/plain", "start_char_idx": 3000, "end_char_idx": 3193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "44d6e092-beab-4ecf-9e69-651c9fc1b4f5": {"__data__": {"id_": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "871a8901-e6c4-4ba9-9040-a15ca2a4e017", "node_type": "1", "metadata": {}, "hash": "7a685878bcbbaad039061ef26a674089d7ce657afc28074fba0cd9f2cddf91af", "class_name": "RelatedNodeInfo"}}, "text": "token, but whenever it is one of the kmost likely\ntokens. For k= 1, this corresponds to regular\ngreedy decoding; we instead use k= 10 to in-\ncrease the disposition of our model to make use of\nthe APIs that it has access to. At the same time,\nwe only at most one API call per input to make\nsure the model does not get stuck in a loop where\nit constantly calls APIs without producing any ac-\ntual output. The effect of these modi\ufb01cations is\nexplored in Section 5.\n4.2.1 LAMA\nWe evaluate our models on the SQuAD, Google-\nRE and T-REx subsets of the LAMA benchmark\n(Petroni et al., 2019). For each of these subsets, the\ntask is to complete a short statement with a miss-\ning fact (e.g., a date or a place). As LAMA was\noriginally designed to evaluate masked language\nmodels (e.g., Devlin et al., 2019), we \ufb01lter out ex-\namples where the mask token is not the \ufb01nal token,\nso that the remaining examples can be processed\nin a left-to-right fashion. To account for different\ntokenizations and added complexity from not in-\nforming the model that a single word is required,\nwe use a slightly more lenient evaluation criterion\nthan exact match and simply check whether the\ncorrect word is within the \ufb01rst \ufb01ve words predicted\nby the model. As LAMA is based on statements\nobtained directly from Wikipedia, we prevent Tool-\nformer from using the Wikipedia Search API to\navoid giving it an unfair advantage.\nResults for all models can be seen in Table 3.\nAll GPT-J models without tool use achieve similar\nperformance. Crucially, Toolformer clearly outper-\nforms these baseline models, improving upon the\nbest baseline by 11.7, 5.2 and 18.6 points, respec-\ntively. It also clearly outperforms OPT (66B) and\nGPT-3 (175B), despite both models being much\nlarger. This is achieved because the model inde-\npendently decides to ask the question answering\ntool for the required information in almost all cases\n(98.1%); for only very few examples, it uses a dif-\nferent tool (0.7%) or no tool at all (1.2%).\n4.2.2 Math Datasets\nWe test mathematical reasoning abilities on ASDiv\n(Miao et al., 2020), SV AMP (Patel et al., 2021) and\nthe MAWPS benchmark (Koncel-Kedziorski et al.,\n2016). We again account for the fact that we test\nall models in a zero-shot setup by using a more\nlenient evaluation criterion: As the required output\nis always a number, we simply check for the \ufb01rstModel SQuAD Google-RE T-REx\nGPT-J 17.8 4.9 31.9\nGPT-J + CC 19.2 5.6 33.2\nToolformer (disabled) 22.1 6.3 34.9\nToolformer 33.8 11.5 53.5\nOPT (66B) 21.6 2.9 30.1\nGPT-3 (175B) 26.8 7.0 39.8\nTable 3: Results on subsets of LAMA. Toolformer uses\nthe question answering tool for most examples, clearly\noutperforming all baselines of the same size and achiev-\ning results competitive with GPT-3 (175B).\nModel ASDiv SVAMP MAWPS\nGPT-J 7.5 5.2 9.9\nGPT-J + CC 9.6 5.0 9.3\nToolformer (disabled) 14.8 6.3 15.0\nToolformer 40.4 29.4 44.0\nOPT (66B) 6.0 4.9 7.9\nGPT-3 (175B) 14.0 10.0 19.8\nTable 4: Results for various benchmarks requiring\nmathematical reasoning. Toolformer makes use of the\ncalculator tool for most examples, clearly outperform-\ning even OPT (66B) and GPT-3 (175B).\nnumber predicted by the model.7\nTable 4 shows results for all benchmarks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "42310718-2ee7-429d-a1d2-8098f04b1da0": {"__data__": {"id_": "42310718-2ee7-429d-a1d2-8098f04b1da0", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}}, "text": "Model ASDiv SVAMP MAWPS\nGPT-J 7.5 5.2 9.9\nGPT-J + CC 9.6 5.0 9.3\nToolformer (disabled) 14.8 6.3 15.0\nToolformer 40.4 29.4 44.0\nOPT (66B) 6.0 4.9 7.9\nGPT-3 (175B) 14.0 10.0 19.8\nTable 4: Results for various benchmarks requiring\nmathematical reasoning. Toolformer makes use of the\ncalculator tool for most examples, clearly outperform-\ning even OPT (66B) and GPT-3 (175B).\nnumber predicted by the model.7\nTable 4 shows results for all benchmarks. While\nGPT-J and GPT-J + CC perform about the same,\nToolformer achieves stronger results even when\nAPI calls are disabled.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "871a8901-e6c4-4ba9-9040-a15ca2a4e017", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f1eb5da8-ba01-4c21-a8a1-af9d42f1465c": {"__data__": {"id_": "f1eb5da8-ba01-4c21-a8a1-af9d42f1465c", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42310718-2ee7-429d-a1d2-8098f04b1da0", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "2f1a1058e6808364264361d1a5895ba44fd74120c2abb03526a1cafce886d2df", "class_name": "RelatedNodeInfo"}}, "text": "We surmise that this is be-\ncause the model is \ufb01netuned on many examples\nof API calls and their results, improving its own\nmathematical capabilities. Nonetheless, allowing\nthe model to make API calls more than doubles per-\nformance for all tasks, and also clearly outperforms\nthe much larger OPT and GPT-3 models. This is\nbecause across all benchmarks, for 97.9% of all\nexamples the model decides to ask the calculator\ntool for help.\n4.2.3 Question Answering\nWe look at Web Questions (Berant et al., 2013),\nNatural Questions (Kwiatkowski et al., 2019) and\nTriviaQA (Joshi et al., 2017), the three question an-\nswering datasets considered by Brown et al. (2020).\nFor evaluation, we check whether the \ufb01rst 20 words\npredicted by a model contain the correct answer\ninstead of requiring an exact match.", "mimetype": "text/plain", "start_char_idx": 567, "end_char_idx": 1364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "871a8901-e6c4-4ba9-9040-a15ca2a4e017", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a1c32316-c73f-462f-8d87-e892b1dcf286": {"__data__": {"id_": "a1c32316-c73f-462f-8d87-e892b1dcf286", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1eb5da8-ba01-4c21-a8a1-af9d42f1465c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "e2d189afdc28bd7a09a0e57d7813afccfcadb09a80191d1782fba7f82f3509ed", "class_name": "RelatedNodeInfo"}}, "text": "For Tool-\nformer, we disable the question answering tool as\n7An exception to this is if the model\u2019s prediction contains\nan equation (e.g., \u201cThe correct answer is 5+3=8\u201d), in which\ncase we consider the \ufb01rst number after the \u201c=\u201d sign to be its\nprediction.", "mimetype": "text/plain", "start_char_idx": 1365, "end_char_idx": 1618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "871a8901-e6c4-4ba9-9040-a15ca2a4e017", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5e9bd650-fe88-42ca-86d3-39af7137ba47": {"__data__": {"id_": "5e9bd650-fe88-42ca-86d3-39af7137ba47", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}}, "text": "Model ASDiv SVAMP MAWPS\nGPT-J 7.5 5.2 9.9\nGPT-J + CC 9.6 5.0 9.3\nToolformer (disabled) 14.8 6.3 15.0\nToolformer 40.4 29.4 44.0\nOPT (66B) 6.0 4.9 7.9\nGPT-3 (175B) 14.0 10.0 19.8\nTable 4: Results for various benchmarks requiring\nmathematical reasoning. Toolformer makes use of the\ncalculator tool for most examples, clearly outperform-\ning even OPT (66B) and GPT-3 (175B).\nnumber predicted by the model.7\nTable 4 shows results for all benchmarks. While\nGPT-J and GPT-J + CC perform about the same,\nToolformer achieves stronger results even when\nAPI calls are disabled. We surmise that this is be-\ncause the model is \ufb01netuned on many examples\nof API calls and their results, improving its own\nmathematical capabilities. Nonetheless, allowing\nthe model to make API calls more than doubles per-\nformance for all tasks, and also clearly outperforms\nthe much larger OPT and GPT-3 models. This is\nbecause across all benchmarks, for 97.9% of all\nexamples the model decides to ask the calculator\ntool for help.\n4.2.3 Question Answering\nWe look at Web Questions (Berant et al., 2013),\nNatural Questions (Kwiatkowski et al., 2019) and\nTriviaQA (Joshi et al., 2017), the three question an-\nswering datasets considered by Brown et al. (2020).\nFor evaluation, we check whether the \ufb01rst 20 words\npredicted by a model contain the correct answer\ninstead of requiring an exact match. For Tool-\nformer, we disable the question answering tool as\n7An exception to this is if the model\u2019s prediction contains\nan equation (e.g., \u201cThe correct answer is 5+3=8\u201d), in which\ncase we consider the \ufb01rst number after the \u201c=\u201d sign to be its\nprediction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "871a8901-e6c4-4ba9-9040-a15ca2a4e017", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "871a8901-e6c4-4ba9-9040-a15ca2a4e017": {"__data__": {"id_": "871a8901-e6c4-4ba9-9040-a15ca2a4e017", "embedding": null, "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1af666f-893f-4303-b5e4-839f3b23f880", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "757d4f8ff6e465b267e6c13c0d2a9e99a931202f1c1a6f282d3b2fc3c5b09a66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1e14bd1a910cca97af0c60d92ba1a4ff6698a6cb5c6276302f2abdd1ef2a23ab", "class_name": "RelatedNodeInfo"}}, "text": "Model ASDiv SVAMP MAWPS\nGPT-J 7.5 5.2 9.9\nGPT-J + CC 9.6 5.0 9.3\nToolformer (disabled) 14.8 6.3 15.0\nToolformer 40.4 29.4 44.0\nOPT (66B) 6.0 4.9 7.9\nGPT-3 (175B) 14.0 10.0 19.8\nTable 4: Results for various benchmarks requiring\nmathematical reasoning. Toolformer makes use of the\ncalculator tool for most examples, clearly outperform-\ning even OPT (66B) and GPT-3 (175B).\nnumber predicted by the model.7\nTable 4 shows results for all benchmarks. While\nGPT-J and GPT-J + CC perform about the same,\nToolformer achieves stronger results even when\nAPI calls are disabled. We surmise that this is be-\ncause the model is \ufb01netuned on many examples\nof API calls and their results, improving its own\nmathematical capabilities. Nonetheless, allowing\nthe model to make API calls more than doubles per-\nformance for all tasks, and also clearly outperforms\nthe much larger OPT and GPT-3 models. This is\nbecause across all benchmarks, for 97.9% of all\nexamples the model decides to ask the calculator\ntool for help.\n4.2.3 Question Answering\nWe look at Web Questions (Berant et al., 2013),\nNatural Questions (Kwiatkowski et al., 2019) and\nTriviaQA (Joshi et al., 2017), the three question an-\nswering datasets considered by Brown et al. (2020).\nFor evaluation, we check whether the \ufb01rst 20 words\npredicted by a model contain the correct answer\ninstead of requiring an exact match. For Tool-\nformer, we disable the question answering tool as\n7An exception to this is if the model\u2019s prediction contains\nan equation (e.g., \u201cThe correct answer is 5+3=8\u201d), in which\ncase we consider the \ufb01rst number after the \u201c=\u201d sign to be its\nprediction.", "mimetype": "text/plain", "start_char_idx": 2749, "end_char_idx": 4367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "871a8901-e6c4-4ba9-9040-a15ca2a4e017", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "faae4f59-5c3f-451c-810b-cd8fc391ab93": {"__data__": {"id_": "faae4f59-5c3f-451c-810b-cd8fc391ab93", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}}, "text": "Model WebQS NQ TriviaQA\nGPT-J 18.5 12.8 43.9\nGPT-J + CC 18.4 12.2 45.6\nToolformer (disabled) 18.9 12.6 46.7\nToolformer 26.3 17.7 48.8\nOPT (66B) 18.6 11.4 45.7\nGPT-3 (175B) 29.0 22.6 65.9\nTable 5: Results for various question answering dataset.\nUsing the Wikipedia search tool for most examples,\nToolformer clearly outperforms baselines of the same\nsize, but falls short of GPT-3 (175B).\nthis would make solving the tasks trivial, especially\ngiven that the underlying QA system was \ufb01netuned\non Natural Questions.\nResults are shown in Table 5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3ea17869-057f-4876-ae38-548cfd0ff08c": {"__data__": {"id_": "3ea17869-057f-4876-ae38-548cfd0ff08c", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faae4f59-5c3f-451c-810b-cd8fc391ab93", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "0a88ab3a59739fed57d47ea3643047c9b494aa2457c287f9a3c447e8b310384f", "class_name": "RelatedNodeInfo"}}, "text": "Results are shown in Table 5. Once again,\nToolformer clearly outperforms all other models\nbased on GPT-J, this time mostly relying on the\nWikipedia search API (99.3%) to \ufb01nd relevant in-\nformation. However, Toolformer still lags behind\nthe much larger GPT-3 (175B) model. This is likely\ndue to both the simplicity of our search engine (in\nmany cases, it returns results that are clearly not\na good match for a given query) and the inability\nof Toolformer to interact with it, e.g., by refor-\nmulating its query if results are not helpful or by\nbrowsing through multiple of the top results. We\nbelieve that adding this functionality is an exciting\ndirection for future work.\n4.2.4 Multilingual Question Answering\nWe evaluate Toolformer and all baseline models\non MLQA (Lewis et al., 2019), a multilingual\nquestion-answering benchmark.", "mimetype": "text/plain", "start_char_idx": 512, "end_char_idx": 1345, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a3d30d56-d6d2-49e7-9034-279d0114b5d4": {"__data__": {"id_": "a3d30d56-d6d2-49e7-9034-279d0114b5d4", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ea17869-057f-4876-ae38-548cfd0ff08c", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a77fde60d080e64221fe13ff65a8aca60cc55a32aba113d9af3bd860a5565dfd", "class_name": "RelatedNodeInfo"}}, "text": "A context para-\ngraph for each question is provided in English,\nwhile the question can be in Arabic, German, Span-\nish, Hindi, Vietnamese, or Simpli\ufb01ed Chinese. In\norder to solve the task, the model needs to be able\nto understand both the paragraph and the question,\nso it may bene\ufb01t from translating the question into\nEnglish. Our evaluation metric is the percentage of\ntimes the model\u2019s generation, capped at 10 words,\ncontains the correct answer.\nResults are shown in Table 6. Using API calls\nconsistently improves Toolformer\u2019s performance\nfor all languages, suggesting that it has learned to\nmake use of the machine translation tool. Depend-\ning on the language, this tool is used for 63.8%\nto 94.9% of all examples; the only exception to\nthis is Hindi, for which the machine translation\ntool is used in only 7.3% of cases. However, Tool-Model Es De Hi Vi Zh Ar\nGPT-J 15.", "mimetype": "text/plain", "start_char_idx": 1346, "end_char_idx": 2221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d75fbf83-be82-413d-9e1a-d1dbb41e3583": {"__data__": {"id_": "d75fbf83-be82-413d-9e1a-d1dbb41e3583", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3d30d56-d6d2-49e7-9034-279d0114b5d4", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8bd254d9b93ae1709dd95096b246944e2b58b275007ae7359ebba11d3bf2c471", "class_name": "RelatedNodeInfo"}}, "text": "However, Tool-Model Es De Hi Vi Zh Ar\nGPT-J 15.2 16.5 1.3 8.2 18.2 8.2\nGPT-J + CC 15.7 14.9 0.5 8.3 13.7 4.6\nToolformer (disabled) 19.8 11.9 1.2 10.1 15.0 3.1\nToolformer 20.6 13.5 1.410.6 16.8 3.7\nOPT (66B) 0.3 0.1 1.1 0.2 0.7 0.1\nGPT-3 (175B) 3.4 1.1 0.1 1.7 17.7 0.1\nGPT-J (All En) 24.3 27.0 23.9 23.3 23.1 23.", "mimetype": "text/plain", "start_char_idx": 2174, "end_char_idx": 2486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a9676371-9f4e-4bf8-bf99-e7a7eb202b39": {"__data__": {"id_": "a9676371-9f4e-4bf8-bf99-e7a7eb202b39", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d75fbf83-be82-413d-9e1a-d1dbb41e3583", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a0492352088407f2d3bc3a3762c8fbaa2f14e0641758d733ab78b6a8be2f0374", "class_name": "RelatedNodeInfo"}}, "text": "3 27.0 23.9 23.3 23.1 23.6\nGPT-3 (All En) 24.7 27.2 26.1 24.9 23.6 24.0\nTable 6: Results on MLQA for Spanish (Es), German\n(De), Hindi (Hi), Vietnamese (Vi), Chinese (Zh) and\nArabic (Ar). While using the machine translation tool\nto translate questions is helpful across all languages,\nfurther pretraining on CCNet deteriorates performance;\nconsequently, Toolformer does not consistently outper-\nform GPT-J. The \ufb01nal two rows correspond to models\nthat are given contexts and questions in English.\nformer does not consistently outperform vanilla\nGPT-J.", "mimetype": "text/plain", "start_char_idx": 2461, "end_char_idx": 3010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4ed49efa-9792-42d5-96d0-4ab1ba720141": {"__data__": {"id_": "4ed49efa-9792-42d5-96d0-4ab1ba720141", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}}, "text": "Model WebQS NQ TriviaQA\nGPT-J 18.5 12.8 43.9\nGPT-J + CC 18.4 12.2 45.6\nToolformer (disabled) 18.9 12.6 46.7\nToolformer 26.3 17.7 48.8\nOPT (66B) 18.6 11.4 45.7\nGPT-3 (175B) 29.0 22.6 65.9\nTable 5: Results for various question answering dataset.\nUsing the Wikipedia search tool for most examples,\nToolformer clearly outperforms baselines of the same\nsize, but falls short of GPT-3 (175B).\nthis would make solving the tasks trivial, especially\ngiven that the underlying QA system was \ufb01netuned\non Natural Questions.\nResults are shown in Table 5. Once again,\nToolformer clearly outperforms all other models\nbased on GPT-J, this time mostly relying on the\nWikipedia search API (99.3%) to \ufb01nd relevant in-\nformation. However, Toolformer still lags behind\nthe much larger GPT-3 (175B) model. This is likely\ndue to both the simplicity of our search engine (in\nmany cases, it returns results that are clearly not\na good match for a given query) and the inability\nof Toolformer to interact with it, e.g., by refor-\nmulating its query if results are not helpful or by\nbrowsing through multiple of the top results. We\nbelieve that adding this functionality is an exciting\ndirection for future work.\n4.2.4 Multilingual Question Answering\nWe evaluate Toolformer and all baseline models\non MLQA (Lewis et al., 2019), a multilingual\nquestion-answering benchmark. A context para-\ngraph for each question is provided in English,\nwhile the question can be in Arabic, German, Span-\nish, Hindi, Vietnamese, or Simpli\ufb01ed Chinese.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4ef7376e-db57-45d0-904f-5863eb3ab4c3": {"__data__": {"id_": "4ef7376e-db57-45d0-904f-5863eb3ab4c3", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ed49efa-9792-42d5-96d0-4ab1ba720141", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "294bbacf36248bda2a7db1d211165d07e105b51767303cb1e3abff5116fedb11", "class_name": "RelatedNodeInfo"}}, "text": "In\norder to solve the task, the model needs to be able\nto understand both the paragraph and the question,\nso it may bene\ufb01t from translating the question into\nEnglish. Our evaluation metric is the percentage of\ntimes the model\u2019s generation, capped at 10 words,\ncontains the correct answer.\nResults are shown in Table 6. Using API calls\nconsistently improves Toolformer\u2019s performance\nfor all languages, suggesting that it has learned to\nmake use of the machine translation tool. Depend-\ning on the language, this tool is used for 63.8%\nto 94.9% of all examples; the only exception to\nthis is Hindi, for which the machine translation\ntool is used in only 7.3% of cases. However, Tool-Model Es De Hi Vi Zh Ar\nGPT-J 15.2 16.5 1.3 8.2 18.2 8.2\nGPT-J + CC 15.7 14.9 0.5 8.3 13.7 4.6\nToolformer (disabled) 19.8 11.9 1.2 10.1 15.0 3.1\nToolformer 20.6 13.5 1.410.6 16.8 3.7\nOPT (66B) 0.3 0.1 1.1 0.2 0.7 0.1\nGPT-3 (175B) 3.4 1.1 0.1 1.7 17.7 0.1\nGPT-J (All En) 24.3 27.0 23.9 23.3 23.1 23.6\nGPT-3 (All En) 24.7 27.2 26.1 24.9 23.6 24.0\nTable 6: Results on MLQA for Spanish (Es), German\n(De), Hindi (Hi), Vietnamese (Vi), Chinese (Zh) and\nArabic (Ar).", "mimetype": "text/plain", "start_char_idx": 1507, "end_char_idx": 2647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "de6dfcb6-a900-463d-8678-200703214071": {"__data__": {"id_": "de6dfcb6-a900-463d-8678-200703214071", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ef7376e-db57-45d0-904f-5863eb3ab4c3", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "e31f70a704449a67fa18e40cb23165edf83a276074905a3d15011a1805031a55", "class_name": "RelatedNodeInfo"}}, "text": "While using the machine translation tool\nto translate questions is helpful across all languages,\nfurther pretraining on CCNet deteriorates performance;\nconsequently, Toolformer does not consistently outper-\nform GPT-J. The \ufb01nal two rows correspond to models\nthat are given contexts and questions in English.\nformer does not consistently outperform vanilla\nGPT-J.", "mimetype": "text/plain", "start_char_idx": 2648, "end_char_idx": 3010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "753ecff5-8408-47de-a3a6-13523cec832e": {"__data__": {"id_": "753ecff5-8408-47de-a3a6-13523cec832e", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78", "node_type": "1", "metadata": {}, "hash": "c317b39e87c96c45dab7646f299c3f44f0528d6a4f8f50c36c622b1c164f3123", "class_name": "RelatedNodeInfo"}}, "text": "Model WebQS NQ TriviaQA\nGPT-J 18.5 12.8 43.9\nGPT-J + CC 18.4 12.2 45.6\nToolformer (disabled) 18.9 12.6 46.7\nToolformer 26.3 17.7 48.8\nOPT (66B) 18.6 11.4 45.7\nGPT-3 (175B) 29.0 22.6 65.9\nTable 5: Results for various question answering dataset.\nUsing the Wikipedia search tool for most examples,\nToolformer clearly outperforms baselines of the same\nsize, but falls short of GPT-3 (175B).\nthis would make solving the tasks trivial, especially\ngiven that the underlying QA system was \ufb01netuned\non Natural Questions.\nResults are shown in Table 5. Once again,\nToolformer clearly outperforms all other models\nbased on GPT-J, this time mostly relying on the\nWikipedia search API (99.3%) to \ufb01nd relevant in-\nformation. However, Toolformer still lags behind\nthe much larger GPT-3 (175B) model. This is likely\ndue to both the simplicity of our search engine (in\nmany cases, it returns results that are clearly not\na good match for a given query) and the inability\nof Toolformer to interact with it, e.g., by refor-\nmulating its query if results are not helpful or by\nbrowsing through multiple of the top results. We\nbelieve that adding this functionality is an exciting\ndirection for future work.\n4.2.4 Multilingual Question Answering\nWe evaluate Toolformer and all baseline models\non MLQA (Lewis et al., 2019), a multilingual\nquestion-answering benchmark. A context para-\ngraph for each question is provided in English,\nwhile the question can be in Arabic, German, Span-\nish, Hindi, Vietnamese, or Simpli\ufb01ed Chinese. In\norder to solve the task, the model needs to be able\nto understand both the paragraph and the question,\nso it may bene\ufb01t from translating the question into\nEnglish. Our evaluation metric is the percentage of\ntimes the model\u2019s generation, capped at 10 words,\ncontains the correct answer.\nResults are shown in Table 6. Using API calls\nconsistently improves Toolformer\u2019s performance\nfor all languages, suggesting that it has learned to\nmake use of the machine translation tool. Depend-\ning on the language, this tool is used for 63.8%\nto 94.9% of all examples; the only exception to\nthis is Hindi, for which the machine translation\ntool is used in only 7.3% of cases. However, Tool-Model Es De Hi Vi Zh Ar\nGPT-J 15.2 16.5 1.3 8.2 18.2 8.2\nGPT-J + CC 15.7 14.9 0.5 8.3 13.7 4.6\nToolformer (disabled) 19.8 11.9 1.2 10.1 15.0 3.1\nToolformer 20.6 13.5 1.410.6 16.8 3.7\nOPT (66B) 0.3 0.1 1.1 0.2 0.7 0.1\nGPT-3 (175B) 3.4 1.1 0.1 1.7 17.7 0.1\nGPT-J (All En) 24.3 27.0 23.9 23.3 23.1 23.6\nGPT-3 (All En) 24.7 27.2 26.1 24.9 23.6 24.0\nTable 6: Results on MLQA for Spanish (Es), German\n(De), Hindi (Hi), Vietnamese (Vi), Chinese (Zh) and\nArabic (Ar). While using the machine translation tool\nto translate questions is helpful across all languages,\nfurther pretraining on CCNet deteriorates performance;\nconsequently, Toolformer does not consistently outper-\nform GPT-J. The \ufb01nal two rows correspond to models\nthat are given contexts and questions in English.\nformer does not consistently outperform vanilla\nGPT-J.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "753ecff5-8408-47de-a3a6-13523cec832e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "09e82015-23ed-49a3-a531-324aa52d9771": {"__data__": {"id_": "09e82015-23ed-49a3-a531-324aa52d9771", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}}, "text": "While using the machine translation tool\nto translate questions is helpful across all languages,\nfurther pretraining on CCNet deteriorates performance;\nconsequently, Toolformer does not consistently outper-\nform GPT-J. The \ufb01nal two rows correspond to models\nthat are given contexts and questions in English.\nformer does not consistently outperform vanilla\nGPT-J. This is mainly because for some languages,\n\ufb01netuning on CCNet deteriorates performance; this\nmight be due to a distribution shift compared to\nGPT-J\u2019s original pretraining data.\nOPT and GPT-3 perform surprisingly weak\nacross all languages, mostly because they fail to\nprovide an answer in English despite being in-\nstructed to do so. A potential reason for GPT-J not\nsuffering from this problem is that it was trained on\nmore multilingual data than both OPT and GPT-3,\nincluding the EuroParl corpus (Koehn, 2005; Gao\net al., 2020).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d81f996d-8944-4161-b284-8f7a23244116": {"__data__": {"id_": "d81f996d-8944-4161-b284-8f7a23244116", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09e82015-23ed-49a3-a531-324aa52d9771", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "845f455c25463c163f86e97a5e88036c804a10a564e02eae5f73c9ff022ef853", "class_name": "RelatedNodeInfo"}}, "text": "As an upper bound, we also evaluate\nGPT-J and GPT-3 on a variant of MLQA where\nboth the context and the question are provided in\nEnglish. In this setup, GPT-3 performs better than\nall other models, supporting our hypothesis that\nits subpar performance on MLQA is due to the\nmultilingual aspect of the task.\n4.2.5 Temporal Datasets\nTo investigate the calendar API\u2019s utility, we eval-\nuate all models on TEMPLAMA (Dhingra et al.,\n2022) and a new dataset that we call DATESET .\nTEMPLAMA is a dataset built from Wikidata that\ncontains cloze queries about facts that change with\ntime (e.g., \u201cCristiano Ronaldo plays for ___\u201d)\nas well as the correct answer for the years be-\ntween 2010 and 2020.", "mimetype": "text/plain", "start_char_idx": 894, "end_char_idx": 1583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7bd9332d-eee2-4f40-8c93-49cd8ee514e1": {"__data__": {"id_": "7bd9332d-eee2-4f40-8c93-49cd8ee514e1", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d81f996d-8944-4161-b284-8f7a23244116", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "7a3d3004cd7bdd6872054bca6fc1ccb704a2dc05bd29ea1067e45a15077351cb", "class_name": "RelatedNodeInfo"}}, "text": "DATESET , described in\nAppendix D, is also generated through a series\nof templates, but populated using a combination\nof random dates/durations (e.g., \u201cWhat day of the\nweek was it 30 days ago?\u201d). Critically, knowing the\ncurrent date is required to answer these questions.", "mimetype": "text/plain", "start_char_idx": 1584, "end_char_idx": 1855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "68810082-d630-4409-8467-dde69491161e": {"__data__": {"id_": "68810082-d630-4409-8467-dde69491161e", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}}, "text": "While using the machine translation tool\nto translate questions is helpful across all languages,\nfurther pretraining on CCNet deteriorates performance;\nconsequently, Toolformer does not consistently outper-\nform GPT-J. The \ufb01nal two rows correspond to models\nthat are given contexts and questions in English.\nformer does not consistently outperform vanilla\nGPT-J. This is mainly because for some languages,\n\ufb01netuning on CCNet deteriorates performance; this\nmight be due to a distribution shift compared to\nGPT-J\u2019s original pretraining data.\nOPT and GPT-3 perform surprisingly weak\nacross all languages, mostly because they fail to\nprovide an answer in English despite being in-\nstructed to do so. A potential reason for GPT-J not\nsuffering from this problem is that it was trained on\nmore multilingual data than both OPT and GPT-3,\nincluding the EuroParl corpus (Koehn, 2005; Gao\net al., 2020). As an upper bound, we also evaluate\nGPT-J and GPT-3 on a variant of MLQA where\nboth the context and the question are provided in\nEnglish. In this setup, GPT-3 performs better than\nall other models, supporting our hypothesis that\nits subpar performance on MLQA is due to the\nmultilingual aspect of the task.\n4.2.5 Temporal Datasets\nTo investigate the calendar API\u2019s utility, we eval-\nuate all models on TEMPLAMA (Dhingra et al.,\n2022) and a new dataset that we call DATESET .\nTEMPLAMA is a dataset built from Wikidata that\ncontains cloze queries about facts that change with\ntime (e.g., \u201cCristiano Ronaldo plays for ___\u201d)\nas well as the correct answer for the years be-\ntween 2010 and 2020. DATESET , described in\nAppendix D, is also generated through a series\nof templates, but populated using a combination\nof random dates/durations (e.g., \u201cWhat day of the\nweek was it 30 days ago?\u201d). Critically, knowing the\ncurrent date is required to answer these questions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78": {"__data__": {"id_": "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78", "embedding": null, "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9624f5061fb1e4e3bfa85157a4bb6473be5f87d2639fbc89179a6111ac3d351", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "753ecff5-8408-47de-a3a6-13523cec832e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "200945f05098e176fbb886c7bc1864de005b413b3477296ac51addbcd9dc324a", "class_name": "RelatedNodeInfo"}}, "text": "While using the machine translation tool\nto translate questions is helpful across all languages,\nfurther pretraining on CCNet deteriorates performance;\nconsequently, Toolformer does not consistently outper-\nform GPT-J. The \ufb01nal two rows correspond to models\nthat are given contexts and questions in English.\nformer does not consistently outperform vanilla\nGPT-J. This is mainly because for some languages,\n\ufb01netuning on CCNet deteriorates performance; this\nmight be due to a distribution shift compared to\nGPT-J\u2019s original pretraining data.\nOPT and GPT-3 perform surprisingly weak\nacross all languages, mostly because they fail to\nprovide an answer in English despite being in-\nstructed to do so. A potential reason for GPT-J not\nsuffering from this problem is that it was trained on\nmore multilingual data than both OPT and GPT-3,\nincluding the EuroParl corpus (Koehn, 2005; Gao\net al., 2020). As an upper bound, we also evaluate\nGPT-J and GPT-3 on a variant of MLQA where\nboth the context and the question are provided in\nEnglish. In this setup, GPT-3 performs better than\nall other models, supporting our hypothesis that\nits subpar performance on MLQA is due to the\nmultilingual aspect of the task.\n4.2.5 Temporal Datasets\nTo investigate the calendar API\u2019s utility, we eval-\nuate all models on TEMPLAMA (Dhingra et al.,\n2022) and a new dataset that we call DATESET .\nTEMPLAMA is a dataset built from Wikidata that\ncontains cloze queries about facts that change with\ntime (e.g., \u201cCristiano Ronaldo plays for ___\u201d)\nas well as the correct answer for the years be-\ntween 2010 and 2020. DATESET , described in\nAppendix D, is also generated through a series\nof templates, but populated using a combination\nof random dates/durations (e.g., \u201cWhat day of the\nweek was it 30 days ago?\u201d). Critically, knowing the\ncurrent date is required to answer these questions.", "mimetype": "text/plain", "start_char_idx": 2648, "end_char_idx": 4503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b51e7f82-20ca-410e-a04a-ef3da9d6935a": {"__data__": {"id_": "b51e7f82-20ca-410e-a04a-ef3da9d6935a", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}}, "text": "Model T EMPLAMA D ATESET\nGPT-J 13.7 3.9\nGPT-J + CC 12.9 2.9\nToolformer (disabled) 12.7 5.9\nToolformer 16.3 27.3\nOPT (66B) 14.5 1.3\nGPT-3 (175B) 15.5 0.8\nTable 7: Results for the temporal datasets. Toolformer\noutperforms all baselines, but does not make use of the\ncalendar tool for T EMPLAMA.\nFor both tasks, we use the same evaluation as for\nthe original LAMA dataset.\nResults shown in Table 7 illustrate that Tool-\nformer outperforms all baselines for both TEM-\nPLAMA andDATESET .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d4f8d082-8d58-4aff-b361-76984409bf9a": {"__data__": {"id_": "d4f8d082-8d58-4aff-b361-76984409bf9a", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b51e7f82-20ca-410e-a04a-ef3da9d6935a", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a4a1006bce6bc04d13ffb84ada73d8eace9ab9319d51cf5ecba2d37fd4412cd0", "class_name": "RelatedNodeInfo"}}, "text": "However, closer inspec-\ntion shows that improvements on TEMPLAMA\ncan not be attributed to the calendar tool, which is\nonly used for 0.2% of all examples, but mostly to\nthe Wikipedia search and question answering tools,\nwhich Toolformer calls the most. This makes sense\ngiven that named entities in TEMPLAMA are often\nso speci\ufb01c and rare that even knowing the exact\ndate alone would be of little help. The best course\nof action for this dataset \u2013 \ufb01rst querying the calen-\ndar API to get the current date, and then querying\nthe question answering system with this date \u2013 is\nnot only prohibited by our restriction of using at\nmost one API call per example, but also hard to\nlearn for Toolformer given that all API calls in its\ntraining data are sampled independently.\nForDATESET , on the other hand, the consider-\nable improvement of Toolformer compared to other\nmodels can be fully accredited to the calendar tool,\nwhich it makes use of for 54.8% of all examples.", "mimetype": "text/plain", "start_char_idx": 483, "end_char_idx": 1444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f7f4c9b2-89bf-42e5-92bf-cd2c785e5fe1": {"__data__": {"id_": "f7f4c9b2-89bf-42e5-92bf-cd2c785e5fe1", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4f8d082-8d58-4aff-b361-76984409bf9a", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "308674f7cee89a6b5d5c6b5686a904c486c1b03e0485e4314d537a10dd37ec37", "class_name": "RelatedNodeInfo"}}, "text": "4.3 Language Modeling\nIn addition to verifying improved performance on\nvarious downstream tasks, we also want to ensure\nthat language modeling performance of Toolformer\ndoes not degrade through our \ufb01netuning with API\ncalls. To this end, we evaluate our models on\ntwo language modeling datasets: WikiText (Mer-\nity et al., 2017) and a subset of 10,000 randomly\nselected documents from CCNet (Wenzek et al.,\n2020) that were not used during training. Perplex-\nities of various models are shown in Table 8.", "mimetype": "text/plain", "start_char_idx": 1445, "end_char_idx": 1947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d4f6dcf7-e9af-4fce-8ffa-c86a5e6375f9": {"__data__": {"id_": "d4f6dcf7-e9af-4fce-8ffa-c86a5e6375f9", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7f4c9b2-89bf-42e5-92bf-cd2c785e5fe1", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d9d9f5a4fd5c9e89f005377cb1540e48497d22c24805a199bcb8dad1c569486e", "class_name": "RelatedNodeInfo"}}, "text": "Perplex-\nities of various models are shown in Table 8. As\none would expect, \ufb01netuning on CCNet leads to\nslightly improved performance on a different CC-\nNet subset, but it slightly deteriorates performance\non WikiText, presumably because the original pre-Model WikiText CCNet\nGPT-J 9.9 10.6\nGPT-J + CC 10.3 10.5\nToolformer (disabled) 10.3 10.5\nTable 8: Perplexities of different models on WikiText\nand our validation subset of CCNet. Adding API calls\ncomes without a cost in terms of perplexity for lan-\nguage modeling without any API calls.\ntraining data for GPT-J is more similar to Wiki-\nText than our randomly selected subset of CCNet.", "mimetype": "text/plain", "start_char_idx": 1893, "end_char_idx": 2532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1929dfa3-53fc-4746-900d-4373d0bc5d6c": {"__data__": {"id_": "1929dfa3-53fc-4746-900d-4373d0bc5d6c", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4f6dcf7-e9af-4fce-8ffa-c86a5e6375f9", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "cb631cc83ad9e9e3f40cb9a3819285d752109d80d3da1a95a1e6a6a9410c8791", "class_name": "RelatedNodeInfo"}}, "text": "Most importantly, however, training on C\u2217(our\ndataset annotated with API calls) does not lead to\nan increase in perplexity compared to training on\nCwhen API calls are disabled at inference time.8\n4.4 Scaling Laws\nWe investigate how the ability to ask external tools\nfor help affects performance as we vary the size\nof our LM. To this end, we apply our approach\nnot just to GPT-J, but also to four smaller mod-\nels from the GPT-2 family (Radford et al., 2019),\nwith 124M, 355M, 775M and 1.6B parameters, re-\nspectively. We do so using only a subset of three\ntools: the question answering system, the calcula-\ntor, and the Wikipedia search engine. Apart from\nthis, we follow the experimental setup described in\nSection 4.1.", "mimetype": "text/plain", "start_char_idx": 2533, "end_char_idx": 3254, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6b040423-74e1-461e-a5bc-43b5fbc4ef64": {"__data__": {"id_": "6b040423-74e1-461e-a5bc-43b5fbc4ef64", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1929dfa3-53fc-4746-900d-4373d0bc5d6c", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "baabb69d727b380e7f89ab7a8a08d670c646c1849bcf00579544db3a271e950f", "class_name": "RelatedNodeInfo"}}, "text": "Apart from\nthis, we follow the experimental setup described in\nSection 4.1.\nFigure 4 shows that the ability to leverage the\nprovided tools only emerges at around 775M pa-\nrameters: smaller models achieve similar perfor-\nmance both with and without tools. An exception\nto this is the Wikipedia search engine used mostly\nfor QA benchmarks; we hypothesize that this is\nbecause the API is comparably easy to use. While\nmodels become better at solving tasks without API\ncalls as they grow in size, their ability to make good\nuse of the provided API improves at the same time.", "mimetype": "text/plain", "start_char_idx": 3179, "end_char_idx": 3749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f3f0d4f0-e794-49a6-b9f4-8c7da98c7c47": {"__data__": {"id_": "f3f0d4f0-e794-49a6-b9f4-8c7da98c7c47", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}}, "text": "Model T EMPLAMA D ATESET\nGPT-J 13.7 3.9\nGPT-J + CC 12.9 2.9\nToolformer (disabled) 12.7 5.9\nToolformer 16.3 27.3\nOPT (66B) 14.5 1.3\nGPT-3 (175B) 15.5 0.8\nTable 7: Results for the temporal datasets. Toolformer\noutperforms all baselines, but does not make use of the\ncalendar tool for T EMPLAMA.\nFor both tasks, we use the same evaluation as for\nthe original LAMA dataset.\nResults shown in Table 7 illustrate that Tool-\nformer outperforms all baselines for both TEM-\nPLAMA andDATESET . However, closer inspec-\ntion shows that improvements on TEMPLAMA\ncan not be attributed to the calendar tool, which is\nonly used for 0.2% of all examples, but mostly to\nthe Wikipedia search and question answering tools,\nwhich Toolformer calls the most. This makes sense\ngiven that named entities in TEMPLAMA are often\nso speci\ufb01c and rare that even knowing the exact\ndate alone would be of little help. The best course\nof action for this dataset \u2013 \ufb01rst querying the calen-\ndar API to get the current date, and then querying\nthe question answering system with this date \u2013 is\nnot only prohibited by our restriction of using at\nmost one API call per example, but also hard to\nlearn for Toolformer given that all API calls in its\ntraining data are sampled independently.\nForDATESET , on the other hand, the consider-\nable improvement of Toolformer compared to other\nmodels can be fully accredited to the calendar tool,\nwhich it makes use of for 54.8% of all examples.\n4.3 Language Modeling\nIn addition to verifying improved performance on\nvarious downstream tasks, we also want to ensure\nthat language modeling performance of Toolformer\ndoes not degrade through our \ufb01netuning with API\ncalls.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "239505fb-6f12-4f51-bc31-00e4ea69afc5": {"__data__": {"id_": "239505fb-6f12-4f51-bc31-00e4ea69afc5", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3f0d4f0-e794-49a6-b9f4-8c7da98c7c47", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "e7104897fe6110e42c3989d45bf19cb256e28c85f6f9ae1705438797880f5b87", "class_name": "RelatedNodeInfo"}}, "text": "To this end, we evaluate our models on\ntwo language modeling datasets: WikiText (Mer-\nity et al., 2017) and a subset of 10,000 randomly\nselected documents from CCNet (Wenzek et al.,\n2020) that were not used during training. Perplex-\nities of various models are shown in Table 8. As\none would expect, \ufb01netuning on CCNet leads to\nslightly improved performance on a different CC-\nNet subset, but it slightly deteriorates performance\non WikiText, presumably because the original pre-Model WikiText CCNet\nGPT-J 9.9 10.6\nGPT-J + CC 10.3 10.5\nToolformer (disabled) 10.3 10.5\nTable 8: Perplexities of different models on WikiText\nand our validation subset of CCNet. Adding API calls\ncomes without a cost in terms of perplexity for lan-\nguage modeling without any API calls.\ntraining data for GPT-J is more similar to Wiki-\nText than our randomly selected subset of CCNet.\nMost importantly, however, training on C\u2217(our\ndataset annotated with API calls) does not lead to\nan increase in perplexity compared to training on\nCwhen API calls are disabled at inference time.8\n4.4 Scaling Laws\nWe investigate how the ability to ask external tools\nfor help affects performance as we vary the size\nof our LM. To this end, we apply our approach\nnot just to GPT-J, but also to four smaller mod-\nels from the GPT-2 family (Radford et al., 2019),\nwith 124M, 355M, 775M and 1.6B parameters, re-\nspectively. We do so using only a subset of three\ntools: the question answering system, the calcula-\ntor, and the Wikipedia search engine. Apart from\nthis, we follow the experimental setup described in\nSection 4.1.\nFigure 4 shows that the ability to leverage the\nprovided tools only emerges at around 775M pa-\nrameters: smaller models achieve similar perfor-\nmance both with and without tools.", "mimetype": "text/plain", "start_char_idx": 1669, "end_char_idx": 3433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "df608f7b-d033-4d87-8a5c-ac945a2a3e4c": {"__data__": {"id_": "df608f7b-d033-4d87-8a5c-ac945a2a3e4c", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "239505fb-6f12-4f51-bc31-00e4ea69afc5", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "24cdc5fa78709aa0516112f11d697836a30ae1c324724a7b2d65b282643d1407", "class_name": "RelatedNodeInfo"}}, "text": "An exception\nto this is the Wikipedia search engine used mostly\nfor QA benchmarks; we hypothesize that this is\nbecause the API is comparably easy to use. While\nmodels become better at solving tasks without API\ncalls as they grow in size, their ability to make good\nuse of the provided API improves at the same time.", "mimetype": "text/plain", "start_char_idx": 3434, "end_char_idx": 3749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fc8231d2-31ec-4cfb-82b6-288b9c53fa42": {"__data__": {"id_": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb7b863d-0d80-467a-a908-12ae47d4c997", "node_type": "1", "metadata": {}, "hash": "75d027a96293e77ab3d4836dabbe9ee4d06d54fa0057b03c563169962c89675c", "class_name": "RelatedNodeInfo"}}, "text": "Model T EMPLAMA D ATESET\nGPT-J 13.7 3.9\nGPT-J + CC 12.9 2.9\nToolformer (disabled) 12.7 5.9\nToolformer 16.3 27.3\nOPT (66B) 14.5 1.3\nGPT-3 (175B) 15.5 0.8\nTable 7: Results for the temporal datasets. Toolformer\noutperforms all baselines, but does not make use of the\ncalendar tool for T EMPLAMA.\nFor both tasks, we use the same evaluation as for\nthe original LAMA dataset.\nResults shown in Table 7 illustrate that Tool-\nformer outperforms all baselines for both TEM-\nPLAMA andDATESET . However, closer inspec-\ntion shows that improvements on TEMPLAMA\ncan not be attributed to the calendar tool, which is\nonly used for 0.2% of all examples, but mostly to\nthe Wikipedia search and question answering tools,\nwhich Toolformer calls the most. This makes sense\ngiven that named entities in TEMPLAMA are often\nso speci\ufb01c and rare that even knowing the exact\ndate alone would be of little help. The best course\nof action for this dataset \u2013 \ufb01rst querying the calen-\ndar API to get the current date, and then querying\nthe question answering system with this date \u2013 is\nnot only prohibited by our restriction of using at\nmost one API call per example, but also hard to\nlearn for Toolformer given that all API calls in its\ntraining data are sampled independently.\nForDATESET , on the other hand, the consider-\nable improvement of Toolformer compared to other\nmodels can be fully accredited to the calendar tool,\nwhich it makes use of for 54.8% of all examples.\n4.3 Language Modeling\nIn addition to verifying improved performance on\nvarious downstream tasks, we also want to ensure\nthat language modeling performance of Toolformer\ndoes not degrade through our \ufb01netuning with API\ncalls. To this end, we evaluate our models on\ntwo language modeling datasets: WikiText (Mer-\nity et al., 2017) and a subset of 10,000 randomly\nselected documents from CCNet (Wenzek et al.,\n2020) that were not used during training. Perplex-\nities of various models are shown in Table 8. As\none would expect, \ufb01netuning on CCNet leads to\nslightly improved performance on a different CC-\nNet subset, but it slightly deteriorates performance\non WikiText, presumably because the original pre-Model WikiText CCNet\nGPT-J 9.9 10.6\nGPT-J + CC 10.3 10.5\nToolformer (disabled) 10.3 10.5\nTable 8: Perplexities of different models on WikiText\nand our validation subset of CCNet. Adding API calls\ncomes without a cost in terms of perplexity for lan-\nguage modeling without any API calls.\ntraining data for GPT-J is more similar to Wiki-\nText than our randomly selected subset of CCNet.\nMost importantly, however, training on C\u2217(our\ndataset annotated with API calls) does not lead to\nan increase in perplexity compared to training on\nCwhen API calls are disabled at inference time.8\n4.4 Scaling Laws\nWe investigate how the ability to ask external tools\nfor help affects performance as we vary the size\nof our LM. To this end, we apply our approach\nnot just to GPT-J, but also to four smaller mod-\nels from the GPT-2 family (Radford et al., 2019),\nwith 124M, 355M, 775M and 1.6B parameters, re-\nspectively. We do so using only a subset of three\ntools: the question answering system, the calcula-\ntor, and the Wikipedia search engine. Apart from\nthis, we follow the experimental setup described in\nSection 4.1.\nFigure 4 shows that the ability to leverage the\nprovided tools only emerges at around 775M pa-\nrameters: smaller models achieve similar perfor-\nmance both with and without tools. An exception\nto this is the Wikipedia search engine used mostly\nfor QA benchmarks; we hypothesize that this is\nbecause the API is comparably easy to use. While\nmodels become better at solving tasks without API\ncalls as they grow in size, their ability to make good\nuse of the provided API improves at the same time.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fb577c4f-c53a-42d4-b33d-2f05f9ce9043": {"__data__": {"id_": "fb577c4f-c53a-42d4-b33d-2f05f9ce9043", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}}, "text": "We do so using only a subset of three\ntools: the question answering system, the calcula-\ntor, and the Wikipedia search engine. Apart from\nthis, we follow the experimental setup described in\nSection 4.1.\nFigure 4 shows that the ability to leverage the\nprovided tools only emerges at around 775M pa-\nrameters: smaller models achieve similar perfor-\nmance both with and without tools. An exception\nto this is the Wikipedia search engine used mostly\nfor QA benchmarks; we hypothesize that this is\nbecause the API is comparably easy to use. While\nmodels become better at solving tasks without API\ncalls as they grow in size, their ability to make good\nuse of the provided API improves at the same time.\nAs a consequence, there remains a large gap be-\ntween predictions with and without API calls even\nfor our biggest model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eb7b863d-0d80-467a-a908-12ae47d4c997", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1965f967-f828-4665-99fc-22f6d7eed72e": {"__data__": {"id_": "1965f967-f828-4665-99fc-22f6d7eed72e", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb577c4f-c53a-42d4-b33d-2f05f9ce9043", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "82ed05993bf4f7ba32624c5de3e04c9d3c5db81ac6f70bddc10b099a614264dc", "class_name": "RelatedNodeInfo"}}, "text": "5 Analysis\nDecoding Strategy We investigate the effect of\nour modi\ufb01ed decoding strategy introduced in Sec-\ntion 4.2, where instead of always generating the\n8We do not evaluate the perplexity of Toolformer with\nAPI calls enabled as computing the probability pM(xt|\nx1,...,x t\u22121)of tokenxtgivenx1,...,x t\u22121would require\nmarginalizing over all potential API calls that the model could\nmake at position t, which is intractable.", "mimetype": "text/plain", "start_char_idx": 819, "end_char_idx": 1242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eb7b863d-0d80-467a-a908-12ae47d4c997", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "158f4f7e-efd5-4d7c-9156-a326e9b5cf3f": {"__data__": {"id_": "158f4f7e-efd5-4d7c-9156-a326e9b5cf3f", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}}, "text": "We do so using only a subset of three\ntools: the question answering system, the calcula-\ntor, and the Wikipedia search engine. Apart from\nthis, we follow the experimental setup described in\nSection 4.1.\nFigure 4 shows that the ability to leverage the\nprovided tools only emerges at around 775M pa-\nrameters: smaller models achieve similar perfor-\nmance both with and without tools. An exception\nto this is the Wikipedia search engine used mostly\nfor QA benchmarks; we hypothesize that this is\nbecause the API is comparably easy to use. While\nmodels become better at solving tasks without API\ncalls as they grow in size, their ability to make good\nuse of the provided API improves at the same time.\nAs a consequence, there remains a large gap be-\ntween predictions with and without API calls even\nfor our biggest model.\n5 Analysis\nDecoding Strategy We investigate the effect of\nour modi\ufb01ed decoding strategy introduced in Sec-\ntion 4.2, where instead of always generating the\n8We do not evaluate the perplexity of Toolformer with\nAPI calls enabled as computing the probability pM(xt|\nx1,...,x t\u22121)of tokenxtgivenx1,...,x t\u22121would require\nmarginalizing over all potential API calls that the model could\nmake at position t, which is intractable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eb7b863d-0d80-467a-a908-12ae47d4c997", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eb7b863d-0d80-467a-a908-12ae47d4c997": {"__data__": {"id_": "eb7b863d-0d80-467a-a908-12ae47d4c997", "embedding": null, "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacad45b-e004-4352-b686-0d5101ffc1c8", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3e7388657ed705b48ad181b13d289790214859830cd0f2a3a524d511608d585d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "713c6e770206493e0c36f4b22aafcfef37e31e83fa9a29aa78616d4b61ef40cf", "class_name": "RelatedNodeInfo"}}, "text": "We do so using only a subset of three\ntools: the question answering system, the calcula-\ntor, and the Wikipedia search engine. Apart from\nthis, we follow the experimental setup described in\nSection 4.1.\nFigure 4 shows that the ability to leverage the\nprovided tools only emerges at around 775M pa-\nrameters: smaller models achieve similar perfor-\nmance both with and without tools. An exception\nto this is the Wikipedia search engine used mostly\nfor QA benchmarks; we hypothesize that this is\nbecause the API is comparably easy to use. While\nmodels become better at solving tasks without API\ncalls as they grow in size, their ability to make good\nuse of the provided API improves at the same time.\nAs a consequence, there remains a large gap be-\ntween predictions with and without API calls even\nfor our biggest model.\n5 Analysis\nDecoding Strategy We investigate the effect of\nour modi\ufb01ed decoding strategy introduced in Sec-\ntion 4.2, where instead of always generating the\n8We do not evaluate the perplexity of Toolformer with\nAPI calls enabled as computing the probability pM(xt|\nx1,...,x t\u22121)of tokenxtgivenx1,...,x t\u22121would require\nmarginalizing over all potential API calls that the model could\nmake at position t, which is intractable.", "mimetype": "text/plain", "start_char_idx": 3052, "end_char_idx": 4294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eb7b863d-0d80-467a-a908-12ae47d4c997", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dac5b250-1ff5-4661-b4d1-78cc80446339": {"__data__": {"id_": "dac5b250-1ff5-4661-b4d1-78cc80446339", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}}, "text": "051015202530\n0200040006000Model Parameters (M)LAMA\n Toolformer Toolformer (disabled) GPT30510152025303540\n0200040006000Model Parameters (M)QA Benchmarks\n051015202530\n0200040006000Model Parameters (M)Math BenchmarksFigure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of\ndifferent sizes and GPT-J \ufb01netuned with our approach, both with and without API calls. While API calls are not\nhelpful to the smallest models, larger models learn how to make good use of them. Even for bigger models, the\ngap between model predictions with and without API calls remains high.\nmost likely token, we generate the <API> token\nif it is one of the kmost likely tokens.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "018da332-d74b-4268-a274-6aa8a1853e0d": {"__data__": {"id_": "018da332-d74b-4268-a274-6aa8a1853e0d", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dac5b250-1ff5-4661-b4d1-78cc80446339", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "e1f4a920e4230e35b5cbc023f0615b7c0cd425014858ccfc4e923b6c7bc2c74d", "class_name": "RelatedNodeInfo"}}, "text": "Table 9\nshows performance on the T-REx subset of LAMA\nand on WebQS for different values of k. As ex-\npected, increasing kleads to the model doing API\ncalls for more examples \u2013 from 40.3% and 8.5%\nwithk= 1(i.e., regular greedy decoding) to 98.1%\nand 100% for k= 10 . While for T-REx, there is\nalready a clear improvement in performance with\ngreedy decoding, on WebQS our model only starts\nto make a substantial number of API calls as we\nslightly increase k. Interestingly, for k= 1 the\nmodel is calibrated to some extent: It decides to\ncall APIs for examples that it would perform partic-\nularly badly on without making API calls.", "mimetype": "text/plain", "start_char_idx": 695, "end_char_idx": 1324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cd564975-4346-4c89-aa8b-00d10dc45fce": {"__data__": {"id_": "cd564975-4346-4c89-aa8b-00d10dc45fce", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "018da332-d74b-4268-a274-6aa8a1853e0d", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c345bf2e71e168378ef4790b85a9f16cabf4b84377987e7f3c79216c10de31be", "class_name": "RelatedNodeInfo"}}, "text": "This can\nbe seen from the fact that performance on examples\nwhere it decides notto make an API call (44.3 and\n19.9) is higher than average performance if no API\ncalls are made at all (34.9 and 18.9). However, this\ncalibration is lost for higher values of k.\nData Quality We qualitatively analyze some\nAPI calls generated with our approach for different\nAPIs. Table 10 shows some examples of texts from\nCCNet augmented with API calls, as well as the\ncorresponding score L\u2212\ni\u2212L+\nithat is used as a \ufb01l-\ntering criterion, and whether the API calls made by\nthe model are intuitively useful in the given context.\nAs can be seen, high values of L\u2212\ni\u2212L+\nitypically\ncorrespond to useful API calls, whereas low values\ncorrespond to API calls that do not provide any in-\nformation that is useful for predicting future tokens.", "mimetype": "text/plain", "start_char_idx": 1325, "end_char_idx": 2139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c2cc36b0-acc8-4468-ab38-576f2984ef74": {"__data__": {"id_": "c2cc36b0-acc8-4468-ab38-576f2984ef74", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd564975-4346-4c89-aa8b-00d10dc45fce", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "11b29d5d25400b7718f11db2888ab53b379af5503179bb2ccb2a38f75ad04ae5", "class_name": "RelatedNodeInfo"}}, "text": "There are some exceptions, e.g., an API call forT-REx WebQS\nk All AC NC % All AC NC %\n0 34.9 \u2013 34.9 0.0 18.9 \u2013 18.9 0.0\n1 47.8 53.0 44.3 40.3 19.3 17.1 19.9 8.5\n3 52.9 58.0 29.0 82.8 26.3 26.5 6.6 99.3\n10 53.5 54.0 22.5 98.1 26.3 26.4 \u2013 100.0\nTable 9: Toolformer results on the T-REx subset of\nLAMA and on WebQS for different values of kused\nduring decoding.", "mimetype": "text/plain", "start_char_idx": 2140, "end_char_idx": 2498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e6cb6fd8-1f23-4ae9-8fbc-5f3c16ff0017": {"__data__": {"id_": "e6cb6fd8-1f23-4ae9-8fbc-5f3c16ff0017", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2cc36b0-acc8-4468-ab38-576f2984ef74", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "6c989c4ca18e1356184416d231a015b0fa66045ef26230409c47d98e086818d4", "class_name": "RelatedNodeInfo"}}, "text": "Numbers shown are overall perfor-\nmance (All), performance on the subset where the\nmodel decides to make an API call (AC) and all re-\nmaining examples (NC), as well as the percentage of\nexamples for which the model decides to call an API\n(%).\n\u201cFast train success\u201d in the fourth example that does\nnot give any relevant information but still reduces\nperplexity. However, some amount of noise in the\nAPI calls that are not \ufb01ltered can actually be useful\nas it forces the model \ufb01netuned on C\u2217to not always\nblindly follow the results of each call it makes.", "mimetype": "text/plain", "start_char_idx": 2499, "end_char_idx": 3050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9e8b6760-8e18-4c71-a410-160ba3aabe59": {"__data__": {"id_": "9e8b6760-8e18-4c71-a410-160ba3aabe59", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}}, "text": "051015202530\n0200040006000Model Parameters (M)LAMA\n Toolformer Toolformer (disabled) GPT30510152025303540\n0200040006000Model Parameters (M)QA Benchmarks\n051015202530\n0200040006000Model Parameters (M)Math BenchmarksFigure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of\ndifferent sizes and GPT-J \ufb01netuned with our approach, both with and without API calls. While API calls are not\nhelpful to the smallest models, larger models learn how to make good use of them. Even for bigger models, the\ngap between model predictions with and without API calls remains high.\nmost likely token, we generate the <API> token\nif it is one of the kmost likely tokens. Table 9\nshows performance on the T-REx subset of LAMA\nand on WebQS for different values of k. As ex-\npected, increasing kleads to the model doing API\ncalls for more examples \u2013 from 40.3% and 8.5%\nwithk= 1(i.e., regular greedy decoding) to 98.1%\nand 100% for k= 10 . While for T-REx, there is\nalready a clear improvement in performance with\ngreedy decoding, on WebQS our model only starts\nto make a substantial number of API calls as we\nslightly increase k. Interestingly, for k= 1 the\nmodel is calibrated to some extent: It decides to\ncall APIs for examples that it would perform partic-\nularly badly on without making API calls. This can\nbe seen from the fact that performance on examples\nwhere it decides notto make an API call (44.3 and\n19.9) is higher than average performance if no API\ncalls are made at all (34.9 and 18.9). However, this\ncalibration is lost for higher values of k.\nData Quality We qualitatively analyze some\nAPI calls generated with our approach for different\nAPIs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ece5763f-7539-4d9c-ba7d-643efb434193": {"__data__": {"id_": "ece5763f-7539-4d9c-ba7d-643efb434193", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e8b6760-8e18-4c71-a410-160ba3aabe59", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "ccbdab0fe14812f05f85c8007560676bb00dc394547f1c57b5014d19799b88e7", "class_name": "RelatedNodeInfo"}}, "text": "Table 10 shows some examples of texts from\nCCNet augmented with API calls, as well as the\ncorresponding score L\u2212\ni\u2212L+\nithat is used as a \ufb01l-\ntering criterion, and whether the API calls made by\nthe model are intuitively useful in the given context.\nAs can be seen, high values of L\u2212\ni\u2212L+\nitypically\ncorrespond to useful API calls, whereas low values\ncorrespond to API calls that do not provide any in-\nformation that is useful for predicting future tokens.\nThere are some exceptions, e.g., an API call forT-REx WebQS\nk All AC NC % All AC NC %\n0 34.9 \u2013 34.9 0.0 18.9 \u2013 18.9 0.0\n1 47.8 53.0 44.3 40.3 19.3 17.1 19.9 8.5\n3 52.9 58.0 29.0 82.8 26.3 26.5 6.6 99.3\n10 53.5 54.0 22.5 98.1 26.3 26.4 \u2013 100.0\nTable 9: Toolformer results on the T-REx subset of\nLAMA and on WebQS for different values of kused\nduring decoding. Numbers shown are overall perfor-\nmance (All), performance on the subset where the\nmodel decides to make an API call (AC) and all re-\nmaining examples (NC), as well as the percentage of\nexamples for which the model decides to call an API\n(%).\n\u201cFast train success\u201d in the fourth example that does\nnot give any relevant information but still reduces\nperplexity. However, some amount of noise in the\nAPI calls that are not \ufb01ltered can actually be useful\nas it forces the model \ufb01netuned on C\u2217to not always\nblindly follow the results of each call it makes.", "mimetype": "text/plain", "start_char_idx": 1684, "end_char_idx": 3050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2a308913-b3f8-40cd-916a-51a7c75994ce": {"__data__": {"id_": "2a308913-b3f8-40cd-916a-51a7c75994ce", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bced9eae-fe6d-48cd-abe4-47907e9cebd1", "node_type": "1", "metadata": {}, "hash": "d2d8f951bae0fee9146a2153552c9551919fff303e6c19195db7686c0ab3f3c7", "class_name": "RelatedNodeInfo"}}, "text": "051015202530\n0200040006000Model Parameters (M)LAMA\n Toolformer Toolformer (disabled) GPT30510152025303540\n0200040006000Model Parameters (M)QA Benchmarks\n051015202530\n0200040006000Model Parameters (M)Math BenchmarksFigure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of\ndifferent sizes and GPT-J \ufb01netuned with our approach, both with and without API calls. While API calls are not\nhelpful to the smallest models, larger models learn how to make good use of them. Even for bigger models, the\ngap between model predictions with and without API calls remains high.\nmost likely token, we generate the <API> token\nif it is one of the kmost likely tokens. Table 9\nshows performance on the T-REx subset of LAMA\nand on WebQS for different values of k. As ex-\npected, increasing kleads to the model doing API\ncalls for more examples \u2013 from 40.3% and 8.5%\nwithk= 1(i.e., regular greedy decoding) to 98.1%\nand 100% for k= 10 . While for T-REx, there is\nalready a clear improvement in performance with\ngreedy decoding, on WebQS our model only starts\nto make a substantial number of API calls as we\nslightly increase k. Interestingly, for k= 1 the\nmodel is calibrated to some extent: It decides to\ncall APIs for examples that it would perform partic-\nularly badly on without making API calls. This can\nbe seen from the fact that performance on examples\nwhere it decides notto make an API call (44.3 and\n19.9) is higher than average performance if no API\ncalls are made at all (34.9 and 18.9). However, this\ncalibration is lost for higher values of k.\nData Quality We qualitatively analyze some\nAPI calls generated with our approach for different\nAPIs. Table 10 shows some examples of texts from\nCCNet augmented with API calls, as well as the\ncorresponding score L\u2212\ni\u2212L+\nithat is used as a \ufb01l-\ntering criterion, and whether the API calls made by\nthe model are intuitively useful in the given context.\nAs can be seen, high values of L\u2212\ni\u2212L+\nitypically\ncorrespond to useful API calls, whereas low values\ncorrespond to API calls that do not provide any in-\nformation that is useful for predicting future tokens.\nThere are some exceptions, e.g., an API call forT-REx WebQS\nk All AC NC % All AC NC %\n0 34.9 \u2013 34.9 0.0 18.9 \u2013 18.9 0.0\n1 47.8 53.0 44.3 40.3 19.3 17.1 19.9 8.5\n3 52.9 58.0 29.0 82.8 26.3 26.5 6.6 99.3\n10 53.5 54.0 22.5 98.1 26.3 26.4 \u2013 100.0\nTable 9: Toolformer results on the T-REx subset of\nLAMA and on WebQS for different values of kused\nduring decoding. Numbers shown are overall perfor-\nmance (All), performance on the subset where the\nmodel decides to make an API call (AC) and all re-\nmaining examples (NC), as well as the percentage of\nexamples for which the model decides to call an API\n(%).\n\u201cFast train success\u201d in the fourth example that does\nnot give any relevant information but still reduces\nperplexity. However, some amount of noise in the\nAPI calls that are not \ufb01ltered can actually be useful\nas it forces the model \ufb01netuned on C\u2217to not always\nblindly follow the results of each call it makes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "379c988d-c250-4ee8-8034-85817c6f7d96": {"__data__": {"id_": "379c988d-c250-4ee8-8034-85817c6f7d96", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}}, "text": "Numbers shown are overall perfor-\nmance (All), performance on the subset where the\nmodel decides to make an API call (AC) and all re-\nmaining examples (NC), as well as the percentage of\nexamples for which the model decides to call an API\n(%).\n\u201cFast train success\u201d in the fourth example that does\nnot give any relevant information but still reduces\nperplexity. However, some amount of noise in the\nAPI calls that are not \ufb01ltered can actually be useful\nas it forces the model \ufb01netuned on C\u2217to not always\nblindly follow the results of each call it makes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bced9eae-fe6d-48cd-abe4-47907e9cebd1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d90a490c-e93d-4e21-b513-54cb95e8dca1": {"__data__": {"id_": "d90a490c-e93d-4e21-b513-54cb95e8dca1", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "379c988d-c250-4ee8-8034-85817c6f7d96", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "688b44677f2b65a6fdb587582d007b69420b9ed116e26ccfd8857d49d78e00ac", "class_name": "RelatedNodeInfo"}}, "text": "6 Related Work\nLanguage Model Pretraining There are various\napproaches that augment language models with\nsome form of additional textual information during\npretraining, including various forms of metadata\n(Keskar et al., 2019), HTML tags (Aghajanyan\net al., 2021), Wikipedia markup (Schick et al.,\n2022), or related texts obtained from an informa-\ntion retrieval system (Guu et al., 2020; Borgeaud\net al., 2021; Izacard et al., 2022). For all of these", "mimetype": "text/plain", "start_char_idx": 552, "end_char_idx": 1003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bced9eae-fe6d-48cd-abe4-47907e9cebd1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d33d77b1-3bfd-4306-b9fe-6dad4bcb3041": {"__data__": {"id_": "d33d77b1-3bfd-4306-b9fe-6dad4bcb3041", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}}, "text": "Numbers shown are overall perfor-\nmance (All), performance on the subset where the\nmodel decides to make an API call (AC) and all re-\nmaining examples (NC), as well as the percentage of\nexamples for which the model decides to call an API\n(%).\n\u201cFast train success\u201d in the fourth example that does\nnot give any relevant information but still reduces\nperplexity. However, some amount of noise in the\nAPI calls that are not \ufb01ltered can actually be useful\nas it forces the model \ufb01netuned on C\u2217to not always\nblindly follow the results of each call it makes.\n6 Related Work\nLanguage Model Pretraining There are various\napproaches that augment language models with\nsome form of additional textual information during\npretraining, including various forms of metadata\n(Keskar et al., 2019), HTML tags (Aghajanyan\net al., 2021), Wikipedia markup (Schick et al.,\n2022), or related texts obtained from an informa-\ntion retrieval system (Guu et al., 2020; Borgeaud\net al., 2021; Izacard et al., 2022). For all of these", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bced9eae-fe6d-48cd-abe4-47907e9cebd1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bced9eae-fe6d-48cd-abe4-47907e9cebd1": {"__data__": {"id_": "bced9eae-fe6d-48cd-abe4-47907e9cebd1", "embedding": null, "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f33d1139-924b-4245-923c-ad7f213b19e1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3370fc268fc2fedc45e99c791af55ab7a05c6d63175d95cf16367cec18cab4b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a308913-b3f8-40cd-916a-51a7c75994ce", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "0a92dce0ddb3cc993b64298ad37006f9c9d1c872799a8d07001d02ad5d31abe3", "class_name": "RelatedNodeInfo"}}, "text": "Numbers shown are overall perfor-\nmance (All), performance on the subset where the\nmodel decides to make an API call (AC) and all re-\nmaining examples (NC), as well as the percentage of\nexamples for which the model decides to call an API\n(%).\n\u201cFast train success\u201d in the fourth example that does\nnot give any relevant information but still reduces\nperplexity. However, some amount of noise in the\nAPI calls that are not \ufb01ltered can actually be useful\nas it forces the model \ufb01netuned on C\u2217to not always\nblindly follow the results of each call it makes.\n6 Related Work\nLanguage Model Pretraining There are various\napproaches that augment language models with\nsome form of additional textual information during\npretraining, including various forms of metadata\n(Keskar et al., 2019), HTML tags (Aghajanyan\net al., 2021), Wikipedia markup (Schick et al.,\n2022), or related texts obtained from an informa-\ntion retrieval system (Guu et al., 2020; Borgeaud\net al., 2021; Izacard et al., 2022). For all of these", "mimetype": "text/plain", "start_char_idx": 2499, "end_char_idx": 3502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bced9eae-fe6d-48cd-abe4-47907e9cebd1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0b0bd11e-5271-49b2-bb51-5894086de834": {"__data__": {"id_": "0b0bd11e-5271-49b2-bb51-5894086de834", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}}, "text": "Example L\u2212\ni\u2212L+\niUseful\nThe Flodden Window (a war memorial dedicated to The Middleton Archers), in the Grade I-listed\nChurch of St Leonard in Middleton is said to be the oldest war memorial in the United King-\ndom. <API> WikiSearch(War memorial Flodden) \u2192Battle of Flodden > Commemoration >\nThe stained-glass Flodden Window in Middleton Parish Church [. . . ] was constructed by Sir\nRichard Assheton in memory of the Battle of Flodden and the archers from Middleton who\nfought in it. </API> Sir Richard Assheton of Middleton (who built St Leonard) was granted\nknighthood [. . . ]5.49 \u0013\nNote: The WL will be open on Friday, <API> Calendar()\u2192Today is Thursday, March 9, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 673, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "94883fb6-7b33-4212-8e1c-6e87aecbe44d": {"__data__": {"id_": "94883fb6-7b33-4212-8e1c-6e87aecbe44d", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b0bd11e-5271-49b2-bb51-5894086de834", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "70fa1b96641a6713e2562868e4da3dd7042bf5fc10583ff898ae0acf71b72f1a", "class_name": "RelatedNodeInfo"}}, "text": "</API> March 10, and Sunday, March 19 for regular hours.2.11 \u0013\nThe Nile has an approximate length of <API> QA(What is the approximate length of the Nile?)\n\u21926,853 km </API> 6,853 kilometers, the White Nile being its main source.2.08 \u0013\nIf Venus had an atmosphere similar to Earth\u2019s then you would expect Venus\u2019 mean temperature to be\n499 K (1.74 x 287) rather than 735 K which is <API> Calculator(735 / 499) \u21921.47</API> 1.47\n(735 / 499) times hotter than it should be.1.59 \u0013\nYou are here: Home / Featured / Catch this fast train to success! <API> WikiSearch(Fast train\nsuccess)\u2192Fast Train > It also peaked at #23 on the Canadian CHUM singles chart, on June\n26, 1971.", "mimetype": "text/plain", "start_char_idx": 674, "end_char_idx": 1338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7c173b0c-9ccc-47e1-bbf1-63cf44c89988": {"__data__": {"id_": "7c173b0c-9ccc-47e1-bbf1-63cf44c89988", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94883fb6-7b33-4212-8e1c-6e87aecbe44d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "2b7e0e7f6ffdfe090c1f32de75e1642e10102be094df6132affa0b0fdc9c7b4a", "class_name": "RelatedNodeInfo"}}, "text": "The success of this single established Myles Goodwyn as the band\u2019s main songwriter,\nand made it possible for April Wine to record a second album. </API> Don\u2019t wait weeks and\n[...]0.92 \u0017\nOs Melhores Escolas em Jersey 2020 <API> MT(Os Melhores Escolas em Jersey) \u2192The Best\nSchools in Jersey </API> On this page you can search for Universities, Colleges and Business\nschools in Jersey0.70 \u0013\nEnjoy these pictures from the <API> Calendar()\u2192Today is Friday, April 19, 2013. </API>\nEaster Egg Hunt.0.33 \u0013\n85 patients (23%) were hospitalised alive and admitted to a hospital ward.", "mimetype": "text/plain", "start_char_idx": 1339, "end_char_idx": 1911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9ad51a02-8bff-4af2-bf92-a9ad6c14736e": {"__data__": {"id_": "9ad51a02-8bff-4af2-bf92-a9ad6c14736e", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c173b0c-9ccc-47e1-bbf1-63cf44c89988", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "7cc5cec44f603267753bec7be3085ff1e22a5d5b4f38274f46469c98d97dc6e7", "class_name": "RelatedNodeInfo"}}, "text": "Of them, <API> Calcula-\ntor(85 / 23)\u21923.70</API> 65% had a cardiac aetiology [...]\u22120.02 \u0017\nBut hey, after the <API> Calendar()\u2192Today is Saturday, June 25, 2011. </API> Disneyland\n\ufb01asco with the \ufb01re drill, I think it\u2019s safe to say Chewey won\u2019t let anyone die in a \ufb01re.\u22120.41 \u0017\nThe last time I was with <API> QA(Who was last time I was with?) \u2192The Last Time </API>\nhim I asked what he likes about me and he said he would tell me one day.\u22121.23 \u0017\nTable 10: Examples of API calls for different tools, sorted by the value of L\u2212\ni\u2212L+\nithat is used as a \ufb01ltering\ncriterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens.", "mimetype": "text/plain", "start_char_idx": 1912, "end_char_idx": 2578, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2162c6c6-34c9-460c-9ac0-95708e608cf0": {"__data__": {"id_": "2162c6c6-34c9-460c-9ac0-95708e608cf0", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ad51a02-8bff-4af2-bf92-a9ad6c14736e", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8b56c2a49c9f13b942573e48ff63205f33606c93986dd120fda8a9a4e03fd054", "class_name": "RelatedNodeInfo"}}, "text": "High values typically correspond to API calls that are intuitively useful for predicting future tokens.\napproaches, additional information is always pro-\nvided, regardless of whether it is helpful or not. In\ncontrast, Toolformer learns for itself to explicitly\nasks for the right information.\nTool Use Several approaches aim to equip LMs\nwith the ability to use external tools such as search\nengines (Komeili et al., 2022; Thoppilan et al.,\n2022; Lazaridou et al., 2022; Shuster et al., 2022;\nYao et al., 2022), web browsers (Nakano et al.,\n2021), calculators (Cobbe et al., 2021; Thoppilan\net al., 2022), translation systems (Thoppilan et al.,\n2022) and Python interpreters (Gao et al., 2022).", "mimetype": "text/plain", "start_char_idx": 2475, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "be169577-b256-46a5-b678-d92a95f0bbc8": {"__data__": {"id_": "be169577-b256-46a5-b678-d92a95f0bbc8", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}}, "text": "Example L\u2212\ni\u2212L+\niUseful\nThe Flodden Window (a war memorial dedicated to The Middleton Archers), in the Grade I-listed\nChurch of St Leonard in Middleton is said to be the oldest war memorial in the United King-\ndom. <API> WikiSearch(War memorial Flodden) \u2192Battle of Flodden > Commemoration >\nThe stained-glass Flodden Window in Middleton Parish Church [. . . ] was constructed by Sir\nRichard Assheton in memory of the Battle of Flodden and the archers from Middleton who\nfought in it. </API> Sir Richard Assheton of Middleton (who built St Leonard) was granted\nknighthood [. . . ]5.49 \u0013\nNote: The WL will be open on Friday, <API> Calendar()\u2192Today is Thursday, March 9, 2017.\n</API> March 10, and Sunday, March 19 for regular hours.2.11 \u0013\nThe Nile has an approximate length of <API> QA(What is the approximate length of the Nile?)\n\u21926,853 km </API> 6,853 kilometers, the White Nile being its main source.2.08 \u0013\nIf Venus had an atmosphere similar to Earth\u2019s then you would expect Venus\u2019 mean temperature to be\n499 K (1.74 x 287) rather than 735 K which is <API> Calculator(735 / 499) \u21921.47</API> 1.47\n(735 / 499) times hotter than it should be.1.59 \u0013\nYou are here: Home / Featured / Catch this fast train to success! <API> WikiSearch(Fast train\nsuccess)\u2192Fast Train > It also peaked at #23 on the Canadian CHUM singles chart, on June\n26, 1971. The success of this single established Myles Goodwyn as the band\u2019s main songwriter,\nand made it possible for April Wine to record a second album.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eb8cdbdd-6831-41d2-8afb-f9c6aced8c20": {"__data__": {"id_": "eb8cdbdd-6831-41d2-8afb-f9c6aced8c20", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be169577-b256-46a5-b678-d92a95f0bbc8", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b6509a146854baf31c1d0a21ef88bcb6b310b5bb2c1febb32867de0dd97b61ba", "class_name": "RelatedNodeInfo"}}, "text": "</API> Don\u2019t wait weeks and\n[...]0.92 \u0017\nOs Melhores Escolas em Jersey 2020 <API> MT(Os Melhores Escolas em Jersey) \u2192The Best\nSchools in Jersey </API> On this page you can search for Universities, Colleges and Business\nschools in Jersey0.70 \u0013\nEnjoy these pictures from the <API> Calendar()\u2192Today is Friday, April 19, 2013. </API>\nEaster Egg Hunt.0.33 \u0013\n85 patients (23%) were hospitalised alive and admitted to a hospital ward. Of them, <API> Calcula-\ntor(85 / 23)\u21923.70</API> 65% had a cardiac aetiology [...]\u22120.02 \u0017\nBut hey, after the <API> Calendar()\u2192Today is Saturday, June 25, 2011. </API> Disneyland\n\ufb01asco with the \ufb01re drill, I think it\u2019s safe to say Chewey won\u2019t let anyone die in a \ufb01re.\u22120.41 \u0017\nThe last time I was with <API> QA(Who was last time I was with?) \u2192The Last Time </API>\nhim I asked what he likes about me and he said he would tell me one day.\u22121.23 \u0017\nTable 10: Examples of API calls for different tools, sorted by the value of L\u2212\ni\u2212L+\nithat is used as a \ufb01ltering\ncriterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens.\napproaches, additional information is always pro-\nvided, regardless of whether it is helpful or not. In\ncontrast, Toolformer learns for itself to explicitly\nasks for the right information.", "mimetype": "text/plain", "start_char_idx": 1485, "end_char_idx": 2767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "151b5f79-30f0-42d5-82a7-42d8963235be": {"__data__": {"id_": "151b5f79-30f0-42d5-82a7-42d8963235be", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb8cdbdd-6831-41d2-8afb-f9c6aced8c20", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "4fc299d7cef82e6ef484c03d4dd3a0000c4d3d0433900dbc537d38f79f454320", "class_name": "RelatedNodeInfo"}}, "text": "In\ncontrast, Toolformer learns for itself to explicitly\nasks for the right information.\nTool Use Several approaches aim to equip LMs\nwith the ability to use external tools such as search\nengines (Komeili et al., 2022; Thoppilan et al.,\n2022; Lazaridou et al., 2022; Shuster et al., 2022;\nYao et al., 2022), web browsers (Nakano et al.,\n2021), calculators (Cobbe et al., 2021; Thoppilan\net al., 2022), translation systems (Thoppilan et al.,\n2022) and Python interpreters (Gao et al., 2022).", "mimetype": "text/plain", "start_char_idx": 2680, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "70c180ef-78bc-4667-bbf1-1f108169db8c": {"__data__": {"id_": "70c180ef-78bc-4667-bbf1-1f108169db8c", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e", "node_type": "1", "metadata": {}, "hash": "4c2df9eb0c95bfd5d051cb18b7f555590f3c70712eaf60002b8427062177ac0b", "class_name": "RelatedNodeInfo"}}, "text": "Example L\u2212\ni\u2212L+\niUseful\nThe Flodden Window (a war memorial dedicated to The Middleton Archers), in the Grade I-listed\nChurch of St Leonard in Middleton is said to be the oldest war memorial in the United King-\ndom. <API> WikiSearch(War memorial Flodden) \u2192Battle of Flodden > Commemoration >\nThe stained-glass Flodden Window in Middleton Parish Church [. . . ] was constructed by Sir\nRichard Assheton in memory of the Battle of Flodden and the archers from Middleton who\nfought in it. </API> Sir Richard Assheton of Middleton (who built St Leonard) was granted\nknighthood [. . . ]5.49 \u0013\nNote: The WL will be open on Friday, <API> Calendar()\u2192Today is Thursday, March 9, 2017.\n</API> March 10, and Sunday, March 19 for regular hours.2.11 \u0013\nThe Nile has an approximate length of <API> QA(What is the approximate length of the Nile?)\n\u21926,853 km </API> 6,853 kilometers, the White Nile being its main source.2.08 \u0013\nIf Venus had an atmosphere similar to Earth\u2019s then you would expect Venus\u2019 mean temperature to be\n499 K (1.74 x 287) rather than 735 K which is <API> Calculator(735 / 499) \u21921.47</API> 1.47\n(735 / 499) times hotter than it should be.1.59 \u0013\nYou are here: Home / Featured / Catch this fast train to success! <API> WikiSearch(Fast train\nsuccess)\u2192Fast Train > It also peaked at #23 on the Canadian CHUM singles chart, on June\n26, 1971. The success of this single established Myles Goodwyn as the band\u2019s main songwriter,\nand made it possible for April Wine to record a second album. </API> Don\u2019t wait weeks and\n[...]0.92 \u0017\nOs Melhores Escolas em Jersey 2020 <API> MT(Os Melhores Escolas em Jersey) \u2192The Best\nSchools in Jersey </API> On this page you can search for Universities, Colleges and Business\nschools in Jersey0.70 \u0013\nEnjoy these pictures from the <API> Calendar()\u2192Today is Friday, April 19, 2013. </API>\nEaster Egg Hunt.0.33 \u0013\n85 patients (23%) were hospitalised alive and admitted to a hospital ward. Of them, <API> Calcula-\ntor(85 / 23)\u21923.70</API> 65% had a cardiac aetiology [...]\u22120.02 \u0017\nBut hey, after the <API> Calendar()\u2192Today is Saturday, June 25, 2011. </API> Disneyland\n\ufb01asco with the \ufb01re drill, I think it\u2019s safe to say Chewey won\u2019t let anyone die in a \ufb01re.\u22120.41 \u0017\nThe last time I was with <API> QA(Who was last time I was with?) \u2192The Last Time </API>\nhim I asked what he likes about me and he said he would tell me one day.\u22121.23 \u0017\nTable 10: Examples of API calls for different tools, sorted by the value of L\u2212\ni\u2212L+\nithat is used as a \ufb01ltering\ncriterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens.\napproaches, additional information is always pro-\nvided, regardless of whether it is helpful or not. In\ncontrast, Toolformer learns for itself to explicitly\nasks for the right information.\nTool Use Several approaches aim to equip LMs\nwith the ability to use external tools such as search\nengines (Komeili et al., 2022; Thoppilan et al.,\n2022; Lazaridou et al., 2022; Shuster et al., 2022;\nYao et al., 2022), web browsers (Nakano et al.,\n2021), calculators (Cobbe et al., 2021; Thoppilan\net al., 2022), translation systems (Thoppilan et al.,\n2022) and Python interpreters (Gao et al., 2022).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bf238218-edeb-48f4-9ab9-e375dcf874cb": {"__data__": {"id_": "bf238218-edeb-48f4-9ab9-e375dcf874cb", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}}, "text": "High values typically correspond to API calls that are intuitively useful for predicting future tokens.\napproaches, additional information is always pro-\nvided, regardless of whether it is helpful or not. In\ncontrast, Toolformer learns for itself to explicitly\nasks for the right information.\nTool Use Several approaches aim to equip LMs\nwith the ability to use external tools such as search\nengines (Komeili et al., 2022; Thoppilan et al.,\n2022; Lazaridou et al., 2022; Shuster et al., 2022;\nYao et al., 2022), web browsers (Nakano et al.,\n2021), calculators (Cobbe et al., 2021; Thoppilan\net al., 2022), translation systems (Thoppilan et al.,\n2022) and Python interpreters (Gao et al., 2022).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f4cfe19c-47e4-4de2-a5fc-b76c3edbab7d": {"__data__": {"id_": "f4cfe19c-47e4-4de2-a5fc-b76c3edbab7d", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf238218-edeb-48f4-9ab9-e375dcf874cb", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "93b5c3f9fc8c65e0695ae72ad23dffedb01199a5a07e277cd70dc897e3cafce2", "class_name": "RelatedNodeInfo"}}, "text": "The way these models learn to use tools can roughly\nbe divided into two approaches: Either they rely on\nlarge amounts of human supervision (Komeili et al.,\n2022; Nakano et al., 2021; Thoppilan et al., 2022)\nor they work by prompting the language model in\na few-shot setup tailored towards a speci\ufb01c task\nwhere it is known a priori which tools needs to beused (Gao et al., 2022; Lazaridou et al., 2022; Yao\net al., 2022). In contrast, the self-supervised nature\nof Toolformer enables it to learn how and when to\nuse tools without requiring a speci\ufb01c prompt that\nshows task-speci\ufb01c examples of how a tool could\nbe used.", "mimetype": "text/plain", "start_char_idx": 695, "end_char_idx": 1312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cdccb3dd-a534-4681-ad6a-34f5942d4535": {"__data__": {"id_": "cdccb3dd-a534-4681-ad6a-34f5942d4535", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4cfe19c-47e4-4de2-a5fc-b76c3edbab7d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "f202673fec140656a39457ef7dfc525886d460a93bb19c77b43cc83b7d60d7e3", "class_name": "RelatedNodeInfo"}}, "text": "Perhaps most closely related to our work\nis TALM (Parisi et al., 2022), an approach that\nuses a similar self-supervised objective for teach-\ning a model to use a calculator and a search engine,\nbut explores this only in settings where a model is\n\ufb01netuned for downstream tasks.\nBootstrapping The idea of using self-training\nand bootstrapping techniques to improve models\nhas been investigated in various contexts, rang-\ning from word sense disambiguation (Yarowsky,\n1995), relation extraction (Brin, 1999; Agichtein\nand Gravano, 2000), parsing (McClosky et al.,\n2006; Reichart and Rappoport, 2007), sequence\ngeneration (He et al., 2020), few-shot text classi-", "mimetype": "text/plain", "start_char_idx": 1313, "end_char_idx": 1971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "981488e8-1b61-4e74-9663-047cfc7d94ad": {"__data__": {"id_": "981488e8-1b61-4e74-9663-047cfc7d94ad", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}}, "text": "High values typically correspond to API calls that are intuitively useful for predicting future tokens.\napproaches, additional information is always pro-\nvided, regardless of whether it is helpful or not. In\ncontrast, Toolformer learns for itself to explicitly\nasks for the right information.\nTool Use Several approaches aim to equip LMs\nwith the ability to use external tools such as search\nengines (Komeili et al., 2022; Thoppilan et al.,\n2022; Lazaridou et al., 2022; Shuster et al., 2022;\nYao et al., 2022), web browsers (Nakano et al.,\n2021), calculators (Cobbe et al., 2021; Thoppilan\net al., 2022), translation systems (Thoppilan et al.,\n2022) and Python interpreters (Gao et al., 2022).\nThe way these models learn to use tools can roughly\nbe divided into two approaches: Either they rely on\nlarge amounts of human supervision (Komeili et al.,\n2022; Nakano et al., 2021; Thoppilan et al., 2022)\nor they work by prompting the language model in\na few-shot setup tailored towards a speci\ufb01c task\nwhere it is known a priori which tools needs to beused (Gao et al., 2022; Lazaridou et al., 2022; Yao\net al., 2022). In contrast, the self-supervised nature\nof Toolformer enables it to learn how and when to\nuse tools without requiring a speci\ufb01c prompt that\nshows task-speci\ufb01c examples of how a tool could\nbe used. Perhaps most closely related to our work\nis TALM (Parisi et al., 2022), an approach that\nuses a similar self-supervised objective for teach-\ning a model to use a calculator and a search engine,\nbut explores this only in settings where a model is\n\ufb01netuned for downstream tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9154fb5a-518d-4931-9581-001103245664": {"__data__": {"id_": "9154fb5a-518d-4931-9581-001103245664", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "981488e8-1b61-4e74-9663-047cfc7d94ad", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "25a5a15c51ed3afebbe856a389224d521372947266bfb39e80819fad260d48b9", "class_name": "RelatedNodeInfo"}}, "text": "Bootstrapping The idea of using self-training\nand bootstrapping techniques to improve models\nhas been investigated in various contexts, rang-\ning from word sense disambiguation (Yarowsky,\n1995), relation extraction (Brin, 1999; Agichtein\nand Gravano, 2000), parsing (McClosky et al.,\n2006; Reichart and Rappoport, 2007), sequence\ngeneration (He et al., 2020), few-shot text classi-", "mimetype": "text/plain", "start_char_idx": 1590, "end_char_idx": 1971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e": {"__data__": {"id_": "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e", "embedding": null, "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ee0818f-da5d-499f-b86a-4e89320b872a", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "305fcc0e275dc8fd16be7f402f983bd084a0f4413f96633db0f95923076c5be0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70c180ef-78bc-4667-bbf1-1f108169db8c", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "2acaedad2580157a7797cda9594941440a50b3dbcc41687cc8bcb4b6fc776dfb", "class_name": "RelatedNodeInfo"}}, "text": "High values typically correspond to API calls that are intuitively useful for predicting future tokens.\napproaches, additional information is always pro-\nvided, regardless of whether it is helpful or not. In\ncontrast, Toolformer learns for itself to explicitly\nasks for the right information.\nTool Use Several approaches aim to equip LMs\nwith the ability to use external tools such as search\nengines (Komeili et al., 2022; Thoppilan et al.,\n2022; Lazaridou et al., 2022; Shuster et al., 2022;\nYao et al., 2022), web browsers (Nakano et al.,\n2021), calculators (Cobbe et al., 2021; Thoppilan\net al., 2022), translation systems (Thoppilan et al.,\n2022) and Python interpreters (Gao et al., 2022).\nThe way these models learn to use tools can roughly\nbe divided into two approaches: Either they rely on\nlarge amounts of human supervision (Komeili et al.,\n2022; Nakano et al., 2021; Thoppilan et al., 2022)\nor they work by prompting the language model in\na few-shot setup tailored towards a speci\ufb01c task\nwhere it is known a priori which tools needs to beused (Gao et al., 2022; Lazaridou et al., 2022; Yao\net al., 2022). In contrast, the self-supervised nature\nof Toolformer enables it to learn how and when to\nuse tools without requiring a speci\ufb01c prompt that\nshows task-speci\ufb01c examples of how a tool could\nbe used. Perhaps most closely related to our work\nis TALM (Parisi et al., 2022), an approach that\nuses a similar self-supervised objective for teach-\ning a model to use a calculator and a search engine,\nbut explores this only in settings where a model is\n\ufb01netuned for downstream tasks.\nBootstrapping The idea of using self-training\nand bootstrapping techniques to improve models\nhas been investigated in various contexts, rang-\ning from word sense disambiguation (Yarowsky,\n1995), relation extraction (Brin, 1999; Agichtein\nand Gravano, 2000), parsing (McClosky et al.,\n2006; Reichart and Rappoport, 2007), sequence\ngeneration (He et al., 2020), few-shot text classi-", "mimetype": "text/plain", "start_char_idx": 2475, "end_char_idx": 4446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a2926084-2577-4d66-84be-054d88ed7c4e": {"__data__": {"id_": "a2926084-2577-4d66-84be-054d88ed7c4e", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}}, "text": "\ufb01cation (Schick and Sch\u00fctze, 2021a) and retrieval\n(Izacard and Grave, 2021) to reasoning (Zelikman\net al., 2022). In a similar spirit to these approaches,\nToolformer is trained on its own predictions after\napplying a perplexity-based \ufb01ltering step.\n7 Limitations\nWhile our approach enables LMs to learn how to\nuse a variety of tools in a self-supervised way, there\nare some clear limitations to what can be achieved\nwith our method in its current form. One such limi-\ntation is the inability of Toolformer to use tools in a\nchain (i.e., using the output of one tool as an input\nfor another tool). This is due to the fact that API\ncalls for each tool are generated independently; as a\nconsequence, there are no examples of chained tool\nuse in the \ufb01netuning dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1419fb3a-983e-425b-bd49-681112be46bc": {"__data__": {"id_": "1419fb3a-983e-425b-bd49-681112be46bc", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2926084-2577-4d66-84be-054d88ed7c4e", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "e7bca7632d3764226c767be941e5618c40a0e2b6a2ccd33fe6224d152bf90319", "class_name": "RelatedNodeInfo"}}, "text": "Our current approach\nalso does not allow the LM to use a tool in an in-\nteractive way \u2013 especially for tools such as search\nengines, that could potentially return hundreds of\ndifferent results, enabling a LM to browse through\nthese results or to re\ufb01ne its search query in a simi-\nlar spirit to Nakano et al. (2021) can be crucial for\ncertain applications. Beyond this, we found models\ntrained with Toolformer to often be sensitive to the\nexact wording of their input when deciding whether\nor not to call an API; this is perhaps unsurprising\ngiven that LMs are known to be very sensitive to\nthe prompt they are provided with in both zero-\nand few-shot settings (Jiang et al., 2020; Schick\nand Sch\u00fctze, 2021a).", "mimetype": "text/plain", "start_char_idx": 765, "end_char_idx": 1473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4c52314f-eba4-4286-aedd-9974022e673d": {"__data__": {"id_": "4c52314f-eba4-4286-aedd-9974022e673d", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1419fb3a-983e-425b-bd49-681112be46bc", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a97649458779b48f8ba60e337a52cfd3d7c6bf48c193a29b44156ea4925f8d56", "class_name": "RelatedNodeInfo"}}, "text": "Depending on the tool, our\nmethod is also very sample-inef\ufb01cient; for example,\nprocessing more than a million documents results\nin only a few thousand examples of useful calls\nto the calculator API. A potential solution to this\nproblem might be to iteratively apply our approach,\nsimilar to how this is done in related bootstrapping\napproaches (Schick and Sch\u00fctze, 2021a; Izacard\nand Grave, 2021; Parisi et al., 2022). Finally, when\ndeciding whether or not to make an API call, Tool-\nformer currently does not take into account the\ntool-dependent, computational cost incurred from\nmaking an API call.\n8 Conclusion\nWe have introduced Toolformer, a language model\nthat learns in a self-supervised way how to use\ndifferent tools such as search engines, calculators,\nand translation systems via simple API calls.", "mimetype": "text/plain", "start_char_idx": 1474, "end_char_idx": 2282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a86cb78c-b2bf-4d49-9312-c04a30768fb3": {"__data__": {"id_": "a86cb78c-b2bf-4d49-9312-c04a30768fb3", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c52314f-eba4-4286-aedd-9974022e673d", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "6bdbe4447f5684451bda90c2a0993a3c6709bd641d75025270828f63a368689e", "class_name": "RelatedNodeInfo"}}, "text": "This\nis done by \ufb01netuning on a large number of sampled\nAPI calls that are \ufb01ltered based on whether theyreduce perplexity on future tokens. Toolformer\nconsiderably improves zero-shot performance of a\n6.7B parameter GPT-J model, enabling it to even\noutperform a much larger GPT-3 model on a range\nof different downstream tasks.\nReferences\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-\nmoyer. 2021. Htlm: Hyper-text pre-training and\nprompting of language models.\nEugene Agichtein and Luis Gravano. 2000. Snowball:\nExtracting relations from large plain-text collections.\nInProceedings of the Fifth ACM Conference on Dig-\nital Libraries , DL \u201900, page 85\u201394, New York, NY ,\nUSA. Association for Computing Machinery.", "mimetype": "text/plain", "start_char_idx": 2283, "end_char_idx": 3044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7cbb4a13-98bf-457f-9ab4-56957dcc8450": {"__data__": {"id_": "7cbb4a13-98bf-457f-9ab4-56957dcc8450", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a86cb78c-b2bf-4d49-9312-c04a30768fb3", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "27d3f40c671577c479745d2063d22b4900a91d94ac25e9ac96f85b6e3f0ffa1c", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computing Machinery.\nRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al.\n1999. Modern information retrieval , volume 463.\nACM press New York.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 1533\u20131544, Seattle, Wash-\nington, USA. Association for Computational Lin-\nguistics.", "mimetype": "text/plain", "start_char_idx": 3008, "end_char_idx": 3471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e89e7795-feb3-47da-8658-cb6d781123a8": {"__data__": {"id_": "e89e7795-feb3-47da-8658-cb6d781123a8", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}}, "text": "\ufb01cation (Schick and Sch\u00fctze, 2021a) and retrieval\n(Izacard and Grave, 2021) to reasoning (Zelikman\net al., 2022). In a similar spirit to these approaches,\nToolformer is trained on its own predictions after\napplying a perplexity-based \ufb01ltering step.\n7 Limitations\nWhile our approach enables LMs to learn how to\nuse a variety of tools in a self-supervised way, there\nare some clear limitations to what can be achieved\nwith our method in its current form. One such limi-\ntation is the inability of Toolformer to use tools in a\nchain (i.e., using the output of one tool as an input\nfor another tool). This is due to the fact that API\ncalls for each tool are generated independently; as a\nconsequence, there are no examples of chained tool\nuse in the \ufb01netuning dataset. Our current approach\nalso does not allow the LM to use a tool in an in-\nteractive way \u2013 especially for tools such as search\nengines, that could potentially return hundreds of\ndifferent results, enabling a LM to browse through\nthese results or to re\ufb01ne its search query in a simi-\nlar spirit to Nakano et al. (2021) can be crucial for\ncertain applications. Beyond this, we found models\ntrained with Toolformer to often be sensitive to the\nexact wording of their input when deciding whether\nor not to call an API; this is perhaps unsurprising\ngiven that LMs are known to be very sensitive to\nthe prompt they are provided with in both zero-\nand few-shot settings (Jiang et al., 2020; Schick\nand Sch\u00fctze, 2021a). Depending on the tool, our\nmethod is also very sample-inef\ufb01cient; for example,\nprocessing more than a million documents results\nin only a few thousand examples of useful calls\nto the calculator API.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "38ecae37-17b7-4d56-9a30-b567e33e77f3": {"__data__": {"id_": "38ecae37-17b7-4d56-9a30-b567e33e77f3", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e89e7795-feb3-47da-8658-cb6d781123a8", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3766f1a3aa8ed6675e5a1b1d6a63c1dec889b9a9898b6fe939b90531bafc0b69", "class_name": "RelatedNodeInfo"}}, "text": "A potential solution to this\nproblem might be to iteratively apply our approach,\nsimilar to how this is done in related bootstrapping\napproaches (Schick and Sch\u00fctze, 2021a; Izacard\nand Grave, 2021; Parisi et al., 2022). Finally, when\ndeciding whether or not to make an API call, Tool-\nformer currently does not take into account the\ntool-dependent, computational cost incurred from\nmaking an API call.\n8 Conclusion\nWe have introduced Toolformer, a language model\nthat learns in a self-supervised way how to use\ndifferent tools such as search engines, calculators,\nand translation systems via simple API calls. This\nis done by \ufb01netuning on a large number of sampled\nAPI calls that are \ufb01ltered based on whether theyreduce perplexity on future tokens. Toolformer\nconsiderably improves zero-shot performance of a\n6.7B parameter GPT-J model, enabling it to even\noutperform a much larger GPT-3 model on a range\nof different downstream tasks.\nReferences\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-\nmoyer. 2021. Htlm: Hyper-text pre-training and\nprompting of language models.\nEugene Agichtein and Luis Gravano. 2000. Snowball:\nExtracting relations from large plain-text collections.\nInProceedings of the Fifth ACM Conference on Dig-\nital Libraries , DL \u201900, page 85\u201394, New York, NY ,\nUSA. Association for Computing Machinery.\nRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al.\n1999. Modern information retrieval , volume 463.\nACM press New York.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 1533\u20131544, Seattle, Wash-\nington, USA.", "mimetype": "text/plain", "start_char_idx": 1673, "end_char_idx": 3426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "541f2839-1cd8-424d-89b0-a9e8e7541585": {"__data__": {"id_": "541f2839-1cd8-424d-89b0-a9e8e7541585", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38ecae37-17b7-4d56-9a30-b567e33e77f3", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "f53df5afed3ff201e85f9973d190ec3d64fbc000592a7c631d1ae23001cf6053", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational Lin-\nguistics.", "mimetype": "text/plain", "start_char_idx": 3427, "end_char_idx": 3471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9f807aaa-6090-46c1-bcab-918f0c0bc976": {"__data__": {"id_": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "024b589f-c94e-495c-971d-a4e679588e40", "node_type": "1", "metadata": {}, "hash": "430731a559047616a960726923a500821c0312a7146d00c4da12c03b56b42c6d", "class_name": "RelatedNodeInfo"}}, "text": "\ufb01cation (Schick and Sch\u00fctze, 2021a) and retrieval\n(Izacard and Grave, 2021) to reasoning (Zelikman\net al., 2022). In a similar spirit to these approaches,\nToolformer is trained on its own predictions after\napplying a perplexity-based \ufb01ltering step.\n7 Limitations\nWhile our approach enables LMs to learn how to\nuse a variety of tools in a self-supervised way, there\nare some clear limitations to what can be achieved\nwith our method in its current form. One such limi-\ntation is the inability of Toolformer to use tools in a\nchain (i.e., using the output of one tool as an input\nfor another tool). This is due to the fact that API\ncalls for each tool are generated independently; as a\nconsequence, there are no examples of chained tool\nuse in the \ufb01netuning dataset. Our current approach\nalso does not allow the LM to use a tool in an in-\nteractive way \u2013 especially for tools such as search\nengines, that could potentially return hundreds of\ndifferent results, enabling a LM to browse through\nthese results or to re\ufb01ne its search query in a simi-\nlar spirit to Nakano et al. (2021) can be crucial for\ncertain applications. Beyond this, we found models\ntrained with Toolformer to often be sensitive to the\nexact wording of their input when deciding whether\nor not to call an API; this is perhaps unsurprising\ngiven that LMs are known to be very sensitive to\nthe prompt they are provided with in both zero-\nand few-shot settings (Jiang et al., 2020; Schick\nand Sch\u00fctze, 2021a). Depending on the tool, our\nmethod is also very sample-inef\ufb01cient; for example,\nprocessing more than a million documents results\nin only a few thousand examples of useful calls\nto the calculator API. A potential solution to this\nproblem might be to iteratively apply our approach,\nsimilar to how this is done in related bootstrapping\napproaches (Schick and Sch\u00fctze, 2021a; Izacard\nand Grave, 2021; Parisi et al., 2022). Finally, when\ndeciding whether or not to make an API call, Tool-\nformer currently does not take into account the\ntool-dependent, computational cost incurred from\nmaking an API call.\n8 Conclusion\nWe have introduced Toolformer, a language model\nthat learns in a self-supervised way how to use\ndifferent tools such as search engines, calculators,\nand translation systems via simple API calls. This\nis done by \ufb01netuning on a large number of sampled\nAPI calls that are \ufb01ltered based on whether theyreduce perplexity on future tokens. Toolformer\nconsiderably improves zero-shot performance of a\n6.7B parameter GPT-J model, enabling it to even\noutperform a much larger GPT-3 model on a range\nof different downstream tasks.\nReferences\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis,\nMandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-\nmoyer. 2021. Htlm: Hyper-text pre-training and\nprompting of language models.\nEugene Agichtein and Luis Gravano. 2000. Snowball:\nExtracting relations from large plain-text collections.\nInProceedings of the Fifth ACM Conference on Dig-\nital Libraries , DL \u201900, page 85\u201394, New York, NY ,\nUSA. Association for Computing Machinery.\nRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al.\n1999. Modern information retrieval , volume 463.\nACM press New York.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 1533\u20131544, Seattle, Wash-\nington, USA. Association for Computational Lin-\nguistics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "aace8115-b0fd-4020-8ee9-b0a0efedbd9d": {"__data__": {"id_": "aace8115-b0fd-4020-8ee9-b0a0efedbd9d", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}}, "text": "Eugene Agichtein and Luis Gravano. 2000. Snowball:\nExtracting relations from large plain-text collections.\nInProceedings of the Fifth ACM Conference on Dig-\nital Libraries , DL \u201900, page 85\u201394, New York, NY ,\nUSA. Association for Computing Machinery.\nRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al.\n1999. Modern information retrieval , volume 463.\nACM press New York.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 1533\u20131544, Seattle, Wash-\nington, USA. Association for Computational Lin-\nguistics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "024b589f-c94e-495c-971d-a4e679588e40", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "401817c7-3efe-4d45-ac61-3f01f236e016": {"__data__": {"id_": "401817c7-3efe-4d45-ac61-3f01f236e016", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aace8115-b0fd-4020-8ee9-b0a0efedbd9d", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "95ba70c0b485ea9ba7f26f0983b31db7555280a91de6b9b1491bafbd9288e8bd", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational Lin-\nguistics.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\nSifre. 2021. Improving language models by retriev-\ning from trillions of tokens.\nSergey Brin. 1999. Extracting patterns and relations\nfrom the world wide web. In The World Wide Web\nand Databases , pages 172\u2013183, Berlin, Heidelberg.", "mimetype": "text/plain", "start_char_idx": 633, "end_char_idx": 1354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "024b589f-c94e-495c-971d-a4e679588e40", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2f3ace41-d30d-434b-ad89-9fb860e0c576": {"__data__": {"id_": "2f3ace41-d30d-434b-ad89-9fb860e0c576", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "401817c7-3efe-4d45-ac61-3f01f236e016", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a372f9136d847c9e29318baf24eb43d7a3c24b8254f218d8eb6e6aaaaea17cb2", "class_name": "RelatedNodeInfo"}}, "text": "Springer Berlin Heidelberg.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems ,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.", "mimetype": "text/plain", "start_char_idx": 1355, "end_char_idx": 2009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "024b589f-c94e-495c-971d-a4e679588e40", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5969b090-d2b3-49b1-8e81-97b43409490c": {"__data__": {"id_": "5969b090-d2b3-49b1-8e81-97b43409490c", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}}, "text": "Eugene Agichtein and Luis Gravano. 2000. Snowball:\nExtracting relations from large plain-text collections.\nInProceedings of the Fifth ACM Conference on Dig-\nital Libraries , DL \u201900, page 85\u201394, New York, NY ,\nUSA. Association for Computing Machinery.\nRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al.\n1999. Modern information retrieval , volume 463.\nACM press New York.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 1533\u20131544, Seattle, Wash-\nington, USA. Association for Computational Lin-\nguistics.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\nSifre. 2021. Improving language models by retriev-\ning from trillions of tokens.\nSergey Brin. 1999. Extracting patterns and relations\nfrom the world wide web. In The World Wide Web\nand Databases , pages 172\u2013183, Berlin, Heidelberg.\nSpringer Berlin Heidelberg.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "024b589f-c94e-495c-971d-a4e679588e40", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a3b606d1-4943-490d-b1b4-853427f660df": {"__data__": {"id_": "a3b606d1-4943-490d-b1b4-853427f660df", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5969b090-d2b3-49b1-8e81-97b43409490c", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "14fa00ab5d67aa276d93fc842a4347f7a75bd5efafce02050dfaccda28accede", "class_name": "RelatedNodeInfo"}}, "text": "Springer Berlin Heidelberg.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems ,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.", "mimetype": "text/plain", "start_char_idx": 1355, "end_char_idx": 2009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "024b589f-c94e-495c-971d-a4e679588e40", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "024b589f-c94e-495c-971d-a4e679588e40": {"__data__": {"id_": "024b589f-c94e-495c-971d-a4e679588e40", "embedding": null, "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "5bce5fd37ef19eeda89a46ae8deccdeb78b3fcb5171a22fd4659f78e40b50339", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f807aaa-6090-46c1-bcab-918f0c0bc976", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "4b516dc67a174b102d1b592962fae0a85c798e8d5a22e429b83521fbcc98bb5f", "class_name": "RelatedNodeInfo"}}, "text": "Eugene Agichtein and Luis Gravano. 2000. Snowball:\nExtracting relations from large plain-text collections.\nInProceedings of the Fifth ACM Conference on Dig-\nital Libraries , DL \u201900, page 85\u201394, New York, NY ,\nUSA. Association for Computing Machinery.\nRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al.\n1999. Modern information retrieval , volume 463.\nACM press New York.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 1533\u20131544, Seattle, Wash-\nington, USA. Association for Computational Lin-\nguistics.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\nSifre. 2021. Improving language models by retriev-\ning from trillions of tokens.\nSergey Brin. 1999. Extracting patterns and relations\nfrom the world wide web. In The World Wide Web\nand Databases , pages 172\u2013183, Berlin, Heidelberg.\nSpringer Berlin Heidelberg.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems ,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.", "mimetype": "text/plain", "start_char_idx": 2794, "end_char_idx": 4803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "024b589f-c94e-495c-971d-a4e679588e40", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "03c3a17b-bcf4-497a-9262-ace694e72ab8": {"__data__": {"id_": "03c3a17b-bcf4-497a-9262-ace694e72ab8", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}}, "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6af94997-024e-4f57-8226-a620fd5cc466": {"__data__": {"id_": "6af94997-024e-4f57-8226-a620fd5cc466", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03c3a17b-bcf4-497a-9262-ace694e72ab8", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "dc704689223d9a9a160c09229e53f97842279f746b195ea577255caed17566c4", "class_name": "RelatedNodeInfo"}}, "text": "David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021.", "mimetype": "text/plain", "start_char_idx": 588, "end_char_idx": 1245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e905615d-0fad-47ce-884e-24ab96caf5ed": {"__data__": {"id_": "e905615d-0fad-47ce-884e-24ab96caf5ed", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6af94997-024e-4f57-8226-a620fd5cc466", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9c956f76cb04552105abcf372b30c6e01018e1f4fc792168b500c1895cb899a8", "class_name": "RelatedNodeInfo"}}, "text": "2021. Training veri\ufb01ers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nMarta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha\nElbayad, Kenneth Hea\ufb01eld, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota.", "mimetype": "text/plain", "start_char_idx": 1240, "end_char_idx": 1973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "28b6c80a-4055-46f8-8c43-33316b6cba34": {"__data__": {"id_": "28b6c80a-4055-46f8-8c43-33316b6cba34", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e905615d-0fad-47ce-884e-24ab96caf5ed", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "68eb07e42663bf6dfe32fdd4e29103e83ef4211ab40ceee510eb619406e5cab2", "class_name": "RelatedNodeInfo"}}, "text": "Associ-\nation for Computational Linguistics.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-aware language\nmodels as temporal knowledge bases. Transactions\nof the Association for Computational Linguistics ,\n10:257\u2013273.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027 .\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022.", "mimetype": "text/plain", "start_char_idx": 1974, "end_char_idx": 2640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cb7f1c57-41a2-48c5-962e-c3d19582ceda": {"__data__": {"id_": "cb7f1c57-41a2-48c5-962e-c3d19582ceda", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28b6c80a-4055-46f8-8c43-33316b6cba34", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "324dfdb9e1b39ea43212a878bc883596a8b6f76993d12af64a3dc174f76cec57", "class_name": "RelatedNodeInfo"}}, "text": "2022. Pal: Program-aided language\nmodels.Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In International Conference\non Learning Representations .\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022.", "mimetype": "text/plain", "start_char_idx": 2635, "end_char_idx": 3059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5f9b0d66-c631-496b-8bf9-68b1c0f6e895": {"__data__": {"id_": "5f9b0d66-c631-496b-8bf9-68b1c0f6e895", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}}, "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training veri\ufb01ers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "835e41bc-8672-4195-9e41-e2bc5633293c": {"__data__": {"id_": "835e41bc-8672-4195-9e41-e2bc5633293c", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f9b0d66-c631-496b-8bf9-68b1c0f6e895", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "2030bd9637638cc97837b850aedfd4f1e16ea2503c47df4e63518bb06715c3a5", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2110.14168 .\nMarta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha\nElbayad, Kenneth Hea\ufb01eld, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-aware language\nmodels as temporal knowledge bases. Transactions\nof the Association for Computational Linguistics ,\n10:257\u2013273.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027 .\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels.Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.", "mimetype": "text/plain", "start_char_idx": 1293, "end_char_idx": 2813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8d460469-f323-42c0-98a0-9661cb23ce93": {"__data__": {"id_": "8d460469-f323-42c0-98a0-9661cb23ce93", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "835e41bc-8672-4195-9e41-e2bc5633293c", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1f515cb638d4d296cde64a65aecbf252e9df84e742af47bd1fe25813bc5f4907", "class_name": "RelatedNodeInfo"}}, "text": "2020. Realm: Retrieval-\naugmented language model pre-training.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In International Conference\non Learning Representations .\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022.", "mimetype": "text/plain", "start_char_idx": 2751, "end_char_idx": 3059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3b7b64a8-0c99-4565-b9ef-b2275e598b3a": {"__data__": {"id_": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f69940f-494d-475a-9735-1fe42ddf93ea", "node_type": "1", "metadata": {}, "hash": "99a8e711b727875a09e9f250bb09e644ac90b22a0c85ddc7aea9bd36283c9c2a", "class_name": "RelatedNodeInfo"}}, "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training veri\ufb01ers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nMarta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha\nElbayad, Kenneth Hea\ufb01eld, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-aware language\nmodels as temporal knowledge bases. Transactions\nof the Association for Computational Linguistics ,\n10:257\u2013273.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027 .\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels.Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In International Conference\non Learning Representations .\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c7de47ce-2321-476f-b53a-d8475e45fce2": {"__data__": {"id_": "c7de47ce-2321-476f-b53a-d8475e45fce2", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2101.00027 .\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels.Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In International Conference\non Learning Representations .\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f69940f-494d-475a-9735-1fe42ddf93ea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d1fd653e-d01a-40b8-88a6-74d87f5ca3e8": {"__data__": {"id_": "d1fd653e-d01a-40b8-88a6-74d87f5ca3e8", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7de47ce-2321-476f-b53a-d8475e45fce2", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "bc76306b84912c9bd9058b50e2ae9613b904f90d5aacea7e94fa62cd54d85013", "class_name": "RelatedNodeInfo"}}, "text": "Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor.\nGautier Izacard and Edouard Grave. 2021. Distilling\nknowledge from reader to retriever for question an-\nswering. In International Conference on Learning\nRepresentations .\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of hallu-\ncination in natural language generation. ACM Com-\nputing Surveys .", "mimetype": "text/plain", "start_char_idx": 566, "end_char_idx": 1263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f69940f-494d-475a-9735-1fe42ddf93ea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "82cb1f04-2a5d-48f9-8427-7dbea46744c4": {"__data__": {"id_": "82cb1f04-2a5d-48f9-8427-7dbea46744c4", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1fd653e-d01a-40b8-88a6-74d87f5ca3e8", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c4e33fa178e7bf811bb0ac55a73dee109de1a2fdd60c9dfc15224421f176879d", "class_name": "RelatedNodeInfo"}}, "text": "ACM Com-\nputing Surveys .\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics , 8:423\u2013438.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601\u20131611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov.\n2016. Fasttext.", "mimetype": "text/plain", "start_char_idx": 1238, "end_char_idx": 1922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f69940f-494d-475a-9735-1fe42ddf93ea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c4c4e23f-bd0d-4315-a21c-a1bc18edcd4b": {"__data__": {"id_": "c4c4e23f-bd0d-4315-a21c-a1bc18edcd4b", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82cb1f04-2a5d-48f9-8427-7dbea46744c4", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d07113d18de99f1d964e304b1fbf0f3b62597790745b37cb91a26e8c7ce243c3", "class_name": "RelatedNodeInfo"}}, "text": "2016. Fasttext. zip: Compressing text classi\ufb01cation\nmodels. arXiv preprint arXiv:1612.03651 .\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCtrl: A conditional transformer language model for\ncontrollable generation.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nmachine translation summit x: papers , pages 79\u201386.\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2022. Internet-augmented dialogue generation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 8460\u20138478, Dublin, Ireland.\nAssociation for Computational Linguistics.", "mimetype": "text/plain", "start_char_idx": 1907, "end_char_idx": 2620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f69940f-494d-475a-9735-1fe42ddf93ea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eb5b4321-6ca3-405a-8e5b-fb4e20fcbf2d": {"__data__": {"id_": "eb5b4321-6ca3-405a-8e5b-fb4e20fcbf2d", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2101.00027 .\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels.Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In International Conference\non Learning Representations .\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor.\nGautier Izacard and Edouard Grave. 2021. Distilling\nknowledge from reader to retriever for question an-\nswering. In International Conference on Learning\nRepresentations .\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of hallu-\ncination in natural language generation. ACM Com-\nputing Surveys .\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics , 8:423\u2013438.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f69940f-494d-475a-9735-1fe42ddf93ea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a2de968d-c52d-4aaf-a0dd-7caa18915893": {"__data__": {"id_": "a2de968d-c52d-4aaf-a0dd-7caa18915893", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb5b4321-6ca3-405a-8e5b-fb4e20fcbf2d", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "493ce048805f9dea8a284e3f3eed04dedffc1953ff5a33aea6359dc95a91f863", "class_name": "RelatedNodeInfo"}}, "text": "Transactions of the Association for\nComputational Linguistics , 8:423\u2013438.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601\u20131611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov.\n2016. Fasttext. zip: Compressing text classi\ufb01cation\nmodels. arXiv preprint arXiv:1612.03651 .\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCtrl: A conditional transformer language model for\ncontrollable generation.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nmachine translation summit x: papers , pages 79\u201386.\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2022. Internet-augmented dialogue generation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 8460\u20138478, Dublin, Ireland.\nAssociation for Computational Linguistics.", "mimetype": "text/plain", "start_char_idx": 1372, "end_char_idx": 2620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f69940f-494d-475a-9735-1fe42ddf93ea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6f69940f-494d-475a-9735-1fe42ddf93ea": {"__data__": {"id_": "6f69940f-494d-475a-9735-1fe42ddf93ea", "embedding": null, "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af26339c-434b-441a-ad5c-2d00a61878ec", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c35aff30e99f17feb7a96e2401b9adc2b652717eba9eff03bc8bf0166df7264d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "51cc11f44b018c9ee7dc6dceda91cef6d0e3853ee3257453585265d68bfd2a15", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2101.00027 .\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels.Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In International Conference\non Learning Representations .\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor.\nGautier Izacard and Edouard Grave. 2021. Distilling\nknowledge from reader to retriever for question an-\nswering. In International Conference on Learning\nRepresentations .\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of hallu-\ncination in natural language generation. ACM Com-\nputing Surveys .\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics , 8:423\u2013438.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601\u20131611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov.\n2016. Fasttext. zip: Compressing text classi\ufb01cation\nmodels. arXiv preprint arXiv:1612.03651 .\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCtrl: A conditional transformer language model for\ncontrollable generation.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nmachine translation summit x: papers , pages 79\u201386.\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2022. Internet-augmented dialogue generation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 8460\u20138478, Dublin, Ireland.\nAssociation for Computational Linguistics.", "mimetype": "text/plain", "start_char_idx": 2494, "end_char_idx": 5114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f69940f-494d-475a-9735-1fe42ddf93ea", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bed28d07-8677-4c0e-82bd-d666cd7ca910": {"__data__": {"id_": "bed28d07-8677-4c0e-82bd-d666cd7ca910", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}}, "text": "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,\nNate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A math word problem repository. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1152\u20131157, San Diego, California. Association for\nComputational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n\ufb01eld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8372e903-14c0-42e6-a54a-44639d2210c6": {"__data__": {"id_": "8372e903-14c0-42e6-a54a-44639d2210c6", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bed28d07-8677-4c0e-82bd-d666cd7ca910", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "2741d24c4d30925f97c72e40ef5ed5607165f79bb1ca5e991db873111a62fc7b", "class_name": "RelatedNodeInfo"}}, "text": "2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics , 7:452\u2013466.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115 .\nPatrick Lewis, Barlas O \u02d8guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. Mlqa: Eval-\nuating cross-lingual extractive question answering.\narXiv preprint arXiv:1910.07475 .", "mimetype": "text/plain", "start_char_idx": 657, "end_char_idx": 1208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e1ae3d8c-a53f-4d2d-a1de-ecee4e3357e5": {"__data__": {"id_": "e1ae3d8c-a53f-4d2d-a1de-ecee4e3357e5", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8372e903-14c0-42e6-a54a-44639d2210c6", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "982dd489f8296b95bf5109eb39f59bfda34f2ceb946bba064b8b05c0ce3655ae", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1910.07475 .\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2021. Few-shot learning with\nmultilingual language models.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factual-\nity in abstractive summarization.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\n2006. Effective self-training for parsing.", "mimetype": "text/plain", "start_char_idx": 1175, "end_char_idx": 1816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "aa942416-de63-44c3-bf82-40e65eafd1cf": {"__data__": {"id_": "aa942416-de63-44c3-bf82-40e65eafd1cf", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1ae3d8c-a53f-4d2d-a1de-ecee4e3357e5", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8a21055d69f0d57048a4055bd057ccf143d6f001c7f4f15edda0c22564280249", "class_name": "RelatedNodeInfo"}}, "text": "2006. Effective self-training for parsing. In Pro-\nceedings of the Human Language Technology Con-\nference of the NAACL, Main Conference , pages 152\u2013\n159, New York City, USA. Association for Compu-\ntational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations .\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and develop-\ning English math word problem solvers. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 975\u2013984, On-\nline. Association for Computational Linguistics.", "mimetype": "text/plain", "start_char_idx": 1774, "end_char_idx": 2461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7cfa7e21-001a-4f83-a5ac-7a708e760dcc": {"__data__": {"id_": "7cfa7e21-001a-4f83-a5ac-7a708e760dcc", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa942416-de63-44c3-bf82-40e65eafd1cf", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3b0f861d0cbc82b384b35336d2e022e9c29d4a36605c404099cc6b2f36869493", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational Linguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.", "mimetype": "text/plain", "start_char_idx": 2419, "end_char_idx": 3226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d5dc6a7d-c845-481d-a7f2-51d4aadc2120": {"__data__": {"id_": "d5dc6a7d-c845-481d-a7f2-51d4aadc2120", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}}, "text": "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,\nNate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A math word problem repository. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1152\u20131157, San Diego, California. Association for\nComputational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n\ufb01eld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics , 7:452\u2013466.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115 .\nPatrick Lewis, Barlas O \u02d8guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. Mlqa: Eval-\nuating cross-lingual extractive question answering.\narXiv preprint arXiv:1910.07475 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "195ed4a0-229d-4464-8682-6b3401b8b754": {"__data__": {"id_": "195ed4a0-229d-4464-8682-6b3401b8b754", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5dc6a7d-c845-481d-a7f2-51d4aadc2120", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b0960ac1c206e3ce04b6aa5b357008557835cdf0f1aae382f017e2b995d2488b", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1910.07475 .\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2021. Few-shot learning with\nmultilingual language models.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factual-\nity in abstractive summarization.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\n2006. Effective self-training for parsing. In Pro-\nceedings of the Human Language Technology Con-\nference of the NAACL, Main Conference , pages 152\u2013\n159, New York City, USA. Association for Compu-\ntational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations .\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and develop-\ning English math word problem solvers. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 975\u2013984, On-\nline. Association for Computational Linguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2021.", "mimetype": "text/plain", "start_char_idx": 1175, "end_char_idx": 2739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ff267bdf-ef1b-49c3-849b-9788c3ba6b1f": {"__data__": {"id_": "ff267bdf-ef1b-49c3-849b-9788c3ba6b1f", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "195ed4a0-229d-4464-8682-6b3401b8b754", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d315a94f91842cc287a509a567cdae45142256f623dab8d32e8c303040fafd35", "class_name": "RelatedNodeInfo"}}, "text": "2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.", "mimetype": "text/plain", "start_char_idx": 2734, "end_char_idx": 3226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cf797a81-a0fc-49b0-8178-33f20d0bc830": {"__data__": {"id_": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f446043-5575-4c7f-9659-57d903610de3", "node_type": "1", "metadata": {}, "hash": "f724c37feacf8d51d7526dd6b0a8f03ecacca41b8c936b3fabf757b8dfb5ed47", "class_name": "RelatedNodeInfo"}}, "text": "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,\nNate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A math word problem repository. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1152\u20131157, San Diego, California. Association for\nComputational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n\ufb01eld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics , 7:452\u2013466.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115 .\nPatrick Lewis, Barlas O \u02d8guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. Mlqa: Eval-\nuating cross-lingual extractive question answering.\narXiv preprint arXiv:1910.07475 .\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2021. Few-shot learning with\nmultilingual language models.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factual-\nity in abstractive summarization.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\n2006. Effective self-training for parsing. In Pro-\nceedings of the Human Language Technology Con-\nference of the NAACL, Main Conference , pages 152\u2013\n159, New York City, USA. Association for Compu-\ntational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations .\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and develop-\ning English math word problem solvers. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 975\u2013984, On-\nline. Association for Computational Linguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d2034f60-24c2-478e-accd-192904ab5b50": {"__data__": {"id_": "d2034f60-24c2-478e-accd-192904ab5b50", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}}, "text": "2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and\nSebastian Riedel. 2021. KILT: a benchmark for\nknowledge intensive language tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f446043-5575-4c7f-9659-57d903610de3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "88d2e849-ff34-4061-bfa7-4d550040b3cd": {"__data__": {"id_": "88d2e849-ff34-4061-bfa7-4d550040b3cd", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2034f60-24c2-478e-accd-192904ab5b50", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c3b9fecb1749b5fe0b7cce967c45c0c096e95df919760363403a301ced7ef063", "class_name": "RelatedNodeInfo"}}, "text": "2021. KILT: a benchmark for\nknowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 2523\u20132544,\nOnline. Association for Computational Linguistics.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP) , pages 2463\u20132473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019.", "mimetype": "text/plain", "start_char_idx": 709, "end_char_idx": 1508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f446043-5575-4c7f-9659-57d903610de3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cb250282-245d-48dd-b92f-98ca6c4d94f6": {"__data__": {"id_": "cb250282-245d-48dd-b92f-98ca6c4d94f6", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88d2e849-ff34-4061-bfa7-4d550040b3cd", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "bbcadda8f5906d969909ff3f8cf595b27e45e3640c8a2728d383ab057079434b", "class_name": "RelatedNodeInfo"}}, "text": "2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog , 1(8):9.\nRoi Reichart and Ari Rappoport. 2007. Self-training\nfor enhancement and domain adaptation of statisti-\ncal parsers trained on small datasets. In Proceed-\nings of the 45th Annual Meeting of the Association of\nComputational Linguistics , pages 616\u2013623, Prague,\nCzech Republic. Association for Computational Lin-\nguistics.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at trec-3. Nist Special Publication Sp ,\n109:109.\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio\nPetroni, Patrick Lewis, Gautier Izacard, Qingfei You,\nChristoforos Nalmpantis, Edouard Grave, and Se-\nbastian Riedel. 2022.", "mimetype": "text/plain", "start_char_idx": 1503, "end_char_idx": 2242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f446043-5575-4c7f-9659-57d903610de3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "af9c2c83-4ae9-45da-b626-8982ded28bec": {"__data__": {"id_": "af9c2c83-4ae9-45da-b626-8982ded28bec", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb250282-245d-48dd-b92f-98ca6c4d94f6", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "fbec958af66325924fdec387042e486e2fc584bd30322fc3369259ea49b07a23", "class_name": "RelatedNodeInfo"}}, "text": "2022. Peer: A collaborative lan-\nguage model.\nTimo Schick and Hinrich Sch\u00fctze. 2021a. Exploiting\ncloze-questions for few-shot text classi\ufb01cation and\nnatural language inference. In Proceedings of the", "mimetype": "text/plain", "start_char_idx": 2237, "end_char_idx": 2435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f446043-5575-4c7f-9659-57d903610de3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3a31a548-ea13-42a6-b639-585cd3ff87cf": {"__data__": {"id_": "3a31a548-ea13-42a6-b639-585cd3ff87cf", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}}, "text": "2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and\nSebastian Riedel. 2021. KILT: a benchmark for\nknowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 2523\u20132544,\nOnline. Association for Computational Linguistics.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP) , pages 2463\u20132473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog , 1(8):9.\nRoi Reichart and Ari Rappoport. 2007.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f446043-5575-4c7f-9659-57d903610de3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "128d2ae6-ee1f-41a1-876f-30413a25d339": {"__data__": {"id_": "128d2ae6-ee1f-41a1-876f-30413a25d339", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a31a548-ea13-42a6-b639-585cd3ff87cf", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "150a7201ebe4358786dd40fdc9e449d459235e990eef567db503d49bc9d85795", "class_name": "RelatedNodeInfo"}}, "text": "Roi Reichart and Ari Rappoport. 2007. Self-training\nfor enhancement and domain adaptation of statisti-\ncal parsers trained on small datasets. In Proceed-\nings of the 45th Annual Meeting of the Association of\nComputational Linguistics , pages 616\u2013623, Prague,\nCzech Republic. Association for Computational Lin-\nguistics.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at trec-3. Nist Special Publication Sp ,\n109:109.\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio\nPetroni, Patrick Lewis, Gautier Izacard, Qingfei You,\nChristoforos Nalmpantis, Edouard Grave, and Se-\nbastian Riedel. 2022. Peer: A collaborative lan-\nguage model.\nTimo Schick and Hinrich Sch\u00fctze. 2021a. Exploiting\ncloze-questions for few-shot text classi\ufb01cation and\nnatural language inference. In Proceedings of the", "mimetype": "text/plain", "start_char_idx": 1586, "end_char_idx": 2435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f446043-5575-4c7f-9659-57d903610de3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6f446043-5575-4c7f-9659-57d903610de3": {"__data__": {"id_": "6f446043-5575-4c7f-9659-57d903610de3", "embedding": null, "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cadce458-1609-48a6-bccb-416dc1147e4e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b476c0834f191abdb1b41a33e3ce2bb88c83069568c71ed493f6e9fc808e6f57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf797a81-a0fc-49b0-8178-33f20d0bc830", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "078b17171d45a76b32ccff5195893265c85e7057411651e18d7d43a14208140e", "class_name": "RelatedNodeInfo"}}, "text": "2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:\nTool augmented language models.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 2080\u20132094, Online.\nAssociation for Computational Linguistics.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and\nSebastian Riedel. 2021. KILT: a benchmark for\nknowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 2523\u20132544,\nOnline. Association for Computational Linguistics.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP) , pages 2463\u20132473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog , 1(8):9.\nRoi Reichart and Ari Rappoport. 2007. Self-training\nfor enhancement and domain adaptation of statisti-\ncal parsers trained on small datasets. In Proceed-\nings of the 45th Annual Meeting of the Association of\nComputational Linguistics , pages 616\u2013623, Prague,\nCzech Republic. Association for Computational Lin-\nguistics.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at trec-3. Nist Special Publication Sp ,\n109:109.\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio\nPetroni, Patrick Lewis, Gautier Izacard, Qingfei You,\nChristoforos Nalmpantis, Edouard Grave, and Se-\nbastian Riedel. 2022. Peer: A collaborative lan-\nguage model.\nTimo Schick and Hinrich Sch\u00fctze. 2021a. Exploiting\ncloze-questions for few-shot text classi\ufb01cation and\nnatural language inference. In Proceedings of the", "mimetype": "text/plain", "start_char_idx": 2734, "end_char_idx": 5169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "6f446043-5575-4c7f-9659-57d903610de3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e76774e7-9714-43ef-8654-df0bde974101": {"__data__": {"id_": "e76774e7-9714-43ef-8654-df0bde974101", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}}, "text": "16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255\u2013269, Online. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021b. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943\u2013\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, William Ngan, Spencer Poff, Naman\nGoyal, Arthur Szlam, Y-Lan Boureau, Melanie Kam-\nbadur, and Jason Weston. 2022.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fc441a0b-53aa-4030-964f-65f23edaf030": {"__data__": {"id_": "fc441a0b-53aa-4030-964f-65f23edaf030", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e76774e7-9714-43ef-8654-df0bde974101", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "cb07fd8a11d6b9b73d4c106260b92407a4202405f9d4700466b73ea97860e9d9", "class_name": "RelatedNodeInfo"}}, "text": "2022. Blenderbot 3: a de-\nployed conversational agent that continually learns\nto responsibly engage.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, De-\nhao Chen, Yuanzhong Xu, Zhifeng Chen, Adam\nRoberts, Maarten Bosma, Vincent Zhao, Yanqi\nZhou, Chung-Ching Chang, Igor Krivokon, Will\nRusch, Marc Pickett, Pranesh Srinivasan, Laichee\nMan, Kathleen Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos,", "mimetype": "text/plain", "start_char_idx": 725, "end_char_idx": 1383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "29807242-edd0-4744-bc79-68e7601a6f7f": {"__data__": {"id_": "29807242-edd0-4744-bc79-68e7601a6f7f", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc441a0b-53aa-4030-964f-65f23edaf030", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "2ef46141b32e0f2a369c19ff95b0597543bf27751fdeb573272d05d2a548e0fb", "class_name": "RelatedNodeInfo"}}, "text": "Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju\nDuke, Johnny Soraker, Ben Zevenbergen, Vinod-\nkumar Prabhakaran, Mark Diaz, Ben Hutchinson,\nKristen Olson, Alejandra Molina, Erin Hoffman-\nJohn, Josh Lee, Lora Aroyo, Ravi Rajakumar,\nAlena Butryna, Matthew Lamm, Viktoriya Kuzmina,\nJoe Fenton, Aaron Cohen, Rachel Bernstein, Ray\nKurzweil, Blaise Aguera-Arcas, Claire Cui, Marian\nCroak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax .", "mimetype": "text/plain", "start_char_idx": 1323, "end_char_idx": 1955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e16a1afa-7e97-48f6-b024-c27a984da441": {"__data__": {"id_": "e16a1afa-7e97-48f6-b024-c27a984da441", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29807242-edd0-4744-bc79-68e7601a6f7f", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "8e818ae8afa8af7c5c160dc7947f3814f672d4c6a2c900ffb6bb03bfc077b9d3", "class_name": "RelatedNodeInfo"}}, "text": "https://github.com/\nkingoflolz/mesh-transformer-jax .\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022.\nEmergent abilities of large language models.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave.", "mimetype": "text/plain", "start_char_idx": 1902, "end_char_idx": 2570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1b66f188-865a-4a74-864c-5ce6ec94f769": {"__data__": {"id_": "1b66f188-865a-4a74-864c-5ce6ec94f769", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e16a1afa-7e97-48f6-b024-c27a984da441", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "17367e66485573d0e2badcef211b611b179de7a1bd05011697891a4aa6ce86cf", "class_name": "RelatedNodeInfo"}}, "text": "2020. CCNet:\nExtracting high quality monolingual datasets fromweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference , pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 189\u2013196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.", "mimetype": "text/plain", "start_char_idx": 2571, "end_char_idx": 3221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9e530225-9fd7-4494-ab92-758053091c8b": {"__data__": {"id_": "9e530225-9fd7-4494-ab92-758053091c8b", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}}, "text": "16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255\u2013269, Online. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021b. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943\u2013\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, William Ngan, Spencer Poff, Naman\nGoyal, Arthur Szlam, Y-Lan Boureau, Melanie Kam-\nbadur, and Jason Weston. 2022. Blenderbot 3: a de-\nployed conversational agent that continually learns\nto responsibly engage.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b693215f-7503-4e8a-b656-de96b96885a5": {"__data__": {"id_": "b693215f-7503-4e8a-b656-de96b96885a5", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e530225-9fd7-4494-ab92-758053091c8b", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d91c2420c21354f933ca58330f92e6d900bc0270dc0ecd05c14d72a78342460f", "class_name": "RelatedNodeInfo"}}, "text": "Romal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, De-\nhao Chen, Yuanzhong Xu, Zhifeng Chen, Adam\nRoberts, Maarten Bosma, Vincent Zhao, Yanqi\nZhou, Chung-Ching Chang, Igor Krivokon, Will\nRusch, Marc Pickett, Pranesh Srinivasan, Laichee\nMan, Kathleen Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju\nDuke, Johnny Soraker, Ben Zevenbergen, Vinod-\nkumar Prabhakaran, Mark Diaz, Ben Hutchinson,\nKristen Olson, Alejandra Molina, Erin Hoffman-\nJohn, Josh Lee, Lora Aroyo, Ravi Rajakumar,\nAlena Butryna, Matthew Lamm, Viktoriya Kuzmina,\nJoe Fenton, Aaron Cohen, Rachel Bernstein, Ray\nKurzweil, Blaise Aguera-Arcas, Claire Cui, Marian\nCroak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax .\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions.", "mimetype": "text/plain", "start_char_idx": 826, "end_char_idx": 2153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1ff4e1ef-6d3f-4db9-9666-3456b7e886f3": {"__data__": {"id_": "1ff4e1ef-6d3f-4db9-9666-3456b7e886f3", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b693215f-7503-4e8a-b656-de96b96885a5", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c37b188e78d8355de564ba082ff3029b32b37b6bbe76588df44cc7e069d0b5c6", "class_name": "RelatedNodeInfo"}}, "text": "2022. Self-instruct: Aligning lan-\nguage model with self generated instructions.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022.\nEmergent abilities of large language models.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets fromweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference , pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 189\u2013196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.", "mimetype": "text/plain", "start_char_idx": 2073, "end_char_idx": 3221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "95637869-494b-461c-afbe-e7b72f65622f": {"__data__": {"id_": "95637869-494b-461c-afbe-e7b72f65622f", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcb51ffb-7d55-490b-b328-7f36b78ea34e", "node_type": "1", "metadata": {}, "hash": "08196a441d540b84b1223e167b2515c9442fe00893615a916da60dde055533fa", "class_name": "RelatedNodeInfo"}}, "text": "16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255\u2013269, Online. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021b. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943\u2013\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, William Ngan, Spencer Poff, Naman\nGoyal, Arthur Szlam, Y-Lan Boureau, Melanie Kam-\nbadur, and Jason Weston. 2022. Blenderbot 3: a de-\nployed conversational agent that continually learns\nto responsibly engage.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, De-\nhao Chen, Yuanzhong Xu, Zhifeng Chen, Adam\nRoberts, Maarten Bosma, Vincent Zhao, Yanqi\nZhou, Chung-Ching Chang, Igor Krivokon, Will\nRusch, Marc Pickett, Pranesh Srinivasan, Laichee\nMan, Kathleen Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju\nDuke, Johnny Soraker, Ben Zevenbergen, Vinod-\nkumar Prabhakaran, Mark Diaz, Ben Hutchinson,\nKristen Olson, Alejandra Molina, Erin Hoffman-\nJohn, Josh Lee, Lora Aroyo, Ravi Rajakumar,\nAlena Butryna, Matthew Lamm, Viktoriya Kuzmina,\nJoe Fenton, Aaron Cohen, Rachel Bernstein, Ray\nKurzweil, Blaise Aguera-Arcas, Claire Cui, Marian\nCroak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-\nguage models for dialog applications.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax .\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022.\nEmergent abilities of large language models.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets fromweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference , pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 189\u2013196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "95637869-494b-461c-afbe-e7b72f65622f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4ab7d399-9410-4926-b423-b29cd7ca54f6": {"__data__": {"id_": "4ab7d399-9410-4926-b423-b29cd7ca54f6", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}}, "text": "2020. CCNet:\nExtracting high quality monolingual datasets fromweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference , pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 189\u2013196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\nGoodman. 2022. Star: Bootstrapping reasoning\nwith reasoning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "dcb51ffb-7d55-490b-b328-7f36b78ea34e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "df5d869b-7454-409b-bb26-4349976a810e": {"__data__": {"id_": "df5d869b-7454-409b-bb26-4349976a810e", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ab7d399-9410-4926-b423-b29cd7ca54f6", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "c4ddb0efe793ed9646183e54c2fa1f25c5bacadb12dafdd54ca888374cfa6cbb", "class_name": "RelatedNodeInfo"}}, "text": "2022. Star: Bootstrapping reasoning\nwith reasoning.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.", "mimetype": "text/plain", "start_char_idx": 708, "end_char_idx": 1097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "dcb51ffb-7d55-490b-b328-7f36b78ea34e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "aa24dff4-2eeb-4c60-9504-c83a56dfacc7": {"__data__": {"id_": "aa24dff4-2eeb-4c60-9504-c83a56dfacc7", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}}, "text": "2020. CCNet:\nExtracting high quality monolingual datasets fromweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference , pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 189\u2013196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\nGoodman. 2022. Star: Bootstrapping reasoning\nwith reasoning.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "dcb51ffb-7d55-490b-b328-7f36b78ea34e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dcb51ffb-7d55-490b-b328-7f36b78ea34e": {"__data__": {"id_": "dcb51ffb-7d55-490b-b328-7f36b78ea34e", "embedding": null, "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d712f9ee8c884885a7e1afe50ef3f51d76d3bbc1cc3e657ce9cb39b658d7269f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95637869-494b-461c-afbe-e7b72f65622f", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "71eaa84680f5f17a1f3f65edb3a225c312bb13125fc20b7278c71598f1f07924", "class_name": "RelatedNodeInfo"}}, "text": "2020. CCNet:\nExtracting high quality monolingual datasets fromweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference , pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 189\u2013196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\nGoodman. 2022. Star: Bootstrapping reasoning\nwith reasoning.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.", "mimetype": "text/plain", "start_char_idx": 2571, "end_char_idx": 3668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "dcb51ffb-7d55-490b-b328-7f36b78ea34e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "85e4eac4-2122-4b06-b9fa-651185dd3070": {"__data__": {"id_": "85e4eac4-2122-4b06-b9fa-651185dd3070", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}}, "text": "A API Details\nWhen sampling and \ufb01ltering API calls, by default\nwe use values of \u03c4s= 0.05and\u03c4f= 1.0\u2013 i.e.,\nwe only make API calls at positions where the\nprobability of the <API> token is at least 5%, and\nwe keep API calls if they reduce the loss by at least\n1.0. We only keep the top k= 5such positions and\nsample up to m= 5 API calls for each position\nidenti\ufb01ed in a piece of text. Due to the heuristic\n\ufb01ltering described below, we generate API calls for\nthe calculator and machine translation system on\nonly a small subset of C; to compensate for this,\nwe set\u03c4s= 0.0,k= 20 andm= 10 for these\ntools. As the resulting sets of API calls are still\ncomparably small, we additionally set \u03c4f= 0.5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3adae9bc-aee4-4b4e-a496-dc12cd9e5c3c": {"__data__": {"id_": "3adae9bc-aee4-4b4e-a496-dc12cd9e5c3c", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85e4eac4-2122-4b06-b9fa-651185dd3070", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "595890ffe9fa533befbb1ec3213b351cae5c65a571d3c3182a3e88470622620f", "class_name": "RelatedNodeInfo"}}, "text": "A.1 Implementation\nQuestion Answering We use the Atlas model of\nIzacard et al. (2022) \ufb01netuned on Natural Ques-\ntions (Kwiatkowski et al., 2019) as our question\nanswering system. For creating C\u2217we use Atlas-\nlarge, enabling us to ef\ufb01ciently process millions\nof API calls; during inference, we use the larger\nAtlas-xxl model.\nCalculator Our calculator is based on a simple\nPython script and only supports the operators \u201c +\u201d,\n\u201c\u2212\u201d, \u201c\u2217\u201d, and \u201c/\u201d. It does not return any result\nfor syntactically invalid equations.", "mimetype": "text/plain", "start_char_idx": 692, "end_char_idx": 1201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d03de5a2-cea4-429a-958c-b442df4a8b1c": {"__data__": {"id_": "d03de5a2-cea4-429a-958c-b442df4a8b1c", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3adae9bc-aee4-4b4e-a496-dc12cd9e5c3c", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "2d031cffad2f5714e3a9dc13cf7ac9e60f217e0ac5d29c4bd1bcfac2eb08b0a0", "class_name": "RelatedNodeInfo"}}, "text": "It does not return any result\nfor syntactically invalid equations. For sampling\nAPI calls, we apply heuristic \ufb01lters to our subset of\nCCNet and only process documents that either (i)\ncontain at least three numbers within a window of\n100 tokens, where one of these numbers is the result\nof applying a mathematical operation to the other\ntwo, (ii) contain one of the sequences \u201c=\u201d, \u201cequals\u201d,\n\u201cequal to\u201d, \u201ctotal of\u201d, \u201caverage of\u201d followed by a\nnumber, or (iii) contain at least three numbers; for\ntexts that only match the last criterion, we only\nkeep a random subset of 1%.\nCalendar For creating our dataset C\u2217, we operate\nunder the assumption that the calendar date in such\ncases should be the date that the document was\ncreated. We approximate this by extracting the date\nfrom the URL, if it is present. We \ufb01lter out texts for\nwhich a date cannot be extracted, leaving around\n18% of the documents.", "mimetype": "text/plain", "start_char_idx": 1135, "end_char_idx": 2032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4f56d800-a5a5-4556-bb4d-c5c6ccb82d4c": {"__data__": {"id_": "4f56d800-a5a5-4556-bb4d-c5c6ccb82d4c", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d03de5a2-cea4-429a-958c-b442df4a8b1c", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a54c3baee9f24101cf10e6332271614e188eff1bb9fa45f6c587b616a9c9fab4", "class_name": "RelatedNodeInfo"}}, "text": "Machine Translation For both training and in-\nference, we use the 600M parameter NLLB (Costa-\njuss\u00e0 et al., 2022) as our machine translation (MT)\nmodel. The source language is automatically de-\ntected using the fastText classi\ufb01er (Joulin et al.,2016), while the target language is always set to\nEnglish. Since most of the CCNet dataset is in\nEnglish, we \ufb01lter out the parts that contain only\nEnglish text before generating API calls. More\nspeci\ufb01cally, we only keep those paragraphs which\ncontain text chunks in a language other than En-\nglish preceded and followed by English text. We\nuse text chunks of size 10 tokens. To determine\nwhether the middle text chunk is in a language\ndifferent than English we again use the fastText\nclassi\ufb01er with a con\ufb01dence greater than 0.8.", "mimetype": "text/plain", "start_char_idx": 2033, "end_char_idx": 2806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6dab964e-a3bb-4239-a2c2-8efc7371f0b3": {"__data__": {"id_": "6dab964e-a3bb-4239-a2c2-8efc7371f0b3", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f56d800-a5a5-4556-bb4d-c5c6ccb82d4c", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "9de301e6d3a472af154b8f7522fe79cf6d2b80cd2cecc9ce6b2b7e55bff037d9", "class_name": "RelatedNodeInfo"}}, "text": "We\nalso \ufb01lter out any text chunks that contain only\nnumbers or special symbols. This \ufb01ltering mecha-\nnism allows us to generate data more ef\ufb01ciently by\nfocusing our API call generations in places where\nthe MT tool is likely to be helpful. After generating\nthe MT API calls, we additionally remove from our\ntraining set those where the input to the MT tool\nappears after the API call but not before it. While\nduring data generation the model can look ahead\nto generate API calls, this is not possible at infer-\nence time, so we want to dissuade the model from\ncalling the API in such cases.\nA.2 Prompts\nBelow, we list the prompts used to sample API\ncalls for each tool considered.\nQuestion Answering We use the following\nprompt for the question answering tool:\nYour task is to add calls to a Question\nAnswering API to a piece of text.\nThe questions should help you get\ninformation required to complete the\ntext.", "mimetype": "text/plain", "start_char_idx": 2807, "end_char_idx": 3717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "27a49fd9-5d17-4339-9d70-5bcd613e1e63": {"__data__": {"id_": "27a49fd9-5d17-4339-9d70-5bcd613e1e63", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}}, "text": "A API Details\nWhen sampling and \ufb01ltering API calls, by default\nwe use values of \u03c4s= 0.05and\u03c4f= 1.0\u2013 i.e.,\nwe only make API calls at positions where the\nprobability of the <API> token is at least 5%, and\nwe keep API calls if they reduce the loss by at least\n1.0. We only keep the top k= 5such positions and\nsample up to m= 5 API calls for each position\nidenti\ufb01ed in a piece of text. Due to the heuristic\n\ufb01ltering described below, we generate API calls for\nthe calculator and machine translation system on\nonly a small subset of C; to compensate for this,\nwe set\u03c4s= 0.0,k= 20 andm= 10 for these\ntools. As the resulting sets of API calls are still\ncomparably small, we additionally set \u03c4f= 0.5.\nA.1 Implementation\nQuestion Answering We use the Atlas model of\nIzacard et al. (2022) \ufb01netuned on Natural Ques-\ntions (Kwiatkowski et al., 2019) as our question\nanswering system. For creating C\u2217we use Atlas-\nlarge, enabling us to ef\ufb01ciently process millions\nof API calls; during inference, we use the larger\nAtlas-xxl model.\nCalculator Our calculator is based on a simple\nPython script and only supports the operators \u201c +\u201d,\n\u201c\u2212\u201d, \u201c\u2217\u201d, and \u201c/\u201d. It does not return any result\nfor syntactically invalid equations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "73d3fb90-1b9c-427c-be38-e492c78466b8": {"__data__": {"id_": "73d3fb90-1b9c-427c-be38-e492c78466b8", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27a49fd9-5d17-4339-9d70-5bcd613e1e63", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "eccc47c16560331eaffa79ef84d73133a31b5e5f6d11496d1cb93162f6f32119", "class_name": "RelatedNodeInfo"}}, "text": "It does not return any result\nfor syntactically invalid equations. For sampling\nAPI calls, we apply heuristic \ufb01lters to our subset of\nCCNet and only process documents that either (i)\ncontain at least three numbers within a window of\n100 tokens, where one of these numbers is the result\nof applying a mathematical operation to the other\ntwo, (ii) contain one of the sequences \u201c=\u201d, \u201cequals\u201d,\n\u201cequal to\u201d, \u201ctotal of\u201d, \u201caverage of\u201d followed by a\nnumber, or (iii) contain at least three numbers; for\ntexts that only match the last criterion, we only\nkeep a random subset of 1%.\nCalendar For creating our dataset C\u2217, we operate\nunder the assumption that the calendar date in such\ncases should be the date that the document was\ncreated. We approximate this by extracting the date\nfrom the URL, if it is present. We \ufb01lter out texts for\nwhich a date cannot be extracted, leaving around\n18% of the documents.\nMachine Translation For both training and in-\nference, we use the 600M parameter NLLB (Costa-\njuss\u00e0 et al., 2022) as our machine translation (MT)\nmodel. The source language is automatically de-\ntected using the fastText classi\ufb01er (Joulin et al.,2016), while the target language is always set to\nEnglish. Since most of the CCNet dataset is in\nEnglish, we \ufb01lter out the parts that contain only\nEnglish text before generating API calls. More\nspeci\ufb01cally, we only keep those paragraphs which\ncontain text chunks in a language other than En-\nglish preceded and followed by English text. We\nuse text chunks of size 10 tokens. To determine\nwhether the middle text chunk is in a language\ndifferent than English we again use the fastText\nclassi\ufb01er with a con\ufb01dence greater than 0.8. We\nalso \ufb01lter out any text chunks that contain only\nnumbers or special symbols.", "mimetype": "text/plain", "start_char_idx": 1135, "end_char_idx": 2886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4f689fb8-1eb5-4f0a-9971-bd95b4594653": {"__data__": {"id_": "4f689fb8-1eb5-4f0a-9971-bd95b4594653", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73d3fb90-1b9c-427c-be38-e492c78466b8", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1c3438a83447073bfb69527109dce65ff7f22fe308e8fc77b3a9583edc9aeb8e", "class_name": "RelatedNodeInfo"}}, "text": "This \ufb01ltering mecha-\nnism allows us to generate data more ef\ufb01ciently by\nfocusing our API call generations in places where\nthe MT tool is likely to be helpful. After generating\nthe MT API calls, we additionally remove from our\ntraining set those where the input to the MT tool\nappears after the API call but not before it. While\nduring data generation the model can look ahead\nto generate API calls, this is not possible at infer-\nence time, so we want to dissuade the model from\ncalling the API in such cases.\nA.2 Prompts\nBelow, we list the prompts used to sample API\ncalls for each tool considered.\nQuestion Answering We use the following\nprompt for the question answering tool:\nYour task is to add calls to a Question\nAnswering API to a piece of text.\nThe questions should help you get\ninformation required to complete the\ntext.", "mimetype": "text/plain", "start_char_idx": 2887, "end_char_idx": 3717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2189a3c2-2140-4315-92c8-d71ad93ff4df": {"__data__": {"id_": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b49c5d25-ae4c-46ed-a686-8e421b1c771c", "node_type": "1", "metadata": {}, "hash": "5354042a831b539eac53b75ccc3a661b4c82e3b8c537058486794bb787385040", "class_name": "RelatedNodeInfo"}}, "text": "A API Details\nWhen sampling and \ufb01ltering API calls, by default\nwe use values of \u03c4s= 0.05and\u03c4f= 1.0\u2013 i.e.,\nwe only make API calls at positions where the\nprobability of the <API> token is at least 5%, and\nwe keep API calls if they reduce the loss by at least\n1.0. We only keep the top k= 5such positions and\nsample up to m= 5 API calls for each position\nidenti\ufb01ed in a piece of text. Due to the heuristic\n\ufb01ltering described below, we generate API calls for\nthe calculator and machine translation system on\nonly a small subset of C; to compensate for this,\nwe set\u03c4s= 0.0,k= 20 andm= 10 for these\ntools. As the resulting sets of API calls are still\ncomparably small, we additionally set \u03c4f= 0.5.\nA.1 Implementation\nQuestion Answering We use the Atlas model of\nIzacard et al. (2022) \ufb01netuned on Natural Ques-\ntions (Kwiatkowski et al., 2019) as our question\nanswering system. For creating C\u2217we use Atlas-\nlarge, enabling us to ef\ufb01ciently process millions\nof API calls; during inference, we use the larger\nAtlas-xxl model.\nCalculator Our calculator is based on a simple\nPython script and only supports the operators \u201c +\u201d,\n\u201c\u2212\u201d, \u201c\u2217\u201d, and \u201c/\u201d. It does not return any result\nfor syntactically invalid equations. For sampling\nAPI calls, we apply heuristic \ufb01lters to our subset of\nCCNet and only process documents that either (i)\ncontain at least three numbers within a window of\n100 tokens, where one of these numbers is the result\nof applying a mathematical operation to the other\ntwo, (ii) contain one of the sequences \u201c=\u201d, \u201cequals\u201d,\n\u201cequal to\u201d, \u201ctotal of\u201d, \u201caverage of\u201d followed by a\nnumber, or (iii) contain at least three numbers; for\ntexts that only match the last criterion, we only\nkeep a random subset of 1%.\nCalendar For creating our dataset C\u2217, we operate\nunder the assumption that the calendar date in such\ncases should be the date that the document was\ncreated. We approximate this by extracting the date\nfrom the URL, if it is present. We \ufb01lter out texts for\nwhich a date cannot be extracted, leaving around\n18% of the documents.\nMachine Translation For both training and in-\nference, we use the 600M parameter NLLB (Costa-\njuss\u00e0 et al., 2022) as our machine translation (MT)\nmodel. The source language is automatically de-\ntected using the fastText classi\ufb01er (Joulin et al.,2016), while the target language is always set to\nEnglish. Since most of the CCNet dataset is in\nEnglish, we \ufb01lter out the parts that contain only\nEnglish text before generating API calls. More\nspeci\ufb01cally, we only keep those paragraphs which\ncontain text chunks in a language other than En-\nglish preceded and followed by English text. We\nuse text chunks of size 10 tokens. To determine\nwhether the middle text chunk is in a language\ndifferent than English we again use the fastText\nclassi\ufb01er with a con\ufb01dence greater than 0.8. We\nalso \ufb01lter out any text chunks that contain only\nnumbers or special symbols. This \ufb01ltering mecha-\nnism allows us to generate data more ef\ufb01ciently by\nfocusing our API call generations in places where\nthe MT tool is likely to be helpful. After generating\nthe MT API calls, we additionally remove from our\ntraining set those where the input to the MT tool\nappears after the API call but not before it. While\nduring data generation the model can look ahead\nto generate API calls, this is not possible at infer-\nence time, so we want to dissuade the model from\ncalling the API in such cases.\nA.2 Prompts\nBelow, we list the prompts used to sample API\ncalls for each tool considered.\nQuestion Answering We use the following\nprompt for the question answering tool:\nYour task is to add calls to a Question\nAnswering API to a piece of text.\nThe questions should help you get\ninformation required to complete the\ntext.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ddcade83-be12-44ec-934a-879165157543": {"__data__": {"id_": "ddcade83-be12-44ec-934a-879165157543", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}}, "text": "This \ufb01ltering mecha-\nnism allows us to generate data more ef\ufb01ciently by\nfocusing our API call generations in places where\nthe MT tool is likely to be helpful. After generating\nthe MT API calls, we additionally remove from our\ntraining set those where the input to the MT tool\nappears after the API call but not before it. While\nduring data generation the model can look ahead\nto generate API calls, this is not possible at infer-\nence time, so we want to dissuade the model from\ncalling the API in such cases.\nA.2 Prompts\nBelow, we list the prompts used to sample API\ncalls for each tool considered.\nQuestion Answering We use the following\nprompt for the question answering tool:\nYour task is to add calls to a Question\nAnswering API to a piece of text.\nThe questions should help you get\ninformation required to complete the\ntext.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "b49c5d25-ae4c-46ed-a686-8e421b1c771c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "feb46246-1f0c-4444-92b1-b964d629350a": {"__data__": {"id_": "feb46246-1f0c-4444-92b1-b964d629350a", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddcade83-be12-44ec-934a-879165157543", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "f191f25cbe7c3c96856c2c8cb79c6780818208642881f2e1fee2e6f00006624c", "class_name": "RelatedNodeInfo"}}, "text": "The questions should help you get\ninformation required to complete the\ntext. You can call the API by writing\n\"[QA(question)]\" where \"question\" is the\nquestion you want to ask. Here are some\nexamples of API calls:\nInput: Joe Biden was born in Scranton,\nPennsylvania.\nOutput: Joe Biden was born in [QA(\"Where\nwas Joe Biden born?\")] Scranton,\n[QA(\"In which state is Scranton?\")]\nPennsylvania.\nInput: Coca-Cola, or Coke, is a\ncarbonated soft drink manufactured by\nthe Coca-Cola Company.\nOutput: Coca-Cola, or [QA(\"What other\nname is Coca-Cola known by?\")] Coke, is\na carbonated soft drink manufactured by\n[QA(\"Who manufactures Coca-Cola?\")] the\nCoca-Cola Company.\nInput: x\nOutput:\nCalculator We use the following prompt for the\ncalculator:\nYour task is to add calls to a\nCalculator API to a piece of text.", "mimetype": "text/plain", "start_char_idx": 754, "end_char_idx": 1555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "b49c5d25-ae4c-46ed-a686-8e421b1c771c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4faf87a1-9a70-438c-831b-522ed8766190": {"__data__": {"id_": "4faf87a1-9a70-438c-831b-522ed8766190", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}}, "text": "This \ufb01ltering mecha-\nnism allows us to generate data more ef\ufb01ciently by\nfocusing our API call generations in places where\nthe MT tool is likely to be helpful. After generating\nthe MT API calls, we additionally remove from our\ntraining set those where the input to the MT tool\nappears after the API call but not before it. While\nduring data generation the model can look ahead\nto generate API calls, this is not possible at infer-\nence time, so we want to dissuade the model from\ncalling the API in such cases.\nA.2 Prompts\nBelow, we list the prompts used to sample API\ncalls for each tool considered.\nQuestion Answering We use the following\nprompt for the question answering tool:\nYour task is to add calls to a Question\nAnswering API to a piece of text.\nThe questions should help you get\ninformation required to complete the\ntext. You can call the API by writing\n\"[QA(question)]\" where \"question\" is the\nquestion you want to ask. Here are some\nexamples of API calls:\nInput: Joe Biden was born in Scranton,\nPennsylvania.\nOutput: Joe Biden was born in [QA(\"Where\nwas Joe Biden born?\")] Scranton,\n[QA(\"In which state is Scranton?\")]\nPennsylvania.\nInput: Coca-Cola, or Coke, is a\ncarbonated soft drink manufactured by\nthe Coca-Cola Company.\nOutput: Coca-Cola, or [QA(\"What other\nname is Coca-Cola known by?\")] Coke, is\na carbonated soft drink manufactured by\n[QA(\"Who manufactures Coca-Cola?\")] the\nCoca-Cola Company.\nInput: x\nOutput:\nCalculator We use the following prompt for the\ncalculator:\nYour task is to add calls to a\nCalculator API to a piece of text.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "b49c5d25-ae4c-46ed-a686-8e421b1c771c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b49c5d25-ae4c-46ed-a686-8e421b1c771c": {"__data__": {"id_": "b49c5d25-ae4c-46ed-a686-8e421b1c771c", "embedding": null, "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2559d92c-a84e-421a-ae06-c8030953b006", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "3d1518efbf47aa786fe2e9540987a79bf989ee5cb7637142cc6a75ce31a83418", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2189a3c2-2140-4315-92c8-d71ad93ff4df", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "98bf53c374372e3a99de7919dd317bc890d1de632442ac9aaf4950ea4bd2be4f", "class_name": "RelatedNodeInfo"}}, "text": "This \ufb01ltering mecha-\nnism allows us to generate data more ef\ufb01ciently by\nfocusing our API call generations in places where\nthe MT tool is likely to be helpful. After generating\nthe MT API calls, we additionally remove from our\ntraining set those where the input to the MT tool\nappears after the API call but not before it. While\nduring data generation the model can look ahead\nto generate API calls, this is not possible at infer-\nence time, so we want to dissuade the model from\ncalling the API in such cases.\nA.2 Prompts\nBelow, we list the prompts used to sample API\ncalls for each tool considered.\nQuestion Answering We use the following\nprompt for the question answering tool:\nYour task is to add calls to a Question\nAnswering API to a piece of text.\nThe questions should help you get\ninformation required to complete the\ntext. You can call the API by writing\n\"[QA(question)]\" where \"question\" is the\nquestion you want to ask. Here are some\nexamples of API calls:\nInput: Joe Biden was born in Scranton,\nPennsylvania.\nOutput: Joe Biden was born in [QA(\"Where\nwas Joe Biden born?\")] Scranton,\n[QA(\"In which state is Scranton?\")]\nPennsylvania.\nInput: Coca-Cola, or Coke, is a\ncarbonated soft drink manufactured by\nthe Coca-Cola Company.\nOutput: Coca-Cola, or [QA(\"What other\nname is Coca-Cola known by?\")] Coke, is\na carbonated soft drink manufactured by\n[QA(\"Who manufactures Coca-Cola?\")] the\nCoca-Cola Company.\nInput: x\nOutput:\nCalculator We use the following prompt for the\ncalculator:\nYour task is to add calls to a\nCalculator API to a piece of text.", "mimetype": "text/plain", "start_char_idx": 2887, "end_char_idx": 4442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "b49c5d25-ae4c-46ed-a686-8e421b1c771c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d5299d0d-6afa-44bd-9244-8961f12cf920": {"__data__": {"id_": "d5299d0d-6afa-44bd-9244-8961f12cf920", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}}, "text": "The calls should help you get\ninformation required to complete the\ntext. You can call the API by writing\n\"[Calculator(expression)]\" where\n\"expression\" is the expression to be\ncomputed. Here are some examples of API\ncalls:\nInput: The number in the next term is 18\n+ 12 x 3 = 54.\nOutput: The number in the next term is\n18 + 12 x 3 = [Calculator(18 + 12 *3)]\n54.\nInput: The population is 658,893 people.\nThis is 11.4% of the national average of\n5,763,868 people.\nOutput: The population is 658,893 people.\nThis is 11.4% of the national average of\n[Calculator(658,893 / 11.4%)] 5,763,868\npeople.\nInput: A total of 252 qualifying matches\nwere played, and 723 goals were scored\n(an average of 2.87 per match).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a1d4862c-4c52-4e59-852c-103fc85c6236": {"__data__": {"id_": "a1d4862c-4c52-4e59-852c-103fc85c6236", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5299d0d-6afa-44bd-9244-8961f12cf920", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b81c578643aca6382bfb2bbc072ff9afe35c8cc6a83479c13e23d8a330e2f078", "class_name": "RelatedNodeInfo"}}, "text": "This is\nthree times less than the 2169 goals\nlast year.\nOutput: A total of 252 qualifying\nmatches were played, and 723 goals were\nscored (an average of [Calculator(723\n/ 252)] 2.87 per match). This is twenty\ngoals more than the [Calculator(723 -\n20)] 703 goals last year.\nInput: I went to Paris in 1994 and\nstayed there until 2011, so in total,\nit was 17 years.\nOutput: I went to Paris in 1994 and\nstayed there until 2011, so in total, it\nwas [Calculator(2011 - 1994)] 17 years.\nInput: From this, we have 4 *30 minutes\n= 120 minutes.\nOutput: From this, we have 4 *30\nminutes = [Calculator(4 *30)] 120\nminutes.", "mimetype": "text/plain", "start_char_idx": 703, "end_char_idx": 1312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ed2e518c-7d2f-4f13-bbf0-d9bdd3521d12": {"__data__": {"id_": "ed2e518c-7d2f-4f13-bbf0-d9bdd3521d12", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1d4862c-4c52-4e59-852c-103fc85c6236", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "dd77a4e3ef813475f753420f5a14752dec5f64df77f7cea6e07231ef3681a059", "class_name": "RelatedNodeInfo"}}, "text": "Input: x\nOutput:\nWikipedia Search We use the following prompt\nfor the Wikipedia search tool:\nYour task is to complete a given piece\nof text. You can use a Wikipedia Search\nAPI to look up information. You can do\nso by writing \"[WikiSearch(term)]\" where\n\"term\" is the search term you want to\nlook up. Here are some examples of API\ncalls:\nInput: The colors on the flag of Ghana\nhave the following meanings: red is for\nthe blood of martyrs, green for forests,\nand gold for mineral wealth.\nOutput: The colors on the flag of Ghana\nhave the following meanings: red is for\n[WikiSearch(\"Ghana flag red meaning\")]\nthe blood of martyrs, green for forests,\nand gold for mineral wealth.\nInput: But what are the risks during\nproduction of nanomaterials? Somenanomaterials may give rise to various\nkinds of lung damage.\nOutput: But what are the risks\nduring production of nanomaterials?", "mimetype": "text/plain", "start_char_idx": 1313, "end_char_idx": 2184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8560e0a6-ab19-46ac-befa-8f8e5694df16": {"__data__": {"id_": "8560e0a6-ab19-46ac-befa-8f8e5694df16", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed2e518c-7d2f-4f13-bbf0-d9bdd3521d12", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1182d004e0b7235c1102acbface6d11a91c5042417040585dbcf1b57228f57b8", "class_name": "RelatedNodeInfo"}}, "text": "Output: But what are the risks\nduring production of nanomaterials?\n[WikiSearch(\"nanomaterial production\nrisks\")] Some nanomaterials may give\nrise to various kinds of lung damage.\nInput: Metformin is the first-line drug\nfor patients with type 2 diabetes and\nobesity.\nOutput: Metformin is the first-line drug\nfor [WikiSearch(\"Metformin first-line\ndrug\")] patients with type 2 diabetes\nand obesity.\nInput: x\nOutput:\nMachine Translation We use the following\nprompt for the machine translation tool:\nYour task is to complete a given piece\nof text by using a Machine Translation\nAPI.\nYou can do so by writing \"[MT(text)]\"\nwhere text is the text to be translated\ninto English.", "mimetype": "text/plain", "start_char_idx": 2118, "end_char_idx": 2787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ca1619f2-7d03-49ef-8ca7-ff25af479e5a": {"__data__": {"id_": "ca1619f2-7d03-49ef-8ca7-ff25af479e5a", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}}, "text": "The calls should help you get\ninformation required to complete the\ntext. You can call the API by writing\n\"[Calculator(expression)]\" where\n\"expression\" is the expression to be\ncomputed. Here are some examples of API\ncalls:\nInput: The number in the next term is 18\n+ 12 x 3 = 54.\nOutput: The number in the next term is\n18 + 12 x 3 = [Calculator(18 + 12 *3)]\n54.\nInput: The population is 658,893 people.\nThis is 11.4% of the national average of\n5,763,868 people.\nOutput: The population is 658,893 people.\nThis is 11.4% of the national average of\n[Calculator(658,893 / 11.4%)] 5,763,868\npeople.\nInput: A total of 252 qualifying matches\nwere played, and 723 goals were scored\n(an average of 2.87 per match). This is\nthree times less than the 2169 goals\nlast year.\nOutput: A total of 252 qualifying\nmatches were played, and 723 goals were\nscored (an average of [Calculator(723\n/ 252)] 2.87 per match). This is twenty\ngoals more than the [Calculator(723 -\n20)] 703 goals last year.\nInput: I went to Paris in 1994 and\nstayed there until 2011, so in total,\nit was 17 years.\nOutput: I went to Paris in 1994 and\nstayed there until 2011, so in total, it\nwas [Calculator(2011 - 1994)] 17 years.\nInput: From this, we have 4 *30 minutes\n= 120 minutes.\nOutput: From this, we have 4 *30\nminutes = [Calculator(4 *30)] 120\nminutes.\nInput: x\nOutput:\nWikipedia Search We use the following prompt\nfor the Wikipedia search tool:\nYour task is to complete a given piece\nof text. You can use a Wikipedia Search\nAPI to look up information.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c39e2c16-2d5f-4530-b557-12d1fe33ca5b": {"__data__": {"id_": "c39e2c16-2d5f-4530-b557-12d1fe33ca5b", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca1619f2-7d03-49ef-8ca7-ff25af479e5a", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "ec5b8def0a801bbd5a7e675f5feea1410342b38b4aa25cae119c08c727b48c52", "class_name": "RelatedNodeInfo"}}, "text": "You can use a Wikipedia Search\nAPI to look up information. You can do\nso by writing \"[WikiSearch(term)]\" where\n\"term\" is the search term you want to\nlook up. Here are some examples of API\ncalls:\nInput: The colors on the flag of Ghana\nhave the following meanings: red is for\nthe blood of martyrs, green for forests,\nand gold for mineral wealth.\nOutput: The colors on the flag of Ghana\nhave the following meanings: red is for\n[WikiSearch(\"Ghana flag red meaning\")]\nthe blood of martyrs, green for forests,\nand gold for mineral wealth.\nInput: But what are the risks during\nproduction of nanomaterials? Somenanomaterials may give rise to various\nkinds of lung damage.\nOutput: But what are the risks\nduring production of nanomaterials?\n[WikiSearch(\"nanomaterial production\nrisks\")] Some nanomaterials may give\nrise to various kinds of lung damage.\nInput: Metformin is the first-line drug\nfor patients with type 2 diabetes and\nobesity.\nOutput: Metformin is the first-line drug\nfor [WikiSearch(\"Metformin first-line\ndrug\")] patients with type 2 diabetes\nand obesity.\nInput: x\nOutput:\nMachine Translation We use the following\nprompt for the machine translation tool:\nYour task is to complete a given piece\nof text by using a Machine Translation\nAPI.\nYou can do so by writing \"[MT(text)]\"\nwhere text is the text to be translated\ninto English.", "mimetype": "text/plain", "start_char_idx": 1454, "end_char_idx": 2787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "50b7165c-a600-4b73-8fe0-2b8fe5afa537": {"__data__": {"id_": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ddd5227-24e4-4355-8a83-fe978655dc05", "node_type": "1", "metadata": {}, "hash": "9fbc74a2197fc3443bcfce98b7aafa55c7276d76f85733fb94bab49c85afca10", "class_name": "RelatedNodeInfo"}}, "text": "The calls should help you get\ninformation required to complete the\ntext. You can call the API by writing\n\"[Calculator(expression)]\" where\n\"expression\" is the expression to be\ncomputed. Here are some examples of API\ncalls:\nInput: The number in the next term is 18\n+ 12 x 3 = 54.\nOutput: The number in the next term is\n18 + 12 x 3 = [Calculator(18 + 12 *3)]\n54.\nInput: The population is 658,893 people.\nThis is 11.4% of the national average of\n5,763,868 people.\nOutput: The population is 658,893 people.\nThis is 11.4% of the national average of\n[Calculator(658,893 / 11.4%)] 5,763,868\npeople.\nInput: A total of 252 qualifying matches\nwere played, and 723 goals were scored\n(an average of 2.87 per match). This is\nthree times less than the 2169 goals\nlast year.\nOutput: A total of 252 qualifying\nmatches were played, and 723 goals were\nscored (an average of [Calculator(723\n/ 252)] 2.87 per match). This is twenty\ngoals more than the [Calculator(723 -\n20)] 703 goals last year.\nInput: I went to Paris in 1994 and\nstayed there until 2011, so in total,\nit was 17 years.\nOutput: I went to Paris in 1994 and\nstayed there until 2011, so in total, it\nwas [Calculator(2011 - 1994)] 17 years.\nInput: From this, we have 4 *30 minutes\n= 120 minutes.\nOutput: From this, we have 4 *30\nminutes = [Calculator(4 *30)] 120\nminutes.\nInput: x\nOutput:\nWikipedia Search We use the following prompt\nfor the Wikipedia search tool:\nYour task is to complete a given piece\nof text. You can use a Wikipedia Search\nAPI to look up information. You can do\nso by writing \"[WikiSearch(term)]\" where\n\"term\" is the search term you want to\nlook up. Here are some examples of API\ncalls:\nInput: The colors on the flag of Ghana\nhave the following meanings: red is for\nthe blood of martyrs, green for forests,\nand gold for mineral wealth.\nOutput: The colors on the flag of Ghana\nhave the following meanings: red is for\n[WikiSearch(\"Ghana flag red meaning\")]\nthe blood of martyrs, green for forests,\nand gold for mineral wealth.\nInput: But what are the risks during\nproduction of nanomaterials? Somenanomaterials may give rise to various\nkinds of lung damage.\nOutput: But what are the risks\nduring production of nanomaterials?\n[WikiSearch(\"nanomaterial production\nrisks\")] Some nanomaterials may give\nrise to various kinds of lung damage.\nInput: Metformin is the first-line drug\nfor patients with type 2 diabetes and\nobesity.\nOutput: Metformin is the first-line drug\nfor [WikiSearch(\"Metformin first-line\ndrug\")] patients with type 2 diabetes\nand obesity.\nInput: x\nOutput:\nMachine Translation We use the following\nprompt for the machine translation tool:\nYour task is to complete a given piece\nof text by using a Machine Translation\nAPI.\nYou can do so by writing \"[MT(text)]\"\nwhere text is the text to be translated\ninto English.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8690bb8c-7fda-4fdb-a6b2-488a83f15b8c": {"__data__": {"id_": "8690bb8c-7fda-4fdb-a6b2-488a83f15b8c", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}}, "text": "Input: But what are the risks during\nproduction of nanomaterials? Somenanomaterials may give rise to various\nkinds of lung damage.\nOutput: But what are the risks\nduring production of nanomaterials?\n[WikiSearch(\"nanomaterial production\nrisks\")] Some nanomaterials may give\nrise to various kinds of lung damage.\nInput: Metformin is the first-line drug\nfor patients with type 2 diabetes and\nobesity.\nOutput: Metformin is the first-line drug\nfor [WikiSearch(\"Metformin first-line\ndrug\")] patients with type 2 diabetes\nand obesity.\nInput: x\nOutput:\nMachine Translation We use the following\nprompt for the machine translation tool:\nYour task is to complete a given piece\nof text by using a Machine Translation\nAPI.\nYou can do so by writing \"[MT(text)]\"\nwhere text is the text to be translated\ninto English.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0ddd5227-24e4-4355-8a83-fe978655dc05", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "73814699-921d-46e1-9447-10a607f71c8c": {"__data__": {"id_": "73814699-921d-46e1-9447-10a607f71c8c", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8690bb8c-7fda-4fdb-a6b2-488a83f15b8c", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "bd93f69046e9ff6db55b57293db7e78f6b15e6c59c8ab5e2f93665a67b928a55", "class_name": "RelatedNodeInfo"}}, "text": "Here are some examples:\nInput: He has published one book: O\nhomem suprimido (\u201cThe Supressed Man\u201d)\nOutput: He has published one book: O\nhomem suprimido [MT(O homem suprimido)]\n(\u201cThe Supressed Man\u201d)\nInput: In Morris de Jonge\u2019s Jeschuah,\nder klassische j\u00fcdische Mann, there is a\ndescription of a Jewish writer\nOutput: In Morris de Jonge\u2019s Jeschuah,\nder klassische j\u00fcdische Mann [MT(der\nklassische j\u00fcdische Mann)],", "mimetype": "text/plain", "start_char_idx": 801, "end_char_idx": 1211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0ddd5227-24e4-4355-8a83-fe978655dc05", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "09c6d98d-3aec-4021-af4c-a9fc01d3f94e": {"__data__": {"id_": "09c6d98d-3aec-4021-af4c-a9fc01d3f94e", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73814699-921d-46e1-9447-10a607f71c8c", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b6b4264ad3eaa2015a9391ef63599509a481d36329760e2b3a24af774517fcb5", "class_name": "RelatedNodeInfo"}}, "text": "there is a\ndescription of a Jewish writer\nInput: \u5357\u4eac\u9ad8\u6df3\u53bf\u4f4f\u623f\u548c\u57ce\u4e61\u5efa\u8bbe\u5c40\u57ce\u5e02\u65b0\n\u533a\u8bbe \u8ba1 a plane of reference Gaochun is\none of seven districts of the provincial\ncapital Nanjing\nOutput: [MT( \u5357\u4eac\u9ad8\u6df3\u53bf\u4f4f\u623f\u548c\u57ce\u4e61\u5efa\u8bbe\u5c40\u57ce\u5e02\u65b0\n\u533a\u8bbe \u8ba1 )] a plane of reference Gaochun is\none of seven districts of the provincial\ncapital Nanjing\nInput: x\nOutput:\nCalendar We use the following prompt for the\ncalendar tool:\nYour task is to add calls to a Calendar\nAPI to a piece of text. The API calls\nshould help you get information required\nto complete the text. You can call the\nAPI by writing \"[Calendar()]\" Here are\nsome examples of API calls:\nInput: Today is the first Friday of the\nyear.\nOutput: Today is the first [Calendar()]\nFriday of the year.", "mimetype": "text/plain", "start_char_idx": 1212, "end_char_idx": 1906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0ddd5227-24e4-4355-8a83-fe978655dc05", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "90321c20-150e-4464-afe4-b8dda897a38e": {"__data__": {"id_": "90321c20-150e-4464-afe4-b8dda897a38e", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}}, "text": "Input: But what are the risks during\nproduction of nanomaterials? Somenanomaterials may give rise to various\nkinds of lung damage.\nOutput: But what are the risks\nduring production of nanomaterials?\n[WikiSearch(\"nanomaterial production\nrisks\")] Some nanomaterials may give\nrise to various kinds of lung damage.\nInput: Metformin is the first-line drug\nfor patients with type 2 diabetes and\nobesity.\nOutput: Metformin is the first-line drug\nfor [WikiSearch(\"Metformin first-line\ndrug\")] patients with type 2 diabetes\nand obesity.\nInput: x\nOutput:\nMachine Translation We use the following\nprompt for the machine translation tool:\nYour task is to complete a given piece\nof text by using a Machine Translation\nAPI.\nYou can do so by writing \"[MT(text)]\"\nwhere text is the text to be translated\ninto English.\nHere are some examples:\nInput: He has published one book: O\nhomem suprimido (\u201cThe Supressed Man\u201d)\nOutput: He has published one book: O\nhomem suprimido [MT(O homem suprimido)]\n(\u201cThe Supressed Man\u201d)\nInput: In Morris de Jonge\u2019s Jeschuah,\nder klassische j\u00fcdische Mann, there is a\ndescription of a Jewish writer\nOutput: In Morris de Jonge\u2019s Jeschuah,\nder klassische j\u00fcdische Mann [MT(der\nklassische j\u00fcdische Mann)], there is a\ndescription of a Jewish writer\nInput: \u5357\u4eac\u9ad8\u6df3\u53bf\u4f4f\u623f\u548c\u57ce\u4e61\u5efa\u8bbe\u5c40\u57ce\u5e02\u65b0\n\u533a\u8bbe \u8ba1 a plane of reference Gaochun is\none of seven districts of the provincial\ncapital Nanjing\nOutput: [MT( \u5357\u4eac\u9ad8\u6df3\u53bf\u4f4f\u623f\u548c\u57ce\u4e61\u5efa\u8bbe\u5c40\u57ce\u5e02\u65b0\n\u533a\u8bbe \u8ba1 )] a plane of reference Gaochun is\none of seven districts of the provincial\ncapital Nanjing\nInput: x\nOutput:\nCalendar We use the following prompt for the\ncalendar tool:\nYour task is to add calls to a Calendar\nAPI to a piece of text.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0ddd5227-24e4-4355-8a83-fe978655dc05", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dc0e3adb-b3fa-464d-ba92-7bebe1ca53f1": {"__data__": {"id_": "dc0e3adb-b3fa-464d-ba92-7bebe1ca53f1", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90321c20-150e-4464-afe4-b8dda897a38e", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "d5717da25f692e5766484c78bd6775b6e7971795e2b55384c31dfd28f54b514d", "class_name": "RelatedNodeInfo"}}, "text": "The API calls\nshould help you get information required\nto complete the text. You can call the\nAPI by writing \"[Calendar()]\" Here are\nsome examples of API calls:\nInput: Today is the first Friday of the\nyear.\nOutput: Today is the first [Calendar()]\nFriday of the year.", "mimetype": "text/plain", "start_char_idx": 1640, "end_char_idx": 1906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0ddd5227-24e4-4355-8a83-fe978655dc05", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0ddd5227-24e4-4355-8a83-fe978655dc05": {"__data__": {"id_": "0ddd5227-24e4-4355-8a83-fe978655dc05", "embedding": null, "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "a8295c83b23d880876cd3b8f22c983c85cd5f06b342a4c5b08282f9436620e56", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "ea06dfa5d2fd79a06ab87d9162536e756db2a7750b9a95bc16552ab356ca313b", "class_name": "RelatedNodeInfo"}}, "text": "Input: But what are the risks during\nproduction of nanomaterials? Somenanomaterials may give rise to various\nkinds of lung damage.\nOutput: But what are the risks\nduring production of nanomaterials?\n[WikiSearch(\"nanomaterial production\nrisks\")] Some nanomaterials may give\nrise to various kinds of lung damage.\nInput: Metformin is the first-line drug\nfor patients with type 2 diabetes and\nobesity.\nOutput: Metformin is the first-line drug\nfor [WikiSearch(\"Metformin first-line\ndrug\")] patients with type 2 diabetes\nand obesity.\nInput: x\nOutput:\nMachine Translation We use the following\nprompt for the machine translation tool:\nYour task is to complete a given piece\nof text by using a Machine Translation\nAPI.\nYou can do so by writing \"[MT(text)]\"\nwhere text is the text to be translated\ninto English.\nHere are some examples:\nInput: He has published one book: O\nhomem suprimido (\u201cThe Supressed Man\u201d)\nOutput: He has published one book: O\nhomem suprimido [MT(O homem suprimido)]\n(\u201cThe Supressed Man\u201d)\nInput: In Morris de Jonge\u2019s Jeschuah,\nder klassische j\u00fcdische Mann, there is a\ndescription of a Jewish writer\nOutput: In Morris de Jonge\u2019s Jeschuah,\nder klassische j\u00fcdische Mann [MT(der\nklassische j\u00fcdische Mann)], there is a\ndescription of a Jewish writer\nInput: \u5357\u4eac\u9ad8\u6df3\u53bf\u4f4f\u623f\u548c\u57ce\u4e61\u5efa\u8bbe\u5c40\u57ce\u5e02\u65b0\n\u533a\u8bbe \u8ba1 a plane of reference Gaochun is\none of seven districts of the provincial\ncapital Nanjing\nOutput: [MT( \u5357\u4eac\u9ad8\u6df3\u53bf\u4f4f\u623f\u548c\u57ce\u4e61\u5efa\u8bbe\u5c40\u57ce\u5e02\u65b0\n\u533a\u8bbe \u8ba1 )] a plane of reference Gaochun is\none of seven districts of the provincial\ncapital Nanjing\nInput: x\nOutput:\nCalendar We use the following prompt for the\ncalendar tool:\nYour task is to add calls to a Calendar\nAPI to a piece of text. The API calls\nshould help you get information required\nto complete the text. You can call the\nAPI by writing \"[Calendar()]\" Here are\nsome examples of API calls:\nInput: Today is the first Friday of the\nyear.\nOutput: Today is the first [Calendar()]\nFriday of the year.", "mimetype": "text/plain", "start_char_idx": 1987, "end_char_idx": 3893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0ddd5227-24e4-4355-8a83-fe978655dc05", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1be362e8-dcfa-479c-ae27-14890c16b8d1": {"__data__": {"id_": "1be362e8-dcfa-479c-ae27-14890c16b8d1", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}}, "text": "Input: The president of the United\nStates is Joe Biden.\nOutput: The president of the United\nStates is [Calendar()] Joe Biden.\nInput: The current day of the week is\nWednesday.\nOutput: The current day of the week is\n[Calendar()] Wednesday.\nInput: The number of days from now until\nChristmas is 30.\nOutput: The number of days from now\nuntil Christmas is [Calendar()] 30.\nInput: The store is never open on the\nweekend, so today it is closed.\nOutput: The store is never open on the\nweekend, so today [Calendar()] it is\nclosed.\nInput: x\nOutput:\nB Toolformer Training\nWe use up to 25k examples per API. Max sequence\nlength 1,024. Effective batch size of 128. All mod-\nels are trained using DeepSpeed\u2019s ZeRO-3 (Rasley\net al., 2020). We used 8 NVIDIA A100 40GB\nGPUs with BF16.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "80982f31-52e8-44b1-a262-88cec327b327": {"__data__": {"id_": "80982f31-52e8-44b1-a262-88cec327b327", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1be362e8-dcfa-479c-ae27-14890c16b8d1", "node_type": "1", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "288f088b2820186877ae61e9c281c5f399eaa682d15e142a4e0b8a85b086da39", "class_name": "RelatedNodeInfo"}}, "text": "We used 8 NVIDIA A100 40GB\nGPUs with BF16. Training up to 2k steps, where\nwe evaluate PPL on a small development set from\nCCNet containing 1,000 examples every 500 steps.\nWe pick the checkpoint that performs best.\nC Zero-Shot Prompts\nC.1 LAMA and T EMPLAMA\nFor both LAMA and TEMPLAMA , given an input\ntextx, we use the following prompt: Please\ncomplete the following text so\nthat it is factually correct: x.\nC.2 Math Benchmarks\nFor all math benchmarks, given a context xand\na question q, our prompt is: x qThe answer\nis.\nC.3 Question Answering\nFor all question answering datasets, including\nDATESET , we simply pre\ufb01x the question with\nAnswer the following question: . We\nappend a question mark if the question does not\nalready end with one.\nC.4 Multilingual Question Answering\nFor MLQA,", "mimetype": "text/plain", "start_char_idx": 725, "end_char_idx": 1511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "60eca99b-1b30-4362-9a03-e1a2aa788d03": {"__data__": {"id_": "60eca99b-1b30-4362-9a03-e1a2aa788d03", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80982f31-52e8-44b1-a262-88cec327b327", "node_type": "1", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b7aa9f238926bce2521c2bf0847bb7706ca3f4e122061c1b1ac07e85b183d053", "class_name": "RelatedNodeInfo"}}, "text": "C.4 Multilingual Question Answering\nFor MLQA, given a context xand a ques-\ntion q, our prompt is: Your task isTemplate Size\nHow many days {ago was, are there until}\n{past_date ,future_date} ?400\nWhat {day of the week, day of the month, month,\nyear} was it ( current_date \u2013 past_date ) {days,\nweeks, months, years} ago?800\nWhat {day of the week, day of the month, month,\nyear} will it be in ( future_date \u2013 current_date )\ndays?800\nWhat day of the week {is, was} it on { past_date ,\nfuture_date} ?400\nWhat {day of the week, day of the month, month,\nyear} {is, was} it {the day before yesterday, yes-\nterday, today, tomorrow, the day after tomorrow}?4,000\nWhat {day of the week, day of the month, month}\n{is, was}holiday this year?1,", "mimetype": "text/plain", "start_char_idx": 1466, "end_char_idx": 2196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d2c3d590-f462-453f-97d6-6a4255ab38f5": {"__data__": {"id_": "d2c3d590-f462-453f-97d6-6a4255ab38f5", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60eca99b-1b30-4362-9a03-e1a2aa788d03", "node_type": "1", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "fd301e45da80b1d0ab70f864f57e79fff1c79370c54da2e2c1b8c35585e0ee54", "class_name": "RelatedNodeInfo"}}, "text": "day of the month, month}\n{is, was}holiday this year?1,800\nHow many {days, weeks, months, years} {ago\nwas, are there until} holiday this year?1,200\nTotal 9,400\nTable 11: Templates used to create D ATESET where\nacurrent_date is randomly selected. For each cur-\nrent_date , a random past_date andfuture_date is gen-\nerated and used to \ufb01ll each template, if relevant. The\nfederal holidays in the United States (e.g., Thanksgiv-\ning) were used in the templates involving holidays.\nto answer a question based on\nthe following paragraph: xNow\nanswer the following question in\nEnglish: q.\nD D ATESET\nDATESET is created by \ufb01rst randomly selecting 500\n\u201ccurrent dates\u201d.", "mimetype": "text/plain", "start_char_idx": 2142, "end_char_idx": 2800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4ea0a703-b576-4406-82a7-baa9ace90f19": {"__data__": {"id_": "4ea0a703-b576-4406-82a7-baa9ace90f19", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2c3d590-f462-453f-97d6-6a4255ab38f5", "node_type": "1", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "b350b598f111a3ce15c1090428334a2f9364fb6b1442b5aa2fd84675febdaf90", "class_name": "RelatedNodeInfo"}}, "text": "For each current date, another rela-\ntively past/future date is randomly selected within\na four-year range, and the two dates are used to \ufb01ll\nthe query templates in Table 11. An example of one\nsuch query using the \ufb01rst template would be, \u201cHow\nmany days ago was August 14, 2020?\u201d If called,\nthe Calendar tool would return the presumed cur-\nrent date (e.g., \u201cToday is Sunday, November 20,\n2020\u201d).", "mimetype": "text/plain", "start_char_idx": 2801, "end_char_idx": 3195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d858b5c1-e18e-4d79-b57c-34b1a758c173": {"__data__": {"id_": "d858b5c1-e18e-4d79-b57c-34b1a758c173", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}}, "text": "Input: The president of the United\nStates is Joe Biden.\nOutput: The president of the United\nStates is [Calendar()] Joe Biden.\nInput: The current day of the week is\nWednesday.\nOutput: The current day of the week is\n[Calendar()] Wednesday.\nInput: The number of days from now until\nChristmas is 30.\nOutput: The number of days from now\nuntil Christmas is [Calendar()] 30.\nInput: The store is never open on the\nweekend, so today it is closed.\nOutput: The store is never open on the\nweekend, so today [Calendar()] it is\nclosed.\nInput: x\nOutput:\nB Toolformer Training\nWe use up to 25k examples per API. Max sequence\nlength 1,024. Effective batch size of 128. All mod-\nels are trained using DeepSpeed\u2019s ZeRO-3 (Rasley\net al., 2020). We used 8 NVIDIA A100 40GB\nGPUs with BF16. Training up to 2k steps, where\nwe evaluate PPL on a small development set from\nCCNet containing 1,000 examples every 500 steps.\nWe pick the checkpoint that performs best.\nC Zero-Shot Prompts\nC.1 LAMA and T EMPLAMA\nFor both LAMA and TEMPLAMA , given an input\ntextx, we use the following prompt: Please\ncomplete the following text so\nthat it is factually correct: x.\nC.2 Math Benchmarks\nFor all math benchmarks, given a context xand\na question q, our prompt is: x qThe answer\nis.\nC.3 Question Answering\nFor all question answering datasets, including\nDATESET , we simply pre\ufb01x the question with\nAnswer the following question: . We\nappend a question mark if the question does not\nalready end with one.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d6e2c0a9-0d7a-425c-bc2e-8afebaa29d61": {"__data__": {"id_": "d6e2c0a9-0d7a-425c-bc2e-8afebaa29d61", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d858b5c1-e18e-4d79-b57c-34b1a758c173", "node_type": "1", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "373d39c4491d227a09056505d33a951772dc1eebf37d9691e9ec28208f2ab01a", "class_name": "RelatedNodeInfo"}}, "text": "We\nappend a question mark if the question does not\nalready end with one.\nC.4 Multilingual Question Answering\nFor MLQA, given a context xand a ques-\ntion q, our prompt is: Your task isTemplate Size\nHow many days {ago was, are there until}\n{past_date ,future_date} ?400\nWhat {day of the week, day of the month, month,\nyear} was it ( current_date \u2013 past_date ) {days,\nweeks, months, years} ago?800\nWhat {day of the week, day of the month, month,\nyear} will it be in ( future_date \u2013 current_date )\ndays?800\nWhat day of the week {is, was} it on { past_date ,\nfuture_date} ?400\nWhat {day of the week, day of the month, month,\nyear} {is, was} it {the day before yesterday, yes-\nterday, today, tomorrow, the day after tomorrow}?4,000\nWhat {day of the week, day of the month, month}\n{is, was}holiday this year?1,800\nHow many {days, weeks, months, years} {ago\nwas, are there until} holiday this year?1,200\nTotal 9,400\nTable 11: Templates used to create D ATESET where\nacurrent_date is randomly selected. For each cur-\nrent_date , a random past_date andfuture_date is gen-\nerated and used to \ufb01ll each template, if relevant. The\nfederal holidays in the United States (e.g., Thanksgiv-\ning) were used in the templates involving holidays.\nto answer a question based on\nthe following paragraph: xNow\nanswer the following question in\nEnglish: q.\nD D ATESET\nDATESET is created by \ufb01rst randomly selecting 500\n\u201ccurrent dates\u201d. For each current date, another rela-\ntively past/future date is randomly selected within\na four-year range, and the two dates are used to \ufb01ll\nthe query templates in Table 11.", "mimetype": "text/plain", "start_char_idx": 1393, "end_char_idx": 2975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6b2f3fbf-578c-4e5e-a1c2-88bf8b7692ba": {"__data__": {"id_": "6b2f3fbf-578c-4e5e-a1c2-88bf8b7692ba", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6e2c0a9-0d7a-425c-bc2e-8afebaa29d61", "node_type": "1", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "dc44022c0c47063c0006d553a1e6e807f8cc2ddf3df7787ce1c4f51f2df88c67", "class_name": "RelatedNodeInfo"}}, "text": "An example of one\nsuch query using the \ufb01rst template would be, \u201cHow\nmany days ago was August 14, 2020?\u201d If called,\nthe Calendar tool would return the presumed cur-\nrent date (e.g., \u201cToday is Sunday, November 20,\n2020\u201d).", "mimetype": "text/plain", "start_char_idx": 2976, "end_char_idx": 3195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "585ead16-a9f5-40ca-9469-4849dbb8ca42": {"__data__": {"id_": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "embedding": null, "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}, "hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "class_name": "RelatedNodeInfo"}}, "text": "Input: The president of the United\nStates is Joe Biden.\nOutput: The president of the United\nStates is [Calendar()] Joe Biden.\nInput: The current day of the week is\nWednesday.\nOutput: The current day of the week is\n[Calendar()] Wednesday.\nInput: The number of days from now until\nChristmas is 30.\nOutput: The number of days from now\nuntil Christmas is [Calendar()] 30.\nInput: The store is never open on the\nweekend, so today it is closed.\nOutput: The store is never open on the\nweekend, so today [Calendar()] it is\nclosed.\nInput: x\nOutput:\nB Toolformer Training\nWe use up to 25k examples per API. Max sequence\nlength 1,024. Effective batch size of 128. All mod-\nels are trained using DeepSpeed\u2019s ZeRO-3 (Rasley\net al., 2020). We used 8 NVIDIA A100 40GB\nGPUs with BF16. Training up to 2k steps, where\nwe evaluate PPL on a small development set from\nCCNet containing 1,000 examples every 500 steps.\nWe pick the checkpoint that performs best.\nC Zero-Shot Prompts\nC.1 LAMA and T EMPLAMA\nFor both LAMA and TEMPLAMA , given an input\ntextx, we use the following prompt: Please\ncomplete the following text so\nthat it is factually correct: x.\nC.2 Math Benchmarks\nFor all math benchmarks, given a context xand\na question q, our prompt is: x qThe answer\nis.\nC.3 Question Answering\nFor all question answering datasets, including\nDATESET , we simply pre\ufb01x the question with\nAnswer the following question: . We\nappend a question mark if the question does not\nalready end with one.\nC.4 Multilingual Question Answering\nFor MLQA, given a context xand a ques-\ntion q, our prompt is: Your task isTemplate Size\nHow many days {ago was, are there until}\n{past_date ,future_date} ?400\nWhat {day of the week, day of the month, month,\nyear} was it ( current_date \u2013 past_date ) {days,\nweeks, months, years} ago?800\nWhat {day of the week, day of the month, month,\nyear} will it be in ( future_date \u2013 current_date )\ndays?800\nWhat day of the week {is, was} it on { past_date ,\nfuture_date} ?400\nWhat {day of the week, day of the month, month,\nyear} {is, was} it {the day before yesterday, yes-\nterday, today, tomorrow, the day after tomorrow}?4,000\nWhat {day of the week, day of the month, month}\n{is, was}holiday this year?1,800\nHow many {days, weeks, months, years} {ago\nwas, are there until} holiday this year?1,200\nTotal 9,400\nTable 11: Templates used to create D ATESET where\nacurrent_date is randomly selected. For each cur-\nrent_date , a random past_date andfuture_date is gen-\nerated and used to \ufb01ll each template, if relevant. The\nfederal holidays in the United States (e.g., Thanksgiv-\ning) were used in the templates involving holidays.\nto answer a question based on\nthe following paragraph: xNow\nanswer the following question in\nEnglish: q.\nD D ATESET\nDATESET is created by \ufb01rst randomly selecting 500\n\u201ccurrent dates\u201d. For each current date, another rela-\ntively past/future date is randomly selected within\na four-year range, and the two dates are used to \ufb01ll\nthe query templates in Table 11. An example of one\nsuch query using the \ufb01rst template would be, \u201cHow\nmany days ago was August 14, 2020?\u201d If called,\nthe Calendar tool would return the presumed cur-\nrent date (e.g., \u201cToday is Sunday, November 20,\n2020\u201d).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "585ead16-a9f5-40ca-9469-4849dbb8ca42", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}}, "docstore/metadata": {"f68d545e-c482-4e5a-916a-b9e6c16a9f89": {"doc_hash": "dcc6f43df45bb78937d522cf21501ea5681bb6dab1765b6895728d2a8fae1d97", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "07629733-1cdf-4951-8385-d355097f1afc": {"doc_hash": "1b5ceabd2b91db9ea5ba094b85b3dbea2baedc3936921e360de28fc17f686e78", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "67d86419-c304-4f5f-bf65-b95021e9de5a": {"doc_hash": "adbf1bb12573abd0c53b2e0b42746a0eae3b8c70db756ac96b82bfc25e826453", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "41e36281-ebb1-409d-872e-2beda113f627": {"doc_hash": "c089e51f9a3ba4199d75ad4bf0ec24067b6068f33ad1c3c904bcd3efd663b1b9", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "902ae4f9-50dc-4a9a-987a-7e473bd1268b": {"doc_hash": "16039ecf85129de279a7e0838b673a31459736c65757a06cac416d5216ebd5f7", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "c9e7a1bb-b3dd-4ac2-a91b-1b4ca5c9ef24": {"doc_hash": "72f6d24c8cd18e9c731447b3a7f1c33b8d9b098d7eabac80f6e5962d3aaeca44", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "a080efe4-9814-4ab8-b4c2-14eae2c14a3e": {"doc_hash": "53889a8c7f2e133ca5c1b7a734b889d4199bcb18f620d5f9a9320467fcd3d8da", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "8237ce55-9fb6-4460-a67b-d57088343ac2": {"doc_hash": "16039ecf85129de279a7e0838b673a31459736c65757a06cac416d5216ebd5f7", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad": {"doc_hash": "9d82f7be555fa18ddb7087a884db320deef60a68c3650d5a14688e20e26ccb49", "ref_doc_id": "9f6b2fe2-dcff-4852-8128-be9219bbbe03"}, "5ef68241-0ade-4ed3-9c11-3cf0daa96848": {"doc_hash": "93c6a50e38dbf4694d920020fab7b12719de559dc62638a4a50e6e8ab18ce226", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "8589b113-0f51-41a6-a845-52c91baf0ace": {"doc_hash": "d8a133bfaf96f444205b75f70fe8426aa1ab898fdb2ea6a7615e160ca253699b", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "f3db2899-4098-49b2-b583-8563ba3a202c": {"doc_hash": "fe7d08e56aeae53e9301c7cccbcafd688075d1af46101f0f1bf434e0bef8a11b", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "50d10b41-b7f2-4af7-b035-75042552f2c6": {"doc_hash": "3b44d67014f106fbe1dd957e65fa7fd5819934076325ef8eb05a9c9ccf25bf44", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "2f2e7161-8da6-4cde-ad3a-c94df2bf01ad": {"doc_hash": "4f0fa375a646f5d20e2752aea6dbd3908fa341549a2d80bcd14ebce3c4582c3c", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "93973990-d406-4646-aeb3-2a788d56b9b5": {"doc_hash": "e2f585c353eb1ae465bb38ac194517860d103bf54b3ee56ff370b96082f50c61", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "977aef4a-99de-4a09-81e1-a66275c5a139": {"doc_hash": "e63c37237b8dd879ffbdba46b21c54ff5a1bb725d5313e2a41aa2102c497476a", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "099523d7-cc92-4292-8d64-e6361875b5f2": {"doc_hash": "eda7c493fad4ef5eaf862f4c94dafc389b741020bf9aab6277531a19964ba443", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "bb23a500-9b12-4d9c-ad23-5b0663f57562": {"doc_hash": "a31887aae52fe81478a7aff313e753af6a2dba26bc6bf274b8389b8ac11f3e74", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd": {"doc_hash": "14393ae97bb69e350ea1b5a2b265b8090b194596f0c908b0c712e1fb6006c0cf", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "6875ac13-4653-4014-8e95-42ee7d3888ec": {"doc_hash": "13fb1969f804e905fd308a7278a84568e786e677ec28a1f9bf47639f14a672a5", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "0f43e3b6-9c7d-4038-a451-5b795042e009": {"doc_hash": "bcd7cbeff36a3de2f79502686ae4555c1226295d42266eac61e2efe7ed39b9c7", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "34b6bed5-921a-4e1e-aaf7-5ca50f098761": {"doc_hash": "c0727a877790aa0e5e5c37e259fcbe74c4f8989625e947c46f670a309faf4160", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "22f394d8-1d10-43a1-aaca-3a52ee53aaea": {"doc_hash": "c0727a877790aa0e5e5c37e259fcbe74c4f8989625e947c46f670a309faf4160", "ref_doc_id": "1440ca32-6bad-42d3-bc57-57bf7af8d27c"}, "7847fadf-8f6a-4ebc-884f-f4a4a228c14d": {"doc_hash": "debf2a14ae2937ab024d7d8a994610a73ddb3acff225045b0707a3f80418c54a", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "4d69f0ae-68c7-45a0-9e82-afa38d368859": {"doc_hash": "db232f822895c2e68004ebec97b864c03ee1dabd2c7db8e4750ff39a3902d07e", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "2b9dc6cf-4ddf-4909-95cf-6c1b1cfe4364": {"doc_hash": "6361fbf864bfa5a16d45b30f3ea64f87c2a81a97df7e5f38dd6fe1604dea2316", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "78af0c97-03ab-4eaf-a74b-5e68db0bc36d": {"doc_hash": "967fe5f5423bddc764bcafb518a41b56b10782ee5daaa554328a9f22a4192887", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "e6cdec83-53d9-4eae-9a5f-f9580755b587": {"doc_hash": "d520514812640524f004c7f7fb18697b43c97392c458b5fb843d751d78e51879", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "6a3e6d03-96b5-4165-9cad-3463da6ea73f": {"doc_hash": "1f5cd1ce675851e0c3f69268de9df644a679be7751f8f9955f75feb2b27bb4fb", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "2edc18d3-cb56-4a31-87cd-ee8879699554": {"doc_hash": "f5b3a99bceb6607133f52c5a41e2299c3ff5e3a0403ceb237b9ddf9af6cee622", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "57d072df-0fcb-452a-a3ba-5e6cce39cec2": {"doc_hash": "2248ee9a02d5454adfadbe6f7332a44ac585c59e219c079e69cc1545ceb28fbc", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "94b5363c-adb9-48a2-ad23-4d5b827a0474": {"doc_hash": "cab46398b77448b717e949ebe83bb0babe51032c12e3be46f971cd90ff04c2bd", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb": {"doc_hash": "c47b0e55adf3dae08781a24c998439f1790e0edeba62d47d789e39e8b2790141", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "129d0d0c-2cec-4683-9586-7b963b83ff6a": {"doc_hash": "84556f0d6d1c5d8609ee060b4651fe7a84030a922c31c8405dcc5b762674123b", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "3576df63-cd81-4595-ab7c-31d3b85c2968": {"doc_hash": "b682d51873a2c40db461b52828bd2a959f73816b592c07067a8922b4e020f8f3", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "4a263539-eef7-4f63-97ef-1ed9520ca575": {"doc_hash": "d27df8385a84e80e364cce22de6184057afa41fd89a40a26497ecee9ac44f6e1", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "a85a5425-c7a6-4b7d-b54e-4dfb4c459381": {"doc_hash": "d27df8385a84e80e364cce22de6184057afa41fd89a40a26497ecee9ac44f6e1", "ref_doc_id": "509897da-3c37-4679-aaec-1ef93cb6ccff"}, "2593dd6c-afbc-4d63-ba3c-e3294e33b9fa": {"doc_hash": "d6eca8d3374dd021d9444ed8f7a392646b6ec1f7041cf3ce3d8e0edc4164bf8a", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "c2990faf-c06f-41ad-add2-163ff7cf180d": {"doc_hash": "3e095dceee63621051dc306a62134cfe74c36d0954bba36628ebb121a9d578c9", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "761989e8-0cfd-4b0b-8939-63d8cf81e5a3": {"doc_hash": "b60f12ba2f285d94e78c9afe52c4cf1b8c7c294802cc34289a3948d001b22811", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "7ec242e1-7b00-485a-aa65-f90dc1c3d3ab": {"doc_hash": "1214790120cbdc4fab39767c49c6f42fc6d8457e7780a4ea199636c0034509b5", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "61715869-a72a-428b-b0eb-99e08b4ba441": {"doc_hash": "8babc56037eb23912bfdf4834bd0fefaf80b9296ca4819646650d1f2d914d290", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "ef00f5f8-9ca0-4f0d-a12c-5ea1c399a65b": {"doc_hash": "40b547062322046c78bb640516207b0de7c81444ffef6470cae16823c08add67", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "1c9ad27a-bed9-42e3-aa8c-b67b552469da": {"doc_hash": "52227a2443ab7fe49b5257fd98e6d09a2a2ddd65ac65c12bd4bee57e8a43ac2f", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "80771438-6266-4cac-9ba7-9a73dcce8afb": {"doc_hash": "bf6ac36c55b945fe0395a4a5aa957b8a1bee9f0023ab1ddf7a29cebcd4ea9a3f", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "bf8a0989-52de-4c7a-9846-6093baf1a5e2": {"doc_hash": "6b0c90fae712d97b01fed2b79bc61fe6e947fa02685b98079de9e07e6e7fc783", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "60ddf73f-8559-43c6-8e99-f698534cebfb": {"doc_hash": "47844d314045ab876e4fc69577206d9f37b2051f778e5fe57e7fc18e120c2273", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "0b2eeaaa-4f2d-4ade-a4cb-54e2f0638357": {"doc_hash": "6913f6306f7531075a77248d71e9eaf4c1d101ac7fe20af81e36b19c277f9b0b", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "e2c92540-a919-42bf-8bab-e6d6c44d4218": {"doc_hash": "da35a5b8e8b689c7d3b114284eae0e79b9fa244f0e266f0e115668cf0b6afe77", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "5b5db312-c332-4b4d-96f2-bf58abaf6547": {"doc_hash": "da35a5b8e8b689c7d3b114284eae0e79b9fa244f0e266f0e115668cf0b6afe77", "ref_doc_id": "dae453b2-ffcb-4225-a910-9c60ebcd1490"}, "be4ee63a-e796-47b1-aa18-2b85313f38f9": {"doc_hash": "6d24cfd76c614cb4e11bbc1b2eae2484ae2bb391c824e1e709b6e404fe8b1b35", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "337acb79-b374-4c66-aa4e-63fa7c9e8d88": {"doc_hash": "c5508d09fcea32d1be25f78e77f63b12bc35d9f4778e6b589b695f7f34c93648", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "995764a7-21bd-4b20-b213-9f3c95a8eca8": {"doc_hash": "008a86ceabfe70525f3a9a472afaeebb7351d3187d951444b79192056cd8009b", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "6a8e3c9f-e1e9-4295-a76d-d33970774e95": {"doc_hash": "228b6f4bd1edbbc1b785aa486abf6099b2915ac051d900cf123aa2d5bd171695", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "7d425980-fabf-4504-b327-179f8aad34ad": {"doc_hash": "e1da473799edd4d64ce1be398cc7b8fa992669b6045d26ef29ea5a5dadba750f", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "50400486-13fd-4062-a0aa-87b26584b149": {"doc_hash": "3cad198a009ac212526b5e3bd3ed32b49ecb67b32970788caa3f5dbb0582bd37", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "2fa245c0-7d2e-42c5-9a38-55eadeb7b48a": {"doc_hash": "8bbae2c2f4f91b5719dbdea7884c69f92097d3b830c5228dd0a4a38cd5de5950", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "ddd3fe98-9660-421d-ae1c-923b8f3bdfaf": {"doc_hash": "b5ff90dcf4fcde700270a16f1f7869dd562c8d9c461dbcd659d1a452b8ed548b", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "0847deed-64a5-4f45-a3b9-aee7ab6d68e3": {"doc_hash": "3cad198a009ac212526b5e3bd3ed32b49ecb67b32970788caa3f5dbb0582bd37", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "f3460b8e-fc3d-4014-ae2d-e0d1524d9946": {"doc_hash": "42ef70ae579b270e489fc930ba168597b6ce26f4ffd4f0a5a486340cd5b4c7d0", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "6d65ecac-3a61-4f0b-9085-b99766b5c037": {"doc_hash": "7898b18607e432f12d91dfcc7942f223f84eaf1b1419670742812ceeb9ddb254", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "f2d1ed81-4f9b-4976-bb7c-c442c1fe8d46": {"doc_hash": "7898b18607e432f12d91dfcc7942f223f84eaf1b1419670742812ceeb9ddb254", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "332430a4-7156-4128-acd4-5e75f80349ca": {"doc_hash": "7898b18607e432f12d91dfcc7942f223f84eaf1b1419670742812ceeb9ddb254", "ref_doc_id": "25d7f236-75ea-4f82-a8ab-3678082838cd"}, "c99dc81c-e321-4622-9cba-e740091c4d8b": {"doc_hash": "8c43b3937165200fca8e5451451f817e5fcbfbdcd5275b2432d0efbd997e6e6f", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "302430ba-129e-4c83-a3d8-978c9203d6b9": {"doc_hash": "7c0639c02ae810e36e16820ed313b76a03a4446e8b3cc76398c04c1200491191", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "3d65f2c3-efa3-44a9-9482-a99f34211d69": {"doc_hash": "a1d478e8112340ffe73081ad3ac5956d4ee2226f98725978aff8d32f32f3521a", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "f2d71a6b-4eca-42bc-9ff3-38dfb02ad177": {"doc_hash": "affc29f70037084d0bd7fdc09de3613df42fcd9c61067fedcdeaf3905ad1c633", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "a0d45704-53ac-496a-8d6d-1976876b6164": {"doc_hash": "b8cd111f800d7d681536c93fa5839f48a0a963687454d9594aa48339ea3df2dc", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "e751c095-4fa8-4db4-8364-eba8859af65f": {"doc_hash": "7101998647e75be18e3b0984bc049b83bdbc08237474a65d3286f71e497e2a1a", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "795e905c-295f-424a-ba98-5c96747b8b6d": {"doc_hash": "853e3f0dc75789735041b9d6a481ebed9aff8f129cd1f06421430a317a5b00f1", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "b5f24363-c544-4756-8b55-7354a4f4e2e0": {"doc_hash": "458a9373f8f60a980d55eb3714ecd1c3a54245f3af97f267f8a05b6edef39046", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "44d6e092-beab-4ecf-9e69-651c9fc1b4f5": {"doc_hash": "1e14bd1a910cca97af0c60d92ba1a4ff6698a6cb5c6276302f2abdd1ef2a23ab", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "42310718-2ee7-429d-a1d2-8098f04b1da0": {"doc_hash": "2f1a1058e6808364264361d1a5895ba44fd74120c2abb03526a1cafce886d2df", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "f1eb5da8-ba01-4c21-a8a1-af9d42f1465c": {"doc_hash": "e2d189afdc28bd7a09a0e57d7813afccfcadb09a80191d1782fba7f82f3509ed", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "a1c32316-c73f-462f-8d87-e892b1dcf286": {"doc_hash": "dd114abf84af38191912ec5310c28943c5bc95e4e47dba6c49b398dc5f977bea", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "5e9bd650-fe88-42ca-86d3-39af7137ba47": {"doc_hash": "14c8aac277bae66b83786c04f19c178ebba177f2191cf2ffb36f9a6472f5ad25", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "871a8901-e6c4-4ba9-9040-a15ca2a4e017": {"doc_hash": "14c8aac277bae66b83786c04f19c178ebba177f2191cf2ffb36f9a6472f5ad25", "ref_doc_id": "c1af666f-893f-4303-b5e4-839f3b23f880"}, "faae4f59-5c3f-451c-810b-cd8fc391ab93": {"doc_hash": "0a88ab3a59739fed57d47ea3643047c9b494aa2457c287f9a3c447e8b310384f", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "3ea17869-057f-4876-ae38-548cfd0ff08c": {"doc_hash": "a77fde60d080e64221fe13ff65a8aca60cc55a32aba113d9af3bd860a5565dfd", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "a3d30d56-d6d2-49e7-9034-279d0114b5d4": {"doc_hash": "8bd254d9b93ae1709dd95096b246944e2b58b275007ae7359ebba11d3bf2c471", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "d75fbf83-be82-413d-9e1a-d1dbb41e3583": {"doc_hash": "a0492352088407f2d3bc3a3762c8fbaa2f14e0641758d733ab78b6a8be2f0374", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "a9676371-9f4e-4bf8-bf99-e7a7eb202b39": {"doc_hash": "071ea1b26968bbd836e91b68f56da351280cddf5c03b5a7c7c3425f5bb62e6e5", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "4ed49efa-9792-42d5-96d0-4ab1ba720141": {"doc_hash": "294bbacf36248bda2a7db1d211165d07e105b51767303cb1e3abff5116fedb11", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "4ef7376e-db57-45d0-904f-5863eb3ab4c3": {"doc_hash": "e31f70a704449a67fa18e40cb23165edf83a276074905a3d15011a1805031a55", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "de6dfcb6-a900-463d-8678-200703214071": {"doc_hash": "5ad7373b1c1180bd3a788b8fc4248eba98dd9c178c7707a8d3c58eb1bf1aa0e1", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "753ecff5-8408-47de-a3a6-13523cec832e": {"doc_hash": "200945f05098e176fbb886c7bc1864de005b413b3477296ac51addbcd9dc324a", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "09e82015-23ed-49a3-a531-324aa52d9771": {"doc_hash": "845f455c25463c163f86e97a5e88036c804a10a564e02eae5f73c9ff022ef853", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "d81f996d-8944-4161-b284-8f7a23244116": {"doc_hash": "7a3d3004cd7bdd6872054bca6fc1ccb704a2dc05bd29ea1067e45a15077351cb", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "7bd9332d-eee2-4f40-8c93-49cd8ee514e1": {"doc_hash": "2c26fd3172c7ac72e36354d51b29038581360668a9544dbd851ffaa737b0591f", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "68810082-d630-4409-8467-dde69491161e": {"doc_hash": "33dc9007b0c37d6ad6069a45e5807cf97472056cbd0353eb8e2ba6bd205e1e06", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78": {"doc_hash": "33dc9007b0c37d6ad6069a45e5807cf97472056cbd0353eb8e2ba6bd205e1e06", "ref_doc_id": "45ae2e2b-0114-4c65-8ac8-00382eabcbb6"}, "b51e7f82-20ca-410e-a04a-ef3da9d6935a": {"doc_hash": "a4a1006bce6bc04d13ffb84ada73d8eace9ab9319d51cf5ecba2d37fd4412cd0", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "d4f8d082-8d58-4aff-b361-76984409bf9a": {"doc_hash": "308674f7cee89a6b5d5c6b5686a904c486c1b03e0485e4314d537a10dd37ec37", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "f7f4c9b2-89bf-42e5-92bf-cd2c785e5fe1": {"doc_hash": "d9d9f5a4fd5c9e89f005377cb1540e48497d22c24805a199bcb8dad1c569486e", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "d4f6dcf7-e9af-4fce-8ffa-c86a5e6375f9": {"doc_hash": "cb631cc83ad9e9e3f40cb9a3819285d752109d80d3da1a95a1e6a6a9410c8791", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "1929dfa3-53fc-4746-900d-4373d0bc5d6c": {"doc_hash": "baabb69d727b380e7f89ab7a8a08d670c646c1849bcf00579544db3a271e950f", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "6b040423-74e1-461e-a5bc-43b5fbc4ef64": {"doc_hash": "282ec11a83768cba31e61b4b9a0af61c2a19afc3e77c310ef05cc35630743411", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "f3f0d4f0-e794-49a6-b9f4-8c7da98c7c47": {"doc_hash": "e7104897fe6110e42c3989d45bf19cb256e28c85f6f9ae1705438797880f5b87", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "239505fb-6f12-4f51-bc31-00e4ea69afc5": {"doc_hash": "24cdc5fa78709aa0516112f11d697836a30ae1c324724a7b2d65b282643d1407", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "df608f7b-d033-4d87-8a5c-ac945a2a3e4c": {"doc_hash": "162055f1ef464b09b941a8c831a30f640fc594e6d42ffbc23ebc069b45276ed7", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "fc8231d2-31ec-4cfb-82b6-288b9c53fa42": {"doc_hash": "713c6e770206493e0c36f4b22aafcfef37e31e83fa9a29aa78616d4b61ef40cf", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "fb577c4f-c53a-42d4-b33d-2f05f9ce9043": {"doc_hash": "82ed05993bf4f7ba32624c5de3e04c9d3c5db81ac6f70bddc10b099a614264dc", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "1965f967-f828-4665-99fc-22f6d7eed72e": {"doc_hash": "eff5af3bcedd084f90163cdf7d4e709cdadd75b2a13f1c450f0096d3ce72bcb6", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "158f4f7e-efd5-4d7c-9156-a326e9b5cf3f": {"doc_hash": "e6ba8aa4cfaed07e68a42da12a2d5e07838415cff5249ca9b82ea5f536baeb1a", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "eb7b863d-0d80-467a-a908-12ae47d4c997": {"doc_hash": "e6ba8aa4cfaed07e68a42da12a2d5e07838415cff5249ca9b82ea5f536baeb1a", "ref_doc_id": "aacad45b-e004-4352-b686-0d5101ffc1c8"}, "dac5b250-1ff5-4661-b4d1-78cc80446339": {"doc_hash": "e1f4a920e4230e35b5cbc023f0615b7c0cd425014858ccfc4e923b6c7bc2c74d", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "018da332-d74b-4268-a274-6aa8a1853e0d": {"doc_hash": "c345bf2e71e168378ef4790b85a9f16cabf4b84377987e7f3c79216c10de31be", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "cd564975-4346-4c89-aa8b-00d10dc45fce": {"doc_hash": "11b29d5d25400b7718f11db2888ab53b379af5503179bb2ccb2a38f75ad04ae5", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "c2cc36b0-acc8-4468-ab38-576f2984ef74": {"doc_hash": "6c989c4ca18e1356184416d231a015b0fa66045ef26230409c47d98e086818d4", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "e6cb6fd8-1f23-4ae9-8fbc-5f3c16ff0017": {"doc_hash": "688b44677f2b65a6fdb587582d007b69420b9ed116e26ccfd8857d49d78e00ac", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "9e8b6760-8e18-4c71-a410-160ba3aabe59": {"doc_hash": "ccbdab0fe14812f05f85c8007560676bb00dc394547f1c57b5014d19799b88e7", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "ece5763f-7539-4d9c-ba7d-643efb434193": {"doc_hash": "4ab00d99bd641e7e6896a1a052760f7f4906a7b89e754f31b2774fa6663f94ed", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "2a308913-b3f8-40cd-916a-51a7c75994ce": {"doc_hash": "0a92dce0ddb3cc993b64298ad37006f9c9d1c872799a8d07001d02ad5d31abe3", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "379c988d-c250-4ee8-8034-85817c6f7d96": {"doc_hash": "688b44677f2b65a6fdb587582d007b69420b9ed116e26ccfd8857d49d78e00ac", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "d90a490c-e93d-4e21-b513-54cb95e8dca1": {"doc_hash": "84cd85e5f4e16242faf8ce1a10f7f8821c132ea90e6df1cdf35676e1900a7373", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "d33d77b1-3bfd-4306-b9fe-6dad4bcb3041": {"doc_hash": "48590e4cde27863d7c6c8855c634084c71bcccf53e99217625f0d83ee6e74a24", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "bced9eae-fe6d-48cd-abe4-47907e9cebd1": {"doc_hash": "48590e4cde27863d7c6c8855c634084c71bcccf53e99217625f0d83ee6e74a24", "ref_doc_id": "f33d1139-924b-4245-923c-ad7f213b19e1"}, "0b0bd11e-5271-49b2-bb51-5894086de834": {"doc_hash": "70fa1b96641a6713e2562868e4da3dd7042bf5fc10583ff898ae0acf71b72f1a", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "94883fb6-7b33-4212-8e1c-6e87aecbe44d": {"doc_hash": "2b7e0e7f6ffdfe090c1f32de75e1642e10102be094df6132affa0b0fdc9c7b4a", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "7c173b0c-9ccc-47e1-bbf1-63cf44c89988": {"doc_hash": "7cc5cec44f603267753bec7be3085ff1e22a5d5b4f38274f46469c98d97dc6e7", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "9ad51a02-8bff-4af2-bf92-a9ad6c14736e": {"doc_hash": "8b56c2a49c9f13b942573e48ff63205f33606c93986dd120fda8a9a4e03fd054", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "2162c6c6-34c9-460c-9ac0-95708e608cf0": {"doc_hash": "93b5c3f9fc8c65e0695ae72ad23dffedb01199a5a07e277cd70dc897e3cafce2", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "be169577-b256-46a5-b678-d92a95f0bbc8": {"doc_hash": "b6509a146854baf31c1d0a21ef88bcb6b310b5bb2c1febb32867de0dd97b61ba", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "eb8cdbdd-6831-41d2-8afb-f9c6aced8c20": {"doc_hash": "4fc299d7cef82e6ef484c03d4dd3a0000c4d3d0433900dbc537d38f79f454320", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "151b5f79-30f0-42d5-82a7-42d8963235be": {"doc_hash": "7a35dac55ee51f638786a0a7ab6ca4ce2a31af4c80589c41398d9e1d2e2ede0f", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "70c180ef-78bc-4667-bbf1-1f108169db8c": {"doc_hash": "2acaedad2580157a7797cda9594941440a50b3dbcc41687cc8bcb4b6fc776dfb", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "bf238218-edeb-48f4-9ab9-e375dcf874cb": {"doc_hash": "93b5c3f9fc8c65e0695ae72ad23dffedb01199a5a07e277cd70dc897e3cafce2", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "f4cfe19c-47e4-4de2-a5fc-b76c3edbab7d": {"doc_hash": "f202673fec140656a39457ef7dfc525886d460a93bb19c77b43cc83b7d60d7e3", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "cdccb3dd-a534-4681-ad6a-34f5942d4535": {"doc_hash": "2422aa43f2f197d90cb6fad8b03426244978941500bf7608e487165ac688a347", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "981488e8-1b61-4e74-9663-047cfc7d94ad": {"doc_hash": "25a5a15c51ed3afebbe856a389224d521372947266bfb39e80819fad260d48b9", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "9154fb5a-518d-4931-9581-001103245664": {"doc_hash": "cd98190333bc0fa86fd824adabee6fb1762c153427a951f8377e7070eff9aa25", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e": {"doc_hash": "7699aaa03712e68e1562460c7946ebdf37a800152846403f6810b93fbd4ea18e", "ref_doc_id": "5ee0818f-da5d-499f-b86a-4e89320b872a"}, "a2926084-2577-4d66-84be-054d88ed7c4e": {"doc_hash": "e7bca7632d3764226c767be941e5618c40a0e2b6a2ccd33fe6224d152bf90319", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "1419fb3a-983e-425b-bd49-681112be46bc": {"doc_hash": "a97649458779b48f8ba60e337a52cfd3d7c6bf48c193a29b44156ea4925f8d56", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "4c52314f-eba4-4286-aedd-9974022e673d": {"doc_hash": "6bdbe4447f5684451bda90c2a0993a3c6709bd641d75025270828f63a368689e", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "a86cb78c-b2bf-4d49-9312-c04a30768fb3": {"doc_hash": "27d3f40c671577c479745d2063d22b4900a91d94ac25e9ac96f85b6e3f0ffa1c", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "7cbb4a13-98bf-457f-9ab4-56957dcc8450": {"doc_hash": "cde1175a5f8607e9b9252925e0b32dae0ed6a5d82705511a9707c2511351a437", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "e89e7795-feb3-47da-8658-cb6d781123a8": {"doc_hash": "3766f1a3aa8ed6675e5a1b1d6a63c1dec889b9a9898b6fe939b90531bafc0b69", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "38ecae37-17b7-4d56-9a30-b567e33e77f3": {"doc_hash": "f53df5afed3ff201e85f9973d190ec3d64fbc000592a7c631d1ae23001cf6053", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "541f2839-1cd8-424d-89b0-a9e8e7541585": {"doc_hash": "d40735b7d77fc5acbe374de3fea35fde74009b07aedbac3c288b06adc2cb2f27", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "9f807aaa-6090-46c1-bcab-918f0c0bc976": {"doc_hash": "4b516dc67a174b102d1b592962fae0a85c798e8d5a22e429b83521fbcc98bb5f", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "aace8115-b0fd-4020-8ee9-b0a0efedbd9d": {"doc_hash": "95ba70c0b485ea9ba7f26f0983b31db7555280a91de6b9b1491bafbd9288e8bd", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "401817c7-3efe-4d45-ac61-3f01f236e016": {"doc_hash": "a372f9136d847c9e29318baf24eb43d7a3c24b8254f218d8eb6e6aaaaea17cb2", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "2f3ace41-d30d-434b-ad89-9fb860e0c576": {"doc_hash": "41a1a653d04086770eaebf242ab8b9d66585b53d70c4c5a5521261a0217d150e", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "5969b090-d2b3-49b1-8e81-97b43409490c": {"doc_hash": "14fa00ab5d67aa276d93fc842a4347f7a75bd5efafce02050dfaccda28accede", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "a3b606d1-4943-490d-b1b4-853427f660df": {"doc_hash": "41a1a653d04086770eaebf242ab8b9d66585b53d70c4c5a5521261a0217d150e", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "024b589f-c94e-495c-971d-a4e679588e40": {"doc_hash": "ad17a18b7ecd5218e7c518abb1d35a0baea4c689beddc3739171f20ba59fb498", "ref_doc_id": "0deb0246-7bf1-4b84-a5c7-03ac11e02509"}, "03c3a17b-bcf4-497a-9262-ace694e72ab8": {"doc_hash": "dc704689223d9a9a160c09229e53f97842279f746b195ea577255caed17566c4", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "6af94997-024e-4f57-8226-a620fd5cc466": {"doc_hash": "9c956f76cb04552105abcf372b30c6e01018e1f4fc792168b500c1895cb899a8", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "e905615d-0fad-47ce-884e-24ab96caf5ed": {"doc_hash": "68eb07e42663bf6dfe32fdd4e29103e83ef4211ab40ceee510eb619406e5cab2", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "28b6c80a-4055-46f8-8c43-33316b6cba34": {"doc_hash": "324dfdb9e1b39ea43212a878bc883596a8b6f76993d12af64a3dc174f76cec57", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "cb7f1c57-41a2-48c5-962e-c3d19582ceda": {"doc_hash": "6e73692021669a761dcf28322b5e76c628984b3a72e5d9b4d288b7b164f9bc4d", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "5f9b0d66-c631-496b-8bf9-68b1c0f6e895": {"doc_hash": "2030bd9637638cc97837b850aedfd4f1e16ea2503c47df4e63518bb06715c3a5", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "835e41bc-8672-4195-9e41-e2bc5633293c": {"doc_hash": "1f515cb638d4d296cde64a65aecbf252e9df84e742af47bd1fe25813bc5f4907", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "8d460469-f323-42c0-98a0-9661cb23ce93": {"doc_hash": "43ff7b99aaf8bc3b12d9eb46443b94d5c0881b2908554004296c8f8985f634d2", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "3b7b64a8-0c99-4565-b9ef-b2275e598b3a": {"doc_hash": "51cc11f44b018c9ee7dc6dceda91cef6d0e3853ee3257453585265d68bfd2a15", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "c7de47ce-2321-476f-b53a-d8475e45fce2": {"doc_hash": "bc76306b84912c9bd9058b50e2ae9613b904f90d5aacea7e94fa62cd54d85013", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "d1fd653e-d01a-40b8-88a6-74d87f5ca3e8": {"doc_hash": "c4e33fa178e7bf811bb0ac55a73dee109de1a2fdd60c9dfc15224421f176879d", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "82cb1f04-2a5d-48f9-8427-7dbea46744c4": {"doc_hash": "d07113d18de99f1d964e304b1fbf0f3b62597790745b37cb91a26e8c7ce243c3", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "c4c4e23f-bd0d-4315-a21c-a1bc18edcd4b": {"doc_hash": "e3930ed8d35237262e4e3cab325870e77f0cad1eaf96283d0ae032cf9780c279", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "eb5b4321-6ca3-405a-8e5b-fb4e20fcbf2d": {"doc_hash": "493ce048805f9dea8a284e3f3eed04dedffc1953ff5a33aea6359dc95a91f863", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "a2de968d-c52d-4aaf-a0dd-7caa18915893": {"doc_hash": "21ca03b80838f4fbf734d5646d6a3df9f76ce12e9731879e1411d6902a5cb962", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "6f69940f-494d-475a-9735-1fe42ddf93ea": {"doc_hash": "718b233bb68a2add7777beefc8a7ec29e61b598c7d455c1a5a12c71628bfe42c", "ref_doc_id": "af26339c-434b-441a-ad5c-2d00a61878ec"}, "bed28d07-8677-4c0e-82bd-d666cd7ca910": {"doc_hash": "2741d24c4d30925f97c72e40ef5ed5607165f79bb1ca5e991db873111a62fc7b", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "8372e903-14c0-42e6-a54a-44639d2210c6": {"doc_hash": "982dd489f8296b95bf5109eb39f59bfda34f2ceb946bba064b8b05c0ce3655ae", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "e1ae3d8c-a53f-4d2d-a1de-ecee4e3357e5": {"doc_hash": "8a21055d69f0d57048a4055bd057ccf143d6f001c7f4f15edda0c22564280249", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "aa942416-de63-44c3-bf82-40e65eafd1cf": {"doc_hash": "3b0f861d0cbc82b384b35336d2e022e9c29d4a36605c404099cc6b2f36869493", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "7cfa7e21-001a-4f83-a5ac-7a708e760dcc": {"doc_hash": "7e091bbf584b03a3cc6dfd38f6a44c14b7545b787898fc03cba75501522a74d0", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "d5dc6a7d-c845-481d-a7f2-51d4aadc2120": {"doc_hash": "b0960ac1c206e3ce04b6aa5b357008557835cdf0f1aae382f017e2b995d2488b", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "195ed4a0-229d-4464-8682-6b3401b8b754": {"doc_hash": "d315a94f91842cc287a509a567cdae45142256f623dab8d32e8c303040fafd35", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "ff267bdf-ef1b-49c3-849b-9788c3ba6b1f": {"doc_hash": "2712224fd650447785ce0bf667409c1fd7f931bce4e14d8be83608c924d65504", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "cf797a81-a0fc-49b0-8178-33f20d0bc830": {"doc_hash": "078b17171d45a76b32ccff5195893265c85e7057411651e18d7d43a14208140e", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "d2034f60-24c2-478e-accd-192904ab5b50": {"doc_hash": "c3b9fecb1749b5fe0b7cce967c45c0c096e95df919760363403a301ced7ef063", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "88d2e849-ff34-4061-bfa7-4d550040b3cd": {"doc_hash": "bbcadda8f5906d969909ff3f8cf595b27e45e3640c8a2728d383ab057079434b", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "cb250282-245d-48dd-b92f-98ca6c4d94f6": {"doc_hash": "fbec958af66325924fdec387042e486e2fc584bd30322fc3369259ea49b07a23", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "af9c2c83-4ae9-45da-b626-8982ded28bec": {"doc_hash": "617c3d219a2cd09edc1891940a61c2d76b82e07ae1d326fe12d0cc0f94fed6e0", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "3a31a548-ea13-42a6-b639-585cd3ff87cf": {"doc_hash": "150a7201ebe4358786dd40fdc9e449d459235e990eef567db503d49bc9d85795", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "128d2ae6-ee1f-41a1-876f-30413a25d339": {"doc_hash": "9fadb28866e20141a86d60b3c8754d13c89453b303a5b139c77db7ee3b3397a2", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "6f446043-5575-4c7f-9659-57d903610de3": {"doc_hash": "95f49f3e593821523a833787752762805793fa7ecdbeb7b1d145e959886f833c", "ref_doc_id": "cadce458-1609-48a6-bccb-416dc1147e4e"}, "e76774e7-9714-43ef-8654-df0bde974101": {"doc_hash": "cb07fd8a11d6b9b73d4c106260b92407a4202405f9d4700466b73ea97860e9d9", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "fc441a0b-53aa-4030-964f-65f23edaf030": {"doc_hash": "2ef46141b32e0f2a369c19ff95b0597543bf27751fdeb573272d05d2a548e0fb", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "29807242-edd0-4744-bc79-68e7601a6f7f": {"doc_hash": "8e818ae8afa8af7c5c160dc7947f3814f672d4c6a2c900ffb6bb03bfc077b9d3", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "e16a1afa-7e97-48f6-b024-c27a984da441": {"doc_hash": "17367e66485573d0e2badcef211b611b179de7a1bd05011697891a4aa6ce86cf", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "1b66f188-865a-4a74-864c-5ce6ec94f769": {"doc_hash": "d6eadd3659c641843d3b8e28b8c6170e4042f6afd13dd62c70fae1f85dc89b97", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "9e530225-9fd7-4494-ab92-758053091c8b": {"doc_hash": "d91c2420c21354f933ca58330f92e6d900bc0270dc0ecd05c14d72a78342460f", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "b693215f-7503-4e8a-b656-de96b96885a5": {"doc_hash": "c37b188e78d8355de564ba082ff3029b32b37b6bbe76588df44cc7e069d0b5c6", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "1ff4e1ef-6d3f-4db9-9666-3456b7e886f3": {"doc_hash": "5fcc5d215319dac4bec59d0aef1b9e72c64a6a8240cd8d6e88210037cf072e41", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "95637869-494b-461c-afbe-e7b72f65622f": {"doc_hash": "71eaa84680f5f17a1f3f65edb3a225c312bb13125fc20b7278c71598f1f07924", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "4ab7d399-9410-4926-b423-b29cd7ca54f6": {"doc_hash": "c4ddb0efe793ed9646183e54c2fa1f25c5bacadb12dafdd54ca888374cfa6cbb", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "df5d869b-7454-409b-bb26-4349976a810e": {"doc_hash": "bf939ea8aba049c8b113a846fe0b605c88a51e8a1e6e546b46a076be4cc02e8d", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "aa24dff4-2eeb-4c60-9504-c83a56dfacc7": {"doc_hash": "98ba44362d8fab7b50ba69cb3910987bd332c672ff1170424ece6c10449b3b1f", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "dcb51ffb-7d55-490b-b328-7f36b78ea34e": {"doc_hash": "98ba44362d8fab7b50ba69cb3910987bd332c672ff1170424ece6c10449b3b1f", "ref_doc_id": "5ade2904-e46b-48a5-a8e7-8f90647e0672"}, "85e4eac4-2122-4b06-b9fa-651185dd3070": {"doc_hash": "595890ffe9fa533befbb1ec3213b351cae5c65a571d3c3182a3e88470622620f", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "3adae9bc-aee4-4b4e-a496-dc12cd9e5c3c": {"doc_hash": "2d031cffad2f5714e3a9dc13cf7ac9e60f217e0ac5d29c4bd1bcfac2eb08b0a0", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "d03de5a2-cea4-429a-958c-b442df4a8b1c": {"doc_hash": "a54c3baee9f24101cf10e6332271614e188eff1bb9fa45f6c587b616a9c9fab4", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "4f56d800-a5a5-4556-bb4d-c5c6ccb82d4c": {"doc_hash": "9de301e6d3a472af154b8f7522fe79cf6d2b80cd2cecc9ce6b2b7e55bff037d9", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "6dab964e-a3bb-4239-a2c2-8efc7371f0b3": {"doc_hash": "79a262d10d8df097be12995b10a4db301360b3c634cac3e3264f1c8eee86c834", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "27a49fd9-5d17-4339-9d70-5bcd613e1e63": {"doc_hash": "eccc47c16560331eaffa79ef84d73133a31b5e5f6d11496d1cb93162f6f32119", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "73d3fb90-1b9c-427c-be38-e492c78466b8": {"doc_hash": "1c3438a83447073bfb69527109dce65ff7f22fe308e8fc77b3a9583edc9aeb8e", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "4f689fb8-1eb5-4f0a-9971-bd95b4594653": {"doc_hash": "f191f25cbe7c3c96856c2c8cb79c6780818208642881f2e1fee2e6f00006624c", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "2189a3c2-2140-4315-92c8-d71ad93ff4df": {"doc_hash": "98bf53c374372e3a99de7919dd317bc890d1de632442ac9aaf4950ea4bd2be4f", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "ddcade83-be12-44ec-934a-879165157543": {"doc_hash": "f191f25cbe7c3c96856c2c8cb79c6780818208642881f2e1fee2e6f00006624c", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "feb46246-1f0c-4444-92b1-b964d629350a": {"doc_hash": "c524b9cec387a8d472ae5643ffcc548d00f9dcddf33e23032364e97855990ca0", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "4faf87a1-9a70-438c-831b-522ed8766190": {"doc_hash": "406d7205937d72b27454ea5ab2e41fe1104bb4f572576e8f3dce8e86b1806f20", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "b49c5d25-ae4c-46ed-a686-8e421b1c771c": {"doc_hash": "406d7205937d72b27454ea5ab2e41fe1104bb4f572576e8f3dce8e86b1806f20", "ref_doc_id": "2559d92c-a84e-421a-ae06-c8030953b006"}, "d5299d0d-6afa-44bd-9244-8961f12cf920": {"doc_hash": "b81c578643aca6382bfb2bbc072ff9afe35c8cc6a83479c13e23d8a330e2f078", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "a1d4862c-4c52-4e59-852c-103fc85c6236": {"doc_hash": "dd77a4e3ef813475f753420f5a14752dec5f64df77f7cea6e07231ef3681a059", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "ed2e518c-7d2f-4f13-bbf0-d9bdd3521d12": {"doc_hash": "1182d004e0b7235c1102acbface6d11a91c5042417040585dbcf1b57228f57b8", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "8560e0a6-ab19-46ac-befa-8f8e5694df16": {"doc_hash": "b26168b41a237755c9797b43244060f42a268bcad6e1d6fe86a4becf266980b7", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "ca1619f2-7d03-49ef-8ca7-ff25af479e5a": {"doc_hash": "ec5b8def0a801bbd5a7e675f5feea1410342b38b4aa25cae119c08c727b48c52", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "c39e2c16-2d5f-4530-b557-12d1fe33ca5b": {"doc_hash": "fbfc8f5f031bc18b93016a6d17ae41c940249b0a615fc8bcc11597317e94fb16", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "50b7165c-a600-4b73-8fe0-2b8fe5afa537": {"doc_hash": "ea06dfa5d2fd79a06ab87d9162536e756db2a7750b9a95bc16552ab356ca313b", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "8690bb8c-7fda-4fdb-a6b2-488a83f15b8c": {"doc_hash": "bd93f69046e9ff6db55b57293db7e78f6b15e6c59c8ab5e2f93665a67b928a55", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "73814699-921d-46e1-9447-10a607f71c8c": {"doc_hash": "b6b4264ad3eaa2015a9391ef63599509a481d36329760e2b3a24af774517fcb5", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "09c6d98d-3aec-4021-af4c-a9fc01d3f94e": {"doc_hash": "5b4e5658de1e46fd51d9f5e2ad20328bfe844bbaf0621c5d798700b2a0402d2a", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "90321c20-150e-4464-afe4-b8dda897a38e": {"doc_hash": "d5717da25f692e5766484c78bd6775b6e7971795e2b55384c31dfd28f54b514d", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "dc0e3adb-b3fa-464d-ba92-7bebe1ca53f1": {"doc_hash": "72f71fb61ca2eb492963b51713b5a917bf046cb8e8c86198f5ff8ebbf6bbe174", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "0ddd5227-24e4-4355-8a83-fe978655dc05": {"doc_hash": "1bf7f644c9d7cf4b6fd24c59b69cef181401f79bec68eef70d8a47d089e4efc9", "ref_doc_id": "eeb29d16-9815-47b2-aa7e-c806c6c951f4"}, "1be362e8-dcfa-479c-ae27-14890c16b8d1": {"doc_hash": "288f088b2820186877ae61e9c281c5f399eaa682d15e142a4e0b8a85b086da39", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}, "80982f31-52e8-44b1-a262-88cec327b327": {"doc_hash": "b7aa9f238926bce2521c2bf0847bb7706ca3f4e122061c1b1ac07e85b183d053", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}, "60eca99b-1b30-4362-9a03-e1a2aa788d03": {"doc_hash": "fd301e45da80b1d0ab70f864f57e79fff1c79370c54da2e2c1b8c35585e0ee54", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}, "d2c3d590-f462-453f-97d6-6a4255ab38f5": {"doc_hash": "b350b598f111a3ce15c1090428334a2f9364fb6b1442b5aa2fd84675febdaf90", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}, "4ea0a703-b576-4406-82a7-baa9ace90f19": {"doc_hash": "74ad8caacf7589dad04d87c8f15bceb040a90b48129c1788c750e66cf08e9843", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}, "d858b5c1-e18e-4d79-b57c-34b1a758c173": {"doc_hash": "373d39c4491d227a09056505d33a951772dc1eebf37d9691e9ec28208f2ab01a", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}, "d6e2c0a9-0d7a-425c-bc2e-8afebaa29d61": {"doc_hash": "dc44022c0c47063c0006d553a1e6e807f8cc2ddf3df7787ce1c4f51f2df88c67", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}, "6b2f3fbf-578c-4e5e-a1c2-88bf8b7692ba": {"doc_hash": "8daa03e17ae030ef37f69e0787e8afcb25171cb0ef1609370f495c9ff5c8e2a2", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}, "585ead16-a9f5-40ca-9469-4849dbb8ca42": {"doc_hash": "1b566f991acdda659b885f3f0d149ed21251badc8b9a32a5b83bb2df0762c55c", "ref_doc_id": "336ee442-6c48-4a5f-a5b2-3b67639bd79d"}}, "docstore/ref_doc_info": {"9f6b2fe2-dcff-4852-8128-be9219bbbe03": {"node_ids": ["f68d545e-c482-4e5a-916a-b9e6c16a9f89", "07629733-1cdf-4951-8385-d355097f1afc", "67d86419-c304-4f5f-bf65-b95021e9de5a", "41e36281-ebb1-409d-872e-2beda113f627", "902ae4f9-50dc-4a9a-987a-7e473bd1268b", "c9e7a1bb-b3dd-4ac2-a91b-1b4ca5c9ef24", "a080efe4-9814-4ab8-b4c2-14eae2c14a3e", "8237ce55-9fb6-4460-a67b-d57088343ac2", "c3c2a25b-9cb6-4ca3-84b4-35068b4a71ad"], "metadata": {"page_label": "1", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "1440ca32-6bad-42d3-bc57-57bf7af8d27c": {"node_ids": ["5ef68241-0ade-4ed3-9c11-3cf0daa96848", "8589b113-0f51-41a6-a845-52c91baf0ace", "f3db2899-4098-49b2-b583-8563ba3a202c", "50d10b41-b7f2-4af7-b035-75042552f2c6", "2f2e7161-8da6-4cde-ad3a-c94df2bf01ad", "93973990-d406-4646-aeb3-2a788d56b9b5", "977aef4a-99de-4a09-81e1-a66275c5a139", "099523d7-cc92-4292-8d64-e6361875b5f2", "bb23a500-9b12-4d9c-ad23-5b0663f57562", "57ee3b01-7124-4a4e-bf1b-4a98d6ee7cbd", "6875ac13-4653-4014-8e95-42ee7d3888ec", "0f43e3b6-9c7d-4038-a451-5b795042e009", "34b6bed5-921a-4e1e-aaf7-5ca50f098761", "22f394d8-1d10-43a1-aaca-3a52ee53aaea"], "metadata": {"page_label": "2", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "509897da-3c37-4679-aaec-1ef93cb6ccff": {"node_ids": ["7847fadf-8f6a-4ebc-884f-f4a4a228c14d", "4d69f0ae-68c7-45a0-9e82-afa38d368859", "2b9dc6cf-4ddf-4909-95cf-6c1b1cfe4364", "78af0c97-03ab-4eaf-a74b-5e68db0bc36d", "e6cdec83-53d9-4eae-9a5f-f9580755b587", "6a3e6d03-96b5-4165-9cad-3463da6ea73f", "2edc18d3-cb56-4a31-87cd-ee8879699554", "57d072df-0fcb-452a-a3ba-5e6cce39cec2", "94b5363c-adb9-48a2-ad23-4d5b827a0474", "1c9bd74f-dddc-428d-9d16-17bbfd5db2bb", "129d0d0c-2cec-4683-9586-7b963b83ff6a", "3576df63-cd81-4595-ab7c-31d3b85c2968", "4a263539-eef7-4f63-97ef-1ed9520ca575", "a85a5425-c7a6-4b7d-b54e-4dfb4c459381"], "metadata": {"page_label": "3", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "dae453b2-ffcb-4225-a910-9c60ebcd1490": {"node_ids": ["2593dd6c-afbc-4d63-ba3c-e3294e33b9fa", "c2990faf-c06f-41ad-add2-163ff7cf180d", "761989e8-0cfd-4b0b-8939-63d8cf81e5a3", "7ec242e1-7b00-485a-aa65-f90dc1c3d3ab", "61715869-a72a-428b-b0eb-99e08b4ba441", "ef00f5f8-9ca0-4f0d-a12c-5ea1c399a65b", "1c9ad27a-bed9-42e3-aa8c-b67b552469da", "80771438-6266-4cac-9ba7-9a73dcce8afb", "bf8a0989-52de-4c7a-9846-6093baf1a5e2", "60ddf73f-8559-43c6-8e99-f698534cebfb", "0b2eeaaa-4f2d-4ade-a4cb-54e2f0638357", "e2c92540-a919-42bf-8bab-e6d6c44d4218", "5b5db312-c332-4b4d-96f2-bf58abaf6547"], "metadata": {"page_label": "4", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "25d7f236-75ea-4f82-a8ab-3678082838cd": {"node_ids": ["be4ee63a-e796-47b1-aa18-2b85313f38f9", "337acb79-b374-4c66-aa4e-63fa7c9e8d88", "995764a7-21bd-4b20-b213-9f3c95a8eca8", "6a8e3c9f-e1e9-4295-a76d-d33970774e95", "7d425980-fabf-4504-b327-179f8aad34ad", "50400486-13fd-4062-a0aa-87b26584b149", "2fa245c0-7d2e-42c5-9a38-55eadeb7b48a", "ddd3fe98-9660-421d-ae1c-923b8f3bdfaf", "0847deed-64a5-4f45-a3b9-aee7ab6d68e3", "f3460b8e-fc3d-4014-ae2d-e0d1524d9946", "6d65ecac-3a61-4f0b-9085-b99766b5c037", "f2d1ed81-4f9b-4976-bb7c-c442c1fe8d46", "332430a4-7156-4128-acd4-5e75f80349ca"], "metadata": {"page_label": "5", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "c1af666f-893f-4303-b5e4-839f3b23f880": {"node_ids": ["c99dc81c-e321-4622-9cba-e740091c4d8b", "302430ba-129e-4c83-a3d8-978c9203d6b9", "3d65f2c3-efa3-44a9-9482-a99f34211d69", "f2d71a6b-4eca-42bc-9ff3-38dfb02ad177", "a0d45704-53ac-496a-8d6d-1976876b6164", "e751c095-4fa8-4db4-8364-eba8859af65f", "795e905c-295f-424a-ba98-5c96747b8b6d", "b5f24363-c544-4756-8b55-7354a4f4e2e0", "44d6e092-beab-4ecf-9e69-651c9fc1b4f5", "42310718-2ee7-429d-a1d2-8098f04b1da0", "f1eb5da8-ba01-4c21-a8a1-af9d42f1465c", "a1c32316-c73f-462f-8d87-e892b1dcf286", "5e9bd650-fe88-42ca-86d3-39af7137ba47", "871a8901-e6c4-4ba9-9040-a15ca2a4e017"], "metadata": {"page_label": "6", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "45ae2e2b-0114-4c65-8ac8-00382eabcbb6": {"node_ids": ["faae4f59-5c3f-451c-810b-cd8fc391ab93", "3ea17869-057f-4876-ae38-548cfd0ff08c", "a3d30d56-d6d2-49e7-9034-279d0114b5d4", "d75fbf83-be82-413d-9e1a-d1dbb41e3583", "a9676371-9f4e-4bf8-bf99-e7a7eb202b39", "4ed49efa-9792-42d5-96d0-4ab1ba720141", "4ef7376e-db57-45d0-904f-5863eb3ab4c3", "de6dfcb6-a900-463d-8678-200703214071", "753ecff5-8408-47de-a3a6-13523cec832e", "09e82015-23ed-49a3-a531-324aa52d9771", "d81f996d-8944-4161-b284-8f7a23244116", "7bd9332d-eee2-4f40-8c93-49cd8ee514e1", "68810082-d630-4409-8467-dde69491161e", "bc603e4e-30c9-4b95-a51e-c0b4b48d3c78"], "metadata": {"page_label": "7", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "aacad45b-e004-4352-b686-0d5101ffc1c8": {"node_ids": ["b51e7f82-20ca-410e-a04a-ef3da9d6935a", "d4f8d082-8d58-4aff-b361-76984409bf9a", "f7f4c9b2-89bf-42e5-92bf-cd2c785e5fe1", "d4f6dcf7-e9af-4fce-8ffa-c86a5e6375f9", "1929dfa3-53fc-4746-900d-4373d0bc5d6c", "6b040423-74e1-461e-a5bc-43b5fbc4ef64", "f3f0d4f0-e794-49a6-b9f4-8c7da98c7c47", "239505fb-6f12-4f51-bc31-00e4ea69afc5", "df608f7b-d033-4d87-8a5c-ac945a2a3e4c", "fc8231d2-31ec-4cfb-82b6-288b9c53fa42", "fb577c4f-c53a-42d4-b33d-2f05f9ce9043", "1965f967-f828-4665-99fc-22f6d7eed72e", "158f4f7e-efd5-4d7c-9156-a326e9b5cf3f", "eb7b863d-0d80-467a-a908-12ae47d4c997"], "metadata": {"page_label": "8", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "f33d1139-924b-4245-923c-ad7f213b19e1": {"node_ids": ["dac5b250-1ff5-4661-b4d1-78cc80446339", "018da332-d74b-4268-a274-6aa8a1853e0d", "cd564975-4346-4c89-aa8b-00d10dc45fce", "c2cc36b0-acc8-4468-ab38-576f2984ef74", "e6cb6fd8-1f23-4ae9-8fbc-5f3c16ff0017", "9e8b6760-8e18-4c71-a410-160ba3aabe59", "ece5763f-7539-4d9c-ba7d-643efb434193", "2a308913-b3f8-40cd-916a-51a7c75994ce", "379c988d-c250-4ee8-8034-85817c6f7d96", "d90a490c-e93d-4e21-b513-54cb95e8dca1", "d33d77b1-3bfd-4306-b9fe-6dad4bcb3041", "bced9eae-fe6d-48cd-abe4-47907e9cebd1"], "metadata": {"page_label": "9", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "5ee0818f-da5d-499f-b86a-4e89320b872a": {"node_ids": ["0b0bd11e-5271-49b2-bb51-5894086de834", "94883fb6-7b33-4212-8e1c-6e87aecbe44d", "7c173b0c-9ccc-47e1-bbf1-63cf44c89988", "9ad51a02-8bff-4af2-bf92-a9ad6c14736e", "2162c6c6-34c9-460c-9ac0-95708e608cf0", "be169577-b256-46a5-b678-d92a95f0bbc8", "eb8cdbdd-6831-41d2-8afb-f9c6aced8c20", "151b5f79-30f0-42d5-82a7-42d8963235be", "70c180ef-78bc-4667-bbf1-1f108169db8c", "bf238218-edeb-48f4-9ab9-e375dcf874cb", "f4cfe19c-47e4-4de2-a5fc-b76c3edbab7d", "cdccb3dd-a534-4681-ad6a-34f5942d4535", "981488e8-1b61-4e74-9663-047cfc7d94ad", "9154fb5a-518d-4931-9581-001103245664", "eaeeee49-b55f-464a-b4c2-f33b9e9ccd8e"], "metadata": {"page_label": "10", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "0deb0246-7bf1-4b84-a5c7-03ac11e02509": {"node_ids": ["a2926084-2577-4d66-84be-054d88ed7c4e", "1419fb3a-983e-425b-bd49-681112be46bc", "4c52314f-eba4-4286-aedd-9974022e673d", "a86cb78c-b2bf-4d49-9312-c04a30768fb3", "7cbb4a13-98bf-457f-9ab4-56957dcc8450", "e89e7795-feb3-47da-8658-cb6d781123a8", "38ecae37-17b7-4d56-9a30-b567e33e77f3", "541f2839-1cd8-424d-89b0-a9e8e7541585", "9f807aaa-6090-46c1-bcab-918f0c0bc976", "aace8115-b0fd-4020-8ee9-b0a0efedbd9d", "401817c7-3efe-4d45-ac61-3f01f236e016", "2f3ace41-d30d-434b-ad89-9fb860e0c576", "5969b090-d2b3-49b1-8e81-97b43409490c", "a3b606d1-4943-490d-b1b4-853427f660df", "024b589f-c94e-495c-971d-a4e679588e40"], "metadata": {"page_label": "11", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "af26339c-434b-441a-ad5c-2d00a61878ec": {"node_ids": ["03c3a17b-bcf4-497a-9262-ace694e72ab8", "6af94997-024e-4f57-8226-a620fd5cc466", "e905615d-0fad-47ce-884e-24ab96caf5ed", "28b6c80a-4055-46f8-8c43-33316b6cba34", "cb7f1c57-41a2-48c5-962e-c3d19582ceda", "5f9b0d66-c631-496b-8bf9-68b1c0f6e895", "835e41bc-8672-4195-9e41-e2bc5633293c", "8d460469-f323-42c0-98a0-9661cb23ce93", "3b7b64a8-0c99-4565-b9ef-b2275e598b3a", "c7de47ce-2321-476f-b53a-d8475e45fce2", "d1fd653e-d01a-40b8-88a6-74d87f5ca3e8", "82cb1f04-2a5d-48f9-8427-7dbea46744c4", "c4c4e23f-bd0d-4315-a21c-a1bc18edcd4b", "eb5b4321-6ca3-405a-8e5b-fb4e20fcbf2d", "a2de968d-c52d-4aaf-a0dd-7caa18915893", "6f69940f-494d-475a-9735-1fe42ddf93ea"], "metadata": {"page_label": "12", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "cadce458-1609-48a6-bccb-416dc1147e4e": {"node_ids": ["bed28d07-8677-4c0e-82bd-d666cd7ca910", "8372e903-14c0-42e6-a54a-44639d2210c6", "e1ae3d8c-a53f-4d2d-a1de-ecee4e3357e5", "aa942416-de63-44c3-bf82-40e65eafd1cf", "7cfa7e21-001a-4f83-a5ac-7a708e760dcc", "d5dc6a7d-c845-481d-a7f2-51d4aadc2120", "195ed4a0-229d-4464-8682-6b3401b8b754", "ff267bdf-ef1b-49c3-849b-9788c3ba6b1f", "cf797a81-a0fc-49b0-8178-33f20d0bc830", "d2034f60-24c2-478e-accd-192904ab5b50", "88d2e849-ff34-4061-bfa7-4d550040b3cd", "cb250282-245d-48dd-b92f-98ca6c4d94f6", "af9c2c83-4ae9-45da-b626-8982ded28bec", "3a31a548-ea13-42a6-b639-585cd3ff87cf", "128d2ae6-ee1f-41a1-876f-30413a25d339", "6f446043-5575-4c7f-9659-57d903610de3"], "metadata": {"page_label": "13", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "5ade2904-e46b-48a5-a8e7-8f90647e0672": {"node_ids": ["e76774e7-9714-43ef-8654-df0bde974101", "fc441a0b-53aa-4030-964f-65f23edaf030", "29807242-edd0-4744-bc79-68e7601a6f7f", "e16a1afa-7e97-48f6-b024-c27a984da441", "1b66f188-865a-4a74-864c-5ce6ec94f769", "9e530225-9fd7-4494-ab92-758053091c8b", "b693215f-7503-4e8a-b656-de96b96885a5", "1ff4e1ef-6d3f-4db9-9666-3456b7e886f3", "95637869-494b-461c-afbe-e7b72f65622f", "4ab7d399-9410-4926-b423-b29cd7ca54f6", "df5d869b-7454-409b-bb26-4349976a810e", "aa24dff4-2eeb-4c60-9504-c83a56dfacc7", "dcb51ffb-7d55-490b-b328-7f36b78ea34e"], "metadata": {"page_label": "14", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "2559d92c-a84e-421a-ae06-c8030953b006": {"node_ids": ["85e4eac4-2122-4b06-b9fa-651185dd3070", "3adae9bc-aee4-4b4e-a496-dc12cd9e5c3c", "d03de5a2-cea4-429a-958c-b442df4a8b1c", "4f56d800-a5a5-4556-bb4d-c5c6ccb82d4c", "6dab964e-a3bb-4239-a2c2-8efc7371f0b3", "27a49fd9-5d17-4339-9d70-5bcd613e1e63", "73d3fb90-1b9c-427c-be38-e492c78466b8", "4f689fb8-1eb5-4f0a-9971-bd95b4594653", "2189a3c2-2140-4315-92c8-d71ad93ff4df", "ddcade83-be12-44ec-934a-879165157543", "feb46246-1f0c-4444-92b1-b964d629350a", "4faf87a1-9a70-438c-831b-522ed8766190", "b49c5d25-ae4c-46ed-a686-8e421b1c771c"], "metadata": {"page_label": "15", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "eeb29d16-9815-47b2-aa7e-c806c6c951f4": {"node_ids": ["d5299d0d-6afa-44bd-9244-8961f12cf920", "a1d4862c-4c52-4e59-852c-103fc85c6236", "ed2e518c-7d2f-4f13-bbf0-d9bdd3521d12", "8560e0a6-ab19-46ac-befa-8f8e5694df16", "ca1619f2-7d03-49ef-8ca7-ff25af479e5a", "c39e2c16-2d5f-4530-b557-12d1fe33ca5b", "50b7165c-a600-4b73-8fe0-2b8fe5afa537", "8690bb8c-7fda-4fdb-a6b2-488a83f15b8c", "73814699-921d-46e1-9447-10a607f71c8c", "09c6d98d-3aec-4021-af4c-a9fc01d3f94e", "90321c20-150e-4464-afe4-b8dda897a38e", "dc0e3adb-b3fa-464d-ba92-7bebe1ca53f1", "0ddd5227-24e4-4355-8a83-fe978655dc05"], "metadata": {"page_label": "16", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}, "336ee442-6c48-4a5f-a5b2-3b67639bd79d": {"node_ids": ["1be362e8-dcfa-479c-ae27-14890c16b8d1", "80982f31-52e8-44b1-a262-88cec327b327", "60eca99b-1b30-4362-9a03-e1a2aa788d03", "d2c3d590-f462-453f-97d6-6a4255ab38f5", "4ea0a703-b576-4406-82a7-baa9ace90f19", "d858b5c1-e18e-4d79-b57c-34b1a758c173", "d6e2c0a9-0d7a-425c-bc2e-8afebaa29d61", "6b2f3fbf-578c-4e5e-a1c2-88bf8b7692ba", "585ead16-a9f5-40ca-9469-4849dbb8ca42"], "metadata": {"page_label": "17", "file_name": "Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_path": "data/pdf/Toolformer Language Models Can Teach Themselves to Use Tools_2302.04761v1.pdf", "file_type": "application/pdf", "file_size": 657966, "creation_date": "2024-10-11", "last_modified_date": "2024-10-11"}}}}