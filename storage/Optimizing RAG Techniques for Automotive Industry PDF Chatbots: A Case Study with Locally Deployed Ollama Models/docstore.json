{"docstore/metadata": {"ce6e11b6-5456-40cf-80b9-6a70be98b35e": {"doc_hash": "a23e8adf162c004ca73b768abbb0da14c7e20c619cdbc6e928fce6ba7c98d7e0"}, "5750a6c7-7487-40e0-9a11-2db70905bcf9": {"doc_hash": "47871a203f97532683f2e21fe7b1ed521dd12bf90b0cd6d2092e52d6c5771dd7"}, "c181dda5-7b43-4f50-ad76-54014acf7fcc": {"doc_hash": "a747839e8576b3928291fa21e410098478bd4bc7ad7abbea5b5ea96b5932915f"}, "a5cc9340-7ff5-46a0-b417-57cbd0f66b55": {"doc_hash": "5f2a6bc17c7d0aaaf62266f83273c815e8fbbae6447e3e9546da462cd041299a"}, "0dd96b01-fe6f-4008-b4dd-5b46f735205d": {"doc_hash": "54817c7a305bf0ea1dfe66b54554edb07f1e2969ef83d8ccb267f666b382072c"}, "f3f0f73b-fbeb-4ed4-a848-ba53b26bc73e": {"doc_hash": "0304f98c06471a27711ee8c5c508c37fcc661cc717db34bbf8e3037d9b223531"}, "3edfdccc-4271-49f4-afdb-5c9f768ae042": {"doc_hash": "735d4f14175a9145ea5c3f227d2c1bf3f44fb379a39214d8e586968c7badabed"}, "f0dc9444-8027-465c-9046-d48b55f3e9f4": {"doc_hash": "cf96661f591c993befce6a1ffc5d84b447d39ef6bed75c6afbd671715927363f"}, "d7e18813-d1de-4559-8d0c-e8c05a6bc913": {"doc_hash": "fe34fa8251870e9bb39d51f8cd5730adcde728e0ee2e9ab1ab027045118fa72c"}, "d726f239-1159-41e0-a763-912f6dae9afd": {"doc_hash": "e77e2067aa1a471f5a29d423d08b66e1b24509f21e44380ca006b304847c8715"}, "c5bbb4e6-7d1e-4473-b747-c66504cd2be8": {"doc_hash": "f6ed64c6996ef3321a96a47858e2fe7086dc9a9a55ca088059c1adaf60f19abd"}, "29a1c38b-957a-426c-9c8d-6423abf653d8": {"doc_hash": "62c6dabdcd084c9dcfa892da9150279df82ddae09cadcb5ab3735ec0c4d00c4f"}, "c483bc16-3673-484d-bd14-18e3bd3081b6": {"doc_hash": "b6b57703deb4a6fa6301469fa2d575e03b211359f3ff577ea1dc47f29b54dc8e"}, "2f49897a-e58b-4ccb-ae83-94618e95a4e7": {"doc_hash": "a9cdc520096fd7b0329b5977f298026186498b9d1d735e21be683384c281bbe6"}, "1d49e4da-7ec1-4018-b386-dbc36f419438": {"doc_hash": "3760a4f72e5a00d098ab7f43b8dfecd030b0c58b186d39fe21a49c92f91d9002"}, "a8b9c775-9cd6-4a80-a5da-1a5c6cc2b47d": {"doc_hash": "dea4eeff56172ddc1bdd77f1308bfd8d4c9e28504c6ce40252d9372e4b6ac162"}, "396ee9db-a91b-4048-b5fa-f87c2706d775": {"doc_hash": "20e4844026231903ec93ec439febcec2b46e8dd1e5bc373fc13c734b31110d84"}, "75126e96-30d3-4574-9ad0-6d5374572d35": {"doc_hash": "267a033d4923521bce5ef036505f10dc3d4628c1f82bc264181f4a2e0760748c"}, "c7d2a465-d5b9-461a-8ba0-8fef81934a15": {"doc_hash": "3215e452ccf8f0c42b68f367d5435541141ecb727113fb49fbd0840aed81785d"}, "f9b354c0-86cc-46b5-9cae-da5bc1733ee5": {"doc_hash": "87d26208ab2cf6752cc20e6aa578a45f5418d04d0d1c30d2daa832e3c5646fff"}, "b1fd74cb-f8d3-4435-8da5-5429b9930493": {"doc_hash": "0bafb7a2715bc64634f2244e002182463e490954461a9b6f9078f7fc5548af10"}, "66c7ad85-a37d-481a-9339-2efe2c239165": {"doc_hash": "4bee1d4dfb3f0fa3e7881d1e27c31b494599506aacb0cc2cc6c7f247c12a84e6"}, "ec1968e7-f49a-4f70-9c73-fa0a7218edf8": {"doc_hash": "ee1bd1e45db798ebb9a13efd15611c4891cd38abf1680cbb9b556f0b2c93eb97"}, "3ddfdb8d-94a2-4676-a775-062b245f9f56": {"doc_hash": "af4c7f48e90cc87ecc99666f353e52db79e5092fe659c661c3f681dbeeaa9461"}, "c0ba6737-26ac-4fef-89c7-b7a7f32a6657": {"doc_hash": "a23e8adf162c004ca73b768abbb0da14c7e20c619cdbc6e928fce6ba7c98d7e0", "ref_doc_id": "ce6e11b6-5456-40cf-80b9-6a70be98b35e"}, "423604e5-a53c-4aa7-bdb5-80b4e956fbbc": {"doc_hash": "47871a203f97532683f2e21fe7b1ed521dd12bf90b0cd6d2092e52d6c5771dd7", "ref_doc_id": "5750a6c7-7487-40e0-9a11-2db70905bcf9"}, "d8a2aaa1-5efe-499e-a8bf-fc2293a63c30": {"doc_hash": "a747839e8576b3928291fa21e410098478bd4bc7ad7abbea5b5ea96b5932915f", "ref_doc_id": "c181dda5-7b43-4f50-ad76-54014acf7fcc"}, "74a17911-035c-44cd-8964-a9401341c19f": {"doc_hash": "5f2a6bc17c7d0aaaf62266f83273c815e8fbbae6447e3e9546da462cd041299a", "ref_doc_id": "a5cc9340-7ff5-46a0-b417-57cbd0f66b55"}, "c80fd143-4f90-4d8f-a696-fb29a2e88711": {"doc_hash": "54817c7a305bf0ea1dfe66b54554edb07f1e2969ef83d8ccb267f666b382072c", "ref_doc_id": "0dd96b01-fe6f-4008-b4dd-5b46f735205d"}, "1309fbe9-b013-46bb-a4dc-55a8da922b8e": {"doc_hash": "0304f98c06471a27711ee8c5c508c37fcc661cc717db34bbf8e3037d9b223531", "ref_doc_id": "f3f0f73b-fbeb-4ed4-a848-ba53b26bc73e"}, "ec92772f-ada0-4049-941a-b6f05455cd94": {"doc_hash": "735d4f14175a9145ea5c3f227d2c1bf3f44fb379a39214d8e586968c7badabed", "ref_doc_id": "3edfdccc-4271-49f4-afdb-5c9f768ae042"}, "08b210da-c8b7-455d-a618-7212054dca90": {"doc_hash": "cf96661f591c993befce6a1ffc5d84b447d39ef6bed75c6afbd671715927363f", "ref_doc_id": "f0dc9444-8027-465c-9046-d48b55f3e9f4"}, "76814b4e-92ae-4fab-b896-083f38b3e06d": {"doc_hash": "fe34fa8251870e9bb39d51f8cd5730adcde728e0ee2e9ab1ab027045118fa72c", "ref_doc_id": "d7e18813-d1de-4559-8d0c-e8c05a6bc913"}, "c72e0a64-de33-45b0-8a24-e208599e9035": {"doc_hash": "e77e2067aa1a471f5a29d423d08b66e1b24509f21e44380ca006b304847c8715", "ref_doc_id": "d726f239-1159-41e0-a763-912f6dae9afd"}, "2e187af4-044e-4cdf-b53e-a2ff30c4e394": {"doc_hash": "f6ed64c6996ef3321a96a47858e2fe7086dc9a9a55ca088059c1adaf60f19abd", "ref_doc_id": "c5bbb4e6-7d1e-4473-b747-c66504cd2be8"}, "ec0b0f65-369c-424e-98b9-375ca3d76d71": {"doc_hash": "62c6dabdcd084c9dcfa892da9150279df82ddae09cadcb5ab3735ec0c4d00c4f", "ref_doc_id": "29a1c38b-957a-426c-9c8d-6423abf653d8"}, "50c9f373-b9be-4162-aca8-1c82a7dc6e12": {"doc_hash": "b6b57703deb4a6fa6301469fa2d575e03b211359f3ff577ea1dc47f29b54dc8e", "ref_doc_id": "c483bc16-3673-484d-bd14-18e3bd3081b6"}, "86a2540d-0b88-4e8c-a71b-8d2a3a746eb3": {"doc_hash": "a9cdc520096fd7b0329b5977f298026186498b9d1d735e21be683384c281bbe6", "ref_doc_id": "2f49897a-e58b-4ccb-ae83-94618e95a4e7"}, "d958ebc5-d6cc-48a9-8b9d-daebf499335d": {"doc_hash": "3760a4f72e5a00d098ab7f43b8dfecd030b0c58b186d39fe21a49c92f91d9002", "ref_doc_id": "1d49e4da-7ec1-4018-b386-dbc36f419438"}, "98efce92-0e3f-4c9a-b385-0ee7de637f00": {"doc_hash": "dea4eeff56172ddc1bdd77f1308bfd8d4c9e28504c6ce40252d9372e4b6ac162", "ref_doc_id": "a8b9c775-9cd6-4a80-a5da-1a5c6cc2b47d"}, "073494f8-bd90-481e-a8b2-3d222179b1b1": {"doc_hash": "20e4844026231903ec93ec439febcec2b46e8dd1e5bc373fc13c734b31110d84", "ref_doc_id": "396ee9db-a91b-4048-b5fa-f87c2706d775"}, "259a00db-3956-4993-9365-3216b237c577": {"doc_hash": "267a033d4923521bce5ef036505f10dc3d4628c1f82bc264181f4a2e0760748c", "ref_doc_id": "75126e96-30d3-4574-9ad0-6d5374572d35"}, "4b3bed41-104f-4a36-a755-b377e2b240b2": {"doc_hash": "3215e452ccf8f0c42b68f367d5435541141ecb727113fb49fbd0840aed81785d", "ref_doc_id": "c7d2a465-d5b9-461a-8ba0-8fef81934a15"}, "f4d93b16-140c-4433-b38d-40f05be4cbb0": {"doc_hash": "87d26208ab2cf6752cc20e6aa578a45f5418d04d0d1c30d2daa832e3c5646fff", "ref_doc_id": "f9b354c0-86cc-46b5-9cae-da5bc1733ee5"}, "f02b4aad-209e-43b9-b564-85d2a4429ebf": {"doc_hash": "0bafb7a2715bc64634f2244e002182463e490954461a9b6f9078f7fc5548af10", "ref_doc_id": "b1fd74cb-f8d3-4435-8da5-5429b9930493"}, "225bf60b-8216-42cd-b4f4-a67858c587ba": {"doc_hash": "4bee1d4dfb3f0fa3e7881d1e27c31b494599506aacb0cc2cc6c7f247c12a84e6", "ref_doc_id": "66c7ad85-a37d-481a-9339-2efe2c239165"}, "6acb82e9-35e8-4018-90e3-62f96dbd1d73": {"doc_hash": "9028382efdc21a474209944d87222705e86d20b923c223f52a86bf69f670f71a", "ref_doc_id": "ec1968e7-f49a-4f70-9c73-fa0a7218edf8"}, "54030730-add3-4de3-bac2-952cbc6eb91b": {"doc_hash": "35881124d2a20a3dc54aa36c505a5bf948c91f2198248f11a04deeb80f0d9a0f", "ref_doc_id": "ec1968e7-f49a-4f70-9c73-fa0a7218edf8"}, "8d7abeff-1764-4331-b429-6d1878260e41": {"doc_hash": "6dc46171aca6f95be98051b88b9151801353d8019d68e39685284b717acfe77f", "ref_doc_id": "ec1968e7-f49a-4f70-9c73-fa0a7218edf8"}, "659d3557-888c-48a6-b8fd-dad0d2e3262e": {"doc_hash": "a5712bdd6df02f88f72bebe4c745dc5f8aecac43d8174b0f3ca18a72681c4874", "ref_doc_id": "3ddfdb8d-94a2-4676-a775-062b245f9f56"}, "14c8dcc5-b0c3-4f7c-8600-91855ca7b3fc": {"doc_hash": "f008fbd61e636321e3950b57b3502159b5a43e211ee99f1a16ee8fde65985755", "ref_doc_id": "3ddfdb8d-94a2-4676-a775-062b245f9f56"}, "c3019942-5a53-4e71-a7f5-80f8f88d9009": {"doc_hash": "0e366b5d6bf730d9f58cd1409dcbcb6db8f98c564bed3812bbc53846c63623f8", "ref_doc_id": "3ddfdb8d-94a2-4676-a775-062b245f9f56"}}, "docstore/data": {"c0ba6737-26ac-4fef-89c7-b7a7f32a6657": {"__data__": {"id_": "c0ba6737-26ac-4fef-89c7-b7a7f32a6657", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce6e11b6-5456-40cf-80b9-6a70be98b35e", "node_type": "4", "metadata": {}, "hash": "a23e8adf162c004ca73b768abbb0da14c7e20c619cdbc6e928fce6ba7c98d7e0", "class_name": "RelatedNodeInfo"}}, "text": "# Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n\n# Optimizing RAG Techniques Based on Locally Deployed Ollama Models\n\n# Authors\n\nFei Liu\n\nChina Automotive Technology & Research Center, liufei@catarc.ac.cn\n\nZejun Kang\n\nChina Automotive Technology & Research Center, kangzejun@catarc.ac.cn\n\nXing Han\n\nChina Automotive Technology & Research Center, hanxing@catarc.ac.cn\n\n# Abstract\n\nWith the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using locally deployed Ollama models.\n\n# Introduction\n\nBased on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding pipelines and an agent supporting self-RAG based on LangGraph best practices.\n\n# Methodology\n\nTo evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive industry dataset.\n\n# Conclusion\n\nOur optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing information processing and intelligent production in the automotive industry.\n\n* Place the footnote text for the author (if applicable) here.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "423604e5-a53c-4aa7-bdb5-80b4e956fbbc": {"__data__": {"id_": "423604e5-a53c-4aa7-bdb5-80b4e956fbbc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5750a6c7-7487-40e0-9a11-2db70905bcf9", "node_type": "4", "metadata": {}, "hash": "47871a203f97532683f2e21fe7b1ed521dd12bf90b0cd6d2092e52d6c5771dd7", "class_name": "RelatedNodeInfo"}}, "text": "# CCS CONCEPTS\n\n\u2022 Computing methodologies \u2022 Artificial intelligence \u2022 Natural language processing \u2022 Natural language generation\n\n# Additional Keywords and Phrases\n\nAutomotive Industry, Langchain, self0rag, 123 1rocessing, RA4, 5llama\n\n# 1 INTRODUCTION\n\n# 1.1 Research Background\n\nThe automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex technical documentation for various processes 71]. This shift encompasses design, manufacturing, and quality control, all of which now heavily depend on efficient information management systems 72]. The growing volume of technical documents, often in 123 format, has created a pressing need for advanced information retrieval and question0answering capabilities in industrial settings 73].\n\nLarge Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating remarkable abilities in tasks such as document understanding and question answering 74]. These models have shown potential in handling the complex, domain0specific language often found in automotive documentation. However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of computational resources and data privacy 75].\n\nAmong the various techniques developed to enhance LLM performance, Retrieval0Augmented 4eneration (RA4) has gained significant attention 76]. RA4 combines the generative capabilities of LLMs with external knowledge retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by Lewis et al., has shown superior performance in generating specific, diverse, and factual language compared to traditional models 77].\n\nThe implementation of RA4 techniques in the automotive industry, however, faces several industry0specific challenges:\n\n1. 2ocument Complexity: Automotive technical documents often feature intricate layouts, including multi0column formats and complex tables. These structural elements pose significant challenges for standard document processing methods 78].\n2. 2ata 1rivacy: The automotive industry deals with highly confidential information related to proprietary designs and manufacturing processes. This necessitates solutions that can operate securely within the company's infrastructure, without relying on external cloud services 79].\n3. Resource Constraints: Many industrial environments operate with limited computational resources. This constraint requires the development of optimized, lightweight models capable of running efficiently on standard hardware 710].\n4. 2omain Specificity: The automotive sector employs a vast array of specialized terminology and concepts. 4eneric language models often lack the specific knowledge required to accurately interpret and respond to queries about automotive processes and specifications 711].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8a2aaa1-5efe-499e-a8bf-fc2293a63c30": {"__data__": {"id_": "d8a2aaa1-5efe-499e-a8bf-fc2293a63c30", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c181dda5-7b43-4f50-ad76-54014acf7fcc", "node_type": "4", "metadata": {}, "hash": "a747839e8576b3928291fa21e410098478bd4bc7ad7abbea5b5ea96b5932915f", "class_name": "RelatedNodeInfo"}}, "text": "# 5. Real-time Performance\n\nIn fast-paced manufacturing environments, the ability to quickly retrieve and process relevant information is crucial. This necessitates high-performance information retrieval systems capable of operating under time constraints 712.\n\nThe open-source large language model service framework 5llama has gained attention for its ability to rapidly deploy LLMs in low-performance environments 713. This framework offers potential solutions to some of the resource constraints faced in industrial settings. However, its application in the context of RA4 for automotive documentation processing remains an area ripe for exploration and optimization.\n\nAs the automotive industry continues to evolve, particularly with the advent of electric and autonomous vehicles, the complexity and volume of technical documentation are expected to increase further 714. This evolution underscores the importance of developing robust, efficient, and secure information retrieval systems tailored to the specific needs of the automotive sector 715.\n\nThe intersection of these technological advancements and industry-specific challenges presents a unique opportunity for research 716. By addressing the particular needs of the automotive industry in the context of RA4 and local LLM deployment, there is potential to significantly enhance information access and utilization in automotive engineering and manufacturing processes 717.\n\n# 1.2 Research Status and Gaps\n\nRecent advancements in RA4 techniques have shown promise in various domains. Jiang et al. proposed 3LARE, which uses predicted next-sentence content to proactively retrieve relevant information 718. Wang et al. introduced 3ILC5, a method for identifying and filtering useful contexts to improve generation quality 719. These approaches demonstrate the potential for more context-aware retrieval in complex document environments, such as those found in automotive engineering.\n\nThe concept of self-reflective RA4, as explored by Asai et al 720, introduces a novel framework designed to enhance the quality and factual accuracy of LLMs through on-demand retrieval and a self-reflection mechanism. This approach could be especially valuable in the automotive context, where precision and accuracy in technical information are paramount.\n\nIn the domain of optimizing RA4 for specific industries, Rajpathak et al 721, proposed a domain-adaptive retrieval method that is particularly relevant for the automotive sector's unique terminology and document structures. Similarly, the work of Siriwardhana et al 722, on improving retrieval efficiency in large-scale industrial datasets offers insights that could be applied to the vast repositories of technical documentation in automotive manufacturing.\n\nThe open-source large language model service framework 5llama has gained attention for its ability to rapidly deploy LLMs in low-performance environments 723. Burgan et al. developed RamChat, an AI chatbot aimed at improving accessibility 724. These developments in local LLM deployment are particularly relevant to the automotive industry's need for on-premises, resource-efficient solutions.\n\nRecent work by Wang et al 725, on on-device language models for function calling of software A1Is presents potential applications in integrating RA4 systems with existing software infrastructure in automotive production environments. This could lead to more seamless integration of AI-powered information retrieval within established industrial processes.\n\nThe challenge of processing complex 123 documents, a common format for technical specifications in the automotive industry, has been addressed by several researchers. Lin et al 726, proposed an advanced 123 parsing.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74a17911-035c-44cd-8964-a9401341c19f": {"__data__": {"id_": "74a17911-035c-44cd-8964-a9401341c19f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5cc9340-7ff5-46a0-b417-57cbd0f66b55", "node_type": "4", "metadata": {}, "hash": "5f2a6bc17c7d0aaaf62266f83273c815e8fbbae6447e3e9546da462cd041299a", "class_name": "RelatedNodeInfo"}}, "text": "# Research on RA4 Systems in Automotive Industry\n\nTechnique that could be adapted to handle the multi-column layouts and intricate tables often found in automotive documentation. Furthermore, the work of Bensch et al [727], on information extraction from semi-structured documents offers promising approaches for dealing with the varied formats of automotive technical literature.\n\nIn the realm of domain-specific language understanding, the research of Baysse et al [728], on fine-tuning language models for specialized industries provides valuable insights that could be applied to tailoring RA4 systems for automotive terminology and concepts. This is complemented by the work of Kumar et al [729], on entity recognition in technical documents, which could enhance the precision of information retrieval in automotive contexts.\n\nThe integration of RA4 with other AI methodologies has also shown promise. For instance, the combination of RA4 with reinforcement learning, as explored by Belhadj et al [730], could lead to more adaptive and context-aware retrieval systems capable of handling the diverse query types encountered in automotive engineering and production.\n\nPrivacy and security concerns, which are paramount in the automotive industry, have been addressed in the context of RA4 by researchers such as Zeng et al [731], who proposed privacy-preserving retrieval methods that could be crucial for protecting proprietary automotive designs and processes.\n\nThe challenge of maintaining coherence in long-form text generation, often necessary when addressing complex automotive queries, has been tackled by researchers like Borgeaud et al [732], whose work on improving long-range dependencies in language models could enhance the quality of responses in automotive RA4 applications.\n\nRecent advancements in few-shot learning, as demonstrated by Izacard et al [733], with 41T03, offer potential for rapidly adapting RA4 systems to new automotive subdomains or emerging technologies without extensive retraining. This could be particularly valuable in the fast-evolving landscape of automotive technology.\n\nThe application of RA4 in multilingual settings, as explored by Ahmad et al [734], is especially relevant for global automotive companies dealing with documentation in multiple languages. Their work on cross-lingual retrieval and generation could facilitate more efficient knowledge sharing across international teams.\n\nIn the domain of optimizing retrieval mechanisms, the research of Su et al [735], on dense retrieval methods offers potential improvements in the speed and accuracy of information lookup, crucial for real-time query resolution in fast-paced automotive production environments.\n\nThe challenge of handling numerical data and calculations, often present in automotive specifications and performance metrics, has been addressed by researchers like Noorbakhsh et al [736], whose work on integrating symbolic mathematics with neural language models could enhance the precision of RA4 systems when dealing with quantitative automotive data.\n\nHowever, despite these advancements, there remains a significant gap in research specifically addressing the unique challenges of implementing RA4 systems in the automotive industry, particularly in resource-constrained, offline environments.\n\n# Significance of Research\n\n1. Providing an effective optimization scheme for local RA4 deployment of 5llama in automotive industrial environments, addressing key challenges in document processing and information retrieval.\n2. Exploring the application of self-RA4 in offline, industry-specific scenarios, offering new insights into function calling implementations for domain-specific tasks.\n3. Contributing to the advancement of intelligent information processing in automotive manufacturing, potentially improving efficiency and accuracy in technical document analysis and query resolution.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3908, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c80fd143-4f90-4d8f-a696-fb29a2e88711": {"__data__": {"id_": "c80fd143-4f90-4d8f-a696-fb29a2e88711", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dd96b01-fe6f-4008-b4dd-5b46f735205d", "node_type": "4", "metadata": {}, "hash": "54817c7a305bf0ea1dfe66b54554edb07f1e2969ef83d8ccb267f666b382072c", "class_name": "RelatedNodeInfo"}}, "text": "# 1.3 Research Objectives and Significance\n\nGiven the identified challenges and research gaps, this study aims to develop a multi-dimensional optimization scheme for applying RA4 technology with Llama in local, low-performance automotive industry environments. Our specific research objectives include:\n\n- Proposing a 123 file processing method optimized for automotive industry documents, capable of handling multi-column layouts and complex tables.\n- Developing an advanced RA4 system based on the Langchain framework, introducing reranking models and BM25 retrievers to build an efficient context compression pipeline.\n- Designing an intelligent agent that supports self-RA4 and exploring a function calling mechanism to enhance Llama's response generation in automotive-specific scenarios.\n- Evaluating the proposed system using a proprietary dataset of automotive industry documents, alongside public datasets, to demonstrate its effectiveness in real-world industrial applications.\n\nThe significance of this research lies in:\n\n- Providing an effective optimization scheme for local RA4 deployment of Llama in automotive industrial environments, addressing key challenges in document processing and information retrieval.\n- Exploring the application of self-RA4 in offline, industry-specific scenarios, offering new insights into function calling implementations for domain-specific tasks.\n- Contributing to the advancement of intelligent information processing in automotive manufacturing, potentially improving efficiency and accuracy in technical document analysis and query resolution.\n\nThis research builds upon and extends existing work in several key areas:\n\n- The potential of RA4 systems to support decision-making processes, a critical application in automotive design and manufacturing, has been explored by researchers such as Damage et al [737]. Their work on using RA4 for few evidences-based reasoning could be adapted to support complex decision-making scenarios in automotive engineering.\n- Recent developments in efficient transformer architectures, such as the work of Zhuang et al [738], on Reformer, offer potential for deploying more powerful RA4 models within the computational constraints of industrial environments.\n- The integration of visual information with text-based retrieval, as explored by Chen et al [739], presents opportunities for enhancing RA4 systems to handle technical diagrams and schematics common in automotive documentation.\n\nBy addressing these research objectives and building upon recent advancements in the field, this study aims to significantly enhance the applicability and effectiveness of RA4 technologies in the automotive industry, potentially transforming how technical information is accessed, processed, and utilized in automotive engineering and manufacturing processes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1309fbe9-b013-46bb-a4dc-55a8da922b8e": {"__data__": {"id_": "1309fbe9-b013-46bb-a4dc-55a8da922b8e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3f0f73b-fbeb-4ed4-a848-ba53b26bc73e", "node_type": "4", "metadata": {}, "hash": "0304f98c06471a27711ee8c5c508c37fcc661cc717db34bbf8e3037d9b223531", "class_name": "RelatedNodeInfo"}}, "text": "# MATERIALS AND METHODS\n\n# 2.1 Foundation\n\nOur research builds upon the Langchain framework and Llama model, adapting them to meet the specific needs of the automotive industry. We began by constructing a preliminary retrieval-based chatbot framework using Langchain's components, which we then optimized for processing automotive technical documents. Figure 1 illustrates the key components of this system. This architecture integrates advanced document loading capabilities for various file formats, efficient text splitting, and a robust retrieval mechanism using the Chroma vector store. The basic framework includes:\n\n|Vector Store (Chroma)|Documents (PDF)|Document Loader (RecursiveCharacterTextSplitter)|Text Splitter|Embeddings (OllamaEmbeddings)|Query|\n|---|---|---|---|---|---|\n|Retriever|Retriever|Retriever|Retriever|Retriever|Retriever|\n|LLM (Ollama)|LLM (Ollama)|LLM (Ollama)|LLM (Ollama)|LLM (Ollama)|ConversationalRetrievalChain|\n| | | | | |Response|\n\nFigure 1: Basic RAG Architecture for Automotive Document Processing.\n\n- Document Loading: Utilizing Langchain's Loader class to recursively load 123 documents from specified directories, simulating the document structure in automotive manufacturing environments.\n- Text Chunking: Implementing an embedded tokenization model to split documents into fixed-length text chunks, optimized for technical specifications and multi-column layouts common in automotive documentation.\n- Vector Storage: Encoding text chunks into semantic vectors and storing them in the Chroma vector database, creating an indexed text retriever tailored for automotive terminology and concepts.\n- Dialogue Generation: Employing Langchain's ConversationalRetrievalChain to process user queries and retrieved context, generating responses using the locally deployed Llama model.\n\nThis foundational setup serves as the baseline for our subsequent optimizations, each designed to address specific challenges in automotive document processing and information retrieval.\n\n# 2.2 PDF File Processing Optimization\n\nTo address the unique challenges posed by automotive industry documents, we developed an enhanced PDF processing method combining PDFMiner and Tabula libraries.\n\n# 2.2.1 Overview of PDFMiner and Tabula\n\nPDFMiner is a Python library used for extracting information from PDF documents. It is capable of extracting text, images, metadata, and structural information from PDF documents. PDFMiner provides both high-level and low-level APIs to satisfy different levels of requirements. Automotive technical documents often feature multi-column layouts.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec92772f-ada0-4049-941a-b6f05455cd94": {"__data__": {"id_": "ec92772f-ada0-4049-941a-b6f05455cd94", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3edfdccc-4271-49f4-afdb-5c9f768ae042", "node_type": "4", "metadata": {}, "hash": "735d4f14175a9145ea5c3f227d2c1bf3f44fb379a39214d8e586968c7badabed", "class_name": "RelatedNodeInfo"}}, "text": "# 1. Introduction\n\nWe implemented a custom algorithm using 123Miner's extract_pages function to accurately extract content while preserving the logical flow of information:\n\n1. Page Segmentation: We analyze each page to identify distinct content regions, including text columns, diagrams, and tables.\n2. Content Ordering: Implementing a left-to-right, top-to-bottom reading order algorithm to ensure proper sequencing of extracted information.\n3. Diagram Extraction: Utilizing 123Miner's image extraction capabilities to preserve technical diagrams crucial for understanding automotive specifications.\n\n# 2. Data Extraction Techniques\n\nTabula is a Java library used for extracting tabular data from PDF files. It provides a Python wrapper, making it more convenient to use Tabula in Python. Tabula can automatically detect table boundaries and convert tabular data into DataFrame objects, facilitating subsequent data analysis and processing.\n\n# 2.2.2 Multi-column Layout and Table Information Recognition Optimization\n\nTables in automotive documents often contain critical data such as part specifications, test results, or compliance information. Our approach uses Tabula's read_pdf function with custom parameters like Algorithm 1:\n\n# ALGORITHM 1: Extract Text and Tables from PDF to Markdown\n\nwrite_2_md(input, output)\npages = extract_pages(input)\nopen output for writing as out\nfor page_num, page in enumerate(pages, start=1), do\nelements = [e for e in page if isinstance(e, LTTextContainer)]\ntables = tabula.read_pdf(input, pages=page_num, multiple_tables=True)\nmid_line = (max(e.bbox[2] for e in elements) + min(e.bbox[0] for e in elements)) / 2\nleft = [e for e in elements if e.bbox[0] < mid_line]\nright = [e for e in elements if e.bbox[0] >= mid_line]\nsort left by -e.bbox[1]\nsort right by -e.bbox[1]\nfor col in [left, right], do\nfor e in col, do\ncleaned_text = clean_text(e.get_text())\nwrite cleaned_text + \"\\n\" to out\nend\nend\nfor table in tables, do\nwrite pd.DataFrame(table).to_markdown(index=False) + \"\\n\" to out\nend\nwrite \"\\n\\n\" to out\nend", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08b210da-c8b7-455d-a618-7212054dca90": {"__data__": {"id_": "08b210da-c8b7-455d-a618-7212054dca90", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0dc9444-8027-465c-9046-d48b55f3e9f4", "node_type": "4", "metadata": {}, "hash": "cf96661f591c993befce6a1ffc5d84b447d39ef6bed75c6afbd671715927363f", "class_name": "RelatedNodeInfo"}}, "text": "# Table Detection and Data Extraction\n\n1. Table Detection: Implementing heuristics to identify table structures within automotive documents, considering common formats used in the industry.\n2. Data Extraction: Converting recognized tables into structured DataFrame objects, preserving relationships between data points.\n3. Contextual Integration: Seamlessly integrating extracted table data with surrounding text to maintain document coherence.\n\nWhen integrating text and table information, we first write the text information extracted by the multi-column layout recognition algorithm into the Markdown file in the order of appearance. Then, after the text information of each page, we convert the table information recognized on that page into Markdown format and write it into the file. By doing so, we can ensure that the content order in the generated Markdown file is consistent with the original file, and the table information can be correctly embedded in the corresponding positions.\n\nTo illustrate the effectiveness of this approach, let's examine two figures that demonstrate the conversion process for a complex academic paper layout. Figure 2 shows a sample page from the original document, which features a challenging two-column layout with embedded tables and various formatting elements. Figure presents the corresponding Markdown output after processing with our optimized method.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76814b4e-92ae-4fab-b896-083f38b3e06d": {"__data__": {"id_": "76814b4e-92ae-4fab-b896-083f38b3e06d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7e18813-d1de-4559-8d0c-e8c05a6bc913", "node_type": "4", "metadata": {}, "hash": "fe34fa8251870e9bb39d51f8cd5730adcde728e0ee2e9ab1ab027045118fa72c", "class_name": "RelatedNodeInfo"}}, "text": "# Results\n\n# 4.1. UCF Crowd Counting\n\nThe UCF dataset is particularly challenging due to the large number of people in the images, the variety of the scenes as well as the low number of training images. We see in Figure that because the UCF dataset has over 1000 people on average in each image, the shapes output by the network in the density map are not as well defined or separated as in the UCSD dataset:\n\nWe report state of the art result on this dataset in Table following the standard protocol of 5-fold cross validation. Our MAE on the dataset is 290.82, which is approximately 5 lower than the previous state of the art, HydraCNN. This is particularly indicative of the power of an aggregated making - use of multicolumn dilation network: Despite not having perspective information, the AMDCN is still able to produce highly accurate density maps for UCF.\n\n# 4.2. TRANCOS Traffic Counting\n\nOur network performs very well on the TRANCOS dataset: Indeed confirmed the GAME score, AMDCN produces the most accurate count and shape combined as compared to other methods. Table shows that we achieve state of the art results as measured by the GAME metric across all levels.\n\n# 4.3. UCSD Crowd Counting\n\nThe Results are shown in Table and Figure. We see that the original split as defined by the creators of the dataset gives us somewhat worse results for counting on this dataset. Results were consistent over multiple trainings. Again, including the perspective map does not seem to increase performance on this dataset: Despite the results are comparable, the proposed network beats the state of the art; For the upscale split the AMDCN is the state of the art by large relative margin. This is compelling because it shows that counting can be achieved without accurate perspective-free.\n\n# 4.4. WorldExpo '10 Crowd Counting\n\nOur network performs reasonably well on the more challenging WorldExpo dataset. While it does not beat the state of the art, our results are comparable. What is more, we do not need to use the perspective maps to obtain these results. As seen in Table, the AMDCN is capable of incorporating the perspective effects without scaling the Gaussians with perspective information. This shows that it is possible to achieve counting results that approach the state of the art with much simpler labels for the counting training data.\n\n# 4.5. Ablation Studies\n\nWe report the results of the ablation studies in Figure. We note from these plots that while there is variation in performance, few trends stand out. Most importantly, the lowest errors are consistently with the combination of larger number of columns and including the aggregator module. Notably for the TRANCOS dataset, including the aggregator consistently improves performance. Generally, the aggregator tends to decrease the variance in performance of the network: Some of the variance that we see in the plots can be explained by: (1) for lower numbers of columns, including an aggregator is not as likely to help as there is not much separation of multiscale information across columns and (2) for the UCSD dataset; there is less of a positive effect than TRANCOS and WorldExpo so perform comparably to simpler networks. These results verify the notion that using more columns increases accuracy; and also support our justification for the use of the aggregator module.\n\n# Table: Mean Absolute Error of Various Methods on UCF Crowds\n\n|Method|MAE|\n|---|---|\n|AMDCN|290.82|\n|HydraCNN|333.73|\n|MCNN|377.60|\n| |467.00|\n| |295.80|\n| |318.10|\n\n# Table: Mean Absolute Error of Various Methods on TRANCOS Traffic\n\n|Method|GAME (L=0)|GAME (L=1)|GAME (L=2)|GAME (L=3)|\n|---|---|---|---|---|\n|AMDCN|9.77|13.16|15.00|15.87|\n|[18]|10.99|13.75|16.69|19.32|\n|[5] SIFT|13.76|16.72|20.72|24.36|\n|RGB|17.68|19.97|23.54|25.84|\n|HOG-2|13.29|18.05|23.65|28.41|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c72e0a64-de33-45b0-8a24-e208599e9035": {"__data__": {"id_": "c72e0a64-de33-45b0-8a24-e208599e9035", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d726f239-1159-41e0-a763-912f6dae9afd", "node_type": "4", "metadata": {}, "hash": "e77e2067aa1a471f5a29d423d08b66e1b24509f21e44380ca006b304847c8715", "class_name": "RelatedNodeInfo"}}, "text": "# Table\n\n|Method|MAE|\n|---|---|\n|AMdON|290.82|\n|Hydrazs [18]|333.73|\n|KON [281]|377.60|\n|[27]|467.00|\n|[23]|295.80|\n| |318.10|\n\n# Results\n\nThe UcF dataset is particularly challenging due to the large number of people in various shapes and the number of training images. Figure shows that because the UCF dataset has 1030 people, the density map is well defined and separated.\n\nDespite not making the previous state of the art, the AMDON implementation is still able to produce highly accurate density maps due to aggregated multicolumn dilation.\n\n# Traffic Counting\n\nThe network performs across the art. Indeed, all levels by the GAME score the #GANET contained measures. AKDCN produces accurate counts compared to other methods. Table shows that.\n\n# Ucsd Crowd Counting Results\n\nTable and Figure show that this dataset gives somewhat worse results. Counting results were consistent over multiple trainings. Including perspective maps seem to increase the dataset's effectiveness.\n\nDespite this, Table and Figure recuts comparable to the state of the art.\n\n# Optimization of Advanced RAG Based on Langchain\n\nTo enhance the RA4 system's performance for automotive industry applications, we introduced several optimizations to the Langchain-based implementation. We introduce the B4E reranker model and BM25 algorithm.\n\nBuilding upon the groundwork laid in previous sections, we first combine the BM25Retriever with the default retriever using EnsembleRetriever in Langchain, assigning different weights to achieve more comprehensive and accurate retrieval. Then, a custom class is designed to integrate the reranking model into the context compression pipeline, further enhancing the retrieval and generation quality of the model.\n\n# Overview of BGE Reranker Model and BM25 Retriever\n\nA reranker model is a general semantic vector model used to optimize the ranking of retrieval results. It can adapt to prioritize automotive-relevant information in retrieved contexts. In this study, we employ BAAI/bge-reranker-large as the reranker model. B4E (BAAI General Embedding) is a reranker model proposed by the Beijing Academy of Artificial Intelligence (BAAI), specifically optimized for Chinese queries.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e187af4-044e-4cdf-b53e-a2ff30c4e394": {"__data__": {"id_": "2e187af4-044e-4cdf-b53e-a2ff30c4e394", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5bbb4e6-7d1e-4473-b747-c66504cd2be8", "node_type": "4", "metadata": {}, "hash": "f6ed64c6996ef3321a96a47858e2fe7086dc9a9a55ca088059c1adaf60f19abd", "class_name": "RelatedNodeInfo"}}, "text": "# BM25 and Context Compression Pipeline\n\nBM25 (Best Matching 25) is a classic bag-of-words retrieval algorithm that evaluates relevance by calculating the term frequency-inverse document frequency (TF-IDF) score between the query and documents. The BM25 retriever can quickly and efficiently filter out the most query-relevant documents from a large-scale text corpus. We introduce this traditional relevance assessment method in combination with the reranker model to further strengthen the effectiveness of the RA4 retriever.\n\n# 2.3.2 Building the Context Compression Pipeline and Custom Class Design\n\nTo optimize the context information processing of the RA4 model, we construct a context compression pipeline DocumentCompressorPipeline that is specifically tailored for automotive technical content. In addition to the collection retrievers mentioned earlier, we also introduce:\n\n- EmbeddingsRedundantFilter: An embedding-based redundancy filter to remove redundant information from the retrieval results.\n- LongContextReorder: Optimizes the order of context information, prioritizing key automotive specifications and procedures.\n- BgeRerank: A custom class that inherits from BaseDocumentCompressor. Since Langchain's official support for the B4E model is relatively limited, the purpose of designing this class is to seamlessly integrate the B4E reranker model into the pipeline. This custom class can improve relevance scoring for automotive queries.\n\nThe following Algorithm 2 is the pseudocode for the BgeRerank class:\n\n# ALGORITHM 2: BgeRerank\n\nBgeRerank(documents, query)\ninitialize model = CrossEncoder(model_name)\ndoc_list = list(documents)\n_docs = [d.page_content for d in doc_list]\nmodel_inputs = [[query, doc] for doc in _docs]\nscores = model.predict(model_inputs)\nresults = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_n]\nfinal_results = []\nfor r in results, do\ndoc = doc_list[r[0]]\ndoc.metadata[\"relevance_score\"] = r[1]\nappend doc to final_results\nend\nreturn final_results\n\nBy overriding the compress_documents method, the B4E model is used to calculate the relevance between the query and documents. The documents are then sorted based on their scores, and the top N documents are selected as the final compression results. This custom class design compensates for the insufficient support for B4E in Langchain, allowing us to flexibly incorporate it into the pipeline structure.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec0b0f65-369c-424e-98b9-375ca3d76d71": {"__data__": {"id_": "ec0b0f65-369c-424e-98b9-375ca3d76d71", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29a1c38b-957a-426c-9c8d-6423abf653d8", "node_type": "4", "metadata": {}, "hash": "62c6dabdcd084c9dcfa892da9150279df82ddae09cadcb5ab3735ec0c4d00c4f", "class_name": "RelatedNodeInfo"}}, "text": "# In summary\n\nThe innovative aspects of this section include: introducing the B4E reranker model and BM25 retriever, building the context compression pipeline, and seamlessly integrating B4E into the Langchain framework through the custom BgeRerank class. This pipeline significantly enhances the quality and relevance of retrieved information, ensuring that the most pertinent automotive technical details are presented to the language model.\n\n# 2.4 Optimization of Advanced RAG Based on Langchain\n\nIn the previous section, we completed the basic design of the RA4 system. However, integrating LLMs into practical applications and constructing end-to-end intelligent systems still present numerous challenges. To address the complex, multi-step problem of querying 123 profiles, which is common in automotive engineering and manufacturing processes, we have developed an advanced Self-RA4 agent based on the Langraph framework.\n\n# 2.4.1 Overview of SELF-RAG\n\nSelf-Reflective Retrieval-Augmented Generation (SELF-RA4) is a novel framework designed to enhance the quality and factual accuracy of LLMs through on-demand retrieval and a self-reflection mechanism. Unlike traditional RA4 methods, SELF-RA4 endows LLMs with the following capabilities:\n\n1. On-demand Retrieval: The LLMs autonomously determines whether to retrieve relevant information from an external knowledge base based on the input it receives.\n2. Self-Reflection: The LLMs evaluates and reflects upon both the retrieved information and its own generated content, thereby improving the quality and reliability of its output.\n\nThe training process of SELF-RA4 consists of two stages:\n\n1. Offline Critic Model Training: An independent critic model is trained to generate \"reflection tokens\". These tokens are inserted into the LLMs' output to guide its self-reflection process.\n2. Generative Model Training: The LLMs is fine-tuned using a corpus that includes reflection tokens and retrieved documents. This enables the LLMs to understand and utilize these tokens, incorporating self-reflection into its generation process.\n\nDuring inference, the LLMs dynamically decides whether to retrieve information based on the requirements of the task at hand. It also leverages the retrieved information and the self-reflection mechanism to generate high-quality output. For instance, in tasks demanding factual accuracy, the LLMs is more inclined to retrieve and utilize relevant information, while in more open-ended tasks, it may prioritize creativity and rely less on retrieval.\n\n# 2.4.2 Agent Design Supporting Self-RAG\n\nConsidering this, based on Langchain, we designed an intelligent Agent class called AgenticRA4, which aims to utilize Self-RA4 technology to answer user questions. AgenticRA4 combines various components from the Langraph and LangChain ecosystems to achieve a modular and scalable question-answering system.\n\nThe core of the AgenticRA4 class is the create_graph method, which defines a workflow based on a directed acyclic graph (DAG). The workflow consists of multiple nodes, each responsible for a specific task in the question-answering process. The nodes are connected by directed edges, forming a complete question-answering flow. The main components in the AgenticRA4 class are as follows:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50c9f373-b9be-4162-aca8-1c82a7dc6e12": {"__data__": {"id_": "50c9f373-b9be-4162-aca8-1c82a7dc6e12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c483bc16-3673-484d-bd14-18e3bd3081b6", "node_type": "4", "metadata": {}, "hash": "b6b57703deb4a6fa6301469fa2d575e03b211359f3ff577ea1dc47f29b54dc8e", "class_name": "RelatedNodeInfo"}}, "text": "# 4raphState\n\nRepresents the current state of the graph, including the user's question, retrieved documents, generated answer, and chat history.\n\n# Node\n\nRepresents a step or task in the question-answering process, accepts the current state as input, performs specific operations, and returns the updated state.\n\nWe designed the AgenticRA4 class to handle sophisticated automotive-related questions. Key components as\n\n# Figure 4:\n\nFlowchart of Self-RAG Agent Implementation via LangGraph.\n\n# 1. Retrieve Node\n\nOptimized to fetch relevant information from automotive technical documents, considering industry-specific terminology and concepts.\n\n# 2. Grade Documents Node\n\nEvaluates retrieved documents based on their relevance to automotive queries, considering factors like technical accuracy and applicability to specific manufacturing processes.\n\n# 3. Generate Node\n\nProduces responses tailored to automotive industry needs, incorporating technical specifications and industry standards searched from documents.\n\n# 4. Transform Query Node\n\nRefines queries to better capture the intent behind automotive-specific questions, improving retrieval accuracy in subsequent iterations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86a2540d-0b88-4e8c-a71b-8d2a3a746eb3": {"__data__": {"id_": "86a2540d-0b88-4e8c-a71b-8d2a3a746eb3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f49897a-e58b-4ccb-ae83-94618e95a4e7", "node_type": "4", "metadata": {}, "hash": "a9cdc520096fd7b0329b5977f298026186498b9d1d735e21be683384c281bbe6", "class_name": "RelatedNodeInfo"}}, "text": "# 2.4.3 Function Calling Design for Optimizing Ollama Output in RAG Scenarios\n\nFunction calling is a powerful technique that significantly enhances the output quality of LLMs in RAG scenarios. By integrating external functions with LLMs, specialized logic and algorithms can be employed to guide the model, resulting in more accurate, coherent, and informative responses. This approach leverages the few-shot learning capabilities of LLMs, enabling them to adapt to new tasks while mitigating their inherent limitations in reasoning and computation.\n\nHowever, most of the relevant work has focused on standard transformer-based LLMs, with a lack of implementation for optimizing llama callbacks, especially in RAG scenarios. Although Langchain officially provides ollamafunction.bind to call functions, further research on this method is currently scarce. Therefore, to enhance the agent's capability in handling automotive-specific tasks, we designed a custom class adapted to ollamafunction.bind, called ChatFunction. It inherits from the BaseFunction class and overrides the __init__ and __call__ methods. The main features of ChatFunction are as follows:\n\n# ALGORITHM 3: ChatFunction\n\nChatFunction(persona, lang, retry)\ninputProp = Property(type=INPUT, desc=InputDesc())\noutputProp = Property(type=OUTPUT, desc=OutputDesc())\nprops = [inputProp, outputProp]\nparams = Parameter(props=props, required=[input, output])\ninitialize BaseFunction(name=ChatFuncName(), desc=ChatFuncDesc(), params)\nend procedure\n\ncall(args)\noutput = args[output]\ninput = args[input]\nif IsEmpty(output) or IsEmpty(input), do\nraise ValueError(MissingArgMessage())\nend\ndetail = GetPromptByRetry(retry)\npersona = GetPersonaString(lang)\nhistory = GetHistoryString(input)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d958ebc5-d6cc-48a9-8b9d-daebf499335d": {"__data__": {"id_": "d958ebc5-d6cc-48a9-8b9d-daebf499335d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d49e4da-7ec1-4018-b386-dbc36f419438", "node_type": "4", "metadata": {}, "hash": "3760a4f72e5a00d098ab7f43b8dfecd030b0c58b186d39fe21a49c92f91d9002", "class_name": "RelatedNodeInfo"}}, "text": "# Current Page\n\n# Procedure and Function Definitions\n\ncurrent = GetCurrentString(input)\nprompt = Format(detail, persona, history, current)\nrespTool = LookupTool(ChatResponseEnhancer())\nreturn {\ntool: respTool,\ntool_input: {\nresponse: output,\nquery: prompt\n}\n}\n\n# GetPromptByRetry(retry)\n\nif retry = 0, do\nreturn GetBriefPrompt()\nelse if retry = 1, do\nreturn GetMediumPrompt()\nelse\nreturn GetDetailedPrompt()\nend\n\n# Initialization and Parameters\n\nThe initialization accepts three parameters: the AI assistant's personality description personality, language preference language, and retry count retry_count.\n\nTwo properties are defined: query_input and output, representing the complete input (including personality description, language instructions, and conversation history) and the AI assistant's response, respectively. This ensures that the model is provided with the necessary background knowledge for each retry, enabling it to generate responses based on the specified role characteristics and language style.\n\nThe constructor of the parent class BaseFunction is called, passing the function name, description, and parameter list.\n\nWhen invoked, it accepts an arguments dictionary containing two required parameters: output and query_input.\n\nAccording to the retry count retry_count in the self0rag Agent in 3.3.2, the level of detail in the answer detail_prompt is dynamically adjusted to modify prompts based on the complexity of automotive queries and the depth of technical detail required.\n\nThe query_input for the next round is constructed by encoding the personality description, language preference, detail level requirement, and current conversation history. To maintain awareness of the ongoing conversation context, crucial for addressing multi-step automotive processes or complex diagnostic queries.\n\nA dictionary is returned, containing the tool field (specifying the callback function name) and the tool_input field (containing the optimized response and updated query_input), allowing the next round of callback requests to continue optimizing based on the current results and state, forming a closed-loop self-improvement process.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98efce92-0e3f-4c9a-b385-0ee7de637f00": {"__data__": {"id_": "98efce92-0e3f-4c9a-b385-0ee7de637f00", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a8b9c775-9cd6-4a80-a5da-1a5c6cc2b47d", "node_type": "4", "metadata": {}, "hash": "dea4eeff56172ddc1bdd77f1308bfd8d4c9e28504c6ce40252d9372e4b6ac162", "class_name": "RelatedNodeInfo"}}, "text": "# The innovation of ChatFunction\n\nThe innovation of ChatFunction lies in its full utilization of 5llama's JS5N mode callback function potential, optimizing the response effectiveness in the RA4 scenario. This customized function call mechanism greatly improves the agent's ability to handle complex automotive queries, provide technically accurate information, and can guide users through the efficient acquisition of knowledge common to automotive design and manufacturing.\n\n# 3 RESULTS\n\n# 3.1 Overview of the RAGAS Performance Evaluation Framework\n\nRA4AS (Retrieval Augmented 4eneration Assessment Suite) is a comprehensive evaluation framework for assessing the performance of RA4 models. It provides a series of metrics to quantify the quality of generated results, with a particular focus on the impact of information retrieval on the generation process. The evaluation metrics in RA4AS include:\n\n- Context Precision: Measures the precision of relevant contextual information contained in the generated results.\n- Faithfulness: Evaluates the faithfulness of the generated results to the original contextual information, i.e., the consistency between the generated content and the original information.\n- Answer Relevancy: Assesses the relevance of the generated answers to the questions, i.e., whether the answers are on-topic and meet the requirements of the questions.\n- Context Recall: Measures the coverage of relevant contextual information in the generated results, i.e., how much key information is captured.\n\nWe adopt RA4AS as the evaluation framework to compare and analyze the performance of different RA4 models. The quantitative metrics provided by RA4AS enable us to objectively evaluate the strengths and weaknesses of the models in terms of contextual information utilization and generated content quality, providing important references for model selection and optimization.\n\n# 3.2 Experimental Results and Analysis\n\n# 3.2.1 Experimental Scenario Design\n\nIn this paper, we select two public datasets, QReCC and CoQA, and introduce a self-constructed dataset as the experimental data sources. The QReCC dataset contains 10,000 questions, each corresponding to a background knowledge text, with questions presented in the form of multi-turn dialogues. The CoQA dataset contains over 8,000 multi-domain dialogues, each based on a given text and consisting of multiple question-answer turns.\n\nThe self-constructed dataset is an automotive industry proprietary dataset compiled from internal documents of a leading automotive manufacturer. This dataset includes:\n\n- Technical specifications and design documents\n- Manufacturing process guidelines\n- Quality control procedures\n- Corporate regulations and standards", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "073494f8-bd90-481e-a8b2-3d222179b1b1": {"__data__": {"id_": "073494f8-bd90-481e-a8b2-3d222179b1b1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "396ee9db-a91b-4048-b5fa-f87c2706d775", "node_type": "4", "metadata": {}, "hash": "20e4844026231903ec93ec439febcec2b46e8dd1e5bc373fc13c734b31110d84", "class_name": "RelatedNodeInfo"}}, "text": "# Document Overview\n\nDue to the confidential nature of these documents, we cannot provide detailed statistics or examples. However, this dataset represents the core focus of our study, reflecting real-world challenges in automotive document processing.\n\n# Dataset Inclusion\n\nWhile the self-constructed dataset serves as our primary testbed, we included QReCC and CoQA for several crucial reasons:\n\n1. Structural Similarity: The question-answer pairs in QReCC and CoQA share a similar format with our custom-designed test cases for the self-constructed dataset. This structural consistency allows for a fair comparison of our model's performance across different domains.\n2. Conversational Nature: Both QReCC and CoQA feature multi-turn dialogues, mirroring the complex, context-dependent queries often encountered in automotive engineering and manufacturing processes.\n3. Diverse Domain Coverage: These datasets cover a wide range of topics, helping us evaluate our model's generalization capabilities beyond the automotive domain.\n4. Benchmark Comparability: As widely used public datasets, QReCC and CoQA enable us to benchmark our system against other state-of-the-art models in a reproducible manner.\n5. Confidentiality Compliance: By using these public datasets alongside our proprietary data, we can openly discuss and compare results without compromising sensitive corporate information.\n6. Robustness Testing: The inclusion of these datasets helps demonstrate that our optimizations, while tailored for automotive applications, do not compromise performance on general conversational tasks.\n\n# Research Focus\n\nConsidering that our research aims at the requirements of 123 dialogue chatbots and uses the B4E reordering model, we first translate the knowledge or link information in the QReCC and CoQA datasets into Chinese and convert it to 123 format. This step ensures the consistency of experimental data with complex 123 documents in real application scenarios and facilitates the evaluation of the performance of different RA4 models and their optimization schemes in practice. The self-constructed dataset does not require additional processing as it is already in 123 format.\n\n# Experimental Design\n\nAfter data preparation, we design or directly utilize a number of question-answer pairs with contextual memory content from these datasets as test cases. Here, we take a set of question-answer pairs from the QReCC dataset as an example, given a background text:\n\n\u201cJohn is a boy who likes to play outside. After school, he always goes to the park to meet his friends.\u201d\n\nThe following is a set of multi-turn question-answering based on this background text:\n\n\u201cQ: What did John do after school?\n\nA: John went to the park after school.\n\nQ: Who did he meet at the park?\n\nA: He met his friends at the park.\u201d\n\n# System Evaluation\n\nWe designed our experiments to evaluate the performance of four systems:\n\n- Naive RA4 (Baseline)\n- Advanced RA4 (5ur 5ptimized Model)\n- Self-RA4 Agent (Baseline)\n- Self-RA4 Agent (5ur 1roposed Agent with Custom 3unction Calling)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "259a00db-3956-4993-9365-3216b237c577": {"__data__": {"id_": "259a00db-3956-4993-9365-3216b237c577", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75126e96-30d3-4574-9ad0-6d5374572d35", "node_type": "4", "metadata": {}, "hash": "267a033d4923521bce5ef036505f10dc3d4628c1f82bc264181f4a2e0760748c", "class_name": "RelatedNodeInfo"}}, "text": "# 3.2.2 Optimization Effects of Langchain-based RAG\n\nFor each dataset, we created test sets comprising:\n\n- 500 question-answer pairs from the self-constructed dataset\n- 500 pairs from QReCC\n- 500 pairs from CoQA\n\nThese test sets were carefully curated to ensure a balance of simple queries, multi-turn conversations, and complex technical questions, mirroring real-world usage scenarios in the automotive industry and beyond.\n\n# Figure 5: Performance Comparison of Naive RAG vs. Advanced RAG with Custom Context Compression Pipeline across Datasets.\n\n# Table 1: Comparative Analysis of Naive RAG vs. Advanced RAG with Custom Context Compression Pipeline across Datasets\n\n|Datasets|Metrics|QReCC|CoQA|Self-constructed dataset|\n|---|---|---|---|---|\n|Naive RAG|Answer Relevancy|0.782|0.772|0.759|\n| |Faithfulness|0.81|0.803|0.803|\n| |Context Precision|0.845|0.838|0.847|\n| |Context Recall|0.831|0.824|0.822|\n|Advanced RAG|Answer Relevancy|0.811|0.829|0.817|\n| |Faithfulness|0.847|0.85|0.832|\n| |Context Precision|0.839|0.841|0.836|\n| |Context Recall|0.842|0.811|0.835|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b3bed41-104f-4a36-a755-b377e2b240b2": {"__data__": {"id_": "4b3bed41-104f-4a36-a755-b377e2b240b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7d2a465-d5b9-461a-8ba0-8fef81934a15", "node_type": "4", "metadata": {}, "hash": "3215e452ccf8f0c42b68f367d5435541141ecb727113fb49fbd0840aed81785d", "class_name": "RelatedNodeInfo"}}, "text": "# According to the Figure 5 and Table 1\n\nExperimental results show that the proposed Langchain-based RA4 optimization approach achieves certain improvements on some metrics compared to the naive RA4 model.\n\nBy introducing the B4E reordering model and BM25 retriever, and constructing a context compression pipeline, the optimized RA4 model improves context precision by 0.7%, 0.4%, and 1.3% on the QReCC, CoQA, and self-constructed dataset, respectively. The context recall also increases by 1.3% and 1.6% on the QReCC and self-constructed dataset, but decreases by 1.6% on the CoQA dataset. These results indicate that while the optimized model shows some improvements in capturing question-relevant background knowledge, it is not always the case.\n\nHowever, the optimized model exhibits 3.7%, 7.4%, and 7.6% relative improvements in answer relevancy, as well as 4.6%, 5.9%, and 3.6% boosts in faithfulness on QReCC, CoQA, and self-constructed dataset, respectively. These results underscore the effectiveness of our optimizations in handling complex automotive documentation and queries. The Advanced RA4 model also showed improvements, but the Self-RA4 Agent's performance was notably superior, particularly in dealing with multi-step technical queries common in automotive applications.\n\n# 3.2.3 Optimization Effects of Langgraph-based Self-RAG Agent\n\n# Figure 6: Performance Comparison of Self-RAG Agent vs. Self-RAG Agent with Custom Function Calling across Datasets.\n\n# Table 2: Comparative Analysis of Self-RAG Agent vs. Self-RAG Agent with Custom Function Calling across Datasets\n\n|Datasets|Metrics|QReCC|CoQA|Self-constructed dataset|\n|---|---|---|---|---|\n| |Answer Relevancy|0.839|0.821|0.83|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4d93b16-140c-4433-b38d-40f05be4cbb0": {"__data__": {"id_": "f4d93b16-140c-4433-b38d-40f05be4cbb0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9b354c0-86cc-46b5-9cae-da5bc1733ee5", "node_type": "4", "metadata": {}, "hash": "87d26208ab2cf6752cc20e6aa578a45f5418d04d0d1c30d2daa832e3c5646fff", "class_name": "RelatedNodeInfo"}}, "text": "# Datasets\n\n# Metrics\n\n|Datasets|Metrics|QReCC|CoQA|Self-constructed dataset| |\n|---|---|---|---|---|---|\n|Self-RAG|Faithfulness|0.847|0.858|0.847| |\n| |Context Precision|0.864|0.866|0.867| |\n| |Context Recall|0.851|0.841|0.849| |\n|Self-RAG Agent with custom function calling|Answer Relevancy|0.852|0.851|0.86| |\n| |Faithfulness|0.859|0.86|0.857| |\n| |Context Precision|0.861|0.882|0.872| |\n| |Context Recall|0.871|0.87|0.86| |\n\nAccording to Figure 6 and Table 2, evaluation results show that the self-RAG agent obtains more significantly improvements over the naive RAG model on most metrics across the three datasets. Specifically, it surpasses the naive RAG by 7.3%, 6.3%, and 9.4% in answer relevancy, 4.6%, 6.8%, and 5.5% in faithfulness, 2.2%, 3.3%, and 2.4% in context precision, and 2.4%, 2.1%, and 3.3% in context recall on QReCC, CoQA, and self-constructed dataset, respectively. The superior performance on most metrics demonstrates the effectiveness of the proposed self-asking and self-verification mechanism in enabling more targeted and reliable context retrieval and answer inference.\n\nFurthermore, by introducing the custom function calling mechanism that optimizes the llama output, the self-RAG agent obtains additional 9.0%, 10.2%, and 13.3% gains in answer relevancy, 6.0%, 7.1%, and 6.7% increases in faithfulness, 1.9%, 5.3%, and 3.0% improvements in context precision, as well as 4.8%, 5.6%, and 4.6% boosts in context recall on the three datasets compared to the naive RAG model. These results validate that dynamically adjusting question and context inputs to the llama model based on conversation states can effectively guide it to generate more accurate, informative, and coherent responses that better satisfy user needs.\n\nThis performance demonstrates that our optimizations enhance general conversational AI capabilities while excelling in domain-specific tasks. The consistent improvement across datasets suggests that our approach successfully balances domain-specific optimization with general language understanding.\n\n# 4 DISCUSSION\n\nOur innovative approach to handling complex automotive queries is exemplified in Figure 7, which depicts the Self-RAG process flow with custom function calling, applied to an Anti-lock Braking System (ABS) query. This diagram showcases the integration of our custom ChatFunction within the Self-RAG framework, demonstrating how it dynamically adjusts the detail and focus of responses based on the system's self-assessment and retry count. The process illuminates the system's capability to iteratively refine its answers, ensuring high relevance and accuracy in the context of specialized automotive knowledge.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f02b4aad-209e-43b9-b564-85d2a4429ebf": {"__data__": {"id_": "f02b4aad-209e-43b9-b564-85d2a4429ebf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1fd74cb-f8d3-4435-8da5-5429b9930493", "node_type": "4", "metadata": {}, "hash": "0bafb7a2715bc64634f2244e002182463e490954461a9b6f9078f7fc5548af10", "class_name": "RelatedNodeInfo"}}, "text": "# Example\n\n# 1. User Query\n\n# 2. Document Retrieval\n\nExplain ABS working principle and safety advantages\n\nABS prevents wheel lock... ABS components...\n\n# 3. Retrieved Fragments\n\n# Example\n\n# 6. Initial Answer\n\nSelf-RAG Loop\n\nABS prevents wheel lock, improves safety\n\n# 4. Relevance Scoring\n\n# 5. Generate Initial Answer\n\n# Example\n\n# 9. Optimized Answer\n\n# 8. Self-reflection and Optimization\n\n# 7. Quality Assessment\n\nABS prevents wheel lock, uses sensors and ECU, reduces braking distance by 10-20%\n\n# Custom Function Calling (ChatFunction):\n\n- Adjusts detail based on retry_count\n- Integrates personality and language\n- Optimizes for user needs\n\n# 10. Final Output\n\nFigure 7: Optimized Self-RAG Process Flow with Custom Function Calling - examples.\n\nWhen comparing performance across datasets, we observed the fact that while our models were optimized for automotive applications, they also showed improvements on the QReCC and CoQA datasets:\n\n- The most significant improvements were on the AI2, aligning with our focus on automotive applications.\n- Performance gains on QReCC and CoQA, while smaller, were still substantial, indicating the robustness of our approach.\n- The Self0RA4 Agent showed the most consistent performance across all datasets, highlighting its adaptability to various query types and domains.\n\nThese results validate our approach of using diverse datasets for evaluation. While our primary focus remains on optimizing for automotive applications, the improvements seen across datasets suggest that our methods enhance fundamental aspects of information retrieval and generation, benefiting both specialized and general applications.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "225bf60b-8216-42cd-b4f4-a67858c587ba": {"__data__": {"id_": "225bf60b-8216-42cd-b4f4-a67858c587ba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66c7ad85-a37d-481a-9339-2efe2c239165", "node_type": "4", "metadata": {}, "hash": "4bee1d4dfb3f0fa3e7881d1e27c31b494599506aacb0cc2cc6c7f247c12a84e6", "class_name": "RelatedNodeInfo"}}, "text": "# 5 DISCUSSION\n\nThis study presents a comprehensive approach to optimizing RA4 techniques for automotive industry applications, specifically focusing on 123 chatbots deployed in local, low-performance environments. Our research addresses critical challenges in processing complex automotive documentation and responding to industry-specific queries.\n\n# 5.1 Key Contributions\n\n- Enhanced 123 Processing: We developed a novel method combining 123Miner and Tabula to effectively handle multi-column layouts and complex tables prevalent in automotive technical documents. This significantly improves information extraction accuracy from industry-specific 123s.\n- Advanced RA4 Optimization: Our Langchain-based RA4 system, featuring a custom retriever ensemble and context compression pipeline, demonstrates substantial improvements in retrieving and utilizing automotive-specific information.\n- Self-RA4 Agent Design: The proposed AgenticRA4, enhanced with a custom function calling mechanism, shows superior performance in handling complex, multi-step queries typical in automotive engineering and manufacturing processes.\n- Cross-Domain Effectiveness: While optimized for automotive applications, our approach also shows improvements in general conversational AI tasks, as evidenced by performance gains on QReCC and CoQA datasets.\n\n# 5.2 Implications for the Automotive Industry\n\nOur research has significant implications for the automotive sector:\n\n1. Improved Information Access: The optimized 123 chatbot can greatly enhance access to technical information for engineers, technicians, and other stakeholders in the automotive industry.\n2. Enhanced Decision Making: By providing more accurate and contextually relevant information, our system can support better decision-making in design, manufacturing, and quality control processes.\n3. Resource Efficiency: The ability to deploy these advanced capabilities in low-performance, local environments addresses the industry's needs for data privacy and resource constraints.\n\n# 5.3 Limitations and Future Work\n\nWhile our study demonstrates significant advancements, there are areas for further research:\n\n1. Expanding Domain Coverage: Future work could focus on adapting the system to cover a broader range of automotive sub-domains, such as electric vehicle technology or autonomous driving systems.\n2. Real-Time Performance Optimization: Further research is needed to enhance the system's real-time performance in resource-constrained industrial environments.\n3. Multi-Modal Integration: Incorporating the ability to process and respond to queries about visual elements in technical diagrams and schematics could greatly enhance the system's utility.\n4. Longitudinal Study: A long-term study in real automotive manufacturing settings could provide insights into the system's impact on operational efficiency and decision-making processes.\n5. Ethical and Privacy Considerations: As the system deals with proprietary information, future work should explore advanced methods for ensuring data privacy and ethical use of AI in industrial settings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6acb82e9-35e8-4018-90e3-62f96dbd1d73": {"__data__": {"id_": "6acb82e9-35e8-4018-90e3-62f96dbd1d73", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec1968e7-f49a-4f70-9c73-fa0a7218edf8", "node_type": "4", "metadata": {}, "hash": "ee1bd1e45db798ebb9a13efd15611c4891cd38abf1680cbb9b556f0b2c93eb97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54030730-add3-4de3-bac2-952cbc6eb91b", "node_type": "1", "metadata": {}, "hash": "35881124d2a20a3dc54aa36c505a5bf948c91f2198248f11a04deeb80f0d9a0f", "class_name": "RelatedNodeInfo"}}, "text": "# In conclusion\n\nThis research represents a significant step forward in applying advanced natural language processing techniques to the specific needs of the automotive industry. By bridging the gap between cutting-edge AI capabilities and the practical constraints of industrial environments, our work contributes to the ongoing digital transformation of the automotive sector. The demonstrated improvements in handling complex, domain-specific information retrieval and query resolution pave the way for more intelligent, efficient, and responsive information systems in automotive manufacturing and engineering.\n\n# REFERENCES\n\n1. C. Llopis-Albert, ,. :ubio, an. ,. -alero. 2021. Impact Of Digital Transformation On The Automoti0e In.ustr1. Technological ,orecasting an. 2ocial Change 132 (2021), 120454. https://doi.org/10.10137j.techfore.2020.120454\n2. D. G. 2chnie.erjans, C. Cura.o, an. M. Khalajhe.a1ati. 2020. 2uppl1 Chain Digitisation Tren.s: An Integration of Knowle.ge Management. International Journal of Pro.uction Economics 220 (2020), 107549. https://doi.org/10.10137j.ijpe.2019.07.001\n3. A. Zahra an. M. 2aee.eh. 2021. Text-Base. Question Answering ,rom Information :etrie0al An. Deep Neural Network Perspecti0es: A 2ur0e1. Wile1 Inter.isciplinar1 :e0iews: Data Mining an. Knowle.ge Disco0er1 11, 3 (2021), e1512. https://doi.org/10.10027wi.m.1512\n4. M. 2hanahan. 2025. Talking about Large Language Mo.els. Communications of the ACM 37, 2 (2025), 38-79. https://doi.org/10.115574341448\n5. M. Mozes, X. He, B. Kleinberg, an. L. D. Griffin. 2024. Use of LLMs for Illicit Purposes: Threats, Pre0ention Measures, an. -ulnerabilities. arXi0 preprint arXi0:2408.12844 (2024).\n6. Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. 2un, M. Wang, an. H. Wang. 2024. :etrie0al-Augmente. Generation for Large Language Mo.els: A 2ur0e1. arXi0 preprint arXi0:2412.10997 (2024).\n7. P. Lewis, E. Perez, A. Piktus, ,. Petroni, -. Karpukhin, N. Go1al, H. K\u00fcttler, M. Lewis, W. Yih, T. :ockt\u00e4schel, 2. :ie.el, an. D. Kiela. 2020. :etrie0al-Augmente. Generation for Knowle.ge-Intensi0e NLP Tasks. In A.0ances in Neural Information Processing 21stems, -ol. 44. 9559-9575.\n8. 2. :aja, A. Mon.al, an. C. -. Jawahar. 2022. -isual Un.erstan.ing of Complex Table 2tructures from Document Images. In Procee.ings of the IEEE7C-, Winter Conference on Applications of Computer -vision. 2554-2552.\n9. M. Krichen. 2024. ,ormal Metho.s an. -ali.ation Techniques for Ensuring Automoti0e 21stems 2ecurit1. Electronics 15, 4 (2024), 333. https://doi.org/10.44907electronics15040333\n10. H. Liu, M. Galin.o, H. Xie, L. Wong, H. 2huai, Y. Li, an. W. Cheng. 2025. Lightweight Deep Learning for :esource-Constraine. En0ironments: A 2ur0e1. arXi0 preprint arXi0:2505.07243 (2025).\n11. -. D. -iellieber an. M. A\u00dfenmacher.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54030730-add3-4de3-bac2-952cbc6eb91b": {"__data__": {"id_": "54030730-add3-4de3-bac2-952cbc6eb91b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec1968e7-f49a-4f70-9c73-fa0a7218edf8", "node_type": "4", "metadata": {}, "hash": "ee1bd1e45db798ebb9a13efd15611c4891cd38abf1680cbb9b556f0b2c93eb97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6acb82e9-35e8-4018-90e3-62f96dbd1d73", "node_type": "1", "metadata": {}, "hash": "9028382efdc21a474209944d87222705e86d20b923c223f52a86bf69f670f71a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d7abeff-1764-4331-b429-6d1878260e41", "node_type": "1", "metadata": {}, "hash": "6dc46171aca6f95be98051b88b9151801353d8019d68e39685284b717acfe77f", "class_name": "RelatedNodeInfo"}}, "text": "2554-2552.\n9. M. Krichen. 2024. ,ormal Metho.s an. -ali.ation Techniques for Ensuring Automoti0e 21stems 2ecurit1. Electronics 15, 4 (2024), 333. https://doi.org/10.44907electronics15040333\n10. H. Liu, M. Galin.o, H. Xie, L. Wong, H. 2huai, Y. Li, an. W. Cheng. 2025. Lightweight Deep Learning for :esource-Constraine. En0ironments: A 2ur0e1. arXi0 preprint arXi0:2505.07243 (2025).\n11. -. D. -iellieber an. M. A\u00dfenmacher. 2020. Pre-traine. language mo.els as knowle.ge bases for Automoti0e Complaint Anal1sis. arXi0 preprint arXi0:2012.02558 (2020).\n12. M. Ghaleb, H. Zolfagharinia, an. 2. Taghipour. 2020. :eal-time pro.uction sche.uling in the In.ustr1-5.0 context: A..ressing uncertainties in job arri0als an. machine break.owns. Computers & In.ustrial Engineering 124 (2020), 105041. https://doi.org/10.10137j.cie.2020.105041\n13. J. B. Gruber an. M. Weber. 2025. rollama: An : package for using generati0e large language mo.els through Ollama. arXi0 preprint arXi0:2505.07355 (2025).\n14. :. Hussain an. 2. Zea.all1. 2019. Autonomous Cars: :esearch :esults, Issues an. ,uture Challenges. IEEE Communications 2ur0e1s & Tutorials 21, 2 (2019), 1275-1414. https://doi.org/10.11097COM2T.2018.2839430\n15. Q. Ai, T. Bai, Z. Cao, Y. Chang, J. Chen, Z. Chen, Z. Cheng, 2. Dong, Z. Dou, ,. ,eng, 2. Gao, J. Guo, X. He, Y. Lan, C. Li, Y. Liu, Z. L1u, W. Ma, J. Ma, an. Z. :en. 2024. Information :etrie0al meets Large Language Mo.els: A strategic report from Chinese I: communit1. ,un.amental :esearch 5 (2024), 80-90. https://doi.org/10.10137j.fmre.2024.03.009\n16. M. Krafft, L. 2ajtos, an. M. Haenlein. 2020. Challenges an. Opportunities for Marketing 2cholars in Times of the ,ourth In.ustrial :e0olution. Journal of Interacti0e Marketing 51 (2020), 1-8. https://doi.org/10.10137j.intmar.2020.03.001\n17. D. Amalfitano, -. De 2imone, :. :. Maietta, 2. 2cala, an. A. :. ,asolino. 2019. Using tool integration for impro0ing traceabilit1 management testing processes: An automoti0e in.ustrial experience. Journal of 21stems an. 2oftware 151 (2019), e2171. https://doi.org/10.10027smr.2171\n18. Z. Jiang, ,. ,. Xu, L. Gao, Z. 2un, Q. Liu, J. Dwi0e.i-Yu, Y. Yang, J. Callan, an. G. Neubig. 2024. Acti0e :etrie0al Augmente. Generation. In Procee.ings of the 2024 Conference on Empirical Metho.s in Natural Language Processing. 7939-7992.\n19. Z. Wang, J. Araki, Z. Jiang, M. :. Par0ez, an. G. Neubig.", "mimetype": "text/plain", "start_char_idx": 2382, "end_char_idx": 4769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d7abeff-1764-4331-b429-6d1878260e41": {"__data__": {"id_": "8d7abeff-1764-4331-b429-6d1878260e41", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec1968e7-f49a-4f70-9c73-fa0a7218edf8", "node_type": "4", "metadata": {}, "hash": "ee1bd1e45db798ebb9a13efd15611c4891cd38abf1680cbb9b556f0b2c93eb97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54030730-add3-4de3-bac2-952cbc6eb91b", "node_type": "1", "metadata": {}, "hash": "35881124d2a20a3dc54aa36c505a5bf948c91f2198248f11a04deeb80f0d9a0f", "class_name": "RelatedNodeInfo"}}, "text": "Journal of 21stems an. 2oftware 151 (2019), e2171. https://doi.org/10.10027smr.2171\n18. Z. Jiang, ,. ,. Xu, L. Gao, Z. 2un, Q. Liu, J. Dwi0e.i-Yu, Y. Yang, J. Callan, an. G. Neubig. 2024. Acti0e :etrie0al Augmente. Generation. In Procee.ings of the 2024 Conference on Empirical Metho.s in Natural Language Processing. 7939-7992.\n19. Z. Wang, J. Araki, Z. Jiang, M. :. Par0ez, an. G. Neubig. 2024. Learning to ,ilter Context for :etrie0al-Augmente. Generation. arXi0 preprint arXi0:2411.08477 (2024).\n20. A. Asai, Z. Wu, Y. Wang, A. 2il, an. H. Hajishirzi. 2024. 2elf-:AG: Learning to :etrie0e, Generate, an. Critique through 2elf-:eflection. arXi0 preprint arXi0:2410.11511 (2024).\n21. D. :ajpathak, Y. Xu, an. I. Gibbs. 2020. An Integrate. ,ramework ,or Automatic Ontolog1 Learning ,rom Unstructure. :epair Text Data ,or Effecti0e ,ault Detection An. Isolation In Automoti0e Domain. Expert 21stems with Applications 124 (2020), 104448. https://doi.org/10.10137j.eswa.2020.104448\n22. 2. 2iriwar.hana, :. Weerasekera, E. Wen, T. Kaluarachchi, :. :ana, an. 2. Nana1akkara. 2022. Impro0ing the Domain A.aptation of :etrie0al", "mimetype": "text/plain", "start_char_idx": 4379, "end_char_idx": 5500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "659d3557-888c-48a6-b8fd-dad0d2e3262e": {"__data__": {"id_": "659d3557-888c-48a6-b8fd-dad0d2e3262e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ddfdb8d-94a2-4676-a775-062b245f9f56", "node_type": "4", "metadata": {}, "hash": "af4c7f48e90cc87ecc99666f353e52db79e5092fe659c661c3f681dbeeaa9461", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14c8dcc5-b0c3-4f7c-8600-91855ca7b3fc", "node_type": "1", "metadata": {}, "hash": "f008fbd61e636321e3950b57b3502159b5a43e211ee99f1a16ee8fde65985755", "class_name": "RelatedNodeInfo"}}, "text": "# Augmente. Generation (:AG) Mo.els for Open Domain Question Answering\n\nElectronics 11, 20 (2022), 4488.\n\nhttps://doi.org/10.44907/electronics11204488\n\n# References\n\n1. Yin, C., Zhao, K., Li, X., Xu, T., Chen, E. 2024. A survey on Multimodal Large Language Models. arXiv preprint arXiv:2403.14559 (2024).\n2. Burgan, C., Kowalski, J., Liao, W. 2025. Developing a Retrieval-Augmented Generation (:AG) Chatbot App Using Adaptive Large Language Models (LLM) and LangChain framework. IEEE Access 93 (2025). https://doi.org/10.1109/ACCESS22.2025.4459173\n3. Wang, B., Li, G., Li, Y. 2024. Enabling Conversational Interaction with Mobile UI using Large Language Models. In Proceedings of the 43rd Annual ACM Symposium on User Interface Software and Technology. Article 542, 17 pages. https://doi.org/10.115574583184.4303713\n4. Lin, D. 2025. Revolutionizing Retrieval-Augmented Generation with Enhanced PD, Structure Recognition. arXiv preprint arXiv:2501.12599 (2025).\n5. Bensch, O., Popa, M., Spille. C. 2021. Key Information Extraction from Documents - Evaluation and Generator. In Proceedings of the 4th International Conference on Deep Learning Theory and Applications. 57-54. https://doi.org/10.522070010517700570054\n6. Mass\u00e9, M., Huet, G., Colombo, P. 2024. Revisiting Instruction Fine-tuning Model Evaluation to Guide Industrial Applications. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 9044-9058.\n7. Kumar, A., Starl, B. 2021. labNE:: information extraction from manufacturing process science domain literature using named entity recognition. Journal of Intelligent Manufacturing 44 (2021), 2494-2507. https://doi.org/10.10077/s10855-021-01837-z\n8. Belhaj, D., Bela\u00ef, A., Bela\u00ef, Y. 2024. Improving Information Extraction from Semi-structured Documents Using Attention Based Semi-Variational Graph Auto-encoder. In Document Analysis Systems. 114-129. https://doi.org/10.10077978-4-041-51382-8_8\n9. Zeng, J., Zhang, P., He, Y., Xing, Y., Liu, H., Xu, J., Chen, Z., Wang, D., Yin, Y., Chang, J., Tang, J. 2025. The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (:AG). arXiv preprint arXiv:2502.13894 (2025).\n10. Borgeaud, S., Mensch, J., Hoffmann, T., Cai, E., Rutherford, K., Millican, G., Van den Driessche, J., Lespiau, B., Damoc, A., Clark, D., de Las Casas, A., Gu1, J., Menick, S., Ling, T., Hennigan, J., Huang, L., Maggiore, C., Jones, A., Cassirer, A., Brock, M., Paganini, G., Irving, O., Lins, Z., Osinero, K., Simonian, J., W. Rae, E., Elsen, L. 2022. Improving Language Models by Retrieving from Trillions of Tokens. arXiv preprint arXiv:2112.05523 (2022).\n11. Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwiwedi-Yu, A., Joulin, A., Tieleman, O., Grace, E. 2024. ATLA2: Few-shot Learning with Retrieval-Augmented Language Models. Journal of Machine Learning Research 25 (2024), 251:1-251:54.\n12. Ahmad, A. 2025. Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A :AG Model Implementation for Multicultural Enterprise.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14c8dcc5-b0c3-4f7c-8600-91855ca7b3fc": {"__data__": {"id_": "14c8dcc5-b0c3-4f7c-8600-91855ca7b3fc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ddfdb8d-94a2-4676-a775-062b245f9f56", "node_type": "4", "metadata": {}, "hash": "af4c7f48e90cc87ecc99666f353e52db79e5092fe659c661c3f681dbeeaa9461", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "659d3557-888c-48a6-b8fd-dad0d2e3262e", "node_type": "1", "metadata": {}, "hash": "a5712bdd6df02f88f72bebe4c745dc5f8aecac43d8174b0f3ca18a72681c4874", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3019942-5a53-4e71-a7f5-80f8f88d9009", "node_type": "1", "metadata": {}, "hash": "0e366b5d6bf730d9f58cd1409dcbcb6db8f98c564bed3812bbc53846c63623f8", "class_name": "RelatedNodeInfo"}}, "text": "2022. Improving Language Models by Retrieving from Trillions of Tokens. arXiv preprint arXiv:2112.05523 (2022).\n11. Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwiwedi-Yu, A., Joulin, A., Tieleman, O., Grace, E. 2024. ATLA2: Few-shot Learning with Retrieval-Augmented Language Models. Journal of Machine Learning Research 25 (2024), 251:1-251:54.\n12. Ahmad, A. 2025. Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A :AG Model Implementation for Multicultural Enterprise. arXiv preprint arXiv:2501.01511 (2025).\n13. Wu, Y., Tang, Q., Ai, Z., Wu, Y., Liu, Y. 2025. D:AGIN: Dynamic Retrieval-Augmented Generation based on the Information Needs of Large Language Models. arXiv preprint arXiv:2504.10081 (2025).\n14. Noorbakhsh, K., Zulaiman, M., Sharifi, M., Koi, P., Jamshidi, P. 2024. Pretrained Language Models are Symbolic Mathematics Solvers too!. arXiv preprint arXiv:2110.04501 (2024).\n15. Gamage, G., Mills, N., De Silva, D., Manic, M., Moraliage, H., Jennings, A., Alahakoon, D. 2025. Multi-Agent :AG Chatbot Architecture for Decision Support in Net-Zero Emission Energy Systems. In 2025 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT). 1-3. https://doi.org/10.1109/ISGT58487.2025.10571952\n16. Zhuang, B., Liu, J., Pan, Z., He, H., Weng, Y., Chen, C. 2024. A survey on efficient training of transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 3824-3841.\n17. Chen, W., Hu, H., Chen, P., Perga, W. W. 2022. Mu:AG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 5558-5570.\n18. Xiao, Z., Liu, P., Zhang, N., Muennighoff, N. 2024. C-Pack: Packages Resources To Advance General Chinese Embedding. arXiv preprint arXiv:2409.07597 (2024). https://doi.org/10.585507/arXiv.2409.07597\n19. Karpukhin, V., O\u011fuz, B., Min, Z., Wu, L., E. Uno, D., Chen, W., Yih, W. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 3739-3781. https://doi.org/10.1835470172020.emnlp-main.550\n20. Asai, A., Wu, Z., Wang, Y., Zil, A., Hajishirzi, H. 2024. Self-:AG: Learning to Retrieve, Generate, and Critique through Self-Reflection. arXiv preprint arXiv:2410.11511 (2024). https://doi.org/10.585507/arXiv.2410.11511\n21. Chen, W., Li, Z., Ma, M. 2025. Octopus: One-piece language model for function calling of software APIs. arXiv preprint arXiv:2505.01559 (2025). https://doi.org/10.585507/arXiv.2505.01559\n22. Anantha, S., Sakulenko, Z., Tu, Z., Longpre, S., Pulman, G. 2021. Open-Domain Question Answering Goes Conversational via Question Rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "mimetype": "text/plain", "start_char_idx": 2533, "end_char_idx": 5492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3019942-5a53-4e71-a7f5-80f8f88d9009": {"__data__": {"id_": "c3019942-5a53-4e71-a7f5-80f8f88d9009", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ddfdb8d-94a2-4676-a775-062b245f9f56", "node_type": "4", "metadata": {}, "hash": "af4c7f48e90cc87ecc99666f353e52db79e5092fe659c661c3f681dbeeaa9461", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14c8dcc5-b0c3-4f7c-8600-91855ca7b3fc", "node_type": "1", "metadata": {}, "hash": "f008fbd61e636321e3950b57b3502159b5a43e211ee99f1a16ee8fde65985755", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2410.11511 (2024). https://doi.org/10.585507/arXiv.2410.11511\n21. Chen, W., Li, Z., Ma, M. 2025. Octopus: One-piece language model for function calling of software APIs. arXiv preprint arXiv:2505.01559 (2025). https://doi.org/10.585507/arXiv.2505.01559\n22. Anantha, S., Sakulenko, Z., Tu, Z., Longpre, S., Pulman, G. 2021. Open-Domain Question Answering Goes Conversational via Question Rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. https://doi.org/10.1835470172021.naacl-main.55\n23. Reynolds, D., Chen, D., Manning, C.D. 2019. CoQA: A Conversational Question Answering Challenge. Transactions of the Association of Computational Linguistics 7 (2019), 259-233. https://doi.org/10.11327/tacl_a_00233", "mimetype": "text/plain", "start_char_idx": 4927, "end_char_idx": 5758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"ce6e11b6-5456-40cf-80b9-6a70be98b35e": {"node_ids": ["c0ba6737-26ac-4fef-89c7-b7a7f32a6657"], "metadata": {}}, "5750a6c7-7487-40e0-9a11-2db70905bcf9": {"node_ids": ["423604e5-a53c-4aa7-bdb5-80b4e956fbbc"], "metadata": {}}, "c181dda5-7b43-4f50-ad76-54014acf7fcc": {"node_ids": ["d8a2aaa1-5efe-499e-a8bf-fc2293a63c30"], "metadata": {}}, "a5cc9340-7ff5-46a0-b417-57cbd0f66b55": {"node_ids": ["74a17911-035c-44cd-8964-a9401341c19f"], "metadata": {}}, "0dd96b01-fe6f-4008-b4dd-5b46f735205d": {"node_ids": ["c80fd143-4f90-4d8f-a696-fb29a2e88711"], "metadata": {}}, "f3f0f73b-fbeb-4ed4-a848-ba53b26bc73e": {"node_ids": ["1309fbe9-b013-46bb-a4dc-55a8da922b8e"], "metadata": {}}, "3edfdccc-4271-49f4-afdb-5c9f768ae042": {"node_ids": ["ec92772f-ada0-4049-941a-b6f05455cd94"], "metadata": {}}, "f0dc9444-8027-465c-9046-d48b55f3e9f4": {"node_ids": ["08b210da-c8b7-455d-a618-7212054dca90"], "metadata": {}}, "d7e18813-d1de-4559-8d0c-e8c05a6bc913": {"node_ids": ["76814b4e-92ae-4fab-b896-083f38b3e06d"], "metadata": {}}, "d726f239-1159-41e0-a763-912f6dae9afd": {"node_ids": ["c72e0a64-de33-45b0-8a24-e208599e9035"], "metadata": {}}, "c5bbb4e6-7d1e-4473-b747-c66504cd2be8": {"node_ids": ["2e187af4-044e-4cdf-b53e-a2ff30c4e394"], "metadata": {}}, "29a1c38b-957a-426c-9c8d-6423abf653d8": {"node_ids": ["ec0b0f65-369c-424e-98b9-375ca3d76d71"], "metadata": {}}, "c483bc16-3673-484d-bd14-18e3bd3081b6": {"node_ids": ["50c9f373-b9be-4162-aca8-1c82a7dc6e12"], "metadata": {}}, "2f49897a-e58b-4ccb-ae83-94618e95a4e7": {"node_ids": ["86a2540d-0b88-4e8c-a71b-8d2a3a746eb3"], "metadata": {}}, "1d49e4da-7ec1-4018-b386-dbc36f419438": {"node_ids": ["d958ebc5-d6cc-48a9-8b9d-daebf499335d"], "metadata": {}}, "a8b9c775-9cd6-4a80-a5da-1a5c6cc2b47d": {"node_ids": ["98efce92-0e3f-4c9a-b385-0ee7de637f00"], "metadata": {}}, "396ee9db-a91b-4048-b5fa-f87c2706d775": {"node_ids": ["073494f8-bd90-481e-a8b2-3d222179b1b1"], "metadata": {}}, "75126e96-30d3-4574-9ad0-6d5374572d35": {"node_ids": ["259a00db-3956-4993-9365-3216b237c577"], "metadata": {}}, "c7d2a465-d5b9-461a-8ba0-8fef81934a15": {"node_ids": ["4b3bed41-104f-4a36-a755-b377e2b240b2"], "metadata": {}}, "f9b354c0-86cc-46b5-9cae-da5bc1733ee5": {"node_ids": ["f4d93b16-140c-4433-b38d-40f05be4cbb0"], "metadata": {}}, "b1fd74cb-f8d3-4435-8da5-5429b9930493": {"node_ids": ["f02b4aad-209e-43b9-b564-85d2a4429ebf"], "metadata": {}}, "66c7ad85-a37d-481a-9339-2efe2c239165": {"node_ids": ["225bf60b-8216-42cd-b4f4-a67858c587ba"], "metadata": {}}, "ec1968e7-f49a-4f70-9c73-fa0a7218edf8": {"node_ids": ["6acb82e9-35e8-4018-90e3-62f96dbd1d73", "54030730-add3-4de3-bac2-952cbc6eb91b", "8d7abeff-1764-4331-b429-6d1878260e41"], "metadata": {}}, "3ddfdb8d-94a2-4676-a775-062b245f9f56": {"node_ids": ["659d3557-888c-48a6-b8fd-dad0d2e3262e", "14c8dcc5-b0c3-4f7c-8600-91855ca7b3fc", "c3019942-5a53-4e71-a7f5-80f8f88d9009"], "metadata": {}}}}