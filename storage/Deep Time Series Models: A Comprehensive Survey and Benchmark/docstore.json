{"docstore/metadata": {"4646fe6d-69f6-47fb-8a07-b9b5556dee31": {"doc_hash": "75c042c5960dee0c8174987a06600379e8534505f4539bd7ab0c2105cb029549"}, "b3b9cc59-e544-4b89-b675-0cc6d3f42333": {"doc_hash": "b382e3ee968725ab573ef160636b7e39a80e23d00f8b010b75bb2d9732154a43"}, "05ea54b4-c39d-40fa-9bfa-d21891ed0ab8": {"doc_hash": "39440827ff07ee423dfc1783a243a8ff51157e20a4e1a8272ec18db99eada161"}, "14fe124f-6d06-4dff-9d80-c251345029a6": {"doc_hash": "7c08d51d256035a14a3531ae4566d50c7e6edc8ac1c9f7d79f5f40c7011954c3"}, "6de13856-3f3f-41be-b8d4-84d08d2bdcbc": {"doc_hash": "76a25d979cb8180363a121e30f6e83206d7e5874992179d4648e24f7211da38b"}, "bfe147d0-16af-4210-924b-1d68aed1ffea": {"doc_hash": "66d3c5eb8a06dd6fdfff50b2e4b3e30c44345f33e74426324f123183dd4739be"}, "0466ba57-a853-4959-a43a-7109aec4a3e5": {"doc_hash": "83d80ec72c46a220718205cde66bfe3e679ddbbb945a13532b24012a8819e883"}, "e2e2bc4a-9b7f-4d48-b52c-427e1694f1ca": {"doc_hash": "992cd36a856384c07edcd25ecd6eb13e30f01d3acdddb36646a34d769c8baf61"}, "47e45b25-ad29-4226-9f87-16c9994ea98e": {"doc_hash": "0f6131d85b7316932c131f1dc25300f1d0e16ddc4bc830f182fec0029223355e"}, "01943cde-58e7-41e0-b2a1-a3973976a0a7": {"doc_hash": "2d84b6ae6864b6a10acfe5abafdd314d4c7c5a53cbb47083ac6e419f0a20647e"}, "5326360c-a26d-47f3-9c7a-86d857923a50": {"doc_hash": "8dd45325345a433daccae8b5c7cd063f409184788c3d882b4c56efd230d6da4c"}, "1f2ceb51-bbbb-433c-9170-b96c39b237b1": {"doc_hash": "e9138944f105971b7de388ddd87b1788c2befa5394b4a1f0f4361f1d4ab9b7a7"}, "2780921a-8944-4247-b1ab-d497c1a9a2c0": {"doc_hash": "b498ebf86f1479ae5ce1add866f56f043496d4c9604a5264d94c0331c44cb07e"}, "01baafe9-b1d0-4f36-8420-8882f1c06f33": {"doc_hash": "7107477183d8e19f2a9dee85f3e1be45b7939ea710972337aca3b36688cff1e7"}, "4c4ba3cc-80d3-4f87-8cd1-fcdadb8267f2": {"doc_hash": "a3bc1fd7bd98ac1bf060844e2b3a1a2c691883ac6663d8146172339d953291e3"}, "d304b9c7-7e0a-4974-b19c-b7233486d6c7": {"doc_hash": "50598629cfcc324c9a3cd5fb7d24068a40da415c26bd7bc68d0b068817fc55ed"}, "a6991055-05ca-46b1-bc0e-a8e474361bf8": {"doc_hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd"}, "355cab74-ec27-4cae-afc8-c66a4a137f48": {"doc_hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563"}, "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9": {"doc_hash": "f6cba8eb167bb42d86a6abce9f753afd601c26351028e82e748f2c449fb76e98"}, "08137354-5b17-48fb-bb75-e73e14190f1f": {"doc_hash": "4183a167e005071191bde7f1090f20802665564657b00299a7c0d964b5ef3fe6"}, "68d1e231-852f-4fb1-916d-29e135943c23": {"doc_hash": "75c042c5960dee0c8174987a06600379e8534505f4539bd7ab0c2105cb029549", "ref_doc_id": "4646fe6d-69f6-47fb-8a07-b9b5556dee31"}, "48afc03c-980b-4381-bb08-0fbc6b46ed7f": {"doc_hash": "13c36d104ef545fa5840ec8e629b19c0e5719cdf2a0d3cf135c2ef022601a29f", "ref_doc_id": "b3b9cc59-e544-4b89-b675-0cc6d3f42333"}, "546deb80-1deb-4342-a37f-906064d7ea29": {"doc_hash": "7fc356da9e40dde4c738955742399c3207efc2f5e1ba5de8c2bde049dbe42d40", "ref_doc_id": "b3b9cc59-e544-4b89-b675-0cc6d3f42333"}, "56c50822-bc3b-489f-a3c7-946943edef72": {"doc_hash": "39440827ff07ee423dfc1783a243a8ff51157e20a4e1a8272ec18db99eada161", "ref_doc_id": "05ea54b4-c39d-40fa-9bfa-d21891ed0ab8"}, "97742285-32ad-4e78-aafc-c282c030f702": {"doc_hash": "1d1dab5124af2d8f9c2ad37e26e34ec0d7eee86f16b0dae150e57d77af523141", "ref_doc_id": "14fe124f-6d06-4dff-9d80-c251345029a6"}, "991b0249-1a37-48de-9e88-a7ab867edc35": {"doc_hash": "b0482959b541d16831c426844d238166f2abdfeb36ae840d44d93b61f64f805d", "ref_doc_id": "14fe124f-6d06-4dff-9d80-c251345029a6"}, "c477450a-0082-441e-ad8d-1b4b8c5c94b2": {"doc_hash": "1b4b73d5e8c0549d82cb9750ace716f19e46184a6151184e8e630241fd274adb", "ref_doc_id": "6de13856-3f3f-41be-b8d4-84d08d2bdcbc"}, "7b63a422-05ff-4edd-9126-a0c2e762b0b0": {"doc_hash": "c4da3b2d46edec6f7b9a663676c07303967672de1bf0f8a3d631c27821b94f65", "ref_doc_id": "6de13856-3f3f-41be-b8d4-84d08d2bdcbc"}, "4e7604e6-60b7-4e25-9a09-0267a3f5f39e": {"doc_hash": "66d3c5eb8a06dd6fdfff50b2e4b3e30c44345f33e74426324f123183dd4739be", "ref_doc_id": "bfe147d0-16af-4210-924b-1d68aed1ffea"}, "765aa66d-4a76-45a0-b0c7-a9b9e37b876a": {"doc_hash": "82fc455cd6a225486590d33c0b0ba119b9d79726be80b0e37f6b2baed3fe0027", "ref_doc_id": "0466ba57-a853-4959-a43a-7109aec4a3e5"}, "8af78ae3-90f4-433e-aeaf-8e4abba4a373": {"doc_hash": "ee029efb875e20c861d7e708f9ef47d1027e66dae839f2a4e630700d0a0cc271", "ref_doc_id": "0466ba57-a853-4959-a43a-7109aec4a3e5"}, "b0ea1e9c-8187-4177-bb3b-880e470a478f": {"doc_hash": "992cd36a856384c07edcd25ecd6eb13e30f01d3acdddb36646a34d769c8baf61", "ref_doc_id": "e2e2bc4a-9b7f-4d48-b52c-427e1694f1ca"}, "d5540e1c-a157-441e-bedc-3afa46cdc743": {"doc_hash": "0f6131d85b7316932c131f1dc25300f1d0e16ddc4bc830f182fec0029223355e", "ref_doc_id": "47e45b25-ad29-4226-9f87-16c9994ea98e"}, "d2120e11-9b01-48ff-884f-762b1d6990c9": {"doc_hash": "f1b2ab53c37ff0f60710bec823833b2d11f9067488cff0f45fb605cf387d9528", "ref_doc_id": "01943cde-58e7-41e0-b2a1-a3973976a0a7"}, "55f10f37-5624-45e3-ab29-a8162918fb53": {"doc_hash": "126a9db7b5740534c746ab8bf491d761f79ed61e69d2bbb5b28340187375d9d1", "ref_doc_id": "01943cde-58e7-41e0-b2a1-a3973976a0a7"}, "e5b60d91-1495-4159-ad71-9251142c0dc4": {"doc_hash": "7ce3a80f0f38bf416ef0cafb69bb3536bf1663bfc467fd6ad189282185c67470", "ref_doc_id": "5326360c-a26d-47f3-9c7a-86d857923a50"}, "a14168c7-d532-4c68-974e-5b38778e27e7": {"doc_hash": "44ca02a9e4b0db24f891bdd31919653ef3f77118eaf411f65d21f2c119d5b2b8", "ref_doc_id": "5326360c-a26d-47f3-9c7a-86d857923a50"}, "77659c59-d4d8-43d1-81a7-9407c6bab1b2": {"doc_hash": "27dc6e9a31c9d63494b5a69da08b106f1305e0466df0310057bfdeac18e2421a", "ref_doc_id": "1f2ceb51-bbbb-433c-9170-b96c39b237b1"}, "d20039e4-1eea-4ba2-ac20-087acbc470c8": {"doc_hash": "85b331091d63ea0cdc50cfbd762ecf92295ff7231b65fd69d8e0a4cbc8cd56db", "ref_doc_id": "1f2ceb51-bbbb-433c-9170-b96c39b237b1"}, "6b6850fd-e78e-4435-aa52-6af74e627fc7": {"doc_hash": "16424320f73b9b55fc1eda39b71c67273a30e20e1e0315622d6254df3fe46476", "ref_doc_id": "2780921a-8944-4247-b1ab-d497c1a9a2c0"}, "55b0cac7-36a7-47d1-a546-2b07054d8b42": {"doc_hash": "7522d3e24bfe1d001064814beb17c2e1e1d3e19940716e09de17dba7bf201d63", "ref_doc_id": "2780921a-8944-4247-b1ab-d497c1a9a2c0"}, "9a61bbda-0c36-42ea-8b39-a14cd988b789": {"doc_hash": "d58032e4ee8e586e3079e22a7653c18a28ea239a22435290e3493c02d9d37241", "ref_doc_id": "01baafe9-b1d0-4f36-8420-8882f1c06f33"}, "feadce1d-bc0b-40a4-b9e8-f57659047c4c": {"doc_hash": "b6bc9d9c1491e012cc554e4f7cfddc30836928b725ef77f49ff53db2000743bf", "ref_doc_id": "01baafe9-b1d0-4f36-8420-8882f1c06f33"}, "3be1ee75-3fc3-4b18-9fd1-f00f2ba533d6": {"doc_hash": "c5bf5f7cbf747f3f800b393ef22d6c1f0e10b40600363c10ccad128b4a6944d2", "ref_doc_id": "4c4ba3cc-80d3-4f87-8cd1-fcdadb8267f2"}, "a653da63-96ef-4247-a710-653a0de7c47c": {"doc_hash": "08ed79cc1c4752fcc40708fefc847facc8e267b235ade0ab6439f955f443ffd3", "ref_doc_id": "4c4ba3cc-80d3-4f87-8cd1-fcdadb8267f2"}, "d634a99c-e520-4ee2-837b-d6ba74ce0cee": {"doc_hash": "7de4fa0f8609ccd0111b44dbc377b073e1bc5fe5e802e341e798794873eb63cc", "ref_doc_id": "d304b9c7-7e0a-4974-b19c-b7233486d6c7"}, "d1a4d301-1839-4edc-8c6d-e390cad5b3f2": {"doc_hash": "1be436630f6cb2c70dbbf62a82d8ca60bef9207c0178b1d6d94432f7f333a87b", "ref_doc_id": "d304b9c7-7e0a-4974-b19c-b7233486d6c7"}, "aa76c58b-da3f-4c78-858f-b2a7fadf22f6": {"doc_hash": "b880dffe7d51cfb75890ee43480202e9537ed10de59c3f339b11d050f778947f", "ref_doc_id": "d304b9c7-7e0a-4974-b19c-b7233486d6c7"}, "0853c091-805b-41fa-8b7c-a3017d3a4b38": {"doc_hash": "fe9d67ecb715a2253218ff3fc302532903b36f90563f1b0111d6335dcb33218e", "ref_doc_id": "a6991055-05ca-46b1-bc0e-a8e474361bf8"}, "6204fdbd-b441-4326-968f-c06e1dbb35b3": {"doc_hash": "a635630396efb22b2c52c3762f609b28883978b3e52d720c1dffed7a3e6a8c83", "ref_doc_id": "a6991055-05ca-46b1-bc0e-a8e474361bf8"}, "dbf6152d-9151-49cf-8a18-dad62084ef85": {"doc_hash": "84430e8ca9c5eb206ae5ca18ba6aaddcf3c32910f906a028f5d9bd5e67a852bc", "ref_doc_id": "a6991055-05ca-46b1-bc0e-a8e474361bf8"}, "066b6b3f-1112-4cc2-b1e7-9c39241b89e6": {"doc_hash": "03358c7197d100c55c6a208c8fa7ac4cf3ef5e181fb9dddcb89cd828023bdedf", "ref_doc_id": "a6991055-05ca-46b1-bc0e-a8e474361bf8"}, "dd613fac-3db7-40d1-bf83-527554e903d2": {"doc_hash": "5d16368df214b5fd9f2af91d1e571cb086710ec9d2e3e70ba15769f0bba85d4c", "ref_doc_id": "355cab74-ec27-4cae-afc8-c66a4a137f48"}, "27c48c02-ace0-4898-bb5a-4881b9b6b0c0": {"doc_hash": "32003f6816b0332ac11db23ced60a7f2512351249a771367c76602b1b735a91c", "ref_doc_id": "355cab74-ec27-4cae-afc8-c66a4a137f48"}, "6eb17b7e-a0ee-426e-b52a-6fa080ab4e0f": {"doc_hash": "30403ea75e12d401eedc9ffc86fa6c78b7f9996ae49838dcaccc79f72d313bba", "ref_doc_id": "355cab74-ec27-4cae-afc8-c66a4a137f48"}, "d8a14ee6-dc8d-4bd1-a826-c279f9a519fe": {"doc_hash": "3360f6122cbecb9382621beff68f3b03e69b7296e7db0fcf3aceb2edd2ee4ea6", "ref_doc_id": "355cab74-ec27-4cae-afc8-c66a4a137f48"}, "7feb97af-07a8-476f-864c-056d6e0de2ce": {"doc_hash": "faca2539750ce8d1286168595f8db9a56654d45cb37c0c4f59f37fdb3a2d721f", "ref_doc_id": "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9"}, "8fd0aa71-d31f-4980-a99f-5ae19b146f79": {"doc_hash": "0d0c4ad1019bd6531002ed16b1ac5e3fb06fd08dee32faa3e41f7a4a36608729", "ref_doc_id": "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9"}, "55efde07-d021-4603-94f1-c0e90e1ac601": {"doc_hash": "5b2eb29730c9f513a66ed06c185dfae5f67fc6fc2dcca0a77c6fcfd1596287ce", "ref_doc_id": "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9"}, "e136041a-e45e-46ca-a3a3-723f30f96627": {"doc_hash": "c7c5fe646430aba7106b58e301fb4b08519c655d5fdeaf3dcdabb9c7a15e8dfa", "ref_doc_id": "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9"}, "5add34ab-22da-4e50-b896-20f0ed259512": {"doc_hash": "ce230988cd5a4e4631f05280b0486912926a5a9f6319cd6e50e61a1a7946858e", "ref_doc_id": "08137354-5b17-48fb-bb75-e73e14190f1f"}, "5861a85c-68de-4188-bb64-dd7f556b39dc": {"doc_hash": "a0500c6c7067c0ab5014ceda8877b6de1c1671d00984e7f86c24b510f48ef1d6", "ref_doc_id": "08137354-5b17-48fb-bb75-e73e14190f1f"}, "1e852dab-f4d0-431b-ace2-8d58941405c7": {"doc_hash": "dafeb3d770d6bf6dedc32b155ec60920259d124b95e4b28a8dc5845f59ff9a16", "ref_doc_id": "08137354-5b17-48fb-bb75-e73e14190f1f"}}, "docstore/data": {"68d1e231-852f-4fb1-916d-29e135943c23": {"__data__": {"id_": "68d1e231-852f-4fb1-916d-29e135943c23", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4646fe6d-69f6-47fb-8a07-b9b5556dee31", "node_type": "4", "metadata": {}, "hash": "75c042c5960dee0c8174987a06600379e8534505f4539bd7ab0c2105cb029549", "class_name": "RelatedNodeInfo"}}, "text": "# Deep Time Series Models: A Comprehensive Survey and Benchmark\n\n# Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, Jianmin Wang\n\n# Abstract\n\nTime series, characterized by a sequence of data points arranged in a discrete-time order, are ubiquitous in real-world applications. Different from other modalities, time series present unique challenges due to their complex and dynamic nature, including the entanglement of nonlinear patterns and time-variant trends. Analyzing time series data is of great significance in real-world scenarios and has been widely studied over centuries. Recent years have witnessed remarkable breakthroughs in the time series community, with techniques shifting from traditional statistical methods to advanced deep learning models. In this paper, we delve into the design of deep time series models across various analysis tasks and review the existing literature from two perspectives: basic modules and model architectures. Further, we develop and release Time Series Library (TSLib) as a fair benchmark of deep time series models for diverse analysis tasks, which implements 24 mainstream models, covers 30 datasets from different domains, and supports five prevalent analysis tasks. Based on TSLib, we thoroughly evaluate 12 advanced deep time series models on different tasks. Empirical results indicate that models with specific structures are well-suited for distinct analytical tasks, which offers insights for research and adoption of deep time series models. Code is available at https://github.com/thuml/Time-Series-Library.\n\n# Index Terms\n\nTime series analysis, deep time series models, survey, benchmark\n\n# 1 INTRODUCTION\n\nTime series refers to a sequence of data points indexed in a discrete-time order, which are omnipresent in real-world applications, such as financial risk assessment, energy sustainability, and weather forecasting. Driven by the increasing availability of vast amounts of time series data across various domains, the community of time series analysis has witnessed tremendous advancements. Compared to image and text data, which have objectively prescribed syntax or intuitive patterns, the semantic information of time series data is primarily derived from the temporal variation. This presents significant challenges in understanding the data, such as identifying sequential dependencies, trends, seasonal patterns, and complicated dynamics. Consequently, analyzing time series data requires sophisticated methods to capture and utilize these complex temporal representations.\n\nGiven the crucial role of time series data in real-world applications, time series analysis has been a longstanding research direction. Time series analysis encompasses the process of analyzing the temporal variation to understand time series data and make accurate predictions and informed decisions. One of the essential cornerstones of time series analysis is discovering the underlying patterns in time series data, which involves the intricate temporal dependencies and variate correlations inherent within the data. By capturing these complex dependencies, time series models can effectively reveal the underlying dynamics, and facilitate various downstream tasks, including forecasting, classification, imputation, and anomaly detection.\n\nTraditional time series methods, such as AutoRegressive Integrated Moving Average (ARIMA), Exponential Smoothing, and Spectral Analysis, have long served as stalwart tools in time series analysis. These models, grounded in statistical methodologies, have been instrumental in discovering patterns, trends, and seasonality within temporal variations. However, their capabilities are hindered due to the inherent limitations of capturing complex nonlinear relationships and long-term dependencies present in real-world time series data. The rigid assumptions of linearity and stationarity that underpin traditional models constrain their adaptability to eventful and evolving data flows.\n\nDeep models have garnered significant attention and achieved remarkable performance across various domains, including natural language processing (NLP), computer vision (CV), and recommendation systems. In recent years, deep learning models have demonstrated their capability to capture the intricate dependencies within time series data, making deep learning models a powerful tool for time series analysis over traditional statistical methods. More recently, Transformer models with attention mechanisms, originally developed for natural language processing tasks, have presented stunning power in processing large-scale data and have also been adapted for learning time series data. These architectures offer the advantage of selectively focusing on different parts of the input sequence, allowing for more nuanced discovery of both temporal and variable dependencies in time series.\n\n# Related Surveys\n\nAlthough various time series models designed for different analysis tasks have emerged in recent years, there is a lack of a comprehensive overview of existing methods, covering both tasks and models. Previous reviews focus exclusively on either a specific model architecture or...\n\n# Footnotes\n\n- Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Jianmin Wang, and Mingsheng Long are with the School of Software, BNRist, Tsinghua University, Beijing 100084, China. E-mail: wangyuxu22@mails.tsinghua.edu.cn.\n- Yuxuan Wang, Haixu Wu and Jiaxiang Dong contributed equally to this work.\n- Corresponding author: Mingsheng Long, mingsheng@tsinghua.edu.cn.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48afc03c-980b-4381-bb08-0fbc6b46ed7f": {"__data__": {"id_": "48afc03c-980b-4381-bb08-0fbc6b46ed7f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3b9cc59-e544-4b89-b675-0cc6d3f42333", "node_type": "4", "metadata": {}, "hash": "b382e3ee968725ab573ef160636b7e39a80e23d00f8b010b75bb2d9732154a43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "546deb80-1deb-4342-a37f-906064d7ea29", "node_type": "1", "metadata": {}, "hash": "7fc356da9e40dde4c738955742399c3207efc2f5e1ba5de8c2bde049dbe42d40", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 2\n\n# TABLE 1\n\nComparison between our work and other related surveys. We present a comprehensive review of tasks and models, with a benchmark provided.\n\n|Survey| | |Analysis Task| | | | |Model Architecture| | |Benchmark|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|Fawaz et al. (2019) [8]| |\u2713| |\u2713|\u2713|\u2713| | | | | |\n|Braei et al. (2020) [9]| | | | |\u2713|\u2713|\u2713|\u2713| | | |\n|Torres et al. (2021) [10]|\u2713| |\u2713|\u2713| | | | | | | |\n|Garc\u00eda et al. (2021) [11]| |\u2713|\u2713|\u2713| | | | | | | |\n|Wen et al. (2022) [12]|\u2713|\u2713| |\u2713| | | | | | | |\n|Jin et al. (2023) [13]|\u2713|\u2713|\u2713| |\u2713| | | | | | |\n|Shao et al. (2023) [14]|\u2713| |\u2713|\u2713|\u2713|\u2713| | | | | |\n|Qiu et al. (2024) [15]|\u2713| |\u2713|\u2713|\u2713|\u2713| | | | | |\n|Our Survey|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713| | | |\n\nan analysis task. For example, [8], [9], [10], [11] reviews deep learning methods for specific time series analysis tasks while failing to include advanced architecture such as Transformer. Several surveys [12], [13] provide up-to-date reviews for time series analysis focusing on specific deep learning architectures (i.e., Graph Neural Network and Transformer). Recently, BasicTS [14] and TFB [15] introduce forecasting benchmarks that enable an unbiased evaluation of existing approaches but do not provide an overview of the architectural design of those deep models.\n\nIn this survey, we provide a comprehensive review of deep time series models for researchers and practitioners, starting from the basic modules to modern architectures. To foster practical applications, a time series benchmark is offered for a fair evaluation and identifying the effective scope of existing models. Our survey is organized as follows. Section 2 provides the background concepts of time series analysis. Section 3 introduces the basic modules that are widely utilized in prevalent deep time series models. Section 4 reviews the existing deep time series models in terms of the architecture design. Section 5 introduces the proposed open-source benchmark\u2014Time Series Library (TSLib)\u2014and presents extensive experimental comparison with detailed analysis. Section 6 provides a brief discussion of future research directions while Section 7 summarizes this survey.\n\n# 2 PRELIMINARIES\n\n# 2.1 Time Series\n\nTime series is a sequence of T observations ordered by time, which can be denoted as X = {x1, x2, ..., xT } \u2208 RT \u00d7C , where xt \u2208 RC represents the observed values at time point t and C is the number of variables. Since time series data are physical measurements obtained from sensors, systems are often recorded with multiple variables. Consequently, real-world time series usually recorded in a multivariate form. Theoretical studies [25], [26] have shown that when there are two or more non-stationary series, a linear combination of them can be stationary. This co-integration property helps in uncovering and modeling long-term relationships among non-stationary series. Therefore, the essence of time series analysis is to capture and utilize the temporal dependencies and inter-variable correlations within the observations.\n\nTemporal Dependency Given the sequential nature inherent in the observations, one evident technological paradigm is to capture the temporal dependence of a set of historical data. The basic idea of temporal dependencies is the intricate correlations between time points or sub-series. Traditional statistical models have laid the groundwork for modeling temporal dependencies. Prominent models include ARIMA (Autoregressive Integrated Moving Average) [1] have been extensively studied for capturing complex temporal patterns in the time series modality. Owing to their simplicity and interpretability, these statistical methods remain popular for tasks where the underlying temporal dynamics do not exhibit high complexity. Considering the high-dimensionality and non-stationarity of real-world time series, the research focus shifted towards deep learning for time series analysis.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "546deb80-1deb-4342-a37f-906064d7ea29": {"__data__": {"id_": "546deb80-1deb-4342-a37f-906064d7ea29", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3b9cc59-e544-4b89-b675-0cc6d3f42333", "node_type": "4", "metadata": {}, "hash": "b382e3ee968725ab573ef160636b7e39a80e23d00f8b010b75bb2d9732154a43", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48afc03c-980b-4381-bb08-0fbc6b46ed7f", "node_type": "1", "metadata": {}, "hash": "13c36d104ef545fa5840ec8e629b19c0e5719cdf2a0d3cf135c2ef022601a29f", "class_name": "RelatedNodeInfo"}}, "text": "This co-integration property helps in uncovering and modeling long-term relationships among non-stationary series. Therefore, the essence of time series analysis is to capture and utilize the temporal dependencies and inter-variable correlations within the observations.\n\nTemporal Dependency Given the sequential nature inherent in the observations, one evident technological paradigm is to capture the temporal dependence of a set of historical data. The basic idea of temporal dependencies is the intricate correlations between time points or sub-series. Traditional statistical models have laid the groundwork for modeling temporal dependencies. Prominent models include ARIMA (Autoregressive Integrated Moving Average) [1] have been extensively studied for capturing complex temporal patterns in the time series modality. Owing to their simplicity and interpretability, these statistical methods remain popular for tasks where the underlying temporal dynamics do not exhibit high complexity. Considering the high-dimensionality and non-stationarity of real-world time series, the research focus shifted towards deep learning for time series analysis. These advanced methods are designed to handle more complex temporal dynamics and offer greater flexibility in capturing the temporal dependency of time series data.\n\nVariate Correlation In addition to capturing temporal dependencies, understanding the variate correlations within high-dimensionality plays a pivotal role in analyzing multivariate time series. These correlations refer to the complex interactions and associations among different variables changing across the time. They provide valuable insights into the underlying dynamics and dependencies among the measurements, enabling a more comprehensive understanding of the latent process. Traditional approaches, such as Vector Autoregressive (VAR) models [27], extend the concept of autoregression to multiple variables and can capture the relationships between multiple quantities as they evolve over time. Technically, VAR represents each variable as a linear combination of its lagged values and the lagged values of all other variables in the model, which results in an inability to capture complex and non-linear relationships. Recently, advanced deep models, such as Graph Neural Networks [28] and Transformers [29], [30], have also been introduced for variate correlation modeling.\n\n# 2.2 Time Series Analysis Tasks\n\nBased on the understanding of underlying patterns and trends within time series data, time series analysis encompasses various downstream applications, including forecasting [31], [32], imputation [33], [34], [35], classification [8], [36], and anomaly detection [5], [37], each serving distinct purposes in diverse application domains.\n\nWe illustrate representative time series analysis tasks in Figure 1. Forecasting is a fundamental task in time.", "mimetype": "text/plain", "start_char_idx": 2824, "end_char_idx": 5714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56c50822-bc3b-489f-a3c7-946943edef72": {"__data__": {"id_": "56c50822-bc3b-489f-a3c7-946943edef72", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05ea54b4-c39d-40fa-9bfa-d21891ed0ab8", "node_type": "4", "metadata": {}, "hash": "39440827ff07ee423dfc1783a243a8ff51157e20a4e1a8272ec18db99eada161", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 3 BASIC MODULES\n\nTime series modeling approaches have evolved significantly, transitioning from traditional statistical models to sophisticated deep learning models. Despite these advancements, many classical tools and analytical algorithms remain widely used and continue to serve as foundational design principles in modern deep models.\n\n# 3.1 Stationarization\n\nAs a foundational concept in time series analysis, stationarity refers to the property of a time series where its statistical properties remain constant over time. A stationary time series has a constant mean and variance, which simplifies statistical analysis and makes it easier to capture the underlying patterns and behavior within a time series. Since many statistics-based time series analysis methods take stationarity as a basic assumption, stationarization of time series data has become an essential module. There are ways of transforming non-stationary time series into stationary. Traditional time series models stationarize the time series through differencing or log-transformation. In recent deep learning approaches, data normalization [38] takes the role of stationarization in a simple but effective way, which standardizes the value distribution of observations while maintaining the intrinsic variations and further helps mitigate the distribution shift between the source and target domains.\n\nThe deep adaptive input normalization (DAIN) layer [39] was proposed to adaptively stationarize time series data according to their original distribution. RevIN [40] introduces reversible instance normalization to time series data, which is an effective normalization-and-denormalization method with learnable affine transforms to make the model bypass the non-stationary inputs. Non-Stationary Transformer [41] (Stationary for short in the following) proposes a simpler but more effective series stationarization technique that improves the predictive capability of non-stationary series without extra parameters. Specifically, for a sequence with T time stamps and C variates X = {X1, X2, ..., XT } \u2208 RT \u00d7C , the outline of Stationary [41] can be summarized as:\n\n\u03bcx = 1/T \u03a3 Xi, \u03c3\u00b2 = 1/T \u03a3 (Xi \u2212 \u03bcx)\u00b2,Y' = p\u03c3x (X \u2212 \u03bcx)\u00b2/(Y' + \u03bcx)\u00b2 + \u03f5\n\nwhere \u03f5 is in a small value for numerical stability. \u03bcx, \u03c3\u00b2 \u2208 R1\u00d7C are the variate-specific mean and variance. To recover the distribution and non-stationarity of the original series, a de-normalization module is further used to augment the model output Y' with mean and variance statistics of inputs. The idea of stationarization and the above-mentioned techniques have been widely used in subsequent deep time series models [23], [30], [42], [43]. Recent SAN [44] rethinks the nature of non-stationary data and tries to split it into non-overlap equally-sized slices and perform normalization on each slice. Specifically, based on the evolving trends of statistical properties, SAN introduces a statistics prediction module to predict the distributions of future slices.\n\n# 3.2 Decomposition\n\nDecomposition [45], [46], as a conventional approach in time series analysis, can disentangle time series into several components with categorized patterns, and works primarily useful for exploring complex series variations. In the previous work, diverse decomposition paradigms are explored.\n\n# 3.2.1 Seasonal-Trend Decomposition\n\nSeasonal-trend decomposition [47], [48] is one of the most common practices to make raw data more predictable, which...", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97742285-32ad-4e78-aafc-c282c030f702": {"__data__": {"id_": "97742285-32ad-4e78-aafc-c282c030f702", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14fe124f-6d06-4dff-9d80-c251345029a6", "node_type": "4", "metadata": {}, "hash": "7c08d51d256035a14a3531ae4566d50c7e6edc8ac1c9f7d79f5f40c7011954c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "991b0249-1a37-48de-9e88-a7ab867edc35", "node_type": "1", "metadata": {}, "hash": "b0482959b541d16831c426844d238166f2abdfeb36ae840d44d93b61f64f805d", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 4\n\ncan separate the series into several different components: trend, seasonal, cyclical, and irregular, namely\n\nX = T + C + S + I,\n\nwhere the trend component T represents the overall long-term pattern of the data over time, the cyclical component C reflects repeated but non-periodic fluctuations within data, the seasonal component S indicates the repetitive patterns over a fixed period, and the irregular component I is the residuals or remainder of the time series after the other components have been removed.\n\nThe trend-seasonality decomposition can be achieved by using mathematical tools such as filters or exponential smoothing [49], [50]. Previous statistical approaches mainly adopt the trend-seasonality decomposition as data preprocessing [51]. In deep models, Autoformer [22] firstly introduces the idea of decomposition to deep learning architecture and proposes a series decomposition block as a basic module to extract the seasonal and trend-cyclical parts of deep features and input series, whose computation process can be formalized as:\n\nXT = AvgPool (Padding(X)),\n\nXS = X \u2212 XT.\n\nThe series decomposition block is concisely implemented based on a temporally average pooling layer with padding operation to keep the sequence length unchanged. This design can capture trends XT, and the remainder is taken as the seasonal part XS. The proposed series decomposition block has been widely used in the follow-up [6], [52], [53], [54], [55] as a native building block of deep models to disentangle the underlying patterns of deep features.\n\n# 3.2.2 Basis Expansion\n\nBasis expansion is a mathematical method used to represent a function or a set of data points in terms of a new set of pre-defined functions. These new functions form a basis for a function space, meaning any function in that space can be expressed as a linear combination of these basis functions. In the context of time series analysis, basis expansion is used to reveal complex non-linear temporal relationships by decomposing the time series into a combination of basic variations, which also enhances interpretability. As a representative model, N-BEATS [56] presents hierarchical decomposition to time series by utilizing a fully connected layer to produce expansion coefficients for both backward and forward forecasts. For l-th blocks in the proposed hierarchical architecture, the operation can be as follows:\n\nXl = Xl\u22121 \u2212 X\u02c6l\u22121\n\nX\u02c6l,\u02c6l = Block(Xl), Yl\n\n\u02c6l\u22121 is the backcast results which restrict the block where X to approximate the input signal Xl\u22121, then Xl removes the portion of well-estimated signal X\u02c6l\u22121 from X\u02c6l\u22121, therefore providing a hierarchical decomposition. Y\u02c6l is the partial forecast based on the decomposed input Xl and the final forecast Y\u02c6l is the sum of all partial forecasts.\n\nSubsequently, N-HiTs [57] redefine the N-BEATS by incorporating subsampling layers before the fully connected blocks, which enhances the input decomposition via multi-frequency data sampling and future predictor via multi-scale interpolation. DEPTS [58] puts forward a novel decoupled formulation for periodic time series by introducing the periodic state as a hidden variable and then develops a deep expansion module on top of residual learning to conduct layer-by-layer expansions between observed signals and hidden periodic states. Similarly, DEWP [59] is also a stack-by-stack expansion model to handle multivariate time series data, where each stack consists of a variable expansion block to capture dependencies among multiple variables and a time expansion block to learn temporal dependencies.\n\n# 3.2.3 Matrix Factorization\n\nThe above-mentioned two decomposition methods are proposed for univariate series or applied to multivariate series in a variate-independent way. Here, we discuss a factorization-based decomposition for multivariate series. Specifically, many multivariate time series data in real-world scenarios can also be referred to as high-dimensional data. They can be formalized in the form of a matrix, whose rows correspond to variate and columns correspond to time points. Since variables in multivariate time series tend to be highly correlated, it can be possibly reduced to a more compact space. Matrix factorization methods [60] work by decomposing the high-dimensional series data into the product of two matrices in a lower-dimensional latent space. For a multivariate time series X \u2208 RT \u00d7C, as shown in Figure 2 the matrix can be approximated by the multiplications of two lower rank embedding matrix, X \u2248 F Xb, in which F \u2208 Rk\u00d7C, Xb \u2208 RT \u00d7k and k is a hyperparameter.\n\nBesides the estimation, there are regularizers to avoid overfitting problems in factorization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "991b0249-1a37-48de-9e88-a7ab867edc35": {"__data__": {"id_": "991b0249-1a37-48de-9e88-a7ab867edc35", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14fe124f-6d06-4dff-9d80-c251345029a6", "node_type": "4", "metadata": {}, "hash": "7c08d51d256035a14a3531ae4566d50c7e6edc8ac1c9f7d79f5f40c7011954c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97742285-32ad-4e78-aafc-c282c030f702", "node_type": "1", "metadata": {}, "hash": "1d1dab5124af2d8f9c2ad37e26e34ec0d7eee86f16b0dae150e57d77af523141", "class_name": "RelatedNodeInfo"}}, "text": "Specifically, many multivariate time series data in real-world scenarios can also be referred to as high-dimensional data. They can be formalized in the form of a matrix, whose rows correspond to variate and columns correspond to time points. Since variables in multivariate time series tend to be highly correlated, it can be possibly reduced to a more compact space. Matrix factorization methods [60] work by decomposing the high-dimensional series data into the product of two matrices in a lower-dimensional latent space. For a multivariate time series X \u2208 RT \u00d7C, as shown in Figure 2 the matrix can be approximated by the multiplications of two lower rank embedding matrix, X \u2248 F Xb, in which F \u2208 Rk\u00d7C, Xb \u2208 RT \u00d7k and k is a hyperparameter.\n\nBesides the estimation, there are regularizers to avoid overfitting problems in factorization. Going beyond the canonical design that takes the squared Frobenius norm as regularizers, Temporal regularized matrix factorization (TRMF) [61] designs an autoregressive-based temporal regularizer to describe temporal dependencies among latent temporal embeddings. Further, [62] extended TRMF with a new spatial autoregressive regularizer to estimate low-rank latent factors by simultaneously learning the spatial and temporal autocorrelations. NoTMF [63] integrates the vector autoregressive process with differencing operations into the classical low-rank matrix factorization framework to better model real-world time series data with trend and seasonality. Eliminating the need for tuning regularization parameters, BTF [64] is a fully Bayesian model that integrates the probabilistic matrix factorization and vector autoregressive process into a single probabilistic graphical model. Instead of using an autoregressive-based temporal regularization,", "mimetype": "text/plain", "start_char_idx": 3932, "end_char_idx": 5727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c477450a-0082-441e-ad8d-1b4b8c5c94b2": {"__data__": {"id_": "c477450a-0082-441e-ad8d-1b4b8c5c94b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6de13856-3f3f-41be-b8d4-84d08d2bdcbc", "node_type": "4", "metadata": {}, "hash": "76a25d979cb8180363a121e30f6e83206d7e5874992179d4648e24f7211da38b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b63a422-05ff-4edd-9126-a0c2e762b0b0", "node_type": "1", "metadata": {}, "hash": "c4da3b2d46edec6f7b9a663676c07303967672de1bf0f8a3d631c27821b94f65", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 5\n\n# DeepGLO\n\nDeepGLO [65] utilizes a temporal convolution network for regularization to capture non-linear dependencies. LSTM-GL-ReMF [66] contains an LSTM-based temporal regularizer to learn complex long-term and short-term non-linear temporal correlations and a Graph Laplacian spatial regularizer [67] to capture spatial correlations.\n\n# 3.3 Fourier Analysis\n\nFourier analysis [68], [69] can convert a physical signal into the Fourier domain to highlight the inherent frequency properties of the original data and has been a well-acknowledged analysis tool in extensive areas. Since time series are usually recorded as a sequence of discrete time points by sampling the original continuous signals, Fourier analysis has become one of the mainstream tools in time series modeling and has been demonstrated favorable effectiveness and efficiency [70], [71]. Introducing the Fourier domain not only augments the representation of the original series but also provides a global view since the frequency spectrum distribution, which can indicate essential periodic properties of time series. In practice, Fast Fourier Transform (FFT) [72] and Wavelet Transform (WT) [73] as the basic algorithms connecting the discrete temporal domain to the frequency domain, have gained increasing popularity in the modular design of deep time series models [74], [75], [76], [77], [78], [79], [80], [81]. Existing approaches can be roughly divided into two categories: time-domain and frequency-domain modeling.\n\n# 3.3.1 Time-Domain Modeling\n\nThe fundamental principle behind the Fourier transform is that sequential data can be decomposed and represented by a series of periodic signals. Consequently, it can be used to identify potentially dominant periods and their corresponding frequencies in the data by analyzing the highest amplitude components. As a typical practice, TimesNet [3] employs the Fast Fourier Transform (FFT) to extract the most significant frequencies with the highest amplitude values, subsequently reshaping the 1D time series data into a 2D space based on the identified periods for better representation learning. Following TimesNet, PDF [82] posits that frequencies with larger values facilitate a more discernible distinction between long-term and short-term relationships.\n\nIn addition to exploiting the information of the sequence obtained by the Fourier Transformer, some works attempt to perform efficient computation through the Fast Fourier Transformer. Auto-correlation is a fundamental concept in time series analysis that measures the dependence between observations at different time points within a sequence of data. The Wiener-Khinchin theorem [83] provides a mathematical relationship between the auto-correlation function and the power spectral density (PSD) of a stationary random process, where the auto-correlation function represents the inverse Fourier transform of the PSD. Taking the data as a real discrete-time process, Autoformer [22] proposes an Auto-Correlation mechanism with an efficient Fast Fourier Transforms to capture the series-wise correlation.\n\nThe frequency-domain representation provides information about the amplitudes and phases, where low-frequency components correspond to slower variations or trends in the signal, and high-frequency components capture fine details or rapid variations. A significant body of work has focused on leveraging frequency-domain information to enhance the model\u2019s capability in capturing temporal dependencies. FiLM [84] introduces Frequency Enhanced Layers (FEL) which combine Fourier analysis with low-rank approximation to keep the part of the representation related to low-frequency Fourier components and the top eigenspace to effectively reduce the noise and boost the training speed. FITS [85] integrates a low-pass filter (LPF) to eliminate high-frequency components above a specified cutoff frequency, thereby compressing the model size while preserving essential information. From an opposite idea, FEDformer [86] posits that retaining only low-frequency components is insufficient for time series modeling, as it may dismiss important fluctuations in the data. Based on the above considerations, to capture the global view of time series, FEDformer represents the series by randomly selecting a constant number of Fourier components, including both high-frequency and low-frequency components.\n\n# 3.3.2 Frequency-Domain Modeling\n\nBuilding on time-frequency analysis in signal processing, several approaches have been developed to study time series simultaneously in both the time and frequency domains. ATFN [87] comprises an augmented sequence-to-sequence model that learns the trending features of complex non-stationary time series, along with a frequency-domain block designed to capture dynamic and intricate periodic patterns. TFAD [88] introduces a time-frequency analysis-based model that employs temporal convolutional networks to learn both time-domain and frequency-domain representations.\n\nSome works have developed specialized deep learning architecture to process the frequency domain of time series. STFNet [75] applies Short-Time Fourier Transform to input signals and applies filtering, convolution, and pooling operations directly in the frequency domain.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b63a422-05ff-4edd-9126-a0c2e762b0b0": {"__data__": {"id_": "7b63a422-05ff-4edd-9126-a0c2e762b0b0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6de13856-3f3f-41be-b8d4-84d08d2bdcbc", "node_type": "4", "metadata": {}, "hash": "76a25d979cb8180363a121e30f6e83206d7e5874992179d4648e24f7211da38b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c477450a-0082-441e-ad8d-1b4b8c5c94b2", "node_type": "1", "metadata": {}, "hash": "1b4b73d5e8c0549d82cb9750ace716f19e46184a6151184e8e630241fd274adb", "class_name": "RelatedNodeInfo"}}, "text": "Based on the above considerations, to capture the global view of time series, FEDformer represents the series by randomly selecting a constant number of Fourier components, including both high-frequency and low-frequency components.\n\n# 3.3.2 Frequency-Domain Modeling\n\nBuilding on time-frequency analysis in signal processing, several approaches have been developed to study time series simultaneously in both the time and frequency domains. ATFN [87] comprises an augmented sequence-to-sequence model that learns the trending features of complex non-stationary time series, along with a frequency-domain block designed to capture dynamic and intricate periodic patterns. TFAD [88] introduces a time-frequency analysis-based model that employs temporal convolutional networks to learn both time-domain and frequency-domain representations.\n\nSome works have developed specialized deep learning architecture to process the frequency domain of time series. STFNet [75] applies Short-Time Fourier Transform to input signals and applies filtering, convolution, and pooling operations directly in the frequency domain. StemGNN [28] combines Graph Fourier Transform (GFT) and Discrete Fourier Transform to model both inter-series correlations and temporal dependencies. EV-FGN [89] uses a 2D discrete Fourier transform on the spatial-temporal plane of the embeddings and performs graph convolutions for capturing the spatial-temporal dependencies simultaneously in the frequency domain. FreTS [90] leverages Discrete Fourier Transform (DFT) to transform the data into the frequency domain spectrum and introduces frequency domain MLPs designed for complex numbers with separated modeling for the real parts and the imaginary parts. FCVAE [91] integrates both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) concurrently. Recent TSLANet [92] propose a lightweight Adaptive Spectral Block (ASB) to replace the self-attention mechanism, which is achieved via Fourier-based multiplications by global and local filters. FourierDiffusion [93] explores extending the score-based SDE formulation of diffusion to complex-valued data and therefore implements time series diffusion in the frequency domain.\n\n# 4 MODEL ARCHITECTURES\n\nAs we have discussed in Section 2, the time series model needs to unearth the intrinsic temporal dependencies and", "mimetype": "text/plain", "start_char_idx": 4242, "end_char_idx": 6627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e7604e6-60b7-4e25-9a09-0267a3f5f39e": {"__data__": {"id_": "4e7604e6-60b7-4e25-9a09-0267a3f5f39e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfe147d0-16af-4210-924b-1d68aed1ffea", "node_type": "4", "metadata": {}, "hash": "66d3c5eb8a06dd6fdfff50b2e4b3e30c44345f33e74426324f123183dd4739be", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 6\n\n|MLP-based|CNN-based|RNN-based|GNN-based|Transformer-based|TSLib Implementation| |\n|---|---|---|---|---|---|---|\n|DLinear|DeepGLO|Autoformer|NSTransformer|TiDE| | |\n|BRITS|LogSparse|Informer|FEDformer|FreTS| | |\n|LSTNet|TFT|N-BEATS|TimeGrad|Pyraformer|Koopa|TimeMixer|\n|2018|2019|2020|2021|2022|2023|2024|\n|DCRNN|MTGNN|GDN|SCINet|Crossformer|iTransformer| |\n|STGCN|ASTGCN|AGCRN| |PatchTST|Mamba| |\n| |Graph WaveNet|StemGNN| |MICN| | |\n| | |THOC| |TimesNet| | |\n\nFig. 3. An overview of representative time series models in chronological order. We mark models with different colors based on their architectures.\n\nIn this section, we provide a technical review of the existing deep time series models. As we have presented in Figure 3, existing works can be classified into five categories based on their backbone architecture, namely MLP-based, RNN-based, CNN-based, GNN-based, and Transformer-based.\n\n# 4.1 Multi-Layer Perceptrons\n\nAs a representation of traditional statistical time series models, the Auto-regressive (AR) model assumes that the model output depends linearly on its own historical values. Inspired by the remarkable performance of auto-regressive models, Multi-Layer Perceptrons (MLP) have become a popular architecture for modeling time series data.\n\nAs a representative work of linear-based models, N-BEATS [24] is a pure MLP-based deep time series model without any time-series-specific knowledge to capture the temporal patterns in time series. Specifically, as described in Equ. (4) N-BEATS consists of deep stacks of fully-connected layers with two residual branches in each layer, one is for the backcast prediction and the other one is the forecast branch. Extending the idea of neural basis expansion analysis, N-HiTs [57] use multi-rate signal sampling and hierarchical interpolation and N-BEATSx [94] incorporate exogenous variables to enhance the prediction.\n\nRecent research by DLinear [52], also referred to as LTSF-Linear, challenges the effectiveness of complicated deep architecture in temporal modeling. It argues a simple linear regression in the raw space that achieves remarkable performance in both modeling and efficiency. As illustrated in Figure 4, prevalent MLP-based deep time series models consist of simple linear layers primarily designed for forecasting tasks. Also lightweight but effective, FITS [85] advocates time series analysis can be treated as interpolation exercises within the complex frequency domain and further introduces a complex-valued linear layer to learn amplitude scaling and phase shift in the frequency domain. Inspired by MLP-Mixer [95] in computer vision, several works have attempted to utilize MLPs to model both temporal and variate dependencies. TSMixer [96] contains interleaving time-mixing and feature-mixing MLPs to extract information from different perspectives. To better model the global dependencies in time series data, FreTS [90] investigates the learned patterns of frequency-domain MLPs which are operated on both inter-series and intra-series scales to capture channel-wise and time-wise dependencies in multivariate data.\n\nRecent works have moved beyond using simple linear layers over discrete time points. TimeMixer suggests that time series exhibit distinct patterns in different sampling scales and proposes an MLP-based multiscale mixing architecture. TiDE [97] incorporates exogenous variables to enhance the time series prediction. Based on Koopman theory and Dynamic Mode Decomposition (DMD) [98], which is a dominant approach for analyzing complicated dynamical systems, Koopa [99] hierarchically disentangles dynamics through an end-to-end predictive training framework and can utilize real-time incoming series for online development.\n\n# 4.2 Recurrent Neural Networks\n\nRecurrent Neural Networks (RNNs) are specifically designed to model sequential data [100], [101], [102], such as natural.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "765aa66d-4a76-45a0-b0c7-a9b9e37b876a": {"__data__": {"id_": "765aa66d-4a76-45a0-b0c7-a9b9e37b876a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0466ba57-a853-4959-a43a-7109aec4a3e5", "node_type": "4", "metadata": {}, "hash": "83d80ec72c46a220718205cde66bfe3e679ddbbb945a13532b24012a8819e883", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8af78ae3-90f4-433e-aeaf-8e4abba4a373", "node_type": "1", "metadata": {}, "hash": "ee029efb875e20c861d7e708f9ef47d1027e66dae839f2a4e630700d0a0cc271", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 7\n\n# Historical Series\n\n# Future\n\n# Generative Outputs\n\nRNN Cell\nRNN Cell\nRNN Cell\nRNN Cell\nRNN Cell\nRNN Cell\n# Deterministic Output\n\nFig. 5. Illustration of RNN-based model in the forecasting task.\n\nlanguage processing [103] and audio modeling [104]. Since time series are also serial in nature, RNNs have emerged as a popular choice for analyzing time series data [105]. Existing RNN-based deep time series models focus on combating the gradient vanishing problem caused by the vanilla recurrent structure and modeling the mutual correlation among multivariate variables. Previous works [106], [107], [108], [109], [110] use variants of RNN to model temporal dependencies.\n\nLSTNet [111] combines the recurrent structure with the convolutional layer to capture both the short-term local dependency between variables and long-term patterns for time series. Moreover, a novel recurrent-skip component based on the periodic pattern is introduced to alleviate gradient vanishing in modeling long-term dependencies. Similarly, DA-RNN [112] combines the recurrent unit with a dual-stage attention mechanism to adaptively extract relevant series at each time step. Beyond deterministic forecasts, DeepAR [113] proposes an auto-regressive recurrent network model to predict the probability distribution of further time points. Technologically, it learns not only the seasonal behavior with time series but dependencies on given covariates across time series, allowing the model to make predictions even when there is little or no historical data.\n\nAlso based on Markovian state representation, the State Space Model (SSM) [114] is another classical mathematical framework that captures the probabilistic dependence between observed measurements in stochastic dynamical systems. Concretely, a single-input single-output (SISO) linear state space model is defined as follows:\n\ndx(t) = Ax(t) + Bu(t),\n\ndt\n\ny(t) = Cx(t) + Du(t),\n\nwhere u(t), x(t), y(t) are input signal, latent state, and output signal respectively. The system is characterized by the matrices A \u2208 RN \u00d7N , B \u2208 RN \u00d71, C \u2208 R1\u00d7N , D \u2208 R1\u00d71 can be learned by the deep neural network.\n\nSSMs have proven their effectiveness and efficiency in processing well-structured time series data, but traditional approaches have to refit each time series sample separately and therefore cannot infer shared patterns from a dataset of similar time series. With the rise of deep learning models, modern SSMs are often implemented in a recurrent manner. By adapting and propagating a deterministic hidden state, RNNs are able to represent long-term dependencies in continuous data which offer an alternative to classical state space models. Therefore, some work [115], [116] have attempted to fuse classical state space models with deep neural networks. Representative like Deep State Spaces Model (DSSM) [117], using a recurrent neural network (RNN) to parametrize a particular linear SSM, takes advantage of incorporating structural assumptions and learning complex patterns. Structured State Space sequence model (S4) [118] introduces a new parameterization for the SSM by conditioning matrix A with a low-rank correction, allowing it to be diagonalized stably, which empowers the model with better long-term modeling capacity. Similar to S4, LS4 [119] is a generative model with latent space evolution following a state space ordinary differential equations (ODE).\n\nRecent work on Mamba [120] has emerged as a powerful method for modeling long-context sequential data while scaling linearly with sequence length. Utilizing a simple selection mechanism that parameterizes the SSM parameters based on the input, Mamba can discern the importance of information in a manner similar to the attention mechanism, posing a potentially effective way to sequential modeling.\n\n# 4.3 Convolutional Neural Networks\n\nSince the semantic information of time series is mainly hidden in the temporal variation, Convolutional neural networks (CNN) [18], [121] have become a competitive backbone for their ability to capture local features and pattern recognition. By leveraging convolutions and hierarchical feature extraction, CNNs have shown remarkable success in various computer vision tasks, such as image classification [122], segmentation [123] and object detection [124].\n\nConsidering the temporal continuity of time series data, previous works [125], [126], [127] apply one-dimensional CNN (1D CNN) to capture the local patterns of time series data. Recent SCINet [128] applies normal convolutions with a hierarchical downsample-convolve-interact architecture to capture dynamic temporal dependencies at different temporal resolutions of time series data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8af78ae3-90f4-433e-aeaf-8e4abba4a373": {"__data__": {"id_": "8af78ae3-90f4-433e-aeaf-8e4abba4a373", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0466ba57-a853-4959-a43a-7109aec4a3e5", "node_type": "4", "metadata": {}, "hash": "83d80ec72c46a220718205cde66bfe3e679ddbbb945a13532b24012a8819e883", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "765aa66d-4a76-45a0-b0c7-a9b9e37b876a", "node_type": "1", "metadata": {}, "hash": "82fc455cd6a225486590d33c0b0ba119b9d79726be80b0e37f6b2baed3fe0027", "class_name": "RelatedNodeInfo"}}, "text": "# 4.3 Convolutional Neural Networks\n\nSince the semantic information of time series is mainly hidden in the temporal variation, Convolutional neural networks (CNN) [18], [121] have become a competitive backbone for their ability to capture local features and pattern recognition. By leveraging convolutions and hierarchical feature extraction, CNNs have shown remarkable success in various computer vision tasks, such as image classification [122], segmentation [123] and object detection [124].\n\nConsidering the temporal continuity of time series data, previous works [125], [126], [127] apply one-dimensional CNN (1D CNN) to capture the local patterns of time series data. Recent SCINet [128] applies normal convolutions with a hierarchical downsample-convolve-interact architecture to capture dynamic temporal dependencies at different temporal resolutions of time series data. Inspired by the idea of masked convolution [129], Wavenet [130] introduces causal convolution and dilated causal convolution to model long-range temporal causality. Similar to Wavenet, Temporal Convolutional Networks (TCN) [131] uses a stack of dilated convolutional kernels with progressively enlarged dilation factors to achieve a large receptive field. However, the limited receptive field of TCN makes it difficult for them to capture global relationships in time series data. Based on TCN, MICN [132] is a local-global convolution network that combines different convolution kernels to model temporal correlation from a local and global perspective. ModernTCN [133] boosts the traditional TCN to capture cross-time and cross-variable dependency by DWConv and ConvFFN separately. Considering that DWConv is proposed to learn temporal information, it is operated variate-independently to learn the temporal dependency of each univariate time series.\n\nBeyond 1D space, motivated by the periodicity properties of time series data, TimesNet [3] transforms the 1D time series X1D data into a set of 2D tensors X2D = {X2D, ..., X2D}1 in each TimesBlock based on the estimated period lengths, where the inter-period variations are presented in tensor columns and inner-period ones are shown in tensor rows. Here k is a hyperparameter, corresponding to multiple 1D-to-2D transformations with different periods. Then it applies", "mimetype": "text/plain", "start_char_idx": 3891, "end_char_idx": 6193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0ea1e9c-8187-4177-bb3b-880e470a478f": {"__data__": {"id_": "b0ea1e9c-8187-4177-bb3b-880e470a478f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2e2bc4a-9b7f-4d48-b52c-427e1694f1ca", "node_type": "4", "metadata": {}, "hash": "992cd36a856384c07edcd25ecd6eb13e30f01d3acdddb36646a34d769c8baf61", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 1D Space\n\n# 1D Series\n\n# 2D Space\n\ndependency, alongside a recurrent neural network to capture temporal dynamics. Similarly, STGCN [139] integrates graph convolutional networks to model the spatial dependencies among traffic sensors with temporal convolutions to capture the temporal dependencies in the traffic time series data. Graph WaveNet [140] combines graph convolution with dilated casual convolution and learns an adaptive dependency matrix through node embedding, enabling the model to automatically capture hidden spatial dependencies in spatial-temporal graph data. Similarly, AGCRN [141] enhances the traditional graph convolutional network with node adaptive parameter learning and data-adaptive graph generation modules, allowing for the automatic capture of spatial and temporal correlations without a pre-defined graph structure. MTGNN [142] introduces a graph learning layer to adaptively learn the graph adjacency matrix, thereby capturing hidden relationships among multivariate time series data. STFGNN [143] employs a Spatial-Temporal Fusion Graph Neural Network with a generated temporal graph to learn localized spatial-temporal heterogeneity and global spatial-temporal homogeneity. StemGNN [28] leverages the advantages of both the Graph Fourier Transform (GFT) and the Discrete Fourier Transform (DFT), modeling multivariate time series in the spectral domain.\n\n# 4.5 Transformers\n\nIn the view of the great success in the field of natural language processing [16], [144], [145], [146], [147] and computer vision [19], [148], [149], [150], Transformers have also emerged as a powerful backbone for time series analysis. Benefiting from the self-attention mechanism [151], Transformer-based models can capture long-term temporal dependencies and complex multivariate correlations. As overviewed in Figure 8, existing Transformer-based time series models can be categorized based on the granularity of representation used in the attention mechanism, namely point-wise, patch-wise, and series-wise approaches.\n\n# 4.5.1 Point-wise Dependency\n\nDue to the serial nature of time series, most existing Transformer-based works use a point-wise representation of time series data and apply attention mechanisms to capture the correlations among different time points. Among these point-wise modeling approaches, Data Embedding is a crucial component that maps the value of time series data to a high-dimensional representation. Given time series X \u2208 RT \u00d7C with corresponding time stamp information Xmark \u2208 RT \u00d7D, where C is the variate number and D is the types of time stamps, the embedding module can be summarized as follow:\n\nHt = Projection(Xt) + PE(Xt) + TE(Xtmark)\n\nwhere Ht \u2208 RT \u00d7dmodel and dmodel is the dimension of the embedded representation, value projection Projection : C 7\u2192 Rdmodel and timestamp embedding TE : RD 7\u2192 Rdmodel are implemented by channel-dimension linear layers, and PE(\u00b7) denotes the absolute position embedding to preserve the sequential context of input series.\n\nThe core goal of GNN architecture is to model the underlying topological relations in multivariate data, therefore existing GNN-based works can be roughly divided into two categories based on whether graph structure is part of the input into the model. DCRNN [138] models the spatial dependency of traffic as a diffusion process on a directed graph and uses diffusion convolution to capture the spatial dependency.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5540e1c-a157-441e-bedc-3afa46cdc743": {"__data__": {"id_": "d5540e1c-a157-441e-bedc-3afa46cdc743", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47e45b25-ad29-4226-9f87-16c9994ea98e", "node_type": "4", "metadata": {}, "hash": "0f6131d85b7316932c131f1dc25300f1d0e16ddc4bc830f182fec0029223355e", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 9\n\n# TABLE 2\n\n# Model cards of Transformer-based deep time series models with architectural details.\n\n|Category|Method|Architecture|Embedding|Attention Mechanism|\n|---|---|---|---|---|\n|Vanilla|Transformer|Enc-Dec|Standard|FullAttention(Q, K, V) = Softmax( QK\u22ba)\u221ad|\n|LogSparse [152]|Dec-only|Standard|Qb, K = CausualCov(H)b|FullAttention(b, K, V) = Softmax( \u221ad ) Q b|\n|Point-wise|Informer [21]|Enc-Dec|Standard|ProbSparse-Attention(Q, K, V) = Softmax( QK\u22ba)V\u221ad|\n| |Pyraformer [153]|Enc-Dec|Standard|Pyramid-Attention(Q, K, V) = Masked(Softmax( QK\u22ba)V)\u221ad|\n| |Autoformer [22]|Enc-Dec|Standard|Auto-Correlation(Q, K, V) = Pk=1 Roll(V, \u03c4) Ri|\n|Patch-wise|Crossformer [29]|Enc-Dec|Patch-Wise|FullAttention(Q, K, V) = Softmax( QK\u22ba)\u221ad|\n| |PatchTST [23]|Enc-only|Patch-Wise|FullAttention(Q, K, V) = Softmax( QK\u22ba)\u221ad|\n|Variate-wise|iTransformer [30]|Enc-only|Variate-Wise|FullAttention(Q, K, V) = Softmax( QK\u22ba)\u221ad|\n\nchance to contain the dominant information in self-attention. Representative Work Based on the proposed sparsity measurement, it further designs a ProbSparse self-attention only using top queries with the biggest measurement results, which can reduce the quadratic complexity in time and memory. Pyraformer [153] constructs a multi-resolution C-ary tree and develops a Pyramidal Attention Mechanism, in which every node can only attend to its neighboring, adjacent, and children nodes. With the calculated mask for attention, Pyraformer can capture both short- and long-temporal dependencies with linear time and space complexity.\n\n# 4.5.2 Patch-wise Dependency\n\nPatch-wise Token PatchTST, Crossformer, TimeXer Patch-based architectures play a crucial role in the Transformer models for both Natural Language Processing (NLP) [16] and Computer Vision (CV) [19]. Since point-wise representations are insufficient to capture local semantic information in temporal data, several studies [23], [154], [155] have been devoted to exploring patch-level temporal dependencies within time series data. Pioneer work Autoformer [22] proposes an Auto-Correlation Mechanism, which captures the series-wise dependencies of time series to replace canonical point-wise self-attention. Based on the stochastic process theory [156], Auto-Correlation utilizes the Fast Fourier Transform to discover the time-delay similarities between different sub-series. A time delay module is further proposed to aggregate the similar sub-series from underlying periods instead of the relation between scattered points, which firstly explores the sub-series level modeling in Transformer-based models.\n\nGiven that the canonical attention approach leads to a quadratic computational complexity, numerous efficient Transformers [21], [22], [86], [153] have been proposed to mitigate the complexity caused by point-wise modeling, which is summarized in Table 2. LogSparse [152] proposes Convolutional Self-Attention to replace canonical attention by employing causal convolutions to produce queries and keys in the self-attention layer. Informer [21] introduces a Query Sparsity Measurement, where a larger value indicates a higher number of patches split, and Pi denotes the i-th patch with sequence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2120e11-9b01-48ff-884f-762b1d6990c9": {"__data__": {"id_": "d2120e11-9b01-48ff-884f-762b1d6990c9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01943cde-58e7-41e0-b2a1-a3973976a0a7", "node_type": "4", "metadata": {}, "hash": "2d84b6ae6864b6a10acfe5abafdd314d4c7c5a53cbb47083ac6e419f0a20647e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55f10f37-5624-45e3-ab29-a8162918fb53", "node_type": "1", "metadata": {}, "hash": "126a9db7b5740534c746ab8bf491d761f79ed61e69d2bbb5b28340187375d9d1", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 10\n\nlength P. The patches are mapped to the latent space through and a learnable position embedding Wpos \u2208 RP 7\u2192 RN.\n\na temporal linear projection PatchEmbed : Rdmodel\u00d7dmodel\n\nBased on the vanilla attention mechanism, PatchTST [23] learns the patch-wise dependencies. Going beyond PatchTST, recent Pathformer [158] proposes a multi-scale Transformer-based model with adaptive pathways. Based on the patch division of different scales, the adaptive pathways select the patch sizes with the top K weights generated by the router to capture multi-scale characteristics.\n\nThe success of PatchTST also benefits from channel-independence design, where each temporal patch-level token only contains information from a single series. In addition to capturing the patch-level temporal dependencies within one single series, recent approaches [54], [157] have endeavored to capture interdependencies among patches from different variables over time. Crossformer [29] introduces a Two-Stage Attention layer containing a Cross-Time Stage and a Cross-Dimension Stage to efficiently capture the cross-time and cross-variate dependencies between each patch token. For the obtained embedded vector H \u2208 RN \u00d7C\u00d7dmodel, the overall attention stage can be described as follow:\n\nZtime = MSAtime (H, H, H)\n\ndim R, Ztime, Z ,time\n\nB = MSA1\n\nZdim = MSA2dim Ztime, B, B\n\nwhere R \u2208 RN\u00d7C\u00d7dmodel is a learnable vector array used as a router to gather information from all dimensions and then distribute the gathered information.\n\n# 4.5.3 Series-wise Dependency\n\nFurther expanding the receptive field, there are also some works that attempt to use the tokenization of the whole time series to capture inter-series dependencies. iTransformer [30] introduce VariateEmbed to multivariate data, and for i-th variable X(i), it can be simply formulated as follows:\n\nH(i) = VariateEmbed(X(i))\n\nwhere VariateEmbed : RT \u2192 Rdmodel is instantiated as trainable linear projector. Based on the global representations of each series, iTransformer utilizes the vanilla Transformer without any architectural modifications to capture mutual correlations in multivariate time series data. Similarly, TimeXer [159] focuses on forecasting with exogenous variables and utilizes patch-level and series-level representations for endogenous and exogenous variables, respectively. Additionally, an endogenous global token is introduced to TimeXer, which serves as a bridge in-between and therefore captures intra-endogenous temporal dependencies and exogenous-to-endogenous correlations jointly.\n\n# 5 TIME SERIES LIBRARY\n\nTime series analysis has emerged as an important research area, attracting significant attention from both academia and industry. Recently, extensive exploration of deep learning based methods for time series analysis has resulted in significant advances. However, the issue of fair benchmarking poses a pressing challenge in this domain. The absence of fair, rational, and comprehensive benchmarks can lead to biased comparisons between different methods and hinder accurate evaluation of their effectiveness, potentially inflating domain advances or hindering practical applications. This presents a substantial obstacle to understanding advances and fostering robust development within the field.\n\nIn the domain of time series analysis, several benchmarks have been proposed, such as DGCRN [160], LibCity [161], DL-Traff [162], TS-bench [163], and BasicTS [14]. More specifically, Autoformer [22] proposed a standard long-term forecasting benchmark covering different practical applications. Further, to verify the generality of different time series analysis models, TimesNet [3] builds a more comprehensive model generalization benchmark covering five mainstream time series analysis tasks. However, these benchmarks typically have some limitations. One issue with current time series benchmarks is their limited coverage of time series analysis tasks and specific domains, which limits their practical applications. Moreover, these benchmarks often fail to provide detailed discussions and comprehensive summaries of task types, model architectures, and specific baseline methods. As a result, they do not effectively guide the design of more efficient time series analysis methods or drive further development in the field.\n\nTo effectively address these issues, we introduce and implement Time Series Library (TSLib), a benchmark for fair and comprehensive comparing and evaluating the performance of deep time series models across various time series analysis tasks. As shown in Figure 9, TSLib encompasses a unified model experiment pipeline, standardized evaluation protocols, extensive and diverse real-world datasets, mainstream and advanced time series analysis models, and unified experimental validation and analysis process.\n\nIn our Time Series Library, we meticulously followed the official codes and implemented 24 widely used and advanced deep time series analysis models. These models are derived from four canonical deep learning architectures. Users can choose from these models based on their specific practical usage scenarios. The code is available at https://github.com/thuml/Time-Series-Library.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55f10f37-5624-45e3-ab29-a8162918fb53": {"__data__": {"id_": "55f10f37-5624-45e3-ab29-a8162918fb53", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01943cde-58e7-41e0-b2a1-a3973976a0a7", "node_type": "4", "metadata": {}, "hash": "2d84b6ae6864b6a10acfe5abafdd314d4c7c5a53cbb47083ac6e419f0a20647e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2120e11-9b01-48ff-884f-762b1d6990c9", "node_type": "1", "metadata": {}, "hash": "f1b2ab53c37ff0f60710bec823833b2d11f9067488cff0f45fb605cf387d9528", "class_name": "RelatedNodeInfo"}}, "text": "As a result, they do not effectively guide the design of more efficient time series analysis methods or drive further development in the field.\n\nTo effectively address these issues, we introduce and implement Time Series Library (TSLib), a benchmark for fair and comprehensive comparing and evaluating the performance of deep time series models across various time series analysis tasks. As shown in Figure 9, TSLib encompasses a unified model experiment pipeline, standardized evaluation protocols, extensive and diverse real-world datasets, mainstream and advanced time series analysis models, and unified experimental validation and analysis process.\n\nIn our Time Series Library, we meticulously followed the official codes and implemented 24 widely used and advanced deep time series analysis models. These models are derived from four canonical deep learning architectures. Users can choose from these models based on their specific practical usage scenarios. The code is available at https://github.com/thuml/Time-Series-Library.\n\nAs follows, we will provide a detailed description of our TSLib, including the design and implementation principles (Section 5.1), the evaluation protocols and metrics (Section 5.2), the dataset descriptions (Section 5.3), and the main results of models with different architectures (Section 5.4).\n\n# 5.1 Design and Implementation Principle\n\nTSLib is designed based on the well-established factory pattern and implements a unified interface between data and model objects, thus enabling a clear separation between deep model creation and usage, promoting modularity and flexibility. By loading different data and model objects and combining specific task heads during model training, TSLib enables different datasets and models to be shared and extended, allowing easy switching between various time series analysis tasks. These design and implementation principles provide enhanced flexibility and scalability for our TSLib. Furthermore, as illustrated in Figure 9, TSLib introduces a unified experimental pipeline covering the overall process of the model training and evaluation, which includes data source, data processing, model training and analysis, and model performance evaluation.", "mimetype": "text/plain", "start_char_idx": 4238, "end_char_idx": 6465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5b60d91-1495-4159-ad71-9251142c0dc4": {"__data__": {"id_": "e5b60d91-1495-4159-ad71-9251142c0dc4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5326360c-a26d-47f3-9c7a-86d857923a50", "node_type": "4", "metadata": {}, "hash": "8dd45325345a433daccae8b5c7cd063f409184788c3d882b4c56efd230d6da4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a14168c7-d532-4c68-974e-5b38778e27e7", "node_type": "1", "metadata": {}, "hash": "44ca02a9e4b0db24f891bdd31919653ef3f77118eaf411f65d21f2c119d5b2b8", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# Performance Evaluation\n\n|Data Sources|Layers|Time Series Analysis Tasks|\n|---|---|---|\n|HealthCare|Normalization|Classification|\n|Electricity|Design Principles|- Accuracy, F1-Score \u2026|\n|Weather|Decomposition|- Mean, Std\u2026|\n|Finance|- Trend, Seasonal, Remainder\u2026|Imputation|\n|Traffic|Fourier Analysis|- MSE, MAE \u2026|\n|e-Market|- FFT, DFT\u2026|Forecasting|\n| |Models|- MSE, MAE, SMAPE \u2026|\n| |Transformer, CNN, RNN, MLP|Anomaly Detection|\n| |iTransformer, SCINeT, LSTNet, DLinear|- Accuracy, F1-Score \u2026|\n| |PatchTST, TCN, DeepAR, N-HiTS| |\n| |FedFormer, MICN, DA-RNN, N-BEATS| |\n| |Autoformer, TimesNet, Mamba, TiDE| |\n\nFig. 9. Architecture and experiment pipeline of Time Series Library. Left: Unified training and evaluation process. Right: Overall Architecture.\n\n# Data Source\n\nOur TSLib provides extensive support for a wide range of diverse and multi-type datasets in a variety of formats, including \".csv\", \".npz\", \".txt\", etc. As shown in Figure 9 and Table 3, TSLib currently supports more than 30 datasets with different sampled frequencies across four mainstream time series analysis tasks, all derived from real-world scenarios in domains such as energy, transportation, economics, weather, and medicine, etc. Moreover, TSLib excels in scalability, allowing for the effortless integration of new data sources of different data types.\n\n# Data Processing\n\nData processing plays a pivotal role in guaranteeing stable training within the realm of time series analysis. Within the Time Series Library, a multitude of data processing steps are conducted, including time window splitting, data batch generation, etc. Subsequently, the raw data is partitioned into separate sets for training, validation, and testing purposes, enabling streamlined model training and equitable comparisons. These steps serve as indispensable prerequisites for attaining precise and dependable results across a range of diverse time series analysis tasks.\n\nMoreover, our TSLib provides additional support for numerous crucial and effective data processing strategies based on different model design principles [39], [40], [41], [45] to enhance model performance and training efficiency. We encapsulate these various design strategies within our basic data processing layer, encompassing techniques such as data normalization, time-frequency decomposition, Fourier analysis, and more. When utilizing TSLib, users have the flexibility to select these strategies to improve training effect based on their specific requirements and objectives.\n\n# Model Training and Analysis\n\nAfter the data processing phase, the raw time series data is transformed into the desired format for model training. Model training forms the crux of the entire experiment pipeline, where we fine-tune the model parameters based on the input to predict the output with minimal error. Our primary goal during model training is to obtain the best possible trainable parameters that result in a significant improvement in model performance. Each model has its own unique design and training objective. The model analysis procedure is to determine the optimal model parameters by comparing the correlation between the training and validation losses. Our TSLib includes complete log printing and result storage functions record and evaluate the training process. By employing rational model analysis techniques, we can efficiently obtain models with superior performance and stronger generalization.\n\n# Performance Evaluation\n\nModel evaluation is a crucial step in verifying the effectiveness and generalization of trained time series models. It involves model prediction and performance evaluation, providing insights into the efficacy of the trained model. In TSLib, we provide evaluation support for four mainstream time series analysis tasks: classification, imputation, forecasting (long-term or short-term), and anomaly detection. Each task comes with its specific evaluation metric, enabling a comprehensive assessment of the performance of models. These metrics play a crucial role in determining the effectiveness of the trained model and its suitability for the intended task.\n\n# 5.2 Evaluation Protocols\n\nIn order to conduct a fair and comprehensive model performance verification, our Time Series Library is designed to provide standardized evaluation protocols for four mainstream time series analysis tasks following [3]. The primary goal of these standardized and unified evaluation protocols is to quantify the effectiveness of different time series analysis methods with varying architectures. Additionally, they provide valuable insights into the strengths and limitations of different methods across diverse time series analysis tasks. By establishing these standardized evaluation protocols, we aim to promote fair comparisons between different methods and improve our understanding of their performance in various time series analysis scenarios.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a14168c7-d532-4c68-974e-5b38778e27e7": {"__data__": {"id_": "a14168c7-d532-4c68-974e-5b38778e27e7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5326360c-a26d-47f3-9c7a-86d857923a50", "node_type": "4", "metadata": {}, "hash": "8dd45325345a433daccae8b5c7cd063f409184788c3d882b4c56efd230d6da4c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5b60d91-1495-4159-ad71-9251142c0dc4", "node_type": "1", "metadata": {}, "hash": "7ce3a80f0f38bf416ef0cafb69bb3536bf1663bfc467fd6ad189282185c67470", "class_name": "RelatedNodeInfo"}}, "text": "In TSLib, we provide evaluation support for four mainstream time series analysis tasks: classification, imputation, forecasting (long-term or short-term), and anomaly detection. Each task comes with its specific evaluation metric, enabling a comprehensive assessment of the performance of models. These metrics play a crucial role in determining the effectiveness of the trained model and its suitability for the intended task.\n\n# 5.2 Evaluation Protocols\n\nIn order to conduct a fair and comprehensive model performance verification, our Time Series Library is designed to provide standardized evaluation protocols for four mainstream time series analysis tasks following [3]. The primary goal of these standardized and unified evaluation protocols is to quantify the effectiveness of different time series analysis methods with varying architectures. Additionally, they provide valuable insights into the strengths and limitations of different methods across diverse time series analysis tasks. By establishing these standardized evaluation protocols, we aim to promote fair comparisons between different methods and improve our understanding of their performance in various time series analysis scenarios.\n\nFor long-term forecasting and imputations, we rely on Mean Square Error (MSE) and Mean Absolute Error (MAE) as the primary evaluation metrics. These metrics help us accurately assess the accuracy of our predictions and imputations. For short-term forecasting, we use the Symmetric Mean Absolute Percentage Error (SMAPE) and Mean Absolute Scaled Error (MASE) as metrics, which focus on absolute errors and reduce the impact of outliers, providing reliable.", "mimetype": "text/plain", "start_char_idx": 3782, "end_char_idx": 5446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77659c59-d4d8-43d1-81a7-9407c6bab1b2": {"__data__": {"id_": "77659c59-d4d8-43d1-81a7-9407c6bab1b2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f2ceb51-bbbb-433c-9170-b96c39b237b1", "node_type": "4", "metadata": {}, "hash": "e9138944f105971b7de388ddd87b1788c2befa5394b4a1f0f4361f1d4ab9b7a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d20039e4-1eea-4ba2-ac20-087acbc470c8", "node_type": "1", "metadata": {}, "hash": "85b331091d63ea0cdc50cfbd762ecf92295ff7231b65fd69d8e0a4cbc8cd56db", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 12\n\n# TABLE 3\n\n# Summary of supported experimental datasets in Time Series Library.\n\n|Task|Dataset|Dimension|Length|Domain|Size|\n|---|---|---|---|---|---|\n|Classification|EthanolConcentration|3|1,751|Alcohol Industry|20.3 MB|\n| |FaceDetection|144|62|Face (250 Hz)|789.1 MB|\n| |Handwriting|3|152|Motion|3.9 MB|\n| |Heartbeat|61|405|Health (0.061 secs)|87.8 MB|\n| |JapaneseVowels|12|29|Voice|1.1 MB|\n| |PEMS-SF|963|144|Transportation (1 day)|420.1 MB|\n| |SelfRegulationSCP1|6|896|Health (256 Hz)|17.8 MB|\n| |SelfRegulationSCP2|7|1,152|Health (256 Hz)|17.7 MB|\n| |SpokenArabicDigits|13|93|Voice (11025 Hz)|37.6 MB|\n| |UWaveGestureLibrary|3|315|Gesture|3.4 MB|\n|Imputation|ETTh1, ETTh2|7|17,420|Electricity (1 hour)|10.4 MB|\n| |ETTm1, ETTm2|7|69,680|Electricity (15 mins)|2.6 MB|\n| |Electricity|321|26,304|Electricity (1 hour)|95.6 MB|\n| |Weather|21|52,696|Environment (10 mins)|7.2 MB|\n| |ETTh1, ETTh2|7|17,420|Electricity (1 hour)|10.4 MB|\n| |ETTm1, ETTm2|7|69,680|Electricity (15 mins)|2.6 MB|\n| |Electricity|321|26,304|Electricity (1 hour)|95.6 MB|\n|Long-term Forecasting|Weather|21|52,696|Environment (10 mins)|7.2 MB|\n| |Traffic|862|17,544|Transportation (1 hour)|136.5 MB|\n| |Exchange|8|7,588|Economic (1 day)|623 KB|\n| |ILI|7|966|Health (1 week)|66 KB|\n| |M4-Yearly|1|6|Demographic| |\n| |M4-Quarterly|1|8|Finance| |\n|Short-term Forecasting|M4-Monthly|1|18|Industry|589.5 MB|\n| |M4-Weakly|1|13|Macro| |\n| |M4-Daily|1|14|Micro| |\n| |M4-Hourly|1|48|Other| |\n| |SMD|38|100|Industry (1 min)|436.4 MB|\n| |MSL|55|100|Industry (1 min)|58.2 MB|\n|Anomaly Detection|SMAP|25|100|Industry (1 min)|113.0 MB|\n| |SwaT|51|100|Industry (1 min)|903.2 MB|\n| |PSM|25|100|Industry (1 min)|107.1 MB|\n\nEvaluations of forecast accuracy across different datasets and methodologies. In the case of time series classification tasks, we utilize Accuracy as the evaluation metric. Accuracy measures the overall prediction performance by calculating the ratio of correctly classified samples to the total number of samples. For anomaly detection, we employ the F1 score to validate the identification of abnormal values. The F1 score represents a balanced combination of precision and recall, offering a comprehensive assessment of a classifier\u2019s performance, especially when dealing with imbalanced classes in the context of anomaly detection.\n\n# 5.3 Datasets\n\nTSLib includes a variety of mainstream datasets across different numbers of samples and categories, a richness of tasks, and a diversity of domains. In this section, we will focus on introducing representative datasets from various time series analysis tasks included in the Time Series Library.\n\nClassification Time series classification aims to assign a label or category to a time series based on its temporal features. To evaluate this capability, we selected ten multi-variate datasets from the UEA Time Series Classification Archive [164], supported in our TSLib. These datasets cover a range of practical tasks, including gesture, action, audio recognition, and medical diagnosis through heartbeat monitoring. We pre-processed the datasets according to the descriptions provided in [165]. Detailed dataset descriptions are shown in Table 3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d20039e4-1eea-4ba2-ac20-087acbc470c8": {"__data__": {"id_": "d20039e4-1eea-4ba2-ac20-087acbc470c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f2ceb51-bbbb-433c-9170-b96c39b237b1", "node_type": "4", "metadata": {}, "hash": "e9138944f105971b7de388ddd87b1788c2befa5394b4a1f0f4361f1d4ab9b7a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77659c59-d4d8-43d1-81a7-9407c6bab1b2", "node_type": "1", "metadata": {}, "hash": "27dc6e9a31c9d63494b5a69da08b106f1305e0466df0310057bfdeac18e2421a", "class_name": "RelatedNodeInfo"}}, "text": "# 5.3 Datasets\n\nTSLib includes a variety of mainstream datasets across different numbers of samples and categories, a richness of tasks, and a diversity of domains. In this section, we will focus on introducing representative datasets from various time series analysis tasks included in the Time Series Library.\n\nClassification Time series classification aims to assign a label or category to a time series based on its temporal features. To evaluate this capability, we selected ten multi-variate datasets from the UEA Time Series Classification Archive [164], supported in our TSLib. These datasets cover a range of practical tasks, including gesture, action, audio recognition, and medical diagnosis through heartbeat monitoring. We pre-processed the datasets according to the descriptions provided in [165]. Detailed dataset descriptions are shown in Table 3.\n\nImputation Due to glitches, the collected time series data may contain partially missing values, posing a challenge for time series analysis. Therefore, time series imputation is a crucial task in real-world applications, aiming to fill in missing values within a time series based on contextual observations from the data. For our benchmark, we selected Electricity Transformer Temperature (ETT) [21], Electricity, and Weather to evaluate the performance of time series imputation tasks with different missing ratios.\n\nForecasting Time series forecasting is an essential task in time series analysis, and it has been widely explored in academic and industry domains. By leveraging historical patterns and trends, the model can predict future values or trends of a time series. Time series forecasting can be broadly divided into two types: long-term forecasting and short-term forecasting. For the long-term time series forecasting task, a wide range of datasets are included in our benchmark, including Electricity Transformer Temperature (ETT), Electricity, Weather, Traffic, Exchange, and Illness (ILI). For the short-term forecasting task, we selected the M4 dataset [166], which comprises six sub-datasets with varying sampling frequencies and domains.", "mimetype": "text/plain", "start_char_idx": 2402, "end_char_idx": 4525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b6850fd-e78e-4435-aa52-6af74e627fc7": {"__data__": {"id_": "6b6850fd-e78e-4435-aa52-6af74e627fc7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2780921a-8944-4247-b1ab-d497c1a9a2c0", "node_type": "4", "metadata": {}, "hash": "b498ebf86f1479ae5ce1add866f56f043496d4c9604a5264d94c0331c44cb07e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55b0cac7-36a7-47d1-a546-2b07054d8b42", "node_type": "1", "metadata": {}, "hash": "7522d3e24bfe1d001064814beb17c2e1e1d3e19940716e09de17dba7bf201d63", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# Long-term Forecasting (Unified hyperparameter)\n\n# Long-term Forecasting (Hyperparameter searching)\n\n# Short-term Forecasting\n\n| |Average MSE|Average MSE|Average MSE| | |\n|---|---|---|---|---|---|\n|StationaryFEDformer|0.589| | |1.803|0.490|\n|N-BEATS|0.559|0.436|0.430| | |\n|0.459|0.394|0.386| | | |\n|0.436|0.429|0.427| | | |\n| |0.362|0.369|0.414| | |\n|0.403| |0.352|1.075| | |\n|0.381|0.332|1.030| | | |\n|1.000|0.373| |0.992| | |\n|0.371|0.317| |0.945| | |\n| |0.313| | |0.922|0.305|\n| | |0.871|0.888|0.890| |\n|0.342| | | | | |\n\n# StationaryFEDformer\n\n# N-BEATS\n\n# Imputation\n\n# Classification\n\n# Anomaly Detection\n\n| |Average Accuracy (%)|Average MSE|\n|---|---|---|\n|StationaryFEDformer|85.24|0.117|\n|N-BEATS|73.67|0.119|\n| |73.17|0.118|\n| |72.50|0.112|\n| |72.66|0.050|\n| |71.71| |\n| |71.07| |\n| |70.67| |\n| |70.71|82.79|\n| |68.96|81.39|\n\n# Fig. 10. Comparison of model performance in Time Series Library.\n\nFull results are averaged from a diverse set of datasets supported by TSLib across four mainstream analysis tasks. In the classification and anomaly detection tasks, higher Accuracy or F1-score indicates better forecasting performance. Conversely, lower average MSE or MAE demonstrate superior model effectiveness for the other tasks. The top three models for each time analysis task are highlighted on a leaderboard, and the optimal model for each time series analysis task is indicated.\n\n# Long-term Forecasting\n\n# (Average MSE)\n\n|Long-term Forecasting (*)|Anomaly Detection|\n|---|---|\n|(Average MSE)|(Average F1-Score)|\n|0.305|86.24|\n|0.386|82.46|\n|0.118|0.922|\n|Short-term Forecasting|69.87|\n|(Average OWA)|0.871|\n|0.050|(Average MSE)|\n\n# Transformer-Based\n\n# RNN-Based\n\n# CNN-Based\n\n# MLP-Based\n\n# Fig. 11. Overall performance of models with different deep architectures.\n\n# Anomaly Detection\n\nAnomaly detection involves identifying unusual or abnormal patterns in a time series. These anomalies can indicate critical events, faults, or outliers that require attention or further investigation. There are some mainstream anomaly detection datasets supported in TSLib, such as Server Machine Dataset (SMD) [167], Mars Science Laboratory rover (MSL) [168], Soil Moisture Active Passive satellite (SMAP) [168], Secure Water Treatment (SWaT) [169], and Pooled Server Metrics (PSM) [170] which are collected from a variety of industrial scenarios.\n\n# 5.4 Main Results\n\nTo examine the strengths and limitations of various methods in mainstream time series analysis tasks, we select twelve representative models from our TSLib. These models encompass four popular deep model architectures and address tasks, including long-term and short-term forecasting, classification, imputation, and anomaly detection.\n\n# Baselines\n\nTo conduct a fair comparative analysis and thoroughly explore the effectiveness of different model architectures in various time series analysis tasks, we conduct comparative comparison experiments using state-of-the-art models designed based on different deep architectures. As shown in Figure 11, we select several advanced and representative Transformer-based models: iTransformer [30], PatchTST [23], Autoformer [22], Non-Stationary Transformer (Stationary) [41], and FEDformer [86] to verify the performance. Additionally, we consider TimesNet [3] and SCINet [128] as the CNN-based models to compare. For RNN-based models, we included the novel and effective Mamba [120]. Finally, we include DLinear [52], N-BEATS [24], and TiDE [97] as representative MLP-based models for analysis.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55b0cac7-36a7-47d1-a546-2b07054d8b42": {"__data__": {"id_": "55b0cac7-36a7-47d1-a546-2b07054d8b42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2780921a-8944-4247-b1ab-d497c1a9a2c0", "node_type": "4", "metadata": {}, "hash": "b498ebf86f1479ae5ce1add866f56f043496d4c9604a5264d94c0331c44cb07e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b6850fd-e78e-4435-aa52-6af74e627fc7", "node_type": "1", "metadata": {}, "hash": "16424320f73b9b55fc1eda39b71c67273a30e20e1e0315622d6254df3fe46476", "class_name": "RelatedNodeInfo"}}, "text": "# Baselines\n\nTo conduct a fair comparative analysis and thoroughly explore the effectiveness of different model architectures in various time series analysis tasks, we conduct comparative comparison experiments using state-of-the-art models designed based on different deep architectures. As shown in Figure 11, we select several advanced and representative Transformer-based models: iTransformer [30], PatchTST [23], Autoformer [22], Non-Stationary Transformer (Stationary) [41], and FEDformer [86] to verify the performance. Additionally, we consider TimesNet [3] and SCINet [128] as the CNN-based models to compare. For RNN-based models, we included the novel and effective Mamba [120]. Finally, we include DLinear [52], N-BEATS [24], and TiDE [97] as representative MLP-based models for analysis. It is important to mention that TiDE [97] is designed to be dependent on specific timestamps, and cannot be easily adapted to part tasks without timestamps, such as short-term forecasting, anomaly detection, and classification.\n\n# Unified Experimental Settings\n\nFor the long-term forecasting task, we conducted two experimental settings to ensure a fair and comprehensive comparison: unified hyperparameter and hyperparameter searching. For the unified hyperparameter, we conducted experiments using a range of standardized hyperparameters set across different datasets. This allows us to accurately evaluate the relative performance of time series models with different deep architectures while keeping other factors constant. As for the \u201chyperparameter searching\u201d scenario, we conducted separate hyperparameter searches for different model architectures and time series datasets. This approach enabled us to identify the best performance of different time series analysis models. By employing above both settings, we obtain a comprehensive understanding of the forecasting performance of different time series models. For the remaining tasks, we maintained the standard experimental settings as outlined in [3] to validate the performance of different time series models.", "mimetype": "text/plain", "start_char_idx": 2794, "end_char_idx": 4868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a61bbda-0c36-42ea-8b39-a14cd988b789": {"__data__": {"id_": "9a61bbda-0c36-42ea-8b39-a14cd988b789", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01baafe9-b1d0-4f36-8420-8882f1c06f33", "node_type": "4", "metadata": {}, "hash": "7107477183d8e19f2a9dee85f3e1be45b7939ea710972337aca3b36688cff1e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "feadce1d-bc0b-40a4-b9e8-f57659047c4c", "node_type": "1", "metadata": {}, "hash": "b6bc9d9c1491e012cc554e4f7cfddc30836928b725ef77f49ff53db2000743bf", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# Overall Results\n\nBased on the overall results of the models across different architectures in Figure 11, we surprisingly find that the MLP-based models, which are generally simpler and have lower computational overhead, perform well on the time series forecasting task. However, these models appear to be less effective in other types of tasks, which requires the model to learn more informative representations. On the contrary, the CNN-based models demonstrate more comprehensive capabilities and excel in classification, imputation, and anomaly detection tasks. The RNN-based models, while performing well on anomaly detection tasks, show limited effectiveness compared to other model architectures. In contrast, the Transformer-based models have demonstrated highly competitive performance across various time series analysis tasks. This can be attributed to the powerful data modeling capabilities inherent in the transformer architecture, which contribute to its overall and consistently superior performance across diverse time series analysis tasks. It further shows that Transformer-based models hold significant research value and application potential in the field of time series analysis and have emerged as a particularly promising option in the time series domain.\n\nAs illustrated in Figure 10, we have also included more detailed results and a top three performance leaderboard for four representative time series analysis tasks. These results clearly show that the Transformer-based models, namely iTransformer [30] and PatchTST [23], exhibit superior forecasting capabilities compared to other models for both long-term and short-term forecasting tasks. This further proves that it is of great significance and value to explore different modeling methods of temporal tokens in time series. Additionally, TimesNet [3] shows a more comprehensive and effective performance covering time series classification, imputation, and anomaly detection tasks. It has pioneered a milestone in the general time series analysis model.\n\nWe believe that TSLib can provide useful start code, valuable insights on model properties and model selection guidance for future research and real-world applications.\n\n# 6 FUTURE DIRECTIONS\n\nIn this section, we provide a discussion on the promising directions for time series analysis.\n\n# 6.1 Time Series Pre-training\n\nA pretraining-finetuning learning paradigm is a two-stage approach commonly used in nature language processing (NLP) [171], [16], [172], [173], [173] and computer vision (CV) [174], [175], [176], [177]. Pre-training establishes the basis of the abilities of Large Models through unsupervised learning [178], [179]. Fine-tuning can improve the performance of the pre-trained model on a specific task or domain.\n\nDue to the limited availability of labeled datasets, self-supervised pre-training [175] has garnered significant attention and has been extensively investigated in the domains of natural language modeling and computer vision. Self-supervised pre-training paradigm significantly reduces labeling expenses and benefits for diverse downstream tasks. Notably, recent research efforts have introduced several self-supervised pre-training methods tailored for time series data, which can be primarily classified into contrastive learning [180] and masked time series modeling [16], [181].\n\nContrastive learning refers to learning the representations of data by contrasting between similar and dissimilar pairs, where similar sample pairs are learned to be close to each other and dissimilar pairs are far apart [180], [182], [183]. Although SimCLR [182] has demonstrated remarkable success in the domain of computer vision, directly applying SimCLR to the field of time series data yields unsatisfactory results due to the insufficient modeling of temporal dependencies. CPC [184] introduced contrastive predictive coding, which utilizes model-predicted features as positive samples and randomly-sampled features as negative samples, to obtain time series representations that are advantageous for downstream tasks. TimeCLR [185] proposes a DTW data augmentation to generate phase shift and amplitude-change phenomena which can preserve time series structure and feature information. TS-TCC [186] employs efficient data augmentations designed for time-series data, and learns discriminative representations from the proposed Temporal Contrasting module and Contextual Contrasting module. TS2Vec [187] employs a hierarchical contrastive learning method and defines the contrastive loss from both instance-wise and patch-wise perspectives across different augmented context views, resulting in a robust contextual representation for each timestamp. Furthermore, LaST [188] takes the idea of variational inference theory [189] and proposes seasonal-trend representations learning and disentanglement mechanisms. CoST [190] proposes a contrastive learning framework to learn disentangled seasonal-trend representations for long sequence time series data. TF-C [191] develop frequency-based contrastive augmentation to leverage rich spectral information and explore time-frequency consistency.\n\nMasked modeling is a reconstruction-based method, which can predict a masked token in a sequence based on the context unmasked part [16], [181], [192].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "feadce1d-bc0b-40a4-b9e8-f57659047c4c": {"__data__": {"id_": "feadce1d-bc0b-40a4-b9e8-f57659047c4c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01baafe9-b1d0-4f36-8420-8882f1c06f33", "node_type": "4", "metadata": {}, "hash": "7107477183d8e19f2a9dee85f3e1be45b7939ea710972337aca3b36688cff1e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a61bbda-0c36-42ea-8b39-a14cd988b789", "node_type": "1", "metadata": {}, "hash": "d58032e4ee8e586e3079e22a7653c18a28ea239a22435290e3493c02d9d37241", "class_name": "RelatedNodeInfo"}}, "text": "TS-TCC [186] employs efficient data augmentations designed for time-series data, and learns discriminative representations from the proposed Temporal Contrasting module and Contextual Contrasting module. TS2Vec [187] employs a hierarchical contrastive learning method and defines the contrastive loss from both instance-wise and patch-wise perspectives across different augmented context views, resulting in a robust contextual representation for each timestamp. Furthermore, LaST [188] takes the idea of variational inference theory [189] and proposes seasonal-trend representations learning and disentanglement mechanisms. CoST [190] proposes a contrastive learning framework to learn disentangled seasonal-trend representations for long sequence time series data. TF-C [191] develop frequency-based contrastive augmentation to leverage rich spectral information and explore time-frequency consistency.\n\nMasked modeling is a reconstruction-based method, which can predict a masked token in a sequence based on the context unmasked part [16], [181], [192]. TST [193] follows the pre-training paradigm proposed in BERT [16] and proposes a pre-training framework for multivariate time series. Further, PatchTST [23] segments the time series data into multiple non-overlapping patches and proposes a patch-level masked modeling approach. SimMTM [194] relates masked modeling to manifold learning and presents a neighborhood aggregation design for reconstruction based on the similarities learned in series-wise representation space. HiMTM [195] proposes a novel hierarchical masked time series pre-training framework to effectively capture the multi-scale characteristics of time series data. TimeSiam [196] constructs an asymmetric masking reconstruction task to capture intrinsic temporal correlations between randomly sampled past and current subseries and learn internal time-dependent representations based on Siamese networks.\n\n# 6.2 Large Time Series Models\n\nWith the advent of Large Language Models (LLMs), the utilization of large-scale models to tackle time series downstream tasks has gained significant attention as the direction of future research. Current researches present the following two possible roadmaps to large time series models.\n\n# 6.2.1 Time Series Foundation Models\n\nRecent advancements in deep learning, particularly with the emergence of foundation models (FMs), have demonstrated significant progress in natural language processing.", "mimetype": "text/plain", "start_char_idx": 4332, "end_char_idx": 6792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3be1ee75-3fc3-4b18-9fd1-f00f2ba533d6": {"__data__": {"id_": "3be1ee75-3fc3-4b18-9fd1-f00f2ba533d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c4ba3cc-80d3-4f87-8cd1-fcdadb8267f2", "node_type": "4", "metadata": {}, "hash": "a3bc1fd7bd98ac1bf060844e2b3a1a2c691883ac6663d8146172339d953291e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a653da63-96ef-4247-a710-653a0de7c47c", "node_type": "1", "metadata": {}, "hash": "08ed79cc1c4752fcc40708fefc847facc8e267b235ade0ab6439f955f443ffd3", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 6.2.1 Generalized Time Series Foundation Models\n\n(NLP) and computer vision (CV) domains [179], [197], [198]. Different from prior deep models, foundation models are pre-trained on massive amounts of data, which enables them to have a wide range of general knowledge learned from diverse domains. Given their success in capturing contextual information and semantic understanding, it is promising to explore a generalized time series foundation model that can effectively learn complex temporal dependencies and capture the underlying dynamics inherent in time series data. Early attempts such as TimeGPT [199], Lag-LlaMa [200], and Timer [155] focus solely on univariate time series data. Nevertheless, in real-world forecasting scenarios, it is crucial to involve additional information that is related to the temporal variation of the target time series and must be taken into account, such as weather conditions or holidays. MOIRAI [201] tries to flatten multivariate time series into a single sequence containing all variate, but its generalization capabilities to other downstream analysis tasks are under-explored. In addition to modeling inter-series dependencies, modeling the relationship between time series and external factors in other can achieve a better understanding of time series data. These external factors may be in the form of other modalities, such as text data or calendar data, and thus multimodal learning is a future trend in the development of a multi-modal time series foundation model.\n\n# 6.2.2 Adaptation of Large Language Models\n\nLarge Language Models (LLMs) have made significant strides in solving various natural language processing tasks. Exemplified by the success of models like GPTs [17], [146] and LLaMA [202], [203], LLMs have proven adept at generalizing to unseen tasks by simply following provided prompts. Therefore, it has become a promising future research direction to unleash the power of LLMs in the field of time series. Here are two paradigms for adapting LLMs.\n\n# Fine-Tuning Pre-trained Language Models\n\nBased on the similar sequential nature, fine-tuning pre-trained language models to equip them with time series analysis capabilities has become a promising research topic. When applying LLMs to time series, it is essential to tokenize the time series before feeding it to a pre-trained model. Thus, adapting an LLM for time series included two key components: time series tokenization and efficient fine-tuning methods for time series analysis tasks. LLM4TS [204] proposes a two-stage fine-tuning strategy, including the time-series alignment stage to align LLMs with the nuances of time series data, and the fine-tuning stage for downstream tasks. LLMTime [205] treats time series forecasting as next-token prediction in text and attempts to encode time series data as a string of numerical digits. Recent Chronos [206] introduces a pre-trained probabilistic time series model based on existing Transformer-based language model architectures. Technologically, Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains the model on these tokenized time series via the cross-entropy loss. Benefiting from the generative capability of LLMs, most of the existing research focuses on time series forecasting tasks. GPT4TS [207] propose a unified framework for diverse time series analysis tasks by using a pre-trained GPT-2 model and fine-tuning the positional embeddings and the parameters of the layer normalization for each analysis task.\n\n# Prompting Large Language Models\n\nRecent Large Language Models exhibit the strong abilities of in-context learning [146] and instruction following [208]. Therefore, the paradigm of leveraging natural language instructions or task examples to guide the model in addressing novel tasks has emerged as a groundbreaking approach [209], [210], [211], which has become a potential solution for time series analysis tasks [212]. Recent literature, such as PromptCast [212], UniTime [213], and TimeLLM [214] focus on investigating a prompt template to enable LLMs to perform the forecasting task. There are other works represented by Autotimes [55], [215], that attempt to design soft prompts for time series data. However, existing prompting approaches are tailored for forecasting, and how to empower LLMs to other time series tasks besides forecasting is relatively unexplored.\n\n# 6.3 Practical Applications\n\n# 6.3.1 Handling Extremely Long Series\n\nDeep time series models have demonstrated remarkable performance across a wide range of downstream tasks, their applicability to longer time series data is often limited by scalability and high computational complexity. In industrial time series analysis, high-frequency sampling results in lengthy historical data, impeding the practical implementation of advanced deep models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a653da63-96ef-4247-a710-653a0de7c47c": {"__data__": {"id_": "a653da63-96ef-4247-a710-653a0de7c47c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c4ba3cc-80d3-4f87-8cd1-fcdadb8267f2", "node_type": "4", "metadata": {}, "hash": "a3bc1fd7bd98ac1bf060844e2b3a1a2c691883ac6663d8146172339d953291e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3be1ee75-3fc3-4b18-9fd1-f00f2ba533d6", "node_type": "1", "metadata": {}, "hash": "c5bf5f7cbf747f3f800b393ef22d6c1f0e10b40600363c10ccad128b4a6944d2", "class_name": "RelatedNodeInfo"}}, "text": "Recent literature, such as PromptCast [212], UniTime [213], and TimeLLM [214] focus on investigating a prompt template to enable LLMs to perform the forecasting task. There are other works represented by Autotimes [55], [215], that attempt to design soft prompts for time series data. However, existing prompting approaches are tailored for forecasting, and how to empower LLMs to other time series tasks besides forecasting is relatively unexplored.\n\n# 6.3 Practical Applications\n\n# 6.3.1 Handling Extremely Long Series\n\nDeep time series models have demonstrated remarkable performance across a wide range of downstream tasks, their applicability to longer time series data is often limited by scalability and high computational complexity. In industrial time series analysis, high-frequency sampling results in lengthy historical data, impeding the practical implementation of advanced deep models. Existing methods usually include patching techniques to enable them to handle long sequences, and when the input length becomes longer, the patch length can be increased accordingly to reduce the computational complexity. However, model performance is closely tied to patch length; hence, solely increasing patch size to reduce complexity may compromise capabilities. Therefore, addressing the limitations of deep models in handling longer time series could be a promising topic.\n\n# 6.3.2 Utilizing Exogenous Variables\n\nSince variations within the time series are often influenced by external factors, it is crucial to include exogenous variables in the analysis to gain a more comprehensive understanding of these factors. Exogenous variables, which are widely discussed in time series prediction tasks, are included in the model for uniform training in modeling time series data, without requiring separate analysis. However, in practical applications, different from multivariate time series analysis, the main variables and covariates usually occupy different positions. Given the crucial role played by exogenous variables in real-world applications, it is essential to explore a unified framework for modeling the relationships between the endogenous and exogenous variants, which allows for a more comprehensive understanding of interrelations and causality among different variants, leading to better and more reliable model performance, as well as interpretability.\n\n# 6.3.3 Processing Heterogeneous Data\n\nIn the field of time series analysis, there is still an unexplored area related to the modeling of heterogeneous time series data. Heterogeneous time series data encompasses a wide range of diverse characteristics, such as varying sampling rates, irregularities, and different length scales. These diverse features make it challenging to develop models that can effectively capture the underlying patterns and relationships.", "mimetype": "text/plain", "start_char_idx": 4049, "end_char_idx": 6889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d634a99c-e520-4ee2-837b-d6ba74ce0cee": {"__data__": {"id_": "d634a99c-e520-4ee2-837b-d6ba74ce0cee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d304b9c7-7e0a-4974-b19c-b7233486d6c7", "node_type": "4", "metadata": {}, "hash": "50598629cfcc324c9a3cd5fb7d24068a40da415c26bd7bc68d0b068817fc55ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1a4d301-1839-4edc-8c6d-e390cad5b3f2", "node_type": "1", "metadata": {}, "hash": "1be436630f6cb2c70dbbf62a82d8ca60bef9207c0178b1d6d94432f7f333a87b", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 6\n\nwithin the data. Moreover, the need for fixed-size inputs in current deep learning models limits their ability to handle the dynamic nature of heterogeneous time series data.\n\nAddressing these challenges requires innovative approaches that can adapt to the unique nature of each individual time series while still capturing the overarching patterns across multiple series. This may involve developing new techniques for feature extraction, incorporating domain knowledge into model design, or exploring alternative architectures that are better suited to handle variable-length inputs.\n\nAs researchers continue to explore this unexplored area in time series analysis, there is potential for significant advances in areas such as finance, healthcare, and environmental monitoring. By improving our ability to model and analyze heterogeneous time series data, we can gain deeper insights into complex systems and make more informed decisions based on predictive analytics. Overall, further research in this area holds great promise for advancing our understanding of temporal data dynamics and enhancing the capabilities of time series modeling in real-world applications.\n\n# 7 CONCLUSION\n\nIn this survey, we provide a systematic review of deep models in time series analysis and introduce Time-Series Library (TSLib) as a fair benchmark for deep time series models across various analysis tasks. Compared with previous reviews that focus on a specific analysis task or model architecture, this paper provides a comprehensive survey and overview of existing deep models for time series analysis, ranging from forecasting, classification, imputation, and anomaly detection. We first present a detailed review of the universal modules that are widely used among time series models, including normalization, decomposition, and Fourier analysis. Next, we summarize existing deep time series models from the perspective of backbone architecture. Based on the review of existing literature, we introduce a practical open-source library, Time Series Library (TSLib), which has included representative deep time series models that can be a fair evaluation benchmark in the field of time series analysis. Finally, we discuss future research directions for deep time series models based on the recent development of the AI community and the practical application needs of time series analysis in real-world scenarios.\n\n# REFERENCES\n\n1. G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series analysis: forecasting and control. John Wiley & Sons, 2015.\n2. J. D. Hamilton, Time series analysis. Princeton university press, 2020.\n3. H. Wu, T. Hu, Y. Liu, H. Zhou, J. Wang, and M. Long, \u201cTimesnet: Temporal 2d-variation modeling for general time series analysis,\u201d in ICLR, 2023.\n4. K. Jin, J. Wi, E. Lee, S. Kang, S. Kim, and Y. Kim, \u201cTrafficbert: Pre-trained model with large-scale data for long-range traffic flow forecasting,\u201d Expert Systems with Applications, vol. 186, p. 115738, 2021.\n5. J. Xu, H. Wu, J. Wang, and M. Long, \u201cAnomaly transformer: Time series anomaly detection with association discrepancy,\u201d in ICLR, 2022.\n6. H. Wu, H. Zhou, M. Long, and J. Wang, \u201cInterpretable weather forecasting for worldwide stations with a unified deep model,\u201d Nature Machine Intelligence, pp. 1\u201310, 2023.\n7. L. H. Koopmans, The spectral analysis of time series. Elsevier, 1995.\n8. H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, \u201cDeep learning for time series classification: a review,\u201d Data mining and knowledge discovery, vol. 33, no. 4, pp. 917\u2013963, 2019.\n9. M. Braei and S. Wagner, \u201cAnomaly detection in univariate time-series: A survey on the state-of-the-art,\u201d arXiv preprint arXiv:2004.00433, 2020.\n10. J. F. Torres, D. Hadjout, A. Sebaa, F. Mart\u00ednez- Alvarez, and A. Troncoso, \u201cDeep learning for time series forecasting: a survey,\u201d Big Data, vol. 9, no. 1, pp. 3\u201321, 2021.\n11. A. Bl\u00e1zquez-Garc\u00eda, A. Conde, U. Mori, and J. A. Lozano, \u201cA review on outlier/anomaly detection in time series data,\u201d Computing Surveys, vol. 54, no. 3, pp. 1\u201333, 2021.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1a4d301-1839-4edc-8c6d-e390cad5b3f2": {"__data__": {"id_": "d1a4d301-1839-4edc-8c6d-e390cad5b3f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d304b9c7-7e0a-4974-b19c-b7233486d6c7", "node_type": "4", "metadata": {}, "hash": "50598629cfcc324c9a3cd5fb7d24068a40da415c26bd7bc68d0b068817fc55ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d634a99c-e520-4ee2-837b-d6ba74ce0cee", "node_type": "1", "metadata": {}, "hash": "7de4fa0f8609ccd0111b44dbc377b073e1bc5fe5e802e341e798794873eb63cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa76c58b-da3f-4c78-858f-b2a7fadf22f6", "node_type": "1", "metadata": {}, "hash": "b880dffe7d51cfb75890ee43480202e9537ed10de59c3f339b11d050f778947f", "class_name": "RelatedNodeInfo"}}, "text": "917\u2013963, 2019.\n9. M. Braei and S. Wagner, \u201cAnomaly detection in univariate time-series: A survey on the state-of-the-art,\u201d arXiv preprint arXiv:2004.00433, 2020.\n10. J. F. Torres, D. Hadjout, A. Sebaa, F. Mart\u00ednez- Alvarez, and A. Troncoso, \u201cDeep learning for time series forecasting: a survey,\u201d Big Data, vol. 9, no. 1, pp. 3\u201321, 2021.\n11. A. Bl\u00e1zquez-Garc\u00eda, A. Conde, U. Mori, and J. A. Lozano, \u201cA review on outlier/anomaly detection in time series data,\u201d Computing Surveys, vol. 54, no. 3, pp. 1\u201333, 2021.\n12. Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun, \u201cTransformers in time series: A survey,\u201d arXiv preprint arXiv:2202.07125, 2022.\n13. M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb, I. King, and S. Pan, \u201cA survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection,\u201d arXiv preprint arXiv:2307.03759, 2023.\n14. Z. Shao, F. Wang, Y. Xu, W. Wei, C. Yu, Z. Zhang, D. Yao, G. Jin, X. Cao, G. Cong et al., \u201cExploring progress in multivariate time series forecasting: Comprehensive benchmarking and heterogeneity analysis,\u201d arXiv preprint arXiv:2310.06119, 2023.\n15. X. Qiu, J. Hu, L. Zhou, X. Wu, J. Du, B. Zhang, C. Guo, A. Zhou, C. S. Jensen, Z. Sheng et al., \u201cTfb: Towards comprehensive and fair benchmarking of time series forecasting methods,\u201d arXiv preprint arXiv:2403.20150, 2024.\n16. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n17. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., \u201cGpt-4 technical report,\u201d arXiv preprint arXiv:2303.08774, 2023.\n18. K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in CVPR, 2016.\n19. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021.\n20. S. Zhang, L. Yao, A. Sun, and Y. Tay, \u201cDeep learning based recommender system: A survey and new perspectives,\u201d Computing surveys, vol. 52, no. 1, pp. 1\u201338, 2019.\n21. H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang, \u201cInformer: Beyond efficient transformer for long sequence time-series forecasting,\u201d in AAAI, 2021.\n22. H. Wu, J. Xu, J. Wang, and M. Long, \u201cAutoformer: Decomposition transformers with auto-correlation for long-term series forecasting,\u201d in NeurIPS, 2021.\n23. Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam, \u201cA time series is worth 64 words: Long-term forecasting with transformers,\u201d in ICLR, 2023.\n24.", "mimetype": "text/plain", "start_char_idx": 3649, "end_char_idx": 6403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa76c58b-da3f-4c78-858f-b2a7fadf22f6": {"__data__": {"id_": "aa76c58b-da3f-4c78-858f-b2a7fadf22f6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d304b9c7-7e0a-4974-b19c-b7233486d6c7", "node_type": "4", "metadata": {}, "hash": "50598629cfcc324c9a3cd5fb7d24068a40da415c26bd7bc68d0b068817fc55ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1a4d301-1839-4edc-8c6d-e390cad5b3f2", "node_type": "1", "metadata": {}, "hash": "1be436630f6cb2c70dbbf62a82d8ca60bef9207c0178b1d6d94432f7f333a87b", "class_name": "RelatedNodeInfo"}}, "text": "52, no. 1, pp. 1\u201338, 2019.\n21. H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang, \u201cInformer: Beyond efficient transformer for long sequence time-series forecasting,\u201d in AAAI, 2021.\n22. H. Wu, J. Xu, J. Wang, and M. Long, \u201cAutoformer: Decomposition transformers with auto-correlation for long-term series forecasting,\u201d in NeurIPS, 2021.\n23. Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam, \u201cA time series is worth 64 words: Long-term forecasting with transformers,\u201d in ICLR, 2023.\n24. B. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio, \u201cN-BEATS: neural basis expansion analysis for interpretable time series forecasting,\u201d in ICLR, 2020.\n25. C. W. Granger, \u201cSome properties of time series data and their use in econometric model specification,\u201d Journal of econometrics, vol. 16, no. 1, pp. 121\u2013130, 1981.\n26. R. F. Engle and C. W. Granger, \u201cCo-integration and error correction: representation, estimation, and testing,\u201d Econometrica: journal of the Econometric Society, pp. 251\u2013276, 1987.\n27. J. H. Stock and M. W. Watson, \u201cVector autoregressions,\u201d Journal of Economic perspectives, vol. 15, no. 4, pp. 101\u2013115, 2001.\n28. D. Cao, Y. Wang, J. Duan, C. Zhang, X. Zhu, C. Huang, Y. Tong, B. Xu, J. Bai, J. Tong et al., \u201cSpectral temporal graph neural network for multivariate time-series forecasting,\u201d in NeurIPS, 2020.\n29. Y. Zhang and J. Yan, \u201cCrossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting,\u201d in ICLR, 2023.\n30. Y. Liu, T. Hu, H. Zhang, H. Wu, S. Wang, L. Ma, and M. Long, \u201citransformer: Inverted transformers are effective for time series forecasting,\u201d arXiv preprint arXiv:2310.06625, 2023.\n31. A. S. Weigend, Time series prediction: forecasting the future and understanding the past. Routledge, 2018.\n32. G. P. Zhang, \u201cTime series forecasting using a hybrid arima and neural network model,\u201d Neurocomputing, vol. 50, pp. 159\u2013175, 2003.", "mimetype": "text/plain", "start_char_idx": 5897, "end_char_idx": 7814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0853c091-805b-41fa-8b7c-a3017d3a4b38": {"__data__": {"id_": "0853c091-805b-41fa-8b7c-a3017d3a4b38", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6991055-05ca-46b1-bc0e-a8e474361bf8", "node_type": "4", "metadata": {}, "hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6204fdbd-b441-4326-968f-c06e1dbb35b3", "node_type": "1", "metadata": {}, "hash": "a635630396efb22b2c52c3762f609b28883978b3e52d720c1dffed7a3e6a8c83", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# References\n\n1. C. Fang and C. Wang, \u201cTime series data imputation: A survey on deep learning approaches,\u201d arXiv preprint arXiv:2011.11347, 2020.\n2. Y. Luo, X. Cai, Y. Zhang, J. Xu et al., \u201cMultivariate time series imputation with generative adversarial networks,\u201d in NeurIPS, 2018.\n3. S. Moritz and T. Bartz-Beielstein, \u201cimputets: time series missing value imputation in r.\u201d R Journal, vol. 9, no. 1, p. 207, 2017.\n4. B. Zhao, H. Lu, S. Chen, J. Liu, and D. Wu, \u201cConvolutional neural networks for time series classification,\u201d Journal of Systems Engineering and Electronics, vol. 28, no. 1, pp. 162\u2013169, 2017.\n5. N. Laptev, S. Amizadeh, and I. Flint, \u201cGeneric and scalable framework for automated time-series anomaly detection,\u201d in SIGKDD, 2015.\n6. D. Ulyanov, A. Vedaldi, and V. Lempitsky, \u201cInstance normalization: The missing ingredient for fast stylization,\u201d arXiv preprint arXiv:1607.08022, 2016.\n7. N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosifidis, \u201cDeep adaptive input normalization for time series forecasting,\u201d IEEE transactions on neural networks and learning systems, vol. 31, no. 9, pp. 3760\u20133765, 2019.\n8. T. Kim, J. Kim, Y. Tae, C. Park, J.-H. Choi, and J. Choo, \u201cReversible instance normalization for accurate time-series forecasting against distribution shift,\u201d in ICLR, 2021.\n9. Y. Liu, H. Wu, J. Wang, and M. Long, \u201cNon-stationary transformers: Exploring the stationarity in time series forecasting,\u201d in NeurIPS, 2022.\n10. J. Gao, W. Hu, and Y. Chen, \u201cClient: Cross-variable linear integrated enhanced transformer for multivariate long-term time series forecasting,\u201d arXiv preprint arXiv:2305.18838, 2023.\n11. H. Wang, Y. Mo, N. Yin, H. Dai, B. Li, S. Fan, and S. Mo, \u201cDance of channel and sequence: An efficient attention-based approach for multivariate time series forecasting,\u201d arXiv preprint arXiv:2312.06220, 2023.\n12. Z. Liu, M. Cheng, Z. Li, Z. Huang, Q. Liu, Y. Xie, and E. Chen, \u201cAdaptive normalization for non-stationary time series forecasting: A temporal slice perspective,\u201d in NeurIPS, 2024.\n13. R. B. Cleveland, W. S. Cleveland, J. E. McRae, and I. Terpenning, \u201cStl: A seasonal-trend decomposition,\u201d J. Off. Stat, vol. 6, no. 1, pp. 3\u201373, 1990.\n14. O. D. Anderson, \u201cTime-series. 2nd edn.\u201d 1976.\n15. C. RB, \u201cStl: A seasonal-trend decomposition procedure based on loess,\u201d J Off Stat, vol. 6, pp. 3\u201373, 1990.\n16. E. B. Dagum and S. Bianconcini, Seasonal adjustment methods and real time trend-cycle estimation. Springer, 2016.\n17. Q. Wen, J. Gao, X. Song, L. Sun, H. Xu, and S. Zhu, \u201cRobuststl: A robust seasonal-trend decomposition algorithm for long time series,\u201d in AAAI, 2019.\n18. A. M. De Livera, R. J. Hyndman, and R. D. Snyder, \u201cForecasting time series with complex seasonal patterns using exponential smoothing,\u201d Journal of the American statistical association, vol. 106, no. 496, pp. 1513\u20131527, 2011.\n19. S. J. Taylor and B. Letham, \u201cForecasting at scale,\u201d The American Statistician, vol. 72, no.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6204fdbd-b441-4326-968f-c06e1dbb35b3": {"__data__": {"id_": "6204fdbd-b441-4326-968f-c06e1dbb35b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6991055-05ca-46b1-bc0e-a8e474361bf8", "node_type": "4", "metadata": {}, "hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0853c091-805b-41fa-8b7c-a3017d3a4b38", "node_type": "1", "metadata": {}, "hash": "fe9d67ecb715a2253218ff3fc302532903b36f90563f1b0111d6335dcb33218e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbf6152d-9151-49cf-8a18-dad62084ef85", "node_type": "1", "metadata": {}, "hash": "84430e8ca9c5eb206ae5ca18ba6aaddcf3c32910f906a028f5d9bd5e67a852bc", "class_name": "RelatedNodeInfo"}}, "text": "16. E. B. Dagum and S. Bianconcini, Seasonal adjustment methods and real time trend-cycle estimation. Springer, 2016.\n17. Q. Wen, J. Gao, X. Song, L. Sun, H. Xu, and S. Zhu, \u201cRobuststl: A robust seasonal-trend decomposition algorithm for long time series,\u201d in AAAI, 2019.\n18. A. M. De Livera, R. J. Hyndman, and R. D. Snyder, \u201cForecasting time series with complex seasonal patterns using exponential smoothing,\u201d Journal of the American statistical association, vol. 106, no. 496, pp. 1513\u20131527, 2011.\n19. S. J. Taylor and B. Letham, \u201cForecasting at scale,\u201d The American Statistician, vol. 72, no. 1, pp. 37\u201345, 2018.\n20. A. Zeng, M. Chen, L. Zhang, and Q. Xu, \u201cAre transformers effective for time series forecasting?\u201d in AAAI, 2023.\n21. H. Cao, Z. Huang, T. Yao, J. Wang, H. He, and Y. Wang, \u201cInparfomer: evolutionary decomposition transformers with interactive parallel attention for long-term time series forecasting,\u201d in AAAI, 2023.\n22. D. Du, B. Su, and Z. Wei, \u201cPreformer: predictive transformer with multi-scale segment-wise correlations for long-term time series forecasting,\u201d in ICASSP, 2023.\n23. D. Cao, F. Jia, S. O. Arik, T. Pfister, Y. Zheng, W. Ye, and Y. Liu, \u201cTempo: Prompt-based generative pre-trained transformer for time series forecasting,\u201d arXiv preprint arXiv:2310.04948, 2023.\n24. B. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio, \u201cN-beats: Neural basis expansion analysis for interpretable time series forecasting,\u201d arXiv preprint arXiv:1905.10437, 2019.\n25. C. Challu, K. G. Olivares, B. N. Oreshkin, F. G. Ramirez, M. M. Canseco, and A. Dubrawski, \u201cNhits: Neural hierarchical interpolation for time series forecasting,\u201d in AAAI, 2023.\n26. W. Fan, S. Zheng, X. Yi, W. Cao, Y. Fu, J. Bian, and T.-Y. Liu, \u201cDepts: Deep expansion learning for periodic time series forecasting,\u201d in ICLR, 2022.\n27. W. Fan, Y. Fu, S. Zheng, J. Bian, Y. Zhou, and H. Xiong, \u201cDewp: Deep expansion learning for wind power forecasting,\u201d ACM Transactions on Knowledge Discovery from Data, vol. 18, no. 3, pp. 1\u201321, 2024.\n28. L. Xiong, X. Chen, T.-K. Huang, J. Schneider, and J. G. Carbonell, \u201cTemporal collaborative filtering with bayesian probabilistic tensor factorization,\u201d in SIAM-SDM, 2010.\n29. H.-F. Yu, N. Rao, and I. S. Dhillon, \u201cTemporal regularized matrix factorization for high-dimensional time series prediction,\u201d in NeurIPS, 2016.\n30. K. Takeuchi, H. Kashima, and N. Ueda, \u201cAutoregressive tensor factorization for spatio-temporal predictions,\u201d in ICDM, 2017.\n31. X. Chen, C. Zhang, X.-L. Zhao, N. Saunier, and L. Sun, \u201cNonstationary temporal matrix factorization for multivariate time series forecasting,\u201d arXiv preprint arXiv:2203.10651, 2022.\n32. X. Chen and L. Sun, \u201cBayesian temporal factorization for multi-dimensional time series prediction,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 4659\u20134673, 2021.\n33. R. Sen, H.-F. Yu, and I. S. Dhillon, \u201cThink globally, act locally: A deep neural network approach to high-dimensional time series forecasting,\u201d in NeurIPS, 2019.\n34. J.-M. Yang, Z.-R.", "mimetype": "text/plain", "start_char_idx": 2437, "end_char_idx": 5489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbf6152d-9151-49cf-8a18-dad62084ef85": {"__data__": {"id_": "dbf6152d-9151-49cf-8a18-dad62084ef85", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6991055-05ca-46b1-bc0e-a8e474361bf8", "node_type": "4", "metadata": {}, "hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6204fdbd-b441-4326-968f-c06e1dbb35b3", "node_type": "1", "metadata": {}, "hash": "a635630396efb22b2c52c3762f609b28883978b3e52d720c1dffed7a3e6a8c83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "066b6b3f-1112-4cc2-b1e7-9c39241b89e6", "node_type": "1", "metadata": {}, "hash": "03358c7197d100c55c6a208c8fa7ac4cf3ef5e181fb9dddcb89cd828023bdedf", "class_name": "RelatedNodeInfo"}}, "text": "31. X. Chen, C. Zhang, X.-L. Zhao, N. Saunier, and L. Sun, \u201cNonstationary temporal matrix factorization for multivariate time series forecasting,\u201d arXiv preprint arXiv:2203.10651, 2022.\n32. X. Chen and L. Sun, \u201cBayesian temporal factorization for multi-dimensional time series prediction,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 4659\u20134673, 2021.\n33. R. Sen, H.-F. Yu, and I. S. Dhillon, \u201cThink globally, act locally: A deep neural network approach to high-dimensional time series forecasting,\u201d in NeurIPS, 2019.\n34. J.-M. Yang, Z.-R. Peng, and L. Lin, \u201cReal-time spatiotemporal prediction and imputation of traffic status based on lstm and graph laplacian regularized matrix factorization,\u201d Transportation Research Part C: Emerging Technologies, vol. 129, p. 103228, 2021.\n35. D. Cai, X. He, J. Han, and T. S. Huang, \u201cGraph regularized nonnegative matrix factorization for data representation,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 33, no. 8, pp. 1548\u20131560, 2010.\n36. I. N. Sneddon, Fourier transforms. Courier Corporation, 1995.\n37. J. W. Goodman, Introduction to Fourier optics. Roberts and Company publishers, 2005.\n38. P. Bloomfield, Fourier analysis of time series: an introduction. John Wiley & Sons, 2004.\n39. K. Samiee, P. Kovacs, and M. Gabbouj, \u201cEpileptic seizure classification of eeg time-series using rational discrete short-time fourier transform,\u201d IEEE transactions on Biomedical Engineering, vol. 62, no. 2, pp. 541\u2013552, 2014.\n40. E. O. Brigham, The fast Fourier transform and its applications. Prentice-Hall, Inc., 1988.\n41. Y. Meyer, Wavelets and operators: volume 1. Cambridge university press, 1992, no. 37.\n42. J. Wang, Z. Wang, J. Li, and J. Wu, \u201cMultilevel wavelet decomposition network for interpretable time series analysis,\u201d in SIGKDD, 2018.\n43. S. Yao, A. Piao, W. Jiang, Y. Zhao, H. Shao, S. Liu, D. Liu, J. Li, T. Wang, S. Hu et al., \u201cStfnets: Learning sensing signals from the time-frequency perspective with short-time fourier neural networks,\u201d in The World Wide Web Conference, 2019, pp. 2192\u20132202.\n44. M. Rhif, A. Ben Abbes, I. R. Farah, B. Mart\u00ednez, and Y. Sang, \u201cWavelet transform application for/in non-stationary time-series analysis: A review,\u201d Applied Sciences, vol. 9, no. 7, p. 1345, 2019.\n45. L. Minhao, A. Zeng, L. Qiuxia, R. Gao, M. Li, J. Qin, and Q. Xu, \u201cT-wavenet: A tree-structured wavelet neural network for time series signal analysis,\u201d in ICLR, 2021.\n46. L. Yang and S. Hong, \u201cUnsupervised time-series representation learning with iterative bilinear temporal-spectral fusion,\u201d in ICML, 2022.\n47. J. Wang, C. Yang, X. Jiang, and J. Wu, \u201cWhen: A wavelet-dtw hybrid attention network for heterogeneous time series analysis,\u201d in SIGKDD, 2023.\n48. F. Yang, X. Li, M. Wang, H. Zang, W. Pang, and M. Wang, \u201cWaveform: Graph enhanced wavelet learning for long sequence forecasting of multivariate time series,\u201d in AAAI, 2023.\n49. K. Yi, Q. Zhang, L. Cao, S. Wang, G. Long, L. Hu, H. He, Z. Niu, W. Fan, and H. Xiong, \u201cA survey on deep learning based time series analysis with frequency transformation,\u201d arXiv preprint arXiv:2302.02173, 2023.\n50.", "mimetype": "text/plain", "start_char_idx": 4911, "end_char_idx": 8075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "066b6b3f-1112-4cc2-b1e7-9c39241b89e6": {"__data__": {"id_": "066b6b3f-1112-4cc2-b1e7-9c39241b89e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6991055-05ca-46b1-bc0e-a8e474361bf8", "node_type": "4", "metadata": {}, "hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbf6152d-9151-49cf-8a18-dad62084ef85", "node_type": "1", "metadata": {}, "hash": "84430e8ca9c5eb206ae5ca18ba6aaddcf3c32910f906a028f5d9bd5e67a852bc", "class_name": "RelatedNodeInfo"}}, "text": "47. J. Wang, C. Yang, X. Jiang, and J. Wu, \u201cWhen: A wavelet-dtw hybrid attention network for heterogeneous time series analysis,\u201d in SIGKDD, 2023.\n48. F. Yang, X. Li, M. Wang, H. Zang, W. Pang, and M. Wang, \u201cWaveform: Graph enhanced wavelet learning for long sequence forecasting of multivariate time series,\u201d in AAAI, 2023.\n49. K. Yi, Q. Zhang, L. Cao, S. Wang, G. Long, L. Hu, H. He, Z. Niu, W. Fan, and H. Xiong, \u201cA survey on deep learning based time series analysis with frequency transformation,\u201d arXiv preprint arXiv:2302.02173, 2023.\n50. T. Dai, B. Wu, P. Liu, N. Li, J. Bao, Y. Jiang, and S.-T. Xia, \u201cPeriodicity decoupling framework for long-term series forecasting,\u201d in ICLR, 2023.\n51. N. Wiener, \u201cGeneralized harmonic analysis,\u201d Acta mathematica, vol. 55, no. 1, pp. 117\u2013258, 1930.\n52. T. Zhou, Z. Ma, Q. Wen, L. Sun, T. Yao, W. Yin, R. Jin et al., \u201cFilm: Frequency improved legendre memory model for long-term time series forecasting,\u201d in NeurIPS, 2022.\n53. Z. Xu, A. Zeng, and Q. Xu, \u201cFits: Modeling time series with 10k parameters,\u201d arXiv preprint arXiv:2307.03756, 2023.", "mimetype": "text/plain", "start_char_idx": 7531, "end_char_idx": 8616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd613fac-3db7-40d1-bf83-527554e903d2": {"__data__": {"id_": "dd613fac-3db7-40d1-bf83-527554e903d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "355cab74-ec27-4cae-afc8-c66a4a137f48", "node_type": "4", "metadata": {}, "hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27c48c02-ace0-4898-bb5a-4881b9b6b0c0", "node_type": "1", "metadata": {}, "hash": "32003f6816b0332ac11db23ced60a7f2512351249a771367c76602b1b735a91c", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# References\n\n1. T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, and R. Jin, \u201cFedformer: Frequency enhanced decomposed transformer for long-term series forecasting,\u201d in ICML, 2022.\n2. Z. Yang, W. Yan, X. Huang, and L. Mei, \u201cAdaptive temporal-frequency network for time-series forecasting,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 4, pp. 1576\u20131587, 2020.\n3. C. Zhang, T. Zhou, Q. Wen, and L. Sun, \u201cTfad: A decomposition time series anomaly detection architecture with time-frequency analysis,\u201d in CIKM, 2022.\n4. K. Yi, Q. Zhang, L. Hu, H. He, N. An, L. Cao, and Z. Niu, \u201cEdge-varying fourier graph networks for multivariate time series forecasting,\u201d arXiv preprint arXiv:2210.03093, 2022.\n5. K. Yi, Q. Zhang, W. Fan, S. Wang, P. Wang, H. He, N. An, D. Lian, L. Cao, and Z. Niu, \u201cFrequency-domain mlps are more effective learners in time series forecasting,\u201d in NeurIPS, 2024.\n6. Z. Wang, C. Pei, M. Ma, X. Wang, Z. Li, D. Pei, S. Rajmohan, D. Zhang, Q. Lin, H. Zhang et al., \u201cRevisiting vae for unsupervised time series anomaly detection: A frequency perspective,\u201d in WWW, 2024.\n7. E. Eldele, M. Ragab, Z. Chen, M. Wu, and X. Li, \u201cTslanet: Rethinking transformers for time series representation learning,\u201d in ICML, 2024.\n8. J. Crabb\u00e9, N. Huynh, J. Stanczuk, and M. van der Schaar, \u201cTime series diffusion in the frequency domain,\u201d in ICML, 2024.\n9. K. G. Olivares, C. Challu, G. Marcjasz, R. Weron, and A. Dubrawski, \u201cNeural basis expansion analysis with exogenous variables: Forecasting electricity prices with nbeatsx,\u201d International Journal of Forecasting, vol. 39, no. 2, pp. 884\u2013900, 2023.\n10. I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al., \u201cMlp-mixer: An all-mlp architecture for vision,\u201d in NeurIPS, 2021.\n11. S.-A. Chen, C.-L. Li, N. Yoder, S. O. Arik, and T. Pfister, \u201cTsmixer: An all-mlp architecture for time series forecasting,\u201d arXiv preprint arXiv:2303.06053, 2023.\n12. A. Das, W. Kong, A. Leach, R. Sen, and R. Yu, \u201cLong-term forecasting with tide: Time-series dense encoder,\u201d arXiv preprint arXiv:2304.08424, 2023.\n13. P. J. Schmid, \u201cDynamic mode decomposition of numerical and experimental data,\u201d Journal of fluid mechanics, vol. 656, pp. 5\u201328, 2010.\n14. Y. Liu, C. Li, J. Wang, and M. Long, \u201cKoopa: Learning non-stationary time series dynamics with koopman predictors,\u201d arXiv preprint arXiv:2305.18803, 2023.\n15. S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n16. I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in NeurIPS, 2014.\n17.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27c48c02-ace0-4898-bb5a-4881b9b6b0c0": {"__data__": {"id_": "27c48c02-ace0-4898-bb5a-4881b9b6b0c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "355cab74-ec27-4cae-afc8-c66a4a137f48", "node_type": "4", "metadata": {}, "hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd613fac-3db7-40d1-bf83-527554e903d2", "node_type": "1", "metadata": {}, "hash": "5d16368df214b5fd9f2af91d1e571cb086710ec9d2e3e70ba15769f0bba85d4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6eb17b7e-a0ee-426e-b52a-6fa080ab4e0f", "node_type": "1", "metadata": {}, "hash": "30403ea75e12d401eedc9ffc86fa6c78b7f9996ae49838dcaccc79f72d313bba", "class_name": "RelatedNodeInfo"}}, "text": "13. P. J. Schmid, \u201cDynamic mode decomposition of numerical and experimental data,\u201d Journal of fluid mechanics, vol. 656, pp. 5\u201328, 2010.\n14. Y. Liu, C. Li, J. Wang, and M. Long, \u201cKoopa: Learning non-stationary time series dynamics with koopman predictors,\u201d arXiv preprint arXiv:2305.18803, 2023.\n15. S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n16. I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in NeurIPS, 2014.\n17. K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder\u2013decoder for statistical machine translation,\u201d in EMNLP, 2014.\n18. R. Nallapati, B. Zhou, C. Gulcehre, and B. Xiang, \u201cAbstractive text summarization using sequence-to-sequence rnns and beyond,\u201d in SIGNLL, 2016.\n19. S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo, A. Courville, and Y. Bengio, \u201cSamplernn: An unconditional end-to-end neural audio generation model,\u201d arXiv preprint arXiv:1612.07837, 2016.\n20. A. Tealab, \u201cTime series forecasting using artificial neural networks methodologies: A systematic review,\u201d Future Computing and Informatics Journal, vol. 3, no. 2, pp. 334\u2013340, 2018.\n21. W. Cao, D. Wang, J. Li, H. Zhou, L. Li, and Y. Li, \u201cBrits: Bidirectional recurrent imputation for time series,\u201d in NeurIPS, 2018.\n22. J. Yoon, W. R. Zame, and M. van der Schaar, \u201cEstimating missing data in temporal data streams using multi-directional recurrent neural networks,\u201d IEEE Transactions on Biomedical Engineering, vol. 66, no. 5, pp. 1477\u20131490, 2018.\n23. Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, \u201cRecurrent neural networks for multivariate time series with missing values,\u201d Scientific reports, vol. 8, no. 1, p. 6085, 2018.\n24. I. M. Baytas, C. Xiao, X. Zhang, F. Wang, A. K. Jain, and J. Zhou, \u201cPatient subtyping via time-aware lstm networks,\u201d in SIGKDD, 2017.\n25. L. Shen, Z. Li, and J. Kwok, \u201cTimeseries anomaly detection using temporal hierarchical one-class network,\u201d in NeurIPS, 2020.\n26. G. Lai, W.-C. Chang, Y. Yang, and H. Liu, \u201cModeling long-and short-term temporal patterns with deep neural networks,\u201d in SIGIR, 2018.\n27. Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G. Cottrell, \u201cA dual-stage attention-based recurrent neural network for time series prediction,\u201d arXiv preprint arXiv:1704.02971, 2017.\n28. D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski, \u201cDeepar: Probabilistic forecasting with autoregressive recurrent networks,\u201d International Journal of Forecasting, vol. 36, no. 3, pp. 1181\u20131191, 2020.\n29. A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9, \u201cCombining recurrent, convolutional, and continuous-time models with linear state space layers,\u201d in NeurIPS, 2021.\n30. M. Fraccaro, S. K. S\u00f8nderby, U. Paquet, and O. Winther, \u201cSequential neural models with stochastic layers,\u201d in NeurIPS, 2016.\n31.", "mimetype": "text/plain", "start_char_idx": 2217, "end_char_idx": 5181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6eb17b7e-a0ee-426e-b52a-6fa080ab4e0f": {"__data__": {"id_": "6eb17b7e-a0ee-426e-b52a-6fa080ab4e0f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "355cab74-ec27-4cae-afc8-c66a4a137f48", "node_type": "4", "metadata": {}, "hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27c48c02-ace0-4898-bb5a-4881b9b6b0c0", "node_type": "1", "metadata": {}, "hash": "32003f6816b0332ac11db23ced60a7f2512351249a771367c76602b1b735a91c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8a14ee6-dc8d-4bd1-a826-c279f9a519fe", "node_type": "1", "metadata": {}, "hash": "3360f6122cbecb9382621beff68f3b03e69b7296e7db0fcf3aceb2edd2ee4ea6", "class_name": "RelatedNodeInfo"}}, "text": "28. D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski, \u201cDeepar: Probabilistic forecasting with autoregressive recurrent networks,\u201d International Journal of Forecasting, vol. 36, no. 3, pp. 1181\u20131191, 2020.\n29. A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9, \u201cCombining recurrent, convolutional, and continuous-time models with linear state space layers,\u201d in NeurIPS, 2021.\n30. M. Fraccaro, S. K. S\u00f8nderby, U. Paquet, and O. Winther, \u201cSequential neural models with stochastic layers,\u201d in NeurIPS, 2016.\n31. R. Krishnan, U. Shalit, and D. Sontag, \u201cStructured inference networks for nonlinear state space models,\u201d in AAAI, 2017.\n32. S. S. Rangapuram, M. W. Seeger, J. Gasthaus, L. Stella, Y. Wang, and T. Januschowski, \u201cDeep state space models for time series forecasting,\u201d in NeurIPS, 2018.\n33. A. Gu, K. Goel, and C. R\u00e9, \u201cEfficiently modeling long sequences with structured state spaces,\u201d in ICLR, 2022.\n34. L. Zhou, M. Poli, W. Xu, S. Massaroli, and S. Ermon, \u201cDeep latent state space models for time-series generation,\u201d in ICML, 2023.\n35. A. Gu and T. Dao, \u201cMamba: Linear-time sequence modeling with selective state spaces,\u201d arXiv preprint arXiv:2312.00752, 2023.\n36. J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, G. Wang, J. Cai et al., \u201cRecent advances in convolutional neural networks,\u201d Pattern recognition, vol. 77, pp. 354\u2013377, 2018.\n37. M. Tan and Q. Le, \u201cEfficientnet: Rethinking model scaling for convolutional neural networks,\u201d in ICML, 2019.\n38. A. Kendall, V. Badrinarayanan, and R. Cipolla, \u201cBayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding,\u201d arXiv preprint arXiv:1511.02680, 2015.\n39. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \u201cYou only look once: Unified, real-time object detection,\u201d in CVPR, 2016.\n40. A. Le Guennec, S. Malinowski, and R. Tavenard, \u201cData augmentation for time series classification using convolutional neural networks,\u201d in ECML/PKDD workshop, 2016.\n41. Z. Cui, W. Chen, and Y. Chen, \u201cMulti-scale convolutional neural networks for time series classification,\u201d arXiv preprint arXiv:1603.06995, 2016.\n42. H. Ismail Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt, J. Weber, G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean, \u201cInceptiontime: Finding alexnet for time series classification,\u201d Data Mining and Knowledge Discovery, vol. 34, no. 6, pp. 1936\u20131962, 2020.\n43. M. Liu, A. Zeng, M. Chen, Z. Xu, Q. Lai, L. Ma, and Q. Xu, \u201cScinet: Time series modeling and forecasting with sample convolution and interaction,\u201d in NeurIPS, 2022.\n44. A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu, \u201cPixel recurrent neural networks,\u201d in ICML, 2016.\n45. A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d arXiv preprint arXiv:1609.03499, 2016.\n46.", "mimetype": "text/plain", "start_char_idx": 4650, "end_char_idx": 7576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8a14ee6-dc8d-4bd1-a826-c279f9a519fe": {"__data__": {"id_": "d8a14ee6-dc8d-4bd1-a826-c279f9a519fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "355cab74-ec27-4cae-afc8-c66a4a137f48", "node_type": "4", "metadata": {}, "hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6eb17b7e-a0ee-426e-b52a-6fa080ab4e0f", "node_type": "1", "metadata": {}, "hash": "30403ea75e12d401eedc9ffc86fa6c78b7f9996ae49838dcaccc79f72d313bba", "class_name": "RelatedNodeInfo"}}, "text": "43. M. Liu, A. Zeng, M. Chen, Z. Xu, Q. Lai, L. Ma, and Q. Xu, \u201cScinet: Time series modeling and forecasting with sample convolution and interaction,\u201d in NeurIPS, 2022.\n44. A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu, \u201cPixel recurrent neural networks,\u201d in ICML, 2016.\n45. A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d arXiv preprint arXiv:1609.03499, 2016.\n46. S. Bai, J. Z. Kolter, and V. Koltun, \u201cAn empirical evaluation of generic convolutional and recurrent networks for sequence modeling,\u201d arXiv preprint arXiv:1803.01271, 2018.\n47. H. Wang, J. Peng, F. Huang, J. Wang, J. Chen, and Y. Xiao, \u201cMicn: Multi-scale local and global context modeling for long-term series forecasting,\u201d in ICLR, 2022.\n48. D. Luo and X. Wang, \u201cModerntcn: A modern pure convolution structure for general time series analysis,\u201d in ICLR, 2024.\n49. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \u201cGoing deeper with convolutions,\u201d in CVPR, 2015.\n50. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking the inception architecture for computer vision,\u201d in CVPR, 2016.\n51. F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, \u201cThe graph neural network model,\u201d IEEE transactions on neural networks, vol. 20, no. 1, pp. 61\u201380, 2008.\n52. T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in ICLR, 2017.", "mimetype": "text/plain", "start_char_idx": 7088, "end_char_idx": 8631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7feb97af-07a8-476f-864c-056d6e0de2ce": {"__data__": {"id_": "7feb97af-07a8-476f-864c-056d6e0de2ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9", "node_type": "4", "metadata": {}, "hash": "f6cba8eb167bb42d86a6abce9f753afd601c26351028e82e748f2c449fb76e98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fd0aa71-d31f-4980-a99f-5ae19b146f79", "node_type": "1", "metadata": {}, "hash": "0d0c4ad1019bd6531002ed16b1ac5e3fb06fd08dee32faa3e41f7a4a36608729", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# References\n\n1. Y. Li, R. Yu, C. Shahabi, and Y. Liu, \u201cDiffusion convolutional recurrent neural network: Data-driven traffic forecasting,\u201d in ICLR, 2018.\n2. B. Yu, H. Yin, and Z. Zhu, \u201cSpatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting,\u201d in IJCAI, 2018.\n3. Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, \u201cGraph wavenet for deep spatial-temporal graph modeling,\u201d in IJCAI, 2019.\n4. L. Bai, L. Yao, C. Li, X. Wang, and C. Wang, \u201cAdaptive graph convolutional recurrent network for traffic forecasting,\u201d in NeurIPS, 2020.\n5. Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang, \u201cConnecting the dots: Multivariate time series forecasting with graph neural networks,\u201d in SIGKDD, 2020.\n6. M. Li and Z. Zhu, \u201cSpatial-temporal fusion graph neural networks for traffic flow forecasting,\u201d in AAAI, 2021.\n7. J. Lu, D. Batra, D. Parikh, and S. Lee, \u201cVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,\u201d in NeurIPS, 2019.\n8. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, \u201cXlnet: Generalized autoregressive pretraining for language understanding,\u201d in NeurIPS, 2019.\n9. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage models are few-shot learners,\u201d in NeurIPS, 2020.\n10. W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., \u201cA survey of large language models,\u201d arXiv preprint arXiv:2303.18223, 2023.\n11. H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou, \u201cTraining data-efficient image transformers & distillation through attention,\u201d in ICML, 2021.\n12. Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in ICCV, 2021.\n13. W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, \u201cPyramid vision transformer: A versatile backbone for dense prediction without convolutions,\u201d in ICCV, 2021.\n14. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in NeurIPS, 2017.\n15. S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan, \u201cEnhancing the locality and breaking the memory bottleneck of transformer on time series forecasting,\u201d in NeurIPS, 2019.\n16. S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and S. Dustdar, \u201cPyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting,\u201d in ICLR, 2021.\n17. T. Zhou, P. Niu, L. Sun, R. Jin et al., \u201cOne fits all: Power general time series analysis by pretrained lm,\u201d in NeurIPS, 2023.\n18.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8fd0aa71-d31f-4980-a99f-5ae19b146f79": {"__data__": {"id_": "8fd0aa71-d31f-4980-a99f-5ae19b146f79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9", "node_type": "4", "metadata": {}, "hash": "f6cba8eb167bb42d86a6abce9f753afd601c26351028e82e748f2c449fb76e98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7feb97af-07a8-476f-864c-056d6e0de2ce", "node_type": "1", "metadata": {}, "hash": "faca2539750ce8d1286168595f8db9a56654d45cb37c0c4f59f37fdb3a2d721f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55efde07-d021-4603-94f1-c0e90e1ac601", "node_type": "1", "metadata": {}, "hash": "5b2eb29730c9f513a66ed06c185dfae5f67fc6fc2dcca0a77c6fcfd1596287ce", "class_name": "RelatedNodeInfo"}}, "text": "15. S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan, \u201cEnhancing the locality and breaking the memory bottleneck of transformer on time series forecasting,\u201d in NeurIPS, 2019.\n16. S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and S. Dustdar, \u201cPyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting,\u201d in ICLR, 2021.\n17. T. Zhou, P. Niu, L. Sun, R. Jin et al., \u201cOne fits all: Power general time series analysis by pretrained lm,\u201d in NeurIPS, 2023.\n18. Y. Liu, H. Zhang, C. Li, X. Huang, J. Wang, and M. Long, \u201cTimer: Transformers for time series analysis at scale,\u201d arXiv preprint arXiv:2402.02368, 2024.\n19. S. Papoulis, Probability, Random Variables and Stochastic Processes by Athanasios. Boston: McGraw-Hill, 2002.\n20. W. Xue, T. Zhou, Q. Wen, J. Gao, B. Ding, and R. Jin, \u201cCard: Channel aligned robust blend transformer for time series forecasting,\u201d in ICLR, 2024.\n21. P. Chen, Y. Zhang, Y. Cheng, Y. Shu, Y. Wang, Q. Wen, B. Yang, and C. Guo, \u201cMulti-scale transformers with adaptive pathways for time series forecasting,\u201d in ICLR, 2023.\n22. Y. Wang, H. Wu, J. Dong, Y. Liu, Y. Qiu, H. Zhang, J. Wang, and M. Long, \u201cTimexer: Empowering transformers for time series forecasting with exogenous variables,\u201d arXiv preprint arXiv:2402.19072, 2024.\n23. F. Li, J. Feng, H. Yan, G. Jin, F. Yang, F. Sun, D. Jin, and Y. Li, \u201cDynamic graph convolutional recurrent network for traffic prediction: Benchmark and solution,\u201d ACM Transactions on Knowledge Discovery from Data, vol. 17, no. 1, pp. 1\u201321, 2023.\n24. J. Wang, J. Jiang, W. Jiang, C. Li, and W. X. Zhao, \u201cLibcity: An open library for traffic prediction,\u201d in SIGSPATIAL, 2021.\n25. R. Jiang, D. Yin, Z. Wang, Y. Wang, J. Deng, H. Liu, Z. Cai, J. Deng, X. Song, and R. Shibasaki, \u201cDl-traff: Survey and benchmark of deep learning models for urban traffic prediction,\u201d in CIKM, 2021.\n26. Y. Hao, X. Qin, Y. Chen, Y. Li, X. Sun, Y. Tao, X. Zhang, and X. Du, \u201cTs-benchmark: A benchmark for time series databases,\u201d in ICDE, 2021.\n27. A. J. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. G. Bostrom, P. Southam, and E. J. Keogh, \u201cThe uea multivariate time series classification archive, 2018,\u201d arXiv preprint arXiv:1811.00075, 2018.\n28. G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, \u201cA transformer-based framework for multivariate time series representation learning,\u201d KDD, 2021.\n29. S. Makridakis, E. Spiliotis, and V. Assimakopoulos, \u201cThe m4 competition: 100,000 time series and 61 forecasting methods,\u201d International Journal of Forecasting, vol. 36, no. 1, pp. 54\u201374, 2020.\n30. Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, \u201cRobust anomaly detection for multivariate time series through stochastic recurrent neural network,\u201d in SIGKDD, 2019.\n31. K. Hundman, V. Constantinou, C. Laporte, I. Colwell, and T. Soderstrom, \u201cDetecting spacecraft anomalies using lstms and nonparametric dynamic thresholding,\u201d in SIGKDD, 2018.\n32.", "mimetype": "text/plain", "start_char_idx": 2304, "end_char_idx": 5268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55efde07-d021-4603-94f1-c0e90e1ac601": {"__data__": {"id_": "55efde07-d021-4603-94f1-c0e90e1ac601", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9", "node_type": "4", "metadata": {}, "hash": "f6cba8eb167bb42d86a6abce9f753afd601c26351028e82e748f2c449fb76e98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fd0aa71-d31f-4980-a99f-5ae19b146f79", "node_type": "1", "metadata": {}, "hash": "0d0c4ad1019bd6531002ed16b1ac5e3fb06fd08dee32faa3e41f7a4a36608729", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e136041a-e45e-46ca-a3a3-723f30f96627", "node_type": "1", "metadata": {}, "hash": "c7c5fe646430aba7106b58e301fb4b08519c655d5fdeaf3dcdabb9c7a15e8dfa", "class_name": "RelatedNodeInfo"}}, "text": "29. S. Makridakis, E. Spiliotis, and V. Assimakopoulos, \u201cThe m4 competition: 100,000 time series and 61 forecasting methods,\u201d International Journal of Forecasting, vol. 36, no. 1, pp. 54\u201374, 2020.\n30. Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, \u201cRobust anomaly detection for multivariate time series through stochastic recurrent neural network,\u201d in SIGKDD, 2019.\n31. K. Hundman, V. Constantinou, C. Laporte, I. Colwell, and T. Soderstrom, \u201cDetecting spacecraft anomalies using lstms and nonparametric dynamic thresholding,\u201d in SIGKDD, 2018.\n32. A. P. Mathur and N. O. Tippenhauer, \u201cSwat: A water treatment testbed for research and training on ics security,\u201d in CySWater, 2016.\n33. A. Abdulaal, Z. Liu, and T. Lancewicki, \u201cPractical approach to asynchronous multivariate time series anomaly detection and localization,\u201d KDD, 2021.\n34. X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, \u201cPre-trained models for natural language processing: A survey,\u201d Science China Technological Sciences, vol. 63, no. 10, pp. 1872\u20131897, 2020.\n35. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d Journal of machine learning research, vol. 21, no. 140, pp. 1\u201367, 2020.\n36. P. P. Ray, \u201cChatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope,\u201d Internet of Things and Cyber-Physical Systems, 2023.\n37. K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in CVPR, 2020.\n38. X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, \u201cSelf-supervised learning: Generative or contrastive,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 1, pp. 857\u2013876, 2021.\n39. Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, \u201cSimmim: A simple framework for masked image modeling,\u201d in CVPR, 2022.\n40. K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in CVPR, 2022.\n41. A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., \u201cImproving language understanding by generative pre-training,\u201d OpenAI, 2018.\n42. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., \u201cLanguage models are unsupervised multitask learners,\u201d OpenAI, vol. 1, no. 8, p. 9, 2019.\n43. A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, \u201cA survey on contrastive self-supervised learning,\u201d Technologies, vol. 9, no. 1, p. 2, 2020.\n44. K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in CVPR, 2022.\n45. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in ICML, 2020.\n46.", "mimetype": "text/plain", "start_char_idx": 4716, "end_char_idx": 7567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e136041a-e45e-46ca-a3a3-723f30f96627": {"__data__": {"id_": "e136041a-e45e-46ca-a3a3-723f30f96627", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9", "node_type": "4", "metadata": {}, "hash": "f6cba8eb167bb42d86a6abce9f753afd601c26351028e82e748f2c449fb76e98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55efde07-d021-4603-94f1-c0e90e1ac601", "node_type": "1", "metadata": {}, "hash": "5b2eb29730c9f513a66ed06c185dfae5f67fc6fc2dcca0a77c6fcfd1596287ce", "class_name": "RelatedNodeInfo"}}, "text": "1, no. 8, p. 9, 2019.\n43. A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, \u201cA survey on contrastive self-supervised learning,\u201d Technologies, vol. 9, no. 1, p. 2, 2020.\n44. K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in CVPR, 2022.\n45. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in ICML, 2020.\n46. Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, \u201cUnsupervised feature learning via non-parametric instance discrimination,\u201d in CVPR, 2018.\n47. A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018.\n48. X. Yang, Z. Zhang, and R. Cui, \u201cTimeclr: A self-supervised contrastive learning framework for univariate time series representation,\u201d Knowledge-Based Systems, vol. 245, p. 108606, 2022.\n49. E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh, X. Li, and C. Guan, \u201cTime-series representation learning via temporal and contextual contrasting,\u201d in IJCAI, 2021.\n50. Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, and B. Xu, \u201cTs2vec: Towards universal representation of time series,\u201d in AAAI, 2022.\n51. Z. Wang, X. Xu, W. Zhang, G. Trajcevski, T. Zhong, and F. Zhou, \u201cLearning latent seasonal-trend representations for time series forecasting,\u201d in NeurIPS, 2022.\n52. D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d arXiv preprint arXiv:1312.6114, 2013.", "mimetype": "text/plain", "start_char_idx": 7107, "end_char_idx": 8611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5add34ab-22da-4e50-b896-20f0ed259512": {"__data__": {"id_": "5add34ab-22da-4e50-b896-20f0ed259512", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08137354-5b17-48fb-bb75-e73e14190f1f", "node_type": "4", "metadata": {}, "hash": "4183a167e005071191bde7f1090f20802665564657b00299a7c0d964b5ef3fe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5861a85c-68de-4188-bb64-dd7f556b39dc", "node_type": "1", "metadata": {}, "hash": "a0500c6c7067c0ab5014ceda8877b6de1c1671d00984e7f86c24b510f48ef1d6", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# References\n\n1. G. Woo, C. Liu, D. Sahoo, A. Kumar, and S. Hoi, \u201cCoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting,\u201d in ICLR, 2022.\n2. X. Zhang, Z. Zhao, T. Tsiligkaridis, and M. Zitnik, \u201cSelf-supervised contrastive pre-training for time series via time-frequency consistency,\u201d in NeurIPS, 2022.\n3. A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, \u201cData2vec: A general framework for self-supervised learning in speech, vision and language,\u201d in ICML, 2022.\n4. G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, \u201cA transformer-based framework for multivariate time series representation learning,\u201d in SIGKDD, 2021.\n5. J. Dong, H. Wu, H. Zhang, L. Zhang, J. Wang, and M. Long, \u201cSimmtm: A simple pre-training framework for masked time-series modeling,\u201d arXiv preprint arXiv:2302.00861, 2023.\n6. S. Zhao, M. Jin, Z. Hou, C. Yang, Z. Li, Q. Wen, and Y. Wang, \u201cHimtm: Hierarchical multi-scale masked time series modeling for long-term forecasting,\u201d arXiv preprint arXiv:2401.05012, 2024.\n7. J. Dong, H. Wu, Y. Wang, Y. Qiu, L. Zhang, J. Wang, and M. Long, \u201cTimesiam: A pre-training framework for siamese time-series modeling,\u201d arXiv preprint arXiv:2402.02475, 2024.\n8. R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., \u201cOn the opportunities and risks of foundation models,\u201d arXiv preprint arXiv:2108.07258, 2021.\n9. A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., \u201cSegment anything,\u201d arXiv preprint arXiv:2304.02643, 2023.\n10. A. Garza and M. Mergenthaler-Canseco, \u201cTimegpt-1,\u201d arXiv preprint arXiv:2310.03589, 2023.\n11. K. Rasul, A. Ashok, A. R. Williams, A. Khorasani, G. Adamopoulos, R. Bhagwatkar, M. Bilo\u0161, H. Ghonia, N. V. Hassen, A. Schneider et al., \u201cLag-llama: Towards foundation models for time series forecasting,\u201d arXiv preprint arXiv:2310.08278, 2023.\n12. G. Woo, C. Liu, A. Kumar, C. Xiong, S. Savarese, and D. Sahoo, \u201cUnified training of universal time series forecasting transformers,\u201d arXiv preprint arXiv:2402.02592, 2024.\n13. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023.\n14. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023.\n15. C. Chang, W.-C.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5861a85c-68de-4188-bb64-dd7f556b39dc": {"__data__": {"id_": "5861a85c-68de-4188-bb64-dd7f556b39dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08137354-5b17-48fb-bb75-e73e14190f1f", "node_type": "4", "metadata": {}, "hash": "4183a167e005071191bde7f1090f20802665564657b00299a7c0d964b5ef3fe6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5add34ab-22da-4e50-b896-20f0ed259512", "node_type": "1", "metadata": {}, "hash": "ce230988cd5a4e4631f05280b0486912926a5a9f6319cd6e50e61a1a7946858e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e852dab-f4d0-431b-ace2-8d58941405c7", "node_type": "1", "metadata": {}, "hash": "dafeb3d770d6bf6dedc32b155ec60920259d124b95e4b28a8dc5845f59ff9a16", "class_name": "RelatedNodeInfo"}}, "text": "13. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023.\n14. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023.\n15. C. Chang, W.-C. Peng, and T.-F. Chen, \u201cLlm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms,\u201d arXiv preprint arXiv:2308.08469, 2023.\n16. S. Q. Nate Gruver, Marc Finzi and A. G. Wilson, \u201cLarge Language Models Are Zero Shot Time Series Forecasters,\u201d in NeurIPS, 2023.\n17. A. F. Ansari, L. Stella, C. Turkmen, X. Zhang, P. Mercado, H. Shen, O. Shchur, S. S. Rangapuram, S. P. Arango, S. Kapoor et al., \u201cChronos: Learning the language of time series,\u201d arXiv preprint arXiv:2403.07815, 2024.\n18. T. Zhou, P. Niu, L. Sun, R. Jin et al., \u201cOne fits all: Power general time series analysis by pretrained lm,\u201d in NeurIPS, 2024.\n19. J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, \u201cFinetuned language models are zero-shot learners,\u201d in ICLR, 2021.\n20. A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, \u201cZero-shot text-to-image generation,\u201d in ICML, 2021.\n21. V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., \u201cMultitask prompted training enables zero-shot task generalization,\u201d arXiv preprint arXiv:2110.08207, 2021.\n22. M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, \u201cVisual prompt tuning,\u201d in European Conference on Computer Vision. Springer, 2022, pp. 709\u2013727.\n23. H. Xue and F. D. Salim, \u201cPrompt-based time series forecasting: A new task and dataset,\u201d arXiv preprint arXiv:2210.08964, 2022.\n24. X. Liu, J. Hu, Y. Li, S. Diao, Y. Liang, B. Hooi, and R. Zimmermann, \u201cUnitime: A language-empowered unified model for cross-domain time series forecasting,\u201d arXiv preprint arXiv:2310.09751, 2023.\n25. M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-Y. Chen, Y. Liang, Y.-F. Li, S. Pan et al., \u201cTime-llm: Time series forecasting by reprogramming large language models,\u201d arXiv preprint arXiv:2310.01728, 2023.\n26. Y. Liu, G. Qin, X. Huang, J. Wang, and M. Long, \u201cAutotimes: Autoregressive time series forecasters via large language models,\u201d arXiv preprint arXiv:2402.02370, 2024.\n\n# Author Biographies\n\nYuxuan Wang received the BE degree from Beihang University in 2022.", "mimetype": "text/plain", "start_char_idx": 2249, "end_char_idx": 4874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e852dab-f4d0-431b-ace2-8d58941405c7": {"__data__": {"id_": "1e852dab-f4d0-431b-ace2-8d58941405c7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08137354-5b17-48fb-bb75-e73e14190f1f", "node_type": "4", "metadata": {}, "hash": "4183a167e005071191bde7f1090f20802665564657b00299a7c0d964b5ef3fe6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5861a85c-68de-4188-bb64-dd7f556b39dc", "node_type": "1", "metadata": {}, "hash": "a0500c6c7067c0ab5014ceda8877b6de1c1671d00984e7f86c24b510f48ef1d6", "class_name": "RelatedNodeInfo"}}, "text": "25. M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-Y. Chen, Y. Liang, Y.-F. Li, S. Pan et al., \u201cTime-llm: Time series forecasting by reprogramming large language models,\u201d arXiv preprint arXiv:2310.01728, 2023.\n26. Y. Liu, G. Qin, X. Huang, J. Wang, and M. Long, \u201cAutotimes: Autoregressive time series forecasters via large language models,\u201d arXiv preprint arXiv:2402.02370, 2024.\n\n# Author Biographies\n\nYuxuan Wang received the BE degree from Beihang University in 2022. She is now working towards the PhD degree in computer software at Tsinghua University. Her research interests include machine learning and time series analysis.\n\nHaixu Wu received the BE degree in software engineering from Tsinghua University in 2020. He is working towards the PhD degree in computer software at Tsinghua University. His research interests include scientific machine learning and spatiotemporal learning.\n\nJiaxiang Dong received the ME degree in computer science and technology from Nankai University in 2018. He is currently working toward the PhD degree in computer software at Tsinghua University. His research interests include machine learning and time series pre-training.\n\nYong Liu received the BE degree in software engineering from Tsinghua University in 2021. He is working towards the PhD degree in computer software at Tsinghua University. His research interests include time series analysis and large time series models.\n\nMingsheng Long received the BE and PhD degrees from Tsinghua University in 2008 and 2014 respectively. He was a visiting researcher with UC Berkeley from 2014 to 2015. He is currently a tenured associate professor with the School of Software, Tsinghua University. He serves as an associate editor of IEEE Transactions on Pattern Analysis and Machine Intelligence and Artificial Intelligence Journal, and as area chairs of major machine learning conferences, including ICML, NeurIPS, and ICLR. His research is dedicated to machine learning theory, algorithms, and models, with special interests in transfer learning and domain adaptation, deep learning and foundation models, scientific learning, and world models.\n\nJianmin Wang received the BE degree from Peking University, China, in 1990, and the ME and PhD degrees in computer software from Tsinghua University, China, in 1992 and 1995, respectively. He is a full professor with the School of Software, Tsinghua University. His research interests include Big Data management systems and large-scale data analytics. He led to developing a product data and lifecycle management system, which has been deployed in hundreds of enterprises in China. He is leading the development of the Tsinghua DataWay Big Data platform in the National Engineering Lab for Big Data Software.", "mimetype": "text/plain", "start_char_idx": 4395, "end_char_idx": 7151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"4646fe6d-69f6-47fb-8a07-b9b5556dee31": {"node_ids": ["68d1e231-852f-4fb1-916d-29e135943c23"], "metadata": {}}, "b3b9cc59-e544-4b89-b675-0cc6d3f42333": {"node_ids": ["48afc03c-980b-4381-bb08-0fbc6b46ed7f", "546deb80-1deb-4342-a37f-906064d7ea29"], "metadata": {}}, "05ea54b4-c39d-40fa-9bfa-d21891ed0ab8": {"node_ids": ["56c50822-bc3b-489f-a3c7-946943edef72"], "metadata": {}}, "14fe124f-6d06-4dff-9d80-c251345029a6": {"node_ids": ["97742285-32ad-4e78-aafc-c282c030f702", "991b0249-1a37-48de-9e88-a7ab867edc35"], "metadata": {}}, "6de13856-3f3f-41be-b8d4-84d08d2bdcbc": {"node_ids": ["c477450a-0082-441e-ad8d-1b4b8c5c94b2", "7b63a422-05ff-4edd-9126-a0c2e762b0b0"], "metadata": {}}, "bfe147d0-16af-4210-924b-1d68aed1ffea": {"node_ids": ["4e7604e6-60b7-4e25-9a09-0267a3f5f39e"], "metadata": {}}, "0466ba57-a853-4959-a43a-7109aec4a3e5": {"node_ids": ["765aa66d-4a76-45a0-b0c7-a9b9e37b876a", "8af78ae3-90f4-433e-aeaf-8e4abba4a373"], "metadata": {}}, "e2e2bc4a-9b7f-4d48-b52c-427e1694f1ca": {"node_ids": ["b0ea1e9c-8187-4177-bb3b-880e470a478f"], "metadata": {}}, "47e45b25-ad29-4226-9f87-16c9994ea98e": {"node_ids": ["d5540e1c-a157-441e-bedc-3afa46cdc743"], "metadata": {}}, "01943cde-58e7-41e0-b2a1-a3973976a0a7": {"node_ids": ["d2120e11-9b01-48ff-884f-762b1d6990c9", "55f10f37-5624-45e3-ab29-a8162918fb53"], "metadata": {}}, "5326360c-a26d-47f3-9c7a-86d857923a50": {"node_ids": ["e5b60d91-1495-4159-ad71-9251142c0dc4", "a14168c7-d532-4c68-974e-5b38778e27e7"], "metadata": {}}, "1f2ceb51-bbbb-433c-9170-b96c39b237b1": {"node_ids": ["77659c59-d4d8-43d1-81a7-9407c6bab1b2", "d20039e4-1eea-4ba2-ac20-087acbc470c8"], "metadata": {}}, "2780921a-8944-4247-b1ab-d497c1a9a2c0": {"node_ids": ["6b6850fd-e78e-4435-aa52-6af74e627fc7", "55b0cac7-36a7-47d1-a546-2b07054d8b42"], "metadata": {}}, "01baafe9-b1d0-4f36-8420-8882f1c06f33": {"node_ids": ["9a61bbda-0c36-42ea-8b39-a14cd988b789", "feadce1d-bc0b-40a4-b9e8-f57659047c4c"], "metadata": {}}, "4c4ba3cc-80d3-4f87-8cd1-fcdadb8267f2": {"node_ids": ["3be1ee75-3fc3-4b18-9fd1-f00f2ba533d6", "a653da63-96ef-4247-a710-653a0de7c47c"], "metadata": {}}, "d304b9c7-7e0a-4974-b19c-b7233486d6c7": {"node_ids": ["d634a99c-e520-4ee2-837b-d6ba74ce0cee", "d1a4d301-1839-4edc-8c6d-e390cad5b3f2", "aa76c58b-da3f-4c78-858f-b2a7fadf22f6"], "metadata": {}}, "a6991055-05ca-46b1-bc0e-a8e474361bf8": {"node_ids": ["0853c091-805b-41fa-8b7c-a3017d3a4b38", "6204fdbd-b441-4326-968f-c06e1dbb35b3", "dbf6152d-9151-49cf-8a18-dad62084ef85", "066b6b3f-1112-4cc2-b1e7-9c39241b89e6"], "metadata": {}}, "355cab74-ec27-4cae-afc8-c66a4a137f48": {"node_ids": ["dd613fac-3db7-40d1-bf83-527554e903d2", "27c48c02-ace0-4898-bb5a-4881b9b6b0c0", "6eb17b7e-a0ee-426e-b52a-6fa080ab4e0f", "d8a14ee6-dc8d-4bd1-a826-c279f9a519fe"], "metadata": {}}, "d08f4211-2fb5-41d2-881d-4be0a1bd3ae9": {"node_ids": ["7feb97af-07a8-476f-864c-056d6e0de2ce", "8fd0aa71-d31f-4980-a99f-5ae19b146f79", "55efde07-d021-4603-94f1-c0e90e1ac601", "e136041a-e45e-46ca-a3a3-723f30f96627"], "metadata": {}}, "08137354-5b17-48fb-bb75-e73e14190f1f": {"node_ids": ["5add34ab-22da-4e50-b896-20f0ed259512", "5861a85c-68de-4188-bb64-dd7f556b39dc", "1e852dab-f4d0-431b-ace2-8d58941405c7"], "metadata": {}}}}