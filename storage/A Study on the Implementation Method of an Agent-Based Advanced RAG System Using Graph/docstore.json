{"docstore/metadata": {"a02d4e34-f474-4935-aaef-8cec4c57c879": {"doc_hash": "532330e410fc1189c39c3d54ba237e42dd9faf016adb6a08e73d5b98d3e212a5"}, "e0d94ca7-e03f-4c62-ae75-5b76a14ed315": {"doc_hash": "3737ba1df978209266acc85ad6746d4288097ea36fbd672d1a760d7901d636f4"}, "1391dafc-7b5a-4343-b003-834a1333ae8d": {"doc_hash": "89466f5339ae9b4acb1b9e14e2f936892d7d61457f6a88626eebbaac0108c5bf"}, "2067add4-2fae-4ea0-bbd8-047962d5c3a9": {"doc_hash": "4186529083896f69f21f412b81a1f0f758e33ebba181bdf9ebd5ed539428da93"}, "c2533e4f-d332-4e54-8073-f0feed11bd9b": {"doc_hash": "50058bf1dd75c22ef2a48d46afea9428bf41fd87476fd60b903d8b491852e235"}, "7b3c6b17-627b-43e2-bdc6-b441024b79c3": {"doc_hash": "b4e789f96c9c0f19538f560aae16f6686a06911972d35f5ef6fcc267c9f92ec2"}, "72c27642-e6d6-4e35-aa91-e32eca38a6f7": {"doc_hash": "74793f8523be9b98b6baa1a23e91fbacf3b45ebc8f880485364ac017790ee974"}, "4a147278-fdae-4c3c-a253-1a15fedba3c0": {"doc_hash": "69fd07780b754810fdf9893699696dc35f4d8c41a85f89c42090e0ea59db3cd0"}, "90cbdbe4-5c92-4dc8-97bb-3c84b0aab59a": {"doc_hash": "578c238d53dd489762de1af3fe999a723479983e438a65d14f8721af3709e143"}, "ce71a438-c5a8-454e-ab36-4ba6fda3ce6a": {"doc_hash": "34b4497d09d031e7d5b709b557ed1644ba7b400d226d27ce2e218d157ae779fc"}, "d1a07607-628d-4bbb-a385-cb65106c8ccc": {"doc_hash": "3fe07728bccc2c163a3527cd99d8eb8706109ae27e8f4c19856a8b10fe3df237"}, "73a20c35-6533-4dd2-ae91-045fadb60ea6": {"doc_hash": "4699976dee04e784d95a85d0b95673c77a9f201368b3dfe103b39fcc381d1614"}, "afd9d996-57c2-475a-aba6-7f93d48f6fae": {"doc_hash": "532330e410fc1189c39c3d54ba237e42dd9faf016adb6a08e73d5b98d3e212a5", "ref_doc_id": "a02d4e34-f474-4935-aaef-8cec4c57c879"}, "81d47669-5f06-44e9-aea9-2a43c1018a74": {"doc_hash": "3737ba1df978209266acc85ad6746d4288097ea36fbd672d1a760d7901d636f4", "ref_doc_id": "e0d94ca7-e03f-4c62-ae75-5b76a14ed315"}, "9df88571-d26b-45fe-b665-5c604a0add95": {"doc_hash": "89466f5339ae9b4acb1b9e14e2f936892d7d61457f6a88626eebbaac0108c5bf", "ref_doc_id": "1391dafc-7b5a-4343-b003-834a1333ae8d"}, "29ecf296-4b93-4948-95f7-68958fe891cd": {"doc_hash": "4186529083896f69f21f412b81a1f0f758e33ebba181bdf9ebd5ed539428da93", "ref_doc_id": "2067add4-2fae-4ea0-bbd8-047962d5c3a9"}, "dbe3e741-aec8-4e7e-bee9-fb9d9abe486f": {"doc_hash": "50058bf1dd75c22ef2a48d46afea9428bf41fd87476fd60b903d8b491852e235", "ref_doc_id": "c2533e4f-d332-4e54-8073-f0feed11bd9b"}, "29cf6dd1-4b27-4be7-a8b0-56803bc449a6": {"doc_hash": "b4e789f96c9c0f19538f560aae16f6686a06911972d35f5ef6fcc267c9f92ec2", "ref_doc_id": "7b3c6b17-627b-43e2-bdc6-b441024b79c3"}, "71f70f1e-fb13-4d41-85e2-b0d3c810bae8": {"doc_hash": "74793f8523be9b98b6baa1a23e91fbacf3b45ebc8f880485364ac017790ee974", "ref_doc_id": "72c27642-e6d6-4e35-aa91-e32eca38a6f7"}, "764d7a9d-ca64-4b70-8090-a622e52f5ace": {"doc_hash": "69fd07780b754810fdf9893699696dc35f4d8c41a85f89c42090e0ea59db3cd0", "ref_doc_id": "4a147278-fdae-4c3c-a253-1a15fedba3c0"}, "b46e6977-1e81-4b83-a028-70f1c0d455c8": {"doc_hash": "578c238d53dd489762de1af3fe999a723479983e438a65d14f8721af3709e143", "ref_doc_id": "90cbdbe4-5c92-4dc8-97bb-3c84b0aab59a"}, "e8607000-92d5-43a1-a938-f66a688ca028": {"doc_hash": "34b4497d09d031e7d5b709b557ed1644ba7b400d226d27ce2e218d157ae779fc", "ref_doc_id": "ce71a438-c5a8-454e-ab36-4ba6fda3ce6a"}, "bd6c1d55-c7b0-402f-a802-2fb54b66a04c": {"doc_hash": "a610145554cc82b0a53d174ad917aba240efa8c4de766e49ced6beae306d01ad", "ref_doc_id": "d1a07607-628d-4bbb-a385-cb65106c8ccc"}, "6fb40d58-aaaa-4c87-94bc-5ca1e159194d": {"doc_hash": "90ae1ee4fa0b2d07a998ae4c6944ba03210ecfc8922f6aba1f289d8230fbc448", "ref_doc_id": "d1a07607-628d-4bbb-a385-cb65106c8ccc"}, "295c0285-31a0-4941-ac27-22e4c96050bc": {"doc_hash": "4699976dee04e784d95a85d0b95673c77a9f201368b3dfe103b39fcc381d1614", "ref_doc_id": "73a20c35-6533-4dd2-ae91-045fadb60ea6"}}, "docstore/data": {"afd9d996-57c2-475a-aba6-7f93d48f6fae": {"__data__": {"id_": "afd9d996-57c2-475a-aba6-7f93d48f6fae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a02d4e34-f474-4935-aaef-8cec4c57c879", "node_type": "4", "metadata": {}, "hash": "532330e410fc1189c39c3d54ba237e42dd9faf016adb6a08e73d5b98d3e212a5", "class_name": "RelatedNodeInfo"}}, "text": "# A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph\n\nCheonsu Jeong1\n\n1 Dr. Jeong is Principal Consultant & the Technical Leader for AI Automation at SAMSUNG SDS;\n\n# Abstract\n\nThis study aims to improve knowledge-based question-answering (QA) systems by overcoming the limitations of existing Retrieval-Augmented Generation (RAG) models and implementing an advanced RAG system based on Graph technology to develop high-quality generative AI services. While existing RAG models demonstrate high accuracy and fluency by utilizing retrieved information, they may suffer from accuracy degradation as they generate responses using pre-loaded knowledge without reprocessing. Additionally, they cannot incorporate real-time data after the RAG configuration stage, leading to issues with contextual understanding and biased information. To address these limitations, this study implemented an enhanced RAG system utilizing Graph technology. This system is designed to efficiently search and utilize information. Specifically, it employs LangGraph to evaluate the reliability of retrieved information and synthesizes diverse data to generate more accurate and enhanced responses. Furthermore, the study provides a detailed explanation of the system's operation, key implementation steps, and examples through implementation code and validation results, thereby enhancing the understanding of advanced RAG technology. This approach offers practical guidelines for implementing advanced RAG systems in corporate services, making it a valuable resource for practical application.\n\n# Keywords\n\nAdvance RAG; Agent RAG; LLM; Generative AI; LangGraph\n\n# I. Introduction\n\nRecent advancements in AI technology have brought significant attention to Generative AI. Generative AI, a form of artificial intelligence that can create new content such as text, images, audio, and video based on vast amounts of trained data models (Jeong, 2023d), is being applied in various fields, including daily conversations, finance, healthcare, education, and entertainment (Ahn & Park, 2023). As generative AI services become more accessible to the general public, the role of generative AI-based chatbots is becoming increasingly important (Adam et al., 2021; Przegalinska et al., 2019; Park, 2024). A chatbot is an intelligent agent that allows users to have conversations typically through text or voice (S\u00e1nchez-D\u00edaz et al., 2018; Jeong & Jeong, 2020). Recently, generative AI chatbots have advanced to the level of analyzing human emotions and intentions to provide responses (Jeong, 2023a). With the advent of large language models (LLMs), these chatbots can now be utilized for automatic dialogue generation and translation (Jeong, 2023b). However, they may generate responses\n\n* Corresponding Author: Cheonsu Jeong; paripal@korea.ac.kr", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81d47669-5f06-44e9-aea9-2a43c1018a74": {"__data__": {"id_": "81d47669-5f06-44e9-aea9-2a43c1018a74", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0d94ca7-e03f-4c62-ae75-5b76a14ed315", "node_type": "4", "metadata": {}, "hash": "3737ba1df978209266acc85ad6746d4288097ea36fbd672d1a760d7901d636f4", "class_name": "RelatedNodeInfo"}}, "text": "# Current Trends in Generative AI and RAG Models\n\nthat conflict with the latest information and have a low understanding of new problems or domains as they rely on previously trained data (Jeong, 2023c). While 2023 was marked by the release of foundational large language models (LLMs) like ChatGPT and Llama-2, experts predict that 2024 will be the year of Retrieval Augmented Generation (RAG) and AI Agents (Skelter Labs, 2024). However, there are several considerations for companies looking to adopt generative AI services. Companies must address concerns such as whether the AI can provide accurate responses based on internal data, the potential risk of internal data leakage, and how to integrate generative AI with corporate systems. Solutions include using domain-specific fine-tuned LLMs and enhancing reliability with RAG that utilizes internal information (Jung, 2024). When domain-specific information is fine-tuned on GPT-4 LLM, accuracy improves from 75% to 81%, and adding RAG can further increase accuracy to 86% (Angels et al., 2024). RAG models are known for effectively combining internal knowledge retrieval and generation to produce more accurate responses. They offer the advantages of source-based fact provision and addressing data freshness issues through the integration of internal and external knowledge bases. However, the effectiveness of RAG models heavily depends on the quality of the database, directly impacting model performance (Kim, 2024). Traditional RAG models load knowledge once and generate responses without reprocessing, leading to accuracy degradation and an inability to reflect real-time data after the RAG configuration. This process can result in inaccurate responses, particularly when generating answers to complex questions, as the initial vectorized knowledge is used without updating with new information. Furthermore, traditional RAG models struggle to handle various types of questions and may suffer from unrelated documents being used in response due to poor retrieval strategies, along with the hallucination issues observed in LLMs.\n\nThe purpose of this study is to improve the traditional RAG model-based knowledge-based QA system (Jeon et al., 2024) and overcome its limitations by accessing real-time data and verifying whether the retrieved documents are genuinely relevant to the questions. By implementing an enhanced RAG system capable of addressing questions about recent events and real-time data, and being less susceptible to hallucinations, this study aims to improve the quality and performance of generative AI services.\n\nThe introduction of this paper explains the research background and objectives, the limitations of existing RAG models, the importance and contributions of the study, and the structure of the paper. The theoretical background reviews the overview of RAG models, advanced RAG approaches, and case studies of existing research improvements. The design of the advanced RAG model covers the composition flow of advanced RAG, the configuration of Agent RAG, and other enhanced features. The implementation of the advanced RAG system details the overview and application of LangGraph, the system implementation process, and the results. The testing section presents the improved results of the implemented code. Finally, the conclusion summarizes the research findings, discusses the limitations, and outlines directions for future research.\n\n# II. Related Work\n\nFor this study, recent key research papers, journals, and articles related to the RAG model were reviewed. This section provides an overview of the RAG model and describes the advancements leading to the development of the Advanced RAG.\n\n# 2.1. Overview of the RAG Model\n\nThe RAG (Retrieval-Augmented Generation) model combines retrieval and generation to produce answers by integrating document retrieval and generation models (Lewis et al., 2020). To generate an answer to a question, the model first retrieves relevant documents and then uses them to produce the response. This process helps in generating accurate answers to questions. The RAG model can handle various types of questions effectively, even when there is a lack of specific domain knowledge. Consequently, it enhances the accuracy and consistency of information compared to traditional generative models.\n\nThe RAG model consists of two main stages:\n\n- Retrieval Stage: Information relevant to the given question is retrieved through a search engine.\n- Generation Stage: Answers are generated based on the retrieved information.\n\n# 2.1.1. RAG Model Implementation Flow\n\nThe RAG model performs text generation tasks by retrieving information from a given source data and using that information to generate the desired text. The data processing for using RAG involves dividing the original data into smaller chunks, embedding the text data by converting it into numerical vectors, and storing these vectors in a vector store (Microsoft, 2023). The implementation flow of a generative AI service based on the RAG model is depicted in Figure 1 (Jeong, 2023e).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9df88571-d26b-45fe-b665-5c604a0add95": {"__data__": {"id_": "9df88571-d26b-45fe-b665-5c604a0add95", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1391dafc-7b5a-4343-b003-834a1333ae8d", "node_type": "4", "metadata": {}, "hash": "89466f5339ae9b4acb1b9e14e2f936892d7d61457f6a88626eebbaac0108c5bf", "class_name": "RelatedNodeInfo"}}, "text": "# Gen = plllorm\n\n# PDF; Website, Vcclor LLM\n\n# Youtube Store\n\n# Source DB\n\n# Gathenno Euaclon sputhltocnuniEmbuddina: VeclatStotuPrexnnirShalci\n\n# Figure 1: Implementation Flow of a RAG-based Generative AI Service\n\n# 2.1.2. RAG-Based Vector Store Types\n\nTo establish a RAG (Retrieval-Augmented Generation) system, a vector database is utilized to store knowledge. A typical vector pipeline for vector databases involves three stages: Indexing, Querying, and Post Processing (Devtorium, 2023). Specifically, RAG-based vector store configurations can be categorized into two types as illustrated in Figure 2: one where all source data is pre-stored in the vector store and another where data is dynamically inserted at query time.\n\n# Figure 2: Configuration Types and Processing Procedures of RAG-Based Vector Stores\n\nWhen a company offers internal knowledge through an Open LLM (Large Language Model), ensuring security is a critical issue, making the use of Local LLMs essential. In this scenario, it is effective to employ multiple Local LLMs, each optimized for different tasks. For instance, one LLM might be specialized in generating database queries to retrieve relevant data from multiple source databases, while another LLM could be developed to provide answers based on specific domain knowledge (Jeong, 2023e).\n\n# 2.2. Prior Research on Advanced RAG\n\n# 2.2.1. Methods to Enhance RAG Performance\n\nThe performance of RAG (Retrieval-Augmented Generation) is influenced by the quality of the data that can be composed into prompts based on the results of question processing from external repositories. Recently, various Advanced RAG (Advanced Retrieval-Augmented Generation) methods have been proposed to address the limitations of conventional RAG. Advanced RAG represents an evolved form of the traditional RAG technique, incorporating various optimization methods to overcome its limitations. Recent research by Yunfan G. et al. introduces an optimization strategy that divides the retrieval process into Pre-Retrieval, Retrieval, and Post-Retrieval stages, significantly enhancing information accuracy and processing efficiency through optimization at each stage (Yunfan G. et al., 2024). Additionally, various improvement methods, such as re-ranking based on relevance to enhance accuracy, have been proposed as strategies for improving the quality of RAG systems (Jang Dong-jin, 2024), as outlined in Table 1 (Matt A., 2023). Frameworks like LangChain or LlamaIndex provide libraries for implementing these strategies, making the implementation process more straightforward.\n\n# Table 1: Methods to Enhance RAG Performance\n\n|Method|Descriptions|\n|---|---|\n|Clean your data|When dealing with conflicting or redundant information, it becomes challenging to find the correct context during retrieval. To ensure accurate responses to queries, it is essential to properly structure the documents themselves. One approach is to create summaries for all documents and use these summaries as context.|\n|Explore different index types|While embedding-based similarity search methods generally perform well, they are not always the best solution. For example, in e-commerce, keyword-based search methods may be more suitable for finding specific items such as products. Many systems employ hybrid approaches, where keyword-based searches are used for specific products, and embedding-based searches are applied for general customer information and support.|\n|Experiment with your chunking approach|Chunk size is critically important, with smaller chunks typically yielding better performance; however, they may also lead to issues related to insufficient surrounding context. Generally, smaller chunk sizes aid search systems in identifying relevant contextual information more effectively.|\n|Play around with your base prompt|To reduce hallucinations, prompting should be designed to ensure that responses are based solely on the given contextual information. For example: \"You are a customer support representative, designed to provide assistance based on factual information only. Please answer queries based on the given context information, not|\n\nCheonsu Jeong", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29ecf296-4b93-4948-95f7-68958fe891cd": {"__data__": {"id_": "29ecf296-4b93-4948-95f7-68958fe891cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2067add4-2fae-4ea0-bbd8-047962d5c3a9", "node_type": "4", "metadata": {}, "hash": "4186529083896f69f21f412b81a1f0f758e33ebba181bdf9ebd5ed539428da93", "class_name": "RelatedNodeInfo"}}, "text": "# III. Design of Advanced RAG\n\n# Models\n\nIn this chapter, various Advanced RAG approaches proposed in previous research are reviewed, and an enhanced RAG system is designed based on these findings. Specifically, we closely analyze methods such as Self-RAG, Corrective RAG, and Adaptive RAG, and present an implementation model as shown in Figure 3 based on the improvements derived from these analyses. The implementation of the Agent RAG system primarily builds on Corrective RAG, while referencing Self-RAG and Adaptive RAG. The workflow to enhance a typical RAG system involves retrieving document chunks from a vector database and then using an LLM to verify the relevance of each retrieved document chunk to the input query. If all retrieved document chunks are relevant, the system can generate responses to find relevant information and using it to refine the answers. This approach can enhance the accuracy and fluency of the responses (Asai A. et al., 2023).\n\n# Notable advanced RAG approaches currently being researched include the following:\n\n- Self-RAG: This method involves re-searching generated responses to find relevant information and using it to refine the answers.\n- Corrective RAG: This approach employs a Corrective Agent to rectify errors in generated responses. The Corrective Agent identifies errors in the responses and retrieves information to correct them, thereby improving the reliability of the answers (Yan, S.Q. et al., 2024).\n- Adaptive RAG: This method involves selecting the appropriate RAG approach based on the type of question. For instance, Self-RAG may be used for factual questions, while Corrective RAG could be employed for opinion-based questions. By choosing the appropriate method according to the question type, the accuracy of the responses can be improved (Jeong, S. et al., 2024).\n\n# Consider query transformations\n\nWhen the context or domain does not align, fine-tuning the embedding model can enhance performance, particularly for domain-specific terminology. For example, this can involve adapting the model to better handle specialized vocabulary pertinent to a specific domain.\n\n# Start using LLM dev. tools\n\nWhen building a RAG system using LlamaIndex or LangChain, debugging tools can be utilized to identify the sources of documents and context.\n\n# 2.2.2. Research on Advanced RAG Types\n\nEdge\nNd\nNot Uzefub\nMxk\nEdge\nUseful\nNot Support\n\nFigure 3: Agent-based advanced RAG workflow\n\nCheonsu Jeong", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbe3e741-aec8-4e7e-bee9-fb9d9abe486f": {"__data__": {"id_": "dbe3e741-aec8-4e7e-bee9-fb9d9abe486f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2533e4f-d332-4e54-8073-f0feed11bd9b", "node_type": "4", "metadata": {}, "hash": "50058bf1dd75c22ef2a48d46afea9428bf41fd87476fd60b903d8b491852e235", "class_name": "RelatedNodeInfo"}}, "text": "# 3.1. Design of Advanced RAG Execution Procedures\n\nThe Advanced RAG model maintains the basic flow of a traditional RAG model while incorporating additional processes after the search stage to enhance the accuracy and consistency of responses. The composition and flow of the Advanced RAG model are designed as follows:\n\n1. Input the question.\n2. Search for information related to the question using a search engine.\n3. Extract relevant information related to the question from the search results using an information extractor.\n4. Generate an answer based on the extracted information.\n5. The Agent refines the answer to enhance its accuracy and fluency.\n6. Output the final answer.\n\n# 3.2. Application of Agent RAG\n\nTo apply the enhanced execution procedures, the Agent RAG framework incorporates the concept of an \"Agent\" into the answer generation process, thereby further improving the accuracy and consistency of responses. The Agent serves as a core element in the answer generation process, fulfilling the following roles:\n\n- Answer Evaluation: Assessing the accuracy, fluency, and reliability of the generated responses.\n- Answer Improvement: Enhancing the responses based on the evaluation results.\n\nFigure 3. Agent-based advanced RAG workflow\n\nThe operation of the Advanced RAG model is as follows:\n\n1. Query Processing: Input the user's query, and analyze and understand the precise meaning of the query through intent recognition and analysis.\n2. Search: Utilize search engines to explore and retrieve information from various sources related to the query.\n3. Candidate Selection: Select information from the search results that is highly relevant and reliable to the query as candidates.\n4. Candidate Ranking: Rank the selected candidates based on their relevance to the query, the reliability of the information, and diversity.\n5. Answer Generation: Use a text generation model to create an answer based on the ranked candidate information.\n6. Answer Updating: Continuously collect new information and update the answer to provide the most current information.\n\n# 3.3. Application of the LangGraph Module\n\nLangGraph is a module released by LangChain designed to build stateful multi-actor applications using LLMs. It is utilized to create Agent and multi-Agent workflows, allowing for the definition of flows that include essential cycles for most Agent architectures, and providing detailed control over the application's flow and state, which is critical for creating reliable Agents (LangGraph, 2024).\n\nBuilt on top of LangChain, LangGraph facilitates the development of AI agents driven by LLMs by creating essential cyclic graphs. LangGraph treats Agent workflows as cyclic graph structures.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29cf6dd1-4b27-4be7-a8b0-56803bc449a6": {"__data__": {"id_": "29cf6dd1-4b27-4be7-a8b0-56803bc449a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b3c6b17-627b-43e2-bdc6-b441024b79c3", "node_type": "4", "metadata": {}, "hash": "b4e789f96c9c0f19538f560aae16f6686a06911972d35f5ef6fcc267c9f92ec2", "class_name": "RelatedNodeInfo"}}, "text": "# Implementation Results of the Advanced RAG System\n\nThis chapter presents the implementation of the RAG model and LangChain framework based on the Advanced RAG concepts introduced in Chapter 3, utilizing data suitable for internal corporate use. The implementation approach and considerations for using LangGraph, which is well-suited for Agent implementation, demonstrated through practical examples.\n\n# 4.1. Development Environment\n\nThe solutions and development platforms applied in this case are based on the framework outlined in Figure 1, and the implementation method utilizing LangGraph and OpenAI LLM is described. The process involves chunking and embedding documents, storing them in ChromaDB, and then transforming them into retrievers for document content search. The results are evaluated, and an Agent RAG Graph is defined and implemented accordingly. The development was carried out using Python, which provides a range of libraries necessary for AI development.\n\nThe development environment for each implementation component is as follows:\n\n- Orchestration Framework: LangChain\n- Agent Graph Workflow: LangGraph\n- Workflow Trace: LangSmith\n- Data Extraction and Chunking: LangChain Modules\n- Embedding: OpenAI\n- Vector Database: Chroma\n- LLM: OpenAI GPT-4-turbo Model\n- Python Development Environment: Google Colab\n\n# 4.2. Results of the step-by-step implementation\n\n# 4.2.1. Installation of Basic Libraries and API Key Setup\n\nBasic libraries, including LangChain for overall orchestration of tasks such as data splitting, OpenAI for API access, ChromaDB for storing RAG knowledge, and a web search library, were installed. To facilitate easy management of knowledge files, Google Drive was integrated with Colab. To ensure security, the keys for various modules were registered in a .env file at a specific location. Figure 4 shows that the .env file containing the keys was successfully read.\n\npip install langchain openai\npip install chromadb pypdf tiktoken\npip install python-dotenv\n# Google Drive Connect\nfrom google.colab import drive\ndrive.mount('/content/drive')\nload_dotenv()  # Load environment variables\n\n# Figure 4: Installation of Basic Libraries and OpenAI API Key Configuration\n\n# 4.2.2. Retriever Implementation\n\nFor managing internal documents, such as 'Dress Code Standards.pdf', the PyPDFLoader is used to load the document from the specified location. Given the document's characteristics, which include tables, the TextSplitter is adjusted. Instead of using the CharacterTextSplitter with a single delimiter (e.g.,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71f70f1e-fb13-4d41-85e2-b0d3c810bae8": {"__data__": {"id_": "71f70f1e-fb13-4d41-85e2-b0d3c810bae8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72c27642-e6d6-4e35-aa91-e32eca38a6f7", "node_type": "4", "metadata": {}, "hash": "74793f8523be9b98b6baa1a23e91fbacf3b45ebc8f880485364ac017790ee974", "class_name": "RelatedNodeInfo"}}, "text": "# 4.2.3. Evaluation of Search Results\n\nTo assess whether the retrieved documents are relevant to the given question, the implementation is as shown in Figure 6.\n\n# Retrieval Process\n\n|Component|Description|\n|---|---|\n|ChatOpenAI|Model for generating responses|\n|Machine Learning|Used for analyzing data|\n|Database|Stores relevant documents|\n|Field|Contains user queries|\n|Output|Results from the retrieval process|\n\nThe retrieved documents are linked to user questions. If the document contains keywords related to the user\u2019s query, it is assessed for relevance with the aim of filtering out irrelevant searches. As depicted in Figure 7, relevance is indicated by assigning a binary score of 'yes' or 'no' (e.g., \u201cGRADE: binary_score='yes\u2019\u201d).\n\n# Relevance Determination\n\n|Query|Considerations|Threshold|Retriever|Document|\n|---|---|---|---|---|\n|Question|Content|Grading|ChatOpenAI|Relevant documents|\n\nSubsequently, a question-answer RAG chain is constructed, similar to traditional RAG systems, to integrate with the AI agent as shown in Figure 8.\n\n# Building the RAG Chain\n\n|Component|Description|\n|---|---|\n|Core Prompt|Generates answers|\n|Output Parser|Formats the output|\n|Context|Provides relevant information|\n\nUsing the constructed question-answer RAG chain, the relevant answer results are obtained as illustrated in Figure 9.\n\n# Answer Processing\n\n|Consideration|Action|\n|---|---|\n|Dress Code|Ensure compliance with business attire|\n\nHowever, as shown in Figure 10, when an out-of-context question is attempted, the response \"RAG does not have relevant information\" is received, indicating that the question cannot be answered.\n\nCheonsu Jeong", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "764d7a9d-ca64-4b70-8090-a622e52f5ace": {"__data__": {"id_": "764d7a9d-ca64-4b70-8090-a622e52f5ace", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a147278-fdae-4c3c-a253-1a15fedba3c0", "node_type": "4", "metadata": {}, "hash": "69fd07780b754810fdf9893699696dc35f4d8c41a85f89c42090e0ea59db3cd0", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 10: Answer Processing for Unrelated Questions\n\nTo improve this, the question can be rephrased into a more optimized version for web search, as shown in Figure 11. Rewriting the question (question rewriting) helps obtain better contextual information from the web. For example, an incomplete question like \"Tell me what is the capital of the country where BTS is located\" can be improved to \"What is the capital of South Korea, the country where BTS is from?\"\n\n# Figure 11: Question Rewriting and Improved Question\n\nAdditionally, the generated answers are evaluated to determine if they contain hallucinations, as depicted in Figure 12.\n\n# Figure 12: Evaluation of Hallucinations\n\nSubsequently, the answers are assessed for their utility in solving the question, as shown in Figure 13. A binary score of 'yes' or 'no' is assigned to indicate whether the answer is useful for solving the question.\n\n# Figure 13: Evaluation of Answer Relevance\n\n# 4.2.4. Definition of the Agent RAG Graph\n\nTo enhance answer retrieval, the Tavily API is used for web searches, and the connection to this API is established. The Graph State of the Agent is defined, where the state object is passed to each node in the graph. Nodes such as Retrieve, generate_answer, grade_documents, and web_search_add are defined as shown in Figure 14.\n\n# Figure 14: Definition of Web Search Tool and Agent Graph State\n\n# Figure 15: Example of Retrieve Node Graph Implementation\n\nThe Agent RAG Graph can be composed of nodes such as Retrieve, grade_documents, rewrite_query, web_search_add, and generate_answer, as illustrated in Figure 14. The State, consisting of a set of messages, is used to store and represent the state of the agent graph as it passes through various nodes. Figure 15 shows the implementation example of the Retrieve Node Graph, which is used to fetch relevant contextual documents from the vector database. It also defines the node classes for document evaluation (grade_documents), question.\n\nCheonsu Jeong", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b46e6977-1e81-4b83-a028-70f1c0d455c8": {"__data__": {"id_": "b46e6977-1e81-4b83-a028-70f1c0d455c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90cbdbe4-5c92-4dc8-97bb-3c84b0aab59a", "node_type": "4", "metadata": {}, "hash": "578c238d53dd489762de1af3fe999a723479983e438a65d14f8721af3709e143", "class_name": "RelatedNodeInfo"}}, "text": "# 4.2.5. Implementation of the Agent RAG Graph\n\nIn the implementation phase of the Agent RAG Graph, LangGraph is used to build the Agent into a graph by utilizing the functions developed in the previous section. This involves placing the Agent into relevant nodes and connecting them with defined edges according to the specified workflow. The Agent performs an action that calls the Retrieve function and then adds output information to the state before invoking the Agent. As shown in Figure 16, the StateGraph class is used to define and manage the state-based graph. The provided code sets up the workflow to define the process for retrieving documents or performing other tasks based on the Agent's decisions.\n\n# Figure 17: Graph of the Answer Generation Process\n\n|Define the nodes| | | | | | | | |\n|---|---|---|---|---|---|---|---|---|\n|Forkflon_agent_rag add_node|retrieve|retrieve|retrieve| | | | | |\n|agent_rag add_node|Grans|docunents|grade_docunents|transform|Qupfigrade|docunents| | |\n|Forkflo_agent_rag add_node|agent|rag|add_node|rewrite_query|search|rerrisearch|Qupryneb|search|\n|Forkflo_agent_rag add_node|generate_answer|generate_answer|generate anshe| | | | | |\n|Bui/d graph| | | | | | | | |\n|Forkflo_agent_rag, set_entry_point|(\"retrieve\")| | | | | | | |\n|Forkflon_agent_rag, add_edge|retrieve|Ocacp|Jocinents| | | | | |\n|agent|[ad|add_conditiona|edgese|grade_docunents|decide|Generalp| | |\n|rpnite|Query|renrite_quety|dpnerat e|ansher|Generale ansher| | | |\n|agent_rag.add_edge|renrite_query|veb_search| | | | | | |\n|agent_rag, add_edge|Sparcn|generate_ansmer| | | | | | |\n|Forkilou aqent|add_conditiona|edgesi|denerate|ansher| | | | |\n|07402|generat ion|docunents_and nmest|cundorted|Oenerate|ansar| | | |\n|useful|END;| | | | | | | |\n|not useful|neb_search| | | | | | | |\n\n# 4.3. Test Results\n\nThe implemented Agent RAG workflow was tested with various questions to improve the accuracy of the answers, and the process of streaming responses to questions can be confirmed through the stream method.\n\n# 4.3.1. Verification of Questions in RAG Knowledge Information\n\nWhen the question \"Tell me the things to consider when choosing a work uniform\" was input for the document \u2018Dress Code Standards\u2019 Figure 18, which is RAG information, the question was rewritten to \"What are the main factors to consider when choosing a work uniform?\" to improve the accuracy of the answer. This resulted in a more accurate response. This process can be confirmed through the streaming of responses to questions using the stream method as shown in Figure 19.\n\nCheonsu Jeong", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8607000-92d5-43a1-a938-f66a688ca028": {"__data__": {"id_": "e8607000-92d5-43a1-a938-f66a688ca028", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce71a438-c5a8-454e-ab36-4ba6fda3ce6a", "node_type": "4", "metadata": {}, "hash": "34b4497d09d031e7d5b709b557ed1644ba7b400d226d27ce2e218d157ae779fc", "class_name": "RelatedNodeInfo"}}, "text": "# Dress Code Guidelines for Work\n\n# Purpose\n\nIndividuality of each employee including professionalism and decorum.\n\n# Considerations for Dress Selection\n\n# Dress Standards\n\nMaintain neat and simple business casual attire, with the option of jeans.\n\nShorts are permissible only from June to September; they should be lightweight, above the knee length (revealing the upper thigh while standing).\n\nAcceptable clothing includes:\n\n- Jacket\n- Casual shirts\n- Clothing that is politically neutral\n- No ripped pants\n- Neat cotton pants\n- Formal shoes\n\n# Attire for Female Employees\n\nBusiness clothing should allow you to comfortably combine tops and bottoms (blouses, etc.).\n\n- Collarless blouses\n- Suit pants\n- Primary color cotton pants\n- No ripped jeans, jean skirts, etc.\n\n# Figures\n\nFigure 18: Dress Code Standards.pdf\n\n# Queries\n\nFor inquiries, please refer to the following:\n\n- Question: \"Tell me what is the capital of the country where BTS is located?\"\n- Rewritten Question: \"What is the capital of South Korea, the country where BTS is from?\"\n- Answer: \"The capital of South Korea is Seoul.\"\n\n# Verification of Questions Not in RAG Knowledge Information\n\nWhen the question \"Tell me what is the capital of the country where BTS is located\" was input, which is not in the RAG knowledge information, it was determined to perform a web search because all Vector RAG documents are irrelevant to the question.\n\n# Conclusion and Discussion\n\nThis study has reviewed various methods to enhance the accuracy of RAG and explored the theoretical background of Advanced RAG models aimed at improving knowledge-based QA systems. Through the implementation of a graph-based Agent RAG system, along with specific implementation code and validation results, this research has demonstrated the feasibility of...\n\nCheonsu Jeong", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd6c1d55-c7b0-402f-a802-2fb54b66a04c": {"__data__": {"id_": "bd6c1d55-c7b0-402f-a802-2fb54b66a04c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1a07607-628d-4bbb-a385-cb65106c8ccc", "node_type": "4", "metadata": {}, "hash": "3fe07728bccc2c163a3527cd99d8eb8706109ae27e8f4c19856a8b10fe3df237", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fb40d58-aaaa-4c87-94bc-5ca1e159194d", "node_type": "1", "metadata": {}, "hash": "90ae1ee4fa0b2d07a998ae4c6944ba03210ecfc8922f6aba1f289d8230fbc448", "class_name": "RelatedNodeInfo"}}, "text": "# An Enhanced RAG System\n\nThe proposed graph-based Advanced RAG system offers a novel approach that significantly improves RAG performance, addressing the limitations of existing RAG models. The experimental results show that this system markedly enhances the accuracy and relevance of responses to user queries. The utilization of LangGraph's graph technology has effectively assessed the reliability of information, contributing to the improvement of information quality through question rewriting and web search optimization. By enhancing real-time data accessibility and strengthening the system's ability to handle various types of questions, the LangGraph-based method has expanded the potential applications of AI-driven customer support and information retrieval. These findings provide a crucial foundation for the advancement of RAG-based generative AI services.\n\nHowever, several limitations remain. The LangGraph-based system is optimized for specific domains, which may result in performance degradation when applied to other fields. Additionally, the system's complexity may require additional resources for implementation and maintenance. Further validation processes are necessary to ensure the accuracy and reliability of real-time data, which could impact overall system performance. Future research should focus on improving the generalizability of graph-based RAG systems. Expanding the system's applicability through testing and optimization across various domains, as well as developing and validating algorithms to enhance real-time data reliability, will be essential for further performance improvements. Lastly, given the rapid advancements in RAG technology, it is crucial to not only keep pace with technological progress but also to deeply understand and continually improve how information is retrieved and how accurate and reliable answers are generated.\n\n# References\n\n1. Adam, M., Wessel, M., & Benlian, A. (2021). AI-based chatbots in customer service and their effects on user compliance. Electronic Markets, 31(2), 427-445.\n2. Ahn, J., & Park, H. (2023). Development of a Case-Based Nursing Education Program Using Generative Artificial Intelligence. Journal of Korean Academic Society of Nursing Education, 29(3), 234-246. https://doi.org/10.5977/jkasne.2023.29.3.234\n3. Angels B., Vinamra B., Renato L., et al., (2024, January 30). RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. arXiv preprint arXiv:2401.08406.\n4. Asai A., Wu Z., Wang Y., Sil A., Hajishirzi H., (2023, October 17). Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. arXiv preprint arXiv:2310.11511.\n5. Devtorium. (2023, July 26). How Vector Databases Can Enhance Custom AI Solutions. https://devtorium.com/blog/how-vector-databases-can-enhance-custom-ai-solutions/\n6. Jang, D. (2024, February 24). Enhancing Search-Augmented Generation (RAG) Performance Using Korean Reranker. Retrieved from https://aws.amazon.com/ko/blogs/tech/korean-reranker-rag/\n7. Jeon, J., Kim, S., Kim, J., & Yoon, S. (2024, January 31). Solving Knowledge-Based QA Problems Using Search-Augmented Generation (RAG) Technology on Web Application Servers (WAS). Proceedings of the Korean Institute of Communications and Information Sciences Conference, Gangwon.\n8. Jeong, C. S., & Jeong, J. H. (2020). A Study on the Method of Implementing an AI Chatbot to Respond to the POST COVID-19 Untact Era, Journal of Information Technology Services, 19(4), 31\u201347. https://doi.org/10.9716/KITS.2020.19.4.031\n9. Jeong, C. S. (2023a). A Study on the RPA Interface Method for Hybrid AI Chatbot Implementation, KIPS Transactions on Software and Data Engineering, 12(1), 41-50. https://doi.org/10.3745/KTSDE.2023.12.1.41\n10. Jeong, C. S. (2023b). A Case Study in Applying Hyperautomation Platform for E2E Business Process Automation, Information Systems Review, 25(2), 31-56. https://doi.org/10.14329/isr.2023.25.2.031\n11. Jeong, C. S. (2023c).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fb40d58-aaaa-4c87-94bc-5ca1e159194d": {"__data__": {"id_": "6fb40d58-aaaa-4c87-94bc-5ca1e159194d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1a07607-628d-4bbb-a385-cb65106c8ccc", "node_type": "4", "metadata": {}, "hash": "3fe07728bccc2c163a3527cd99d8eb8706109ae27e8f4c19856a8b10fe3df237", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd6c1d55-c7b0-402f-a802-2fb54b66a04c", "node_type": "1", "metadata": {}, "hash": "a610145554cc82b0a53d174ad917aba240efa8c4de766e49ced6beae306d01ad", "class_name": "RelatedNodeInfo"}}, "text": "https://doi.org/10.9716/KITS.2020.19.4.031\n9. Jeong, C. S. (2023a). A Study on the RPA Interface Method for Hybrid AI Chatbot Implementation, KIPS Transactions on Software and Data Engineering, 12(1), 41-50. https://doi.org/10.3745/KTSDE.2023.12.1.41\n10. Jeong, C. S. (2023b). A Case Study in Applying Hyperautomation Platform for E2E Business Process Automation, Information Systems Review, 25(2), 31-56. https://doi.org/10.14329/isr.2023.25.2.031\n11. Jeong, C. S. (2023c). A Study on the Service Integration of Traditional Chatbot and ChatGPT, Journal of Information Technology Applications & Management, 3(4), 11-28. https://doi.org/10.21219/jitam.2023.30.4.001\n12. Jeong, C. S. (2023d). A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture. Advances in Artificial Intelligence and Machine Learning, 3(4). 1588-1618. https://dx.doi.org/10.54364/AAIML.2023.1191\n13. Jeong, C. S. (2023e). Generative AI service implementation using LLM application architecture: based on RAG model and LangChain framework. Journal of Intelligence\n\nCheonsu Jeong", "mimetype": "text/plain", "start_char_idx": 3490, "end_char_idx": 4598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "295c0285-31a0-4941-ac27-22e4c96050bc": {"__data__": {"id_": "295c0285-31a0-4941-ac27-22e4c96050bc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73a20c35-6533-4dd2-ae91-045fadb60ea6", "node_type": "4", "metadata": {}, "hash": "4699976dee04e784d95a85d0b95673c77a9f201368b3dfe103b39fcc381d1614", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Jeong, C. S. (2023). Domain-specialized LLM: Financial fine-tuning and utilization method using Mistral 7B. Journal of Intelligence and Information Systems, 29(4), 129-164. https://dx.doi.org/10.13088/jiis.2023.29.4.129\n2. Jeong, C. S. (2024). Domain-specialized LLM: Financial fine-tuning and utilization method using Mistral 7B. Journal of Intelligence and Information Systems, 30(1), 93-120. https://dx.doi.org/10.13088/jiis.2024.30.1.093\n3. Jeong, S., Baek, J., Cho, S., Hwang S.J., Park, J.C., Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park. (2024, March 28). Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity. arXiv preprint arXiv:2403.14403.\n4. Kim, J. (2024). A Study on Data Chunking Strategies to Enhance LLM Service Quality Using RAG Techniques. Master's Thesis, Korea University, Seoul.\n5. LangGraph. (2024, July 22). LangGraph Overview. https://langchain-ai.github.io/langgraph/\n6. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., & Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.\n7. Matt, A. (2023, September 19). 10 Ways to Improve the Performance of Retrieval Augmented Generation Systems. https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c\n8. Microsoft. (2023, August 01). Retrieval Augmented Generation using Azure Machine Learning prompt flow. https://learn.microsoft.com/en-us/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2\n9. Park, E. (2024). The Impact of Customers' Regulatory Focus and Familiarity with Generative AI-Based Chatbots on Privacy Disclosure Intent: Focusing on Privacy Calculus Theory. Journal of Knowledge Management Research, 25(2), 49-68. https://doi.org/10.15813/kmr.2024.25.2.003\n10. Przegalinska, A., Ciechanowski, L., Stroz, A., Gloor, P., & Mazurek, G. (2019). In bot we trust: A new methodology of chatbot performance measures. Business Horizons, 62(6), 785-797.\n11. S\u00e1nchez-D\u00edaz, X., Ayala-Bastidas, G., Fonseca-Ortiz, P., Garrido, L. (2018). A Knowledge-Based Methodology for Building a Conversational Chatbot as an Intelligent Tutor, Advances in Computational Intelligence, Vol. 11289. 165-175. https://doi.org/10.1007/978-3-030-04497-8_14\n12. Skelter Labs. (2024, January 5). 2024 Year Of The RAG: Reasons for RAG's Attention and Future Trends. Retrieved from https://www.skelterlabs.com/blog/2024-year-of-the-rag\n13. Yan S.Q., Gu J.C., Zhu Y., Ling Z.H. (2024, March 27). Corrective Retrieval Augmented Generation. arXiv preprint arXiv:2401.15884.\n14. Yunfan G., Yun X., Xinyu G., Kangxiang J., Jinliu P., Yuxi B., Yi D., Jiawei S., Meng W., Haofen W. (2024, March 27). Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv preprint arXiv:2312.10997.\n15. Anderson JC. Current status of chorion villus biopsy. In: Tudenhope D, Chenoweth J, editors. Proceedings of the 4th Congress of the Australian Perinatal Society; 1986. Brisbane, Queensland: Australian Perinatal Society; 1987. p. 190-6.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"a02d4e34-f474-4935-aaef-8cec4c57c879": {"node_ids": ["afd9d996-57c2-475a-aba6-7f93d48f6fae"], "metadata": {}}, "e0d94ca7-e03f-4c62-ae75-5b76a14ed315": {"node_ids": ["81d47669-5f06-44e9-aea9-2a43c1018a74"], "metadata": {}}, "1391dafc-7b5a-4343-b003-834a1333ae8d": {"node_ids": ["9df88571-d26b-45fe-b665-5c604a0add95"], "metadata": {}}, "2067add4-2fae-4ea0-bbd8-047962d5c3a9": {"node_ids": ["29ecf296-4b93-4948-95f7-68958fe891cd"], "metadata": {}}, "c2533e4f-d332-4e54-8073-f0feed11bd9b": {"node_ids": ["dbe3e741-aec8-4e7e-bee9-fb9d9abe486f"], "metadata": {}}, "7b3c6b17-627b-43e2-bdc6-b441024b79c3": {"node_ids": ["29cf6dd1-4b27-4be7-a8b0-56803bc449a6"], "metadata": {}}, "72c27642-e6d6-4e35-aa91-e32eca38a6f7": {"node_ids": ["71f70f1e-fb13-4d41-85e2-b0d3c810bae8"], "metadata": {}}, "4a147278-fdae-4c3c-a253-1a15fedba3c0": {"node_ids": ["764d7a9d-ca64-4b70-8090-a622e52f5ace"], "metadata": {}}, "90cbdbe4-5c92-4dc8-97bb-3c84b0aab59a": {"node_ids": ["b46e6977-1e81-4b83-a028-70f1c0d455c8"], "metadata": {}}, "ce71a438-c5a8-454e-ab36-4ba6fda3ce6a": {"node_ids": ["e8607000-92d5-43a1-a938-f66a688ca028"], "metadata": {}}, "d1a07607-628d-4bbb-a385-cb65106c8ccc": {"node_ids": ["bd6c1d55-c7b0-402f-a802-2fb54b66a04c", "6fb40d58-aaaa-4c87-94bc-5ca1e159194d"], "metadata": {}}, "73a20c35-6533-4dd2-ae91-045fadb60ea6": {"node_ids": ["295c0285-31a0-4941-ac27-22e4c96050bc"], "metadata": {}}}}