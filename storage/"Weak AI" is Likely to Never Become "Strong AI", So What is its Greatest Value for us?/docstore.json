{"docstore/metadata": {"df00ac50-61b0-4923-b855-15bb7066d27f": {"doc_hash": "43fcbaf6b2647b08c4e3722db9672a4e54bd90085ad50eb701650750434c509a"}, "d8850640-e1e5-4d63-a1b6-b0f874a8eac7": {"doc_hash": "6cd12d9c0749d39671f9a9e094212289c34c2ba057b4e0502e77f2ce6ce02fed"}, "1aa02e05-6795-4fb4-992d-ce56d9fc6590": {"doc_hash": "ca9d3583962f6888b09659920290d0b59a977ab0da9c6582d6a3eee21d1ccd89"}, "8db11412-4ed1-4c39-91dc-1c155d197d36": {"doc_hash": "95ac358bc219b476177ea99f84bc1a2107242b498f089f0379af0bdb6a80075e"}, "bf4f6a01-18f1-4d3a-8c8d-8710bf99147f": {"doc_hash": "40d2e806636370700174ff136ff612f035ba871b5f7658fbfb9283bee1d6c3aa"}, "fa86fa59-3492-42b9-a42b-f32b374b009b": {"doc_hash": "477b973cce4001a36ccc3d84301b49e39432cc81d849c2d90506a3ceee57d707"}, "e9bd26e1-2124-42b9-afcc-d7a70506bee3": {"doc_hash": "4e670de985bf94887739d00bd3fa60d72931fd2992f19e3a46b701b2ceaf6c04"}, "406e41f3-651e-4898-94b5-f147dd7d0c94": {"doc_hash": "43fcbaf6b2647b08c4e3722db9672a4e54bd90085ad50eb701650750434c509a", "ref_doc_id": "df00ac50-61b0-4923-b855-15bb7066d27f"}, "f4f8d8ba-cc15-4577-8f9d-46627cee7bb3": {"doc_hash": "6cd12d9c0749d39671f9a9e094212289c34c2ba057b4e0502e77f2ce6ce02fed", "ref_doc_id": "d8850640-e1e5-4d63-a1b6-b0f874a8eac7"}, "e07be232-0bfb-4f45-afb3-c3c0f2f1a620": {"doc_hash": "ca9d3583962f6888b09659920290d0b59a977ab0da9c6582d6a3eee21d1ccd89", "ref_doc_id": "1aa02e05-6795-4fb4-992d-ce56d9fc6590"}, "f410b3a1-4fdc-41cc-9b03-cd544517154a": {"doc_hash": "95ac358bc219b476177ea99f84bc1a2107242b498f089f0379af0bdb6a80075e", "ref_doc_id": "8db11412-4ed1-4c39-91dc-1c155d197d36"}, "5d670843-a531-41bb-ba7d-569228c2306c": {"doc_hash": "40d2e806636370700174ff136ff612f035ba871b5f7658fbfb9283bee1d6c3aa", "ref_doc_id": "bf4f6a01-18f1-4d3a-8c8d-8710bf99147f"}, "35353932-e819-45a6-bcc2-9e1668d2dda2": {"doc_hash": "477b973cce4001a36ccc3d84301b49e39432cc81d849c2d90506a3ceee57d707", "ref_doc_id": "fa86fa59-3492-42b9-a42b-f32b374b009b"}, "1368b4b0-62b1-47a6-9eab-0be9f626bebe": {"doc_hash": "a21f87be1d7c7e0fbc2b066f74f81c60f0919308e1e456d4be2cbd27ca4d411c", "ref_doc_id": "e9bd26e1-2124-42b9-afcc-d7a70506bee3"}, "893cf908-f7a3-4fcf-b871-a10111e5b0f6": {"doc_hash": "5bfa94367e0221f9505312a5a1e82a071e116a67c28ec1b9a78b8eb479d97d5f", "ref_doc_id": "e9bd26e1-2124-42b9-afcc-d7a70506bee3"}}, "docstore/data": {"406e41f3-651e-4898-94b5-f147dd7d0c94": {"__data__": {"id_": "406e41f3-651e-4898-94b5-f147dd7d0c94", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df00ac50-61b0-4923-b855-15bb7066d27f", "node_type": "4", "metadata": {}, "hash": "43fcbaf6b2647b08c4e3722db9672a4e54bd90085ad50eb701650750434c509a", "class_name": "RelatedNodeInfo"}}, "text": "# arXiv:2103.15294v1 [cs.AI] 29 Mar 2021\n\n# \u201cWeak AI\u201d is Likely to Never Become \u201cStrong AI\u201d, So What is its Greatest Value for us?\n\n# starBin Liu\n\nFirst posted March 30th, 2021\n\n# Abstract\n\nAI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, \u201cStarcraft\u201d and poker), and protein structure prediction. However, at the same time, AI is also bearing serious controversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In this paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research, termed \u201cweak AI\u201d and \u201cstrong AI\u201d (a.k.a. artificial general intelligence); (3) clarifies how to judge which paradigm a research work should be classified into; (4) discusses what is the greatest value of \u201cweak AI\u201d if it has no chance to develop into \u201cstrong AI\u201d.\n\n# Index Terms\n\nArtificial intelligence, artificial general intelligence, deep learning, weak AI, strong AI\n\n# I. INTRODUCTION\n\nThe last decade has seen impressive applications of AI represented mostly by deep neural networks, i.e., deep learning [1]. The striking point lies in that the computing agent has reached and even surpassed humans in many tasks, e.g., image classification [2], speech recognition [3, 4], games [5\u20137], protein structure prediction [8]. Even ten years ago, it was hard to imagine that AI would achieve so many amazing breakthroughs.\n\nOn the other side, AI is also bearing serious controversies during the same period. Among the critics, Judea Pearl, a pioneer for probabilistic reasoning in AI and a winner of the Turing award, argues that \u201c... all the impressive achievements of deep learning amount to just curve fitting,\u201d and a necessary ability to be supplemented for AI is causal reasoning [9, 10]. Gary Marcus, a professor of cognitive science, starB. Liu is with Zhejiang Lab, Hangzhou, China. e-mail: bins@ieee.org or liubin@zhejianglab.com.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4f8d8ba-cc15-4577-8f9d-46627cee7bb3": {"__data__": {"id_": "f4f8d8ba-cc15-4577-8f9d-46627cee7bb3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8850640-e1e5-4d63-a1b6-b0f874a8eac7", "node_type": "4", "metadata": {}, "hash": "6cd12d9c0749d39671f9a9e094212289c34c2ba057b4e0502e77f2ce6ce02fed", "class_name": "RelatedNodeInfo"}}, "text": "# 2\n\nsummarizes ten limitations of deep learning [11], namely, \u201c... it is data-hungry, ... it has limited capacity for transfer, ... it has no natural way to deal with hierarchical structure, ... it struggles with open-ended inference, ... it is not sufficiently transparent, ... it has not been well integrated with prior knowledge, ... it cannot inherently distinguish causation from correlation, ... it presumes a largely stable world, in ways that may be problematic, ... it works well as an approximation, but its answers often cannot be fully trusted, ... it is difficult to engineer with\u201d. In a recent issue of the journal Frontiers in Psychology, another cognitive scientist J. Mark Bishop argues that AI \u201cis stupid and causal reasoning will not fix it\u201d [12].\n\nIn this paper, I attempt to concisely respond to current controversies about AI. Specifically, I emphasize discrimination between two paradigms of AI research, namely \u201cweak AI\u201d and \u201cstrong AI\u201d (Section II); provide a conceptual guide to judge which paradigm a research work should be classified into (Section II-A), explain why controversies about AI last (Section III), present major views on whether \u201cweak AI\u201d will grow into \u201cstrong AI\u201d (Section IV) and discuss what is the greatest value of \u201cweak AI\u201d if it has no chance to become \u201cstrong AI\u201d (Section V).\n\n# II. WHAT DO \u201cWEAK AI\u201d AND \u201cSTRONG AI\u201d MEAN?\n\n\u201cWeak AI\u201d and \u201cStrong AI\u201d are two terms coined by John Searle in the \u201cChinese room argument\u201d (CRA) [13]. CRA is a thought experiment as follows: \u201cSearle imagines himself alone in a room following a computer program for responding to Chinese characters slipped under the door. Searle understands nothing of Chinese, and yet, by following the program for manipulating symbols and numerals just as a computer does, he sends appropriate strings of Chinese characters back out under the door, and this leads those outside to mistakenly suppose there is a Chinese speaker in the room\u201d [14]. The term \u201cstrong AI\u201d entails that, \u201c... the computer is not merely a tool in the study of the mind; rather, the appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.\u201d In contrast, the term \u201cweak AI\u201d implies that \u201c... the principal value of the computer in the study of the mind is that it gives us a very powerful tool.\u201d J. Mark Bishop summarizes that \u201cweak AI focuses on epistemic issues relating to engineering a simulation of human intelligent behavior, whereas strong AI, in seeking to engineer a computational system with all the causal power of a mind, focuses on the ontological\u201d [12].\n\nI borrow the terms \u201cweak AI\u201d and \u201cstrong AI\u201d here without an intent to discuss CRA. See related discussions in e.g., [15\u201318].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e07be232-0bfb-4f45-afb3-c3c0f2f1a620": {"__data__": {"id_": "e07be232-0bfb-4f45-afb3-c3c0f2f1a620", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1aa02e05-6795-4fb4-992d-ce56d9fc6590", "node_type": "4", "metadata": {}, "hash": "ca9d3583962f6888b09659920290d0b59a977ab0da9c6582d6a3eee21d1ccd89", "class_name": "RelatedNodeInfo"}}, "text": "# Simply put, \u201cweak AI\u201d represents computational systems that exhibit as if they own human intelligence, but they do not. In contrast, \u201cstrong AI\u201d represents computational systems that have human intelligence. Correspondingly, all AI research can be categorized into two paradigms: one is targeted for realizing \u201cstrong AI\u201d; and the other produces advanced \u201cweak AI\u201d systems to meet a variety of practical needs.\n\n# A. How to Judge a Research Work Belongs to Which Paradigm?\n\nThe biggest motivation for realizing \u201cstrong AI\u201d is to answer the question: what are the generation mechanisms of humansquoteright intelligence and how to implement these mechanisms with a machine. Therefore, given a research work, it is easy to judge whether it belongs to the \u201cstrong AI\u201d paradigm. If this work provides any new and useful clue for us to answer the above question, it falls within the \u201cstrong AI\u201d paradigm; otherwise, it belongs to the \u201cweak AI\u201d paradigm.\n\nBased on the above method, part of the (especially early) works on neural networks that deepen our understanding of the working mechanism of biological neural systems, surely belongs to the \u201cstrong AI\u201d paradigm. On the other hand, most research works that involve artificial neural networks and deep learning, even if they are proposed under the inspiration of research on neuroscience, cognitive science, behavior psychology, they belong to the \u201cweak AI\u201d paradigm as long as they do not give us any new insight on the generation mechanisms of humansquoteright intelligence or on how to better implement mechanisms that have already been found.\n\n# III. WHY CONTROVERSIES ABOUT AI LAST?\n\nIn controversies about AI, party A believes that AI has made substantial progress in the past decade; party B doubts or even negates the development of AI.\n\nI argue that controversies arise mainly because these two parties mix two different concepts, \u201cweak AI\u201d and \u201cstrong AI\u201d, together, when they talk about AI. The fact is that \u201cweak AI\u201d has made substantial progress in the past decade, while \u201cstrong AI\u201d has not. Party A thinks that \u201cweak AI\u201d is an important member of the AI family; progress gained from \u201cweak AI\u201d also belongs to this AI family. In contrast, in the mind of Party B, there always exists one ideal form of AI, namely a realized \u201cstrong AI\u201d, and the \u201cdistance\u201d between current AI and this ideal AI is treated as a criterion for evaluating current AI. Compared with decades ago, current AI still lacks basic human-level abilities such as causal reasoning [9], robust decision making [19], commonsense utilization [20], and knowledge transfer, which implies", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f410b3a1-4fdc-41cc-9b03-cd544517154a": {"__data__": {"id_": "f410b3a1-4fdc-41cc-9b03-cd544517154a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8db11412-4ed1-4c39-91dc-1c155d197d36", "node_type": "4", "metadata": {}, "hash": "95ac358bc219b476177ea99f84bc1a2107242b498f089f0379af0bdb6a80075e", "class_name": "RelatedNodeInfo"}}, "text": "# IV. WILL \u201cWEAK AI\u201d GROW INTO \u201cSTRONG AI\u201d?\n\nA metaphor is often used to reply to this question: the relationship between \u201cweak AI\u201d and \u201cstrong AI\u201d is like that between flying machines and birds. Flying machines are not developed by accurately mimicking birds' flying. Birds perform much better in maneuvering than the most advanced flying machine today. Birds can flexibly re-purpose their behaviors while flying machines cannot. But the appearance of flying machines has met demands of speedy transportation and others. People may think that, since it is unlikely and not necessary for flying machines to develop into birds, then similarly, \u201cweak AI\u201d is unlikely and not necessary to grow into \u201cstrong AI\u201d.\n\nTo formally consider whether \u201cweak AI\u201d will grow into \u201cstrong AI\u201d, let recall the Turing test [21] and the CRA (mentioned in Section II). An example statement of the Turing test is as follows [22]: \u201cOriginally known as the Imitation Game, the test evaluates if a machine's behavior can be distinguished from a human. In this test, there is a person known as the \u201cinterrogator\u201d who seeks to identify a difference between computer-generated output and human-generated ones through a series of questions. If the interrogator cannot reliably discern the machines from human subjects, the machine passes the test. However, if the evaluator can identify the human responses correctly, then this eliminates the machine from being categorized as intelligent.\u201d Through the lens of CRA, Searle argues that the Turing test has serious flaws, as passing the test does not indicate that the machine has consciousness or understanding. The absence of an effective evaluation method hampers the development of \u201cstrong AI\u201d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d670843-a531-41bb-ba7d-569228c2306c": {"__data__": {"id_": "5d670843-a531-41bb-ba7d-569228c2306c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf4f6a01-18f1-4d3a-8c8d-8710bf99147f", "node_type": "4", "metadata": {}, "hash": "40d2e806636370700174ff136ff612f035ba871b5f7658fbfb9283bee1d6c3aa", "class_name": "RelatedNodeInfo"}}, "text": "# Besides, philosophers and cognitive scientists often use Godel's first incompleteness theorem [23] to argue that a machine cannot generate human's consciousness or understanding. See related discussions in e.g., [12].\n\n# V. WHAT IS THE GREATEST VALUE OF \u201cWEAK AI\u201d FOR US?\n\nIn his most recent paper, Geoffrey Hinton states that \u201cThe difference between science and philosophy is that experiments can show that extremely plausible ideas are just wrong and extremely implausible ones, like learning an entire complicated system by end-to-end gradient descent, are just right\u201d [24]. In [25], Judea Pearl argues that \u201cModern connectionism has in fact been viewed as a Triumph of Radical Empiricism over its rationalistic rivals. Indeed, the ability to emulate knowledge acquisition processes on digital machines offer enormously flexible testing grounds in which philosophical theories about the balance between empiricism and innateness can be submitted to experimental evaluation on digital machines.\u201d\n\nCombining their arguments, one can see that they both attribute recent deep learning's success as a success of empiricism which is data-driven, other than driven by philosophical theory or intuition.\n\nA very important lesson that can be learned from the fast-pacing development and applications of AI in the past decade is that deep learning running on big enough data can produce unexpected shortcuts to solve extremely difficult problems. For example, by combining deep learning, reinforcement learning [26], and Monte Carlo tree search [27], a computer program AlphaGO [28] can win the human champion without having to understand any of the Go-playing strategies that have been accumulated by humans for more than four thousand years. The Generative Pre-trained Transformer 3 (GPT-3) [29] can generate human-like texts through deep learning without having to understand any syntax or semantics underlying the texts.\n\nIt is shown that the greatest value of \u201cweak AI\u201d represented by deep learning lies in that it provides scalable, less-labor-involved, accurate, and generalizable tools for distilling, representing and then exploiting patterns hidden from big data. Although such \u201cweak AI\u201d has no real intelligence, to a large extent it meets urgent needs for scalable, efficient, and accurate processing of big data.\n\nIn a foreseeable future, \u201cweak AI\u201d is likely to become more robustly (with e.g., portfolio [19] or dynamic portfolio methods [30\u201335]), while a big challenge is how to model \u201cunknown unknowns\u201d; it will perform more automatically through e.g., auto machine learning [36], but it cannot become completely automatic provided that \u201cstrong AI\u201d is realized [37]; it may perform as if it owns abilities of cognition and understanding, but it does not.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35353932-e819-45a6-bcc2-9e1668d2dda2": {"__data__": {"id_": "35353932-e819-45a6-bcc2-9e1668d2dda2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa86fa59-3492-42b9-a42b-f32b374b009b", "node_type": "4", "metadata": {}, "hash": "477b973cce4001a36ccc3d84301b49e39432cc81d849c2d90506a3ceee57d707", "class_name": "RelatedNodeInfo"}}, "text": "# VI. CONCLUSIONS\n\nAI has made great progress in the past decade. It has influenced almost all facets of human society by providing more efficient algorithmic solutions to representation, management, analysis of multi-modal big data. Controversies about AI last mainly because \u201cweak AI\u201d becomes so strong while \u201cstrong AI\u201d is almost as weak as it was decades ago. Almost all breakthroughs of AI that have attracted the public attention in the past decade are within the \u201cweak AI\u201d paradigm. \u201cWeak AI\u201d is developing much faster than expected. Even ten years ago, one could not imagine that a computer program would beat the human champion soon in playing Go. In contrast, the \u201cfruits\u201d people have got from the \u201cstrong AI\u201d paradigm are not so striking as from \u201cweak AI\u201d. I suggest, when talking about AI in the future, one should better make a statement in advance whether this talk is about \u201cweak AI\u201d or \u201cstrong AI\u201d. In this way, more focused and constructive discussions can be expected.\n\nIn a foreseeable future, \u201cweak AI\u201d cannot develop into \u201cstrong AI\u201d (see why in Section IV), but it provides a channel to synthesize advances obtained from related disciplines such as cloud computing, computer storage, high-speed wireless mobile communications. Through this synthesis of technologies, more advanced algorithmic tools will be developed in the \u201cweak AI\u201d paradigm, then \u201cweak AI\u201d will continue to influence human society more profoundly, through big data. The man-computer symbiosis world that Licklider predicted more than sixty years ago [38] is becoming a reality.\n\n# REFERENCES\n\n1. Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.\n2. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d Advances in neural information processing systems, vol. 25, pp. 1097\u20131105, 2012.\n3. W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke, \u201cThe microsoft 2017 conversational speech recognition system,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5934\u20135938.\n4. G. Saon, G. Kurata, T. Sercu et al., \u201cEnglish conversational telephone speech recognition by humans and machines,\u201d arXiv preprint arXiv:1703.02136, 2017.\n5. D. Silver, T. Hubert, J. Schrittwieser et al., \u201cA general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\u201d Science, vol. 362, no. 6419, pp. 1140\u20131144, 2018.\n6. N. Brown and T. Sandholm, \u201cSuperhuman ai for heads-up no-limit poker: Libratus beats top professionals,\u201d Science, vol. 359, no. 6374, pp. 418\u2013424, 2018.\n7. O. Vinyals, I. Babuschkin, W. M. Czarnecki et al., \u201cGrandmaster level in starcraft ii using multi-agent reinforcement learning,\u201d Nature, vol. 575, no. 7782, pp. 350\u2013354, 2019.\n8. A. W. Senior, R. Evans, J. Jumper et al., \u201cImproved protein structure prediction using potentials from deep learning,\u201d Nature, vol. 577, no. 7792, pp. 706\u2013710, 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1368b4b0-62b1-47a6-9eab-0be9f626bebe": {"__data__": {"id_": "1368b4b0-62b1-47a6-9eab-0be9f626bebe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9bd26e1-2124-42b9-afcc-d7a70506bee3", "node_type": "4", "metadata": {}, "hash": "4e670de985bf94887739d00bd3fa60d72931fd2992f19e3a46b701b2ceaf6c04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "893cf908-f7a3-4fcf-b871-a10111e5b0f6", "node_type": "1", "metadata": {}, "hash": "5bfa94367e0221f9505312a5a1e82a071e116a67c28ec1b9a78b8eb479d97d5f", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. J. Pearl, \u201cThe limitations of opaque learning machines,\u201d Possible minds: twenty-five ways of looking at AI, pp. 13\u201319, 2019.\n2. J. Pearl and D. Mackenzie, \u201cAi canquoterightt reason why,\u201d Wall Street Journal, 2018.\n3. G. Marcus, \u201cDeep learning: A critical appraisal,\u201d arXiv preprint arXiv:1801.00631, 2018.\n4. J. M. Bishop, \u201cArtificial intelligence is stupid and causal reasoning will not fix it,\u201d Frontiers in Psychology, vol. 11, pp. 1\u201318, 2021.\n5. S. John, \u201cMinds, brains, and programs,\u201d Behavioral and Brain Sciences, vol. 3, no. 3, pp. 417\u2013457, 1980.\n6. D. Cole, \u201cThe chinese room argument,\u201d https://plato.stanford.edu/entries/chinese-room/.\n7. G. Rey, \u201cWhatquoterights really going on in Searlequoterights \u201cChinese room\u201d,\u201d Philosophical Studies, vol. 50, no. 2, pp. 169\u201385, 1986.\n8. M. J. Shaffer, \u201cA logical hole in the chinese room,\u201d Minds and Machines, vol. 19, no. 2, pp. 229\u2013235, 2009.\n9. A. Sloman and M. Croucher, \u201cHow to turn an information processor into an understander,\u201d Behavioral and Brain Sciences, vol. 3, no. 3, pp. 447\u2013448, 1980.\n10. M. A. Boden, Computer models of mind: Computational approaches in theoretical psychology. Cambridge University Press, 1988.\n11. T. G. Dietterich, \u201cSteps toward robust artificial intelligence,\u201d AI Magazine, vol. 38, no. 3, pp. 3\u201324, 2017.\n12. G. Marcus, \u201cThe next decade in AI: four steps towards robust artificial intelligence,\u201d arXiv preprint arXiv:2002.06177, 2020.\n13. A. M. Turing, \u201cComputing machinery and intelligence,\u201d in Parsing the turing test. Springer, 2009, pp. 23\u201365.\n14. IBM Cloud Education, \u201cStrong AI,\u201d https://www.ibm.com/cloud/learn/strong-ai.\n15. P. Raatikainen, \u201cGdieresisodelquoterights incompleteness theorems,\u201d https://plato.stanford.edu/entries/goedel-incompleteness/.\n16. G. Hinton, \u201cHow to represent part-whole hierarchies in a neural network,\u201d arXiv preprint arXiv:2102.12627, 2021.\n17. J. Pearl, \u201cRadical empiricism and machine learning research,\u201d Causal Analysis in Theory and Practice (Blog), vol. 26, 2020.\n18. R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.\n19. S. Gelly and D. Silver, \u201cMonte-carlo tree search and rapid action value estimation in computer go,\u201d Artificial Intelligence, vol. 175, no. 11, pp. 1856\u20131875, 2011.\n20. D. Silver, A. Huang, C. J. Maddison et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.\n21. T. B. Brown, B. Mann, N. Ryder et al., \u201cLanguage models are few-shot learners,\u201d arXiv preprint arXiv:2005.14165, 2020.\n22. B. Liu, Y. Qi, and K. Chen, \u201cSequential online prediction in the presence of outliers and change points: an instant temporal structure learning approach,\u201d Neurocomputing, vol. 413, pp. 240\u2013258, 2020.\n23. Y. Qi, B. Liu, Y. Wang, and G. Pan, \u201cDynamic ensemble modeling approach to nonstationary neural decoding in brain-computer interfaces,\u201d Advances in neural information processing systems, pp. 6087\u20136096, 2019.\n24. B. Liu, \u201cRobust particle filter by dynamic averaging of multiple noise models,\u201d in IEEE Intquoterightl Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 4034\u20134038.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "893cf908-f7a3-4fcf-b871-a10111e5b0f6": {"__data__": {"id_": "893cf908-f7a3-4fcf-b871-a10111e5b0f6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9bd26e1-2124-42b9-afcc-d7a70506bee3", "node_type": "4", "metadata": {}, "hash": "4e670de985bf94887739d00bd3fa60d72931fd2992f19e3a46b701b2ceaf6c04", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1368b4b0-62b1-47a6-9eab-0be9f626bebe", "node_type": "1", "metadata": {}, "hash": "a21f87be1d7c7e0fbc2b066f74f81c60f0919308e1e456d4be2cbd27ca4d411c", "class_name": "RelatedNodeInfo"}}, "text": "22. B. Liu, Y. Qi, and K. Chen, \u201cSequential online prediction in the presence of outliers and change points: an instant temporal structure learning approach,\u201d Neurocomputing, vol. 413, pp. 240\u2013258, 2020.\n23. Y. Qi, B. Liu, Y. Wang, and G. Pan, \u201cDynamic ensemble modeling approach to nonstationary neural decoding in brain-computer interfaces,\u201d Advances in neural information processing systems, pp. 6087\u20136096, 2019.\n24. B. Liu, \u201cRobust particle filter by dynamic averaging of multiple noise models,\u201d in IEEE Intquoterightl Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 4034\u20134038.\n25. Y. Dai and B. Liu, \u201cRobust video object tracking via bayesian model averaging-based feature fusion,\u201d Optical Engineering, vol. 55, no. 8, pp. 1\u201311, 2016.\n26. B. Liu, \u201cData-driven model set design for model averaged particle filter,\u201d in IEEE Intquoterightl Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 5835\u20135839.\n27. \u2014\u2014, \u201cInstantaneous frequency tracking under model uncertainty via dynamic model averaging and particle filtering,\u201d IEEE Trans. on Wireless Communications, vol. 10, no. 6, pp. 1810\u20131819, 2011.\n28. F. Hutter, L. Kotthoff, and J. Vanschoren, Automated machine learning: methods, systems, challenges. Springer Nature, 2019.\n29. B. Liu, \u201cA very brief and critical discussion on automl,\u201d arXiv preprint arXiv:1811.03822, 2018.\n30. J. Licklider, \u201cMan-computer symbiosis,\u201d IRE Transactions on human factors in electronics, no. 1, pp. 4\u201311, 1960.", "mimetype": "text/plain", "start_char_idx": 2559, "end_char_idx": 4055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"df00ac50-61b0-4923-b855-15bb7066d27f": {"node_ids": ["406e41f3-651e-4898-94b5-f147dd7d0c94"], "metadata": {}}, "d8850640-e1e5-4d63-a1b6-b0f874a8eac7": {"node_ids": ["f4f8d8ba-cc15-4577-8f9d-46627cee7bb3"], "metadata": {}}, "1aa02e05-6795-4fb4-992d-ce56d9fc6590": {"node_ids": ["e07be232-0bfb-4f45-afb3-c3c0f2f1a620"], "metadata": {}}, "8db11412-4ed1-4c39-91dc-1c155d197d36": {"node_ids": ["f410b3a1-4fdc-41cc-9b03-cd544517154a"], "metadata": {}}, "bf4f6a01-18f1-4d3a-8c8d-8710bf99147f": {"node_ids": ["5d670843-a531-41bb-ba7d-569228c2306c"], "metadata": {}}, "fa86fa59-3492-42b9-a42b-f32b374b009b": {"node_ids": ["35353932-e819-45a6-bcc2-9e1668d2dda2"], "metadata": {}}, "e9bd26e1-2124-42b9-afcc-d7a70506bee3": {"node_ids": ["1368b4b0-62b1-47a6-9eab-0be9f626bebe", "893cf908-f7a3-4fcf-b871-a10111e5b0f6"], "metadata": {}}}}