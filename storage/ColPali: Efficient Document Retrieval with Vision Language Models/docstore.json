{"docstore/metadata": {"4fdbe99b-a72f-4ea9-9d58-1efbbc0e8af8": {"doc_hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b"}, "7da3d723-b838-417c-bbe8-a24a50b96ea4": {"doc_hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d"}, "60e079ef-a199-49c3-ac39-40f24a2eccfa": {"doc_hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f"}, "128bdb06-8cf0-4df1-b56d-fdd8ffd2769d": {"doc_hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe"}, "689b0e3e-8117-4195-8759-dc5bb0960102": {"doc_hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc"}, "a2b2a7b7-fcec-4af1-b893-a7a5f1b6fb6d": {"doc_hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4"}, "292d5589-3184-4ff3-b433-03daeaee1cba": {"doc_hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3"}, "b0570d2b-7f87-4bb8-a029-0c83c23aea38": {"doc_hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84"}, "69d4d7ff-482f-4aa8-afeb-114aae7cfe25": {"doc_hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0"}, "4b8f6a82-8d7d-4def-8709-c09dc876bd15": {"doc_hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377"}, "7394abf3-6e8c-4750-86fd-81dbee960fa4": {"doc_hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2"}, "9133d5b3-b632-40b6-b387-ac0329d734a4": {"doc_hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b"}, "554e003a-18f7-4226-84fe-3021466167a3": {"doc_hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb"}, "817bba4f-a97f-4a48-90a0-de713b5685c6": {"doc_hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073"}, "506cce11-d72a-405c-a08a-4ef5f19ad9bc": {"doc_hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160"}, "0996ba57-66b0-4db1-b8b3-cd9fa2e3b876": {"doc_hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28"}, "297ac6a3-89db-4283-91b6-8ff124b88fd0": {"doc_hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777"}, "765b95d7-7a8e-473e-9cbe-e435fe78268a": {"doc_hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2"}, "c478a956-8f9e-492d-a6c1-6ae1cd7e6a9a": {"doc_hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451"}, "43782e9d-aa09-436f-a65d-7b7a5b82c66e": {"doc_hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc"}, "53de1f52-55a7-4ee8-acb3-ee46a58ec801": {"doc_hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b", "ref_doc_id": "4fdbe99b-a72f-4ea9-9d58-1efbbc0e8af8"}, "ab723b02-84f5-4cd1-b044-da180d75c1de": {"doc_hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d", "ref_doc_id": "7da3d723-b838-417c-bbe8-a24a50b96ea4"}, "67bb8be7-226b-42ad-990a-f0c7d684cf67": {"doc_hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f", "ref_doc_id": "60e079ef-a199-49c3-ac39-40f24a2eccfa"}, "96a27d0c-a728-4b25-89e5-dda0dc780acc": {"doc_hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe", "ref_doc_id": "128bdb06-8cf0-4df1-b56d-fdd8ffd2769d"}, "7485555b-1e26-4200-be0d-c7adb35c5fa1": {"doc_hash": "b5899512ab126ca7b0cc5fe2a825e271dfbea6ea3e49469597376829a500a0db", "ref_doc_id": "689b0e3e-8117-4195-8759-dc5bb0960102"}, "29fed939-f494-4404-9c83-59f8cc3f195a": {"doc_hash": "7e3111e040a674730fa702d7902f35c16b8c433334150ac0ae151ea4fab47ce1", "ref_doc_id": "689b0e3e-8117-4195-8759-dc5bb0960102"}, "04708a6c-cbd4-40e2-8ea8-cc6581ba3884": {"doc_hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4", "ref_doc_id": "a2b2a7b7-fcec-4af1-b893-a7a5f1b6fb6d"}, "9a223960-346f-4cef-a8bc-87e7b0ecc114": {"doc_hash": "e95fa380b755b4c34d72a363da79a8c57174e8d488e9a48e2fd996853cc786e8", "ref_doc_id": "292d5589-3184-4ff3-b433-03daeaee1cba"}, "9f24de3e-503b-4b1f-a34c-49acca2618cd": {"doc_hash": "231a44d96b44ef7cef97f4d52fc7bf0c58617c4a316ef1d4aacccf09c35c8c27", "ref_doc_id": "292d5589-3184-4ff3-b433-03daeaee1cba"}, "b91907ea-bb44-484d-9ea0-eab8caaecb6a": {"doc_hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84", "ref_doc_id": "b0570d2b-7f87-4bb8-a029-0c83c23aea38"}, "49c440da-96b9-4eaa-a6ae-f8e875b0490d": {"doc_hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0", "ref_doc_id": "69d4d7ff-482f-4aa8-afeb-114aae7cfe25"}, "0834c725-cb08-4a22-9914-c39947db97f7": {"doc_hash": "73a1d174247c6265657797b59c09c64dc8a0dbbe89ce49cf76e1b363e58e5267", "ref_doc_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15"}, "1964f7aa-67af-4dcf-ac5b-e38fd8537b77": {"doc_hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "ref_doc_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15"}, "d2ba09d9-02c0-4b40-b767-fd4af26b92b4": {"doc_hash": "a33129b68cd64ada00b9243cecd3233f935c166bfedda87a4cb2921a33d65367", "ref_doc_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15"}, "7d1603c3-1bf2-453e-b9e9-5616896ab5ea": {"doc_hash": "f30400ea5c39a8dda2862c4f5a6ed31adb53965d1dfd780b04bd6e1abb680842", "ref_doc_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4"}, "e73b2524-cd6a-4cde-bd02-ca033aff2175": {"doc_hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "ref_doc_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4"}, "3316462b-be37-4152-a821-a2024ad0fca3": {"doc_hash": "5b6cd1ec36c79c626e603648645c27d2c8bdf7405d134fa5ee8d442654faaccb", "ref_doc_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4"}, "2795c181-5047-471d-ba11-0b48062aed3c": {"doc_hash": "f5cd5448c9452cbe6dbac1ac03e442b08b61542ba0c8aa8aaaffc80752815b7e", "ref_doc_id": "9133d5b3-b632-40b6-b387-ac0329d734a4"}, "f6edfd89-c126-44b0-a3a1-72394bb84c8c": {"doc_hash": "275baea2efd06dc7d04371ac8ef8f549db38d8eac1441e281d806a98e93a9ea8", "ref_doc_id": "9133d5b3-b632-40b6-b387-ac0329d734a4"}, "202f20dd-0083-4fed-b098-19c2a7e14b05": {"doc_hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb", "ref_doc_id": "554e003a-18f7-4226-84fe-3021466167a3"}, "5b670c60-7ad0-420b-9ca5-3d0d78cc0186": {"doc_hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073", "ref_doc_id": "817bba4f-a97f-4a48-90a0-de713b5685c6"}, "061e8f8a-fb51-4d61-bc52-5234557555aa": {"doc_hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160", "ref_doc_id": "506cce11-d72a-405c-a08a-4ef5f19ad9bc"}, "9895c678-e654-479a-9334-f328d52c3c90": {"doc_hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28", "ref_doc_id": "0996ba57-66b0-4db1-b8b3-cd9fa2e3b876"}, "dbc872ae-050b-4958-b89a-c9dc31e20af4": {"doc_hash": "00119033c6c21ba634c0f31ec079ed765db6f54b4f9569e4c2947c753f6644b0", "ref_doc_id": "297ac6a3-89db-4283-91b6-8ff124b88fd0"}, "c3c7e4b2-e0be-4da7-bd73-5dc10b360a1b": {"doc_hash": "62816e0b531a77b544cbbfd001f5c8c7767e6aa087dc8bc5a6d4f92170b12b5e", "ref_doc_id": "297ac6a3-89db-4283-91b6-8ff124b88fd0"}, "77083868-7a42-40f0-bcb1-256f532565ba": {"doc_hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2", "ref_doc_id": "765b95d7-7a8e-473e-9cbe-e435fe78268a"}, "d806e310-b5a0-4251-984a-5ac4f109537e": {"doc_hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451", "ref_doc_id": "c478a956-8f9e-492d-a6c1-6ae1cd7e6a9a"}, "78bffaad-0be5-4871-9c47-4aabb1dd228f": {"doc_hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc", "ref_doc_id": "43782e9d-aa09-436f-a65d-7b7a5b82c66e"}}, "docstore/data": {"53de1f52-55a7-4ee8-acb3-ee46a58ec801": {"__data__": {"id_": "53de1f52-55a7-4ee8-acb3-ee46a58ec801", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fdbe99b-a72f-4ea9-9d58-1efbbc0e8af8", "node_type": "4", "metadata": {}, "hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b", "class_name": "RelatedNodeInfo"}}, "text": "# ColPali: Efficient Document Retrieval with Vision Language Models\n\n# Manuel Faysse* 1,3 Hugues Sibille\u22171,4 Tony Wu\u22171 Bilel Omrani1\n\n# Gautier Viaud1 C\u00e9line Hudelot3 Pierre Colombo2,3\n\n# 1Illuin Technology 2Equall.ai\n\n# 3CentraleSup\u00e9lec, Paris-Saclay 4ETH Z\u00fcrich\n\n# manuel.faysse@centralesupelec.fr\n\n# Abstract\n\nDocuments are visually rich structures that convey information through text, as well as tables, figures, page layouts, or fonts. While modern document retrieval systems exhibit strong performance on query-to-text matching, they struggle to exploit visual cues efficiently, hindering their performance on practical document retrieval applications such as Retrieval Augmented Generation. To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieving tasks spanning multiple domains, languages, and settings. The inherent shortcomings of modern systems motivate the introduction of a new retrieval model architecture, ColPali, which leverages the document understanding capabilities of recent Vision Language Models to produce high-quality contextualized embeddings solely from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically faster and end-to-end trainable. We release all project artifacts at https://huggingface.co/vidore.\n\n# 1 Introduction\n\nDocument Retrieval consists in matching a user query to relevant documents in a given corpus. It is central to many industrial applications, either as a standalone ranking system (search engines) or as part of more complex information extraction or Retrieval Augmented Generation (RAG) pipelines. Over recent years, pretrained language models have enabled large improvements in text embedding models. In practical industrial settings, however, the main performance bottleneck for efficient document retrieval is not in embedding model performance but in the prior data ingestion pipeline. To optimize a standard PDF document, many steps are required. First, PDF parsers or Optical Character Recognition (OCR) systems are used to extract words from the pages. Document layout detection models can then be run to segment paragraphs, titles, and other page objects such as tables, figures, and headers. A chunking strategy is then defined to group text passages with some semantic coherence, and modern retrieval setups may even integrate a captioning step to describe visually rich elements in a natural language form, more suitable for embedding models. In our experiments (Table 2), we typically find that optimizing the ingestion pipeline yields much greater performance on visually rich document retrieval than optimizing the text embedding model.\n\n# Contribution 1: ViDoRe\n\nIn this work, we argue that document retrieval systems should not be evaluated solely on the capabilities of text embedding models (Bajaj et al., 2016; Thakur et al., 2021; Muennighoff et al., 2022), but should also", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab723b02-84f5-4cd1-b044-da180d75c1de": {"__data__": {"id_": "ab723b02-84f5-4cd1-b044-da180d75c1de", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7da3d723-b838-417c-bbe8-a24a50b96ea4", "node_type": "4", "metadata": {}, "hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d", "class_name": "RelatedNodeInfo"}}, "text": "# Standard Retrieval\n\n# m 0.66 NDCG@5\n\n|Method|Offline|Online|Similarity Score|\n|---|---|---|---|\n|ColPali (ours)|0.81 NDCG@5|Vision LLM| |\n|MaxSim| |What| |\n|Page|7.22s|Query|22ms|\n|OCR Detection| | | |\n\nFigure 2: ColPali simplifies document retrieval w.r.t. standard retrieval methods while achieving stronger performances with better latencies. Latencies and results are detailed in section 5 and subsection B.5.\n\nConsider the context and visual elements of the documents to be retrieved. To this end, we create and openly release ViDoRe, a comprehensive benchmark to evaluate systems on page-level document retrieval with a wide coverage of domains, visual elements, and languages. ViDoRe targets practical document retrieval settings, in which user queries may require both textual and visual understanding to be correctly matched to relevant documents. We highlight the shortcomings of current text-centric systems in these settings.1\n\n# Contribution 2: ColPali\n\nWe propose a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents purely from their visual features, allowing for subsequent fast query matching with late interaction mechanisms (Khattab and Zaharia, 2020). Our method, ColPali, outperforms all other retrieval systems on ViDoRe while being fast and end-to-end trainable. We release models and code at https://huggingface.co/vidore.\n\n# 2 Problem Formulation & Related Work\n\n# Problem Setting\n\nIn our setting, a retrieval system scores how relevant a document d from corpus D is with respect to a query q. Computing the similarity score s(q, d) \u2208 R+ for each of the |D| documents in the corpus creates a ranking we can use to extract the most relevant documents. In this work, we focus on page-level retrieval: given a query, is the correct document page retrieved by the system? For coherence with existing literature, we further use the term document to refer to individual pages, i.e. the atomic retrieved elements in our setting. As we focus on practical industrial retrieval applications (RAG, search engines) with potentially large corpora sizes, latency constraints are imposed on scoring systems. Most current retrieval systems can be decomposed into (1) an offline indexation phase in which a document index is built and (2) an online querying phase in which a query is matched to documents from the index and where low latency is vital to the user experience.\n\nEfficient document retrieval systems exhibit joint properties of high retrieval performance (R1), low latency during querying (R2), and high throughput during indexation (R3).\n\n# 2.1 Textual Retrieval Methods\n\n# Document Retrieval in Text Space\n\nStatistical methods based on word frequency like TF-IDF (Sparck Jones, 1972) and BM25 (Robertson et al., 1994) are still widely used due to their simplicity.\n\n1 The benchmark leaderboard is hosted publicly at https://huggingface.co/spaces/vidore/vidore-leaderboard to encourage further developments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67bb8be7-226b-42ad-990a-f0c7d684cf67": {"__data__": {"id_": "67bb8be7-226b-42ad-990a-f0c7d684cf67", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60e079ef-a199-49c3-ac39-40f24a2eccfa", "node_type": "4", "metadata": {}, "hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f", "class_name": "RelatedNodeInfo"}}, "text": "# 2.2 Integrating Visual features\n\n# Contrastive Vision Language Models\n\nMapping latent representations of textual content to corresponding representations of visual content has been done by aligning disjoint visual and text encoders through contrastive losses (Radford et al., 2021; Zhai et al., 2023). While some OCR capabilities exist in these models, the visual component is often not optimized for text understanding. The Fine-grained Interactive Language-Image Pre-training (Yao et al., 2021) framework extends the late interaction mechanism to cross-modal vision-language models, relying on max similarity operations between text tokens and image patches.\n\n# Visually Rich Document Understanding\n\nTo go beyond text, some document-focused models jointly encode text tokens alongside visual or document layout features (Appalaraju et al., 2021; Kim et al., 2021; Huang et al., 2022; Tang et al., 2022). Large Language transformer Models (LLMs) with strong reasoning capabilities have recently been combined with Vision Transformers (ViTs) (Dosovitskiy et al., 2020) to create VLMs (Alayrac et al., 2022; Liu et al., 2023b; Bai et al., 2023; Lauren\u00e7on et al., 2024) where image patch vectors from contrastively trained ViT models (Zhai et al., 2023) are fed as input embeddings to the language model and concatenated with the text-token embeddings.\n\n# PaliGemma\n\nThe PaliGemma-3B model (Lucas Beyer* et al., 2024) extends concepts from Pali3 (Chen et al., 2023), and projects SigLIP-So400m/14 (Alabdulmohsin et al., 2023) patch embeddings into Gemma-2B\u2019s text vector space (Gemma Team et al., 2024). Along with its reasonable size w.r.t. other performant VLMs, an interesting property of PaliGemma\u2019s text model is that it is fine-tuned with full-block attention on the prefix (instruction text and image tokens). VLMs display enhanced capabilities in Visual Question Answering, captioning, and document understanding (Yue et al., 2023), but are not optimized for retrieval tasks.\n\n# 3 The ViDoRe Benchmark\n\nExisting benchmarks for contrastive vision-language models primarily evaluate retrieval for natural images (Lin et al., 2014; Borchmann et al., 2021; Thapliyal et al., 2022). On the other hand, textual retrieval benchmarks (Muennighoff et al., 2022) are evaluated at the textual passage level and are not tailored for document retrieval tasks. We fill the gap with ViDoRe, a comprehensive benchmark for document retrieval using visual features.\n\n# 3.1 Benchmark Design\n\nViDoRe is designed to comprehensively evaluate retrieval systems on their capacity to match queries to relevant documents at the page level. This benchmark encompasses multiple orthogonal subtasks, with focuses on various modalities - text, figures, infographics, tables; thematic domains - medical,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96a27d0c-a728-4b25-89e5-dda0dc780acc": {"__data__": {"id_": "96a27d0c-a728-4b25-89e5-dda0dc780acc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "128bdb06-8cf0-4df1-b56d-fdd8ffd2769d", "node_type": "4", "metadata": {}, "hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe", "class_name": "RelatedNodeInfo"}}, "text": "# Dataset\n\n|Dataset|# Queries|Domain|\n|---|---|---|\n|Academic Tasks| | |\n|DocVQA (eng)|500 (500)|Industrial|\n|InfoVQA (eng)|500 (500)|Infographics|\n|TAT-DQA (eng)|1600 (1600)|Varied Modalities|\n|arXiVQA (eng)|500 (500)|Scientific Figures|\n|TabFQuAD (fra)|210 (210)|Tables|\n|Practical Tasks| | |\n|Energy (eng)|100 (1000)|Scientific|\n|Government (eng)|100 (1000)|Administrative|\n|Healthcare (eng)|100 (1000)|Medical|\n|AI (eng)|100 (1000)|Scientific|\n|Shift Project (fra)|100 (1000)|Environment|\n\nTable 1: ViDoRe comprehensively evaluates multimodal retrieval methods. The size of the document corpus is indicated in parentheses.\n\n# Academic Tasks\n\nWe repurpose widely used visual question-answering benchmarks for retrieval tasks: for each page-question-answer triplet, we use the question as the query, and the associated page as the gold document (Table 1). These academic datasets either focus on single specific modalities (Mathew et al., 2020, 2021; Li et al., 2024) or target more varied visually rich documents (Zhu et al., 2022). Moreover, we consider TabFQuAD, a human-labeled dataset on tables extracted from French industrial PDF documents released with this work. Details can be found in subsection A.1.\n\n# Practical tasks\n\nWe construct topic-specific retrieval benchmarks spanning multiple domains to go beyond repurposed QA datasets and evaluate retrieval in more realistic industrial situations (e.g. RAG). To achieve this, we collect publicly accessible PDF documents and generate queries pertaining to document pages using Claude-3 Sonnet, a high-quality proprietary vision-language model (Anthropic, 2024). In total, we collect 1,000 document pages per topic, which we associate with 100 queries extensively filtered for quality and relevance by human annotators. The corpus topics are intentionally specific to maximize syntactic proximity between documents, creating challenging retrieval tasks and covering an array of orthogonal domains (Table 1). Query-page pair examples are shown in Appendix E.2.\n\n# Evaluation Metrics\n\nWe evaluate performance on our benchmark (Requirement R1) using standard metrics from the retrieval literature (NDCG, Recall@K, MRR). We report NDCG@5 values as the main performance metric in this work and release the complete sets of results along with the models.\n\nTo validate compliance with practical industrial constraints, we also consider query latencies (R2) and indexing throughputs (R3).\n\n# 3.2 Assessing Current Systems\n\n# Unstructured\n\nWe evaluate retrieval systems representative of those found in standard industrial RAG pipelines. As is common practice, we rely on the Unstructured off-the-shelf tool in the highest resolution settings to construct high-quality text chunks from PDF documents. Unstructured orchestrates the document parsing pipeline, relying on deep learning vision models to detect titles and document layouts (Ge et al., 2021), OCR engines (Smith, 2007) to extract text in non-native PDFs, specialized methods or models to detect and reconstruct tables, and implements a chunking strategy (by-title) that leverages the detected document structure to preserve section boundaries when concatenating texts. As is common practice, in our simplest Unstructured configuration (text-only), only textual elements are kept, and figures, images, and tables are considered noisy information and are filtered out.\n\n# Unstructured + X\n\nWhile Unstructured is a strong baseline by itself, we further augment Unstructured\u2019s output by integrating the visual elements. In (+ OCR), tables, charts, and images are run through an OCR engine, processed by Unstructured, and chunked independently. In (+ Captioning), we set up a fully-fledged captioning strategy (Zhao et al., 2023), in which we feed visual elements to a strong proprietary Vision Language Model (Claude-3 Sonnet (Anthropic, 2024)) to obtain highly detailed textual descriptions of the elements. Both strategies aim to integrate visual elements in the retrieval pipeline but incur significant latency and resource costs (subsection 5.2).\n\n# Embedding Model\n\nTo embed textual chunks, we evaluate Okapi BM25, the de facto standard sparse statistical retrieval method, and the dense encoder of BGE-M3 (Chen et al., 2024), a multilingual neural method with SOTA performance in its size category. Chunks are embedded and scored independently, and page-level scores are obtained by.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7485555b-1e26-4200-be0d-c7adb35c5fa1": {"__data__": {"id_": "7485555b-1e26-4200-be0d-c7adb35c5fa1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "689b0e3e-8117-4195-8759-dc5bb0960102", "node_type": "4", "metadata": {}, "hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29fed939-f494-4404-9c83-59f8cc3f195a", "node_type": "1", "metadata": {}, "hash": "7e3111e040a674730fa702d7902f35c16b8c433334150ac0ae151ea4fab47ce1", "class_name": "RelatedNodeInfo"}}, "text": "max-pooling over the page\u2019s chunk scores.5\n\n# Contrastive VLMs\n\nWe also evaluate the strongest available vision-language embedding models; Jina CLIP (Koukounas et al., 2024), Nomic Embed Vision (Nomic, 2024), and SigLIP-So400m/14 (Alabdulmohsin et al., 2023).\n\n# Results\n\nFrom a performance perspective, best results are obtained by combining the Unstructured parser with visual information, either from captioning strategies or by running OCR on the visual elements (Table 2). Little difference is seen between BM25 and BGE-M3 embeddings highlighting the visual information bottleneck. Contrastive VLMs lag behind. Beyond retrieval performance (R1), the indexing latencies (R2) reported in Figure 3 illustrate that PDF parsing pipelines can be very lengthy, especially when incorporating OCR or captioning strategies. Querying latencies at runtime (R3) are very good for all evaluated systems (\u2264 22 ms on NVIDIA L4) due to fast query encoding and cosine similarity matching.\n\n|PDF Parser|Latency (s)|\n|---|---|\n|PDF Parser|(7.22s)|\n|Siglip|(0.12s)|\n|ColPali|(0.39s)|\n\n0 1 2 3 4 5 6 7\n\nLatency (s)\n\nLayout Detection OCR Captioning Page Encoding\n\n# Figure 3\n\nOffline indexing with ColPali is much simpler and faster compared to standard retrieval methods. Indexing speeds reported are computed on Nvidia L4 GPUs and detailed in subsection B.5.\n\n# 4 Late interaction based Vision Retrieval\n\n# 4.1 Architecture\n\nVision-Language Models. Encouraged by their strong document understanding capabilities, we propose adapting recent VLMs for retrieval. The key concept is to leverage the alignment between output embeddings of text and image tokens acquired during multi-modal finetuning. To this extent, we introduce ColPali, a Paligemma-3B extension that is capable of generating ColBERT-style multi-vector representations of text and images (Figure 2). PaliGemma-3B is a strong candidate due to its small size, the many released checkpoints fine-tuned for different image resolutions and tasks,\n\n5We empirically validated the max-pooling strategy over sub-page chunks to be more effective than concatenating all page chunks before embedding pagewise.\n\nand the promising performances on various document understanding benchmarks. We add a projection layer to map the output language modeling embeddings to a vector space of reduced dimension D = 128 as used in the ColBERT paper (Khattab and Zaharia, 2020) to keep lightweight bag-of-embedding representations.\n\n# Late Interaction\n\nGiven query q and document d, we denote as Eq \u2208 RNq \u00d7D and Ed \u2208 RNd\u00d7D their respective multi-vector representation in the common embedding space RD. The late interaction operator, LI (q, d), is the sum over all query vectors Ed(j), of its maximum dot product \u27e8\u00b7|\u00b7\u27e9 with each of the Nd document embedding vectors Ed(1:Nd).\n\nLI (q, d) = \u2211i\u2208[|1,Nq|] maxj\u2208[|1,Nd|] \u27e8Eq(i)|Ed(j)\u27e9\n\n# Contrastive Loss\n\nThe Late Interaction operation is fully differentiable, enabling backpropagation. Let a batch {qk, dk}k\u2208[|1,b|] composed of b query-page pairs, where for all k \u2208 [|1, b|], the document page dk is the document corresponding to query qk. Following Khattab and Zaharia (2020), we define our in-batch contrastive loss L as the softmaxed cross-entropy of the positive scores\n\nL+ = LI (dk, qk) w.r.t. to the maximal negative scores L- = maxl,l\u2260k LI (qk, pl).\n\n# 4.2 Model training\n\nDataset. Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages6. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nParameters. All models are trained for 1 epoch on the train set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29fed939-f494-4404-9c83-59f8cc3f195a": {"__data__": {"id_": "29fed939-f494-4404-9c83-59f8cc3f195a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "689b0e3e-8117-4195-8759-dc5bb0960102", "node_type": "4", "metadata": {}, "hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7485555b-1e26-4200-be0d-c7adb35c5fa1", "node_type": "1", "metadata": {}, "hash": "b5899512ab126ca7b0cc5fe2a825e271dfbea6ea3e49469597376829a500a0db", "class_name": "RelatedNodeInfo"}}, "text": "to the maximal negative scores L- = maxl,l\u2260k LI (qk, pl).\n\n# 4.2 Model training\n\nDataset. Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages6. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nParameters. All models are trained for 1 epoch on the train set. Unless specified otherwise, we train models in bfloat16 format, use low-rank adapters (LoRA, Hu et al. (2021)) with \u03b1 = 32 and r = 32 on the transformer layers from the language model,\n\n6Multilingual data is present in the pretraining corpus of the language model (Gemma-2B) and potentially occurs during PaliGemma-3B\u2019s multimodal training.", "mimetype": "text/plain", "start_char_idx": 3266, "end_char_idx": 4349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04708a6c-cbd4-40e2-8ea8-cc6581ba3884": {"__data__": {"id_": "04708a6c-cbd4-40e2-8ea8-cc6581ba3884", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2b2a7b7-fcec-4af1-b893-a7a5f1b6fb6d", "node_type": "4", "metadata": {}, "hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4", "class_name": "RelatedNodeInfo"}}, "text": "# Results\n\n# 5.1 Performance (R1)\n\nWe iteratively construct ColPali, starting from an off-the-shelf SigLIP model (Table 2).\n\n# BiSigLIP: Improving a strong model.\n\nSigLIP7 is a strong vision-language bi-encoder model, pre-trained on the English split of WebLI (Chen et al., 2023), a corpus of billions of image-text pairs. We find that SigLIP largely outperforms both Jina CLIP and Nomic-vision on document retrieval tasks. Further fine-tuning the textual component of this model on our document-oriented dataset (BiSigLIP) yields clear improvements across the board, particularly on figure retrieval (ArxivQA) and table retrieval tasks (TabFQuAD).\n\n# BiPali: Pairing with a language model.\n\nIn the PaliGemma model architecture, SigLIP-generated patch embeddings are fed to a text language model to obtain LLM contextualized output patch embeddings.8 We average pool these representations to obtain a single dense vector, effectively creating a PaliGemma bi-encoder model (BiPali). After fine-tuning on the training dataset, we obtain a model that performs slightly worse in English than the tuned BiSigLIP variant. This can be explained by the fact that contrary to SigLIP, the original PaliGemma is not trained on contrastive matching tasks, but rather on next token prediction. Our contrastive fine-tuning phase on 100K images to transform PaliGemma into a bi-encoder is 5 orders of magnitude smaller than SigLIP\u2019s original contrastive training. However, we see notable improvements in French tasks, indicating that BiPali\u2019s LLM (Gemma 2B) helps multilingual text understanding. This is particularly notable as our training dataset does not contain non-English samples.\n\n# ColPali: Adding Late Interaction.\n\nOne benefit of inputting image patch embeddings through a language model is that they are natively mapped to a latent space similar to textual input (query). This enables leveraging the ColBERT strategy to compute interactions between text tokens and image patches, which enables a step-change improvement in performance compared to BiPali. Results in Table 2 show that our ColPali model also largely outperforms the strong baselines based on Unstructured and captioning, as well as all evaluated text-image embedding models. The difference is particularly stark on the more visually complex benchmark tasks, such as InfographicVQA, ArxivQA, and TabFQuAD representing respectively infographics, figures, and tables. However, text-centric documents are also better retrieved by the ColPali models across all evaluated domains and languages, making our approach the overall best-performing document-retrieval model.\n\n# Negative Results.\n\nFor extensiveness, we also train ColSigLIP, a late interaction variant of the BiSigLIP model but obtain abysmal performances. We attribute this to the large gaps w.r.t. SigLIP\u2019s pre-training, in which only a pooled latent representation is used in the contrastive loss, which does not optimize the representations of individual patch and token embeddings. Similarly, we train a BiSigLIPP aliGemma variant, in which we retrieve the image representations from the SigLIP model that has been further updated by PaliGemma fine-tuning, and use the text representations from PaliGemma\u2019s text model. After fine-tuning on our dataset, performance is severely inferior to SigLIP vanilla which simply encodes with SigLIP\u2019s original text and vision components. This indicates a logical misalignment between SigLIP embeddings, and Gemma embeddings after PaliGemma training. We detail these results in Table 5.\n\n# 5.2 Latencies & Memory Footprint\n\n# Online Querying. (R2)\n\nLogically, querying latencies differ between ColPali and a BGE-M3 embedding model. For BGE, encoding takes about 22 ms for 15 tokens, while encoding a query with ColPali\u2019s language model takes about 30 ms9. For smaller corpus sizes, computing the late interaction operation induces marginally small overheads (\u2248 1 ms per 1000 pages in the corpus), and the cosine similarity computation between bi-encoder vectors.\n\n9Computed for a batch size of 1 (online), and averaged over 1000 queries. See subsection B.5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a223960-346f-4cef-a8bc-87e7b0ecc114": {"__data__": {"id_": "9a223960-346f-4cef-a8bc-87e7b0ecc114", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "292d5589-3184-4ff3-b433-03daeaee1cba", "node_type": "4", "metadata": {}, "hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f24de3e-503b-4b1f-a34c-49acca2618cd", "node_type": "1", "metadata": {}, "hash": "231a44d96b44ef7cef97f4d52fc7bf0c58617c4a316ef1d4aacccf09c35c8c27", "class_name": "RelatedNodeInfo"}}, "text": "# Table 2: Comprehensive evaluation of baseline models and our proposed method on ViDoRe.\n\nResults are presented using NDCG@5 metrics, and illustrate the impact of different components. Text-only metrics are not computed for benchmarks with only visual elements.\n\n|Model|Metrics| | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy Gov.| |Health.|Avg.| |\n|- BM25|-|34.1|-|-|44.0|59.6|90.4|78.3|78.8|82.6| | |\n|- BGE-M3|-|28.4\u21935.7|-|-|36.1\u21937.9|68.5\u21918.9|88.4\u21932.0|76.8\u21931.5|77.7\u21931.1|84.6\u21912.0| | |\n|- BM25|31.6|36.8|62.9|46.5|62.7|64.3|92.8|85.9|83.9|87.2|65.5| |\n|- BGE-M3|31.4\u21930.2|25.7\u219311.1|60.1\u21932.8|70.8\u219124.3|50.5\u219312.2|73.2\u21918.9|90.2\u21932.6|83.6\u21932.3|84.9\u21911.0|91.1\u21913.9|66.1\u21910.6| |\n|- BM25|40.1|38.4|70.0|35.4|61.5|60.9|88.0|84.7|82.7|89.2|65.1| |\n|- BGE-M3|35.7\u21934.4|32.9\u21935.4|71.9\u21911.9|69.1\u219133.7|43.8\u219317.7|73.1\u219112.2|88.8\u21910.8|83.3\u21931.4|80.4\u21932.3|91.3\u21912.1|67.0\u21911.9| |\n|Contrastive VLMs|Jina-CLIP|25.4|11.9|35.5|20.2|3.3|3.8|15.2|19.7|21.4|20.8|17.7|\n|Nomic-vision|17.1|10.7|30.1|16.3|2.7|1.1|12.9|10.9|11.4|15.7|12.9| |\n|SigLIP (Vanilla)|43.2|30.3|64.1|58.1|26.2|18.7|62.5|65.7|66.1|79.1|51.4| |\n|Ours|SigLIP (Vanilla)|43.2|30.3|64.1|58.1|26.2|18.7|62.5|65.7|66.1|79.1|51.4|\n|BiSigLIP (+fine-tuning)|58.5\u219115.3|32.9\u21912.6|70.5\u21916.4|62.7\u21914.6|30.5\u21914.3|26.5\u21917.8|74.3\u219111.8|73.7\u21918.0|74.2\u21918.1|82.3\u21913.2|58.6\u21917.2| |\n|BiPali (+LLM)|56.5\u2193-2.0|30.0\u2193-2.9|67.4\u2193-3.1|76.9\u219114.2|33.4\u21912.9|43.7\u219117.2|71.2\u2193-3.1|61.9\u2193-11.7|73.8\u2193-0.4|73.6\u2193-8.8|58.8\u21910.2| |\n|ColPali (+Late Inter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f24de3e-503b-4b1f-a34c-49acca2618cd": {"__data__": {"id_": "9f24de3e-503b-4b1f-a34c-49acca2618cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "292d5589-3184-4ff3-b433-03daeaee1cba", "node_type": "4", "metadata": {}, "hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a223960-346f-4cef-a8bc-87e7b0ecc114", "node_type": "1", "metadata": {}, "hash": "e95fa380b755b4c34d72a363da79a8c57174e8d488e9a48e2fd996853cc786e8", "class_name": "RelatedNodeInfo"}}, "text": ")|79.1\u219122.6|54.4\u219124.5|81.8\u219114.4|83.9\u21917.0|65.8\u219132.4|73.2\u219129.5|96.2\u219125.0|91.0\u219129.1|92.7\u219118.9|94.4\u219120.8|81.3\u219122.5| |\n\n# 5.3 Interpretability\n\nBy superimposing the late interaction heatmap on top of the original image, we can visualize the most salient image patches with respect to each term of the query, yielding interpretable insights into model focus zones. As epitomized in Figure 1, we observe ColPali exhibits strong OCR capabilities as both the words \"hourly\" and \"hours\" present a high similarity score with the query token &lt;_hour&gt;. We also note particular focus on other non-trivial image features such as the x-axis representing hours being salient. Other visualization examples with similar trends of the model transcending pure OCR are shown in Appendix C.\n\n# 6 Ablation study\n\nShould we scale models or patch numbers? We train a variant of PaliGemma with half the number of image patches (512). While there is a clear performance degradation w.r.t. to the 1024-patch ColPali model (Figure 4), memory usage is much lower.\n\nWhile another PaliGemma variant exists with 2048 patches, the different training datamix and the large memory requirements make this model impractical for both training.", "mimetype": "text/plain", "start_char_idx": 1511, "end_char_idx": 2719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b91907ea-bb44-484d-9ea0-eab8caaecb6a": {"__data__": {"id_": "b91907ea-bb44-484d-9ea0-eab8caaecb6a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0570d2b-7f87-4bb8-a029-0c83c23aea38", "node_type": "4", "metadata": {}, "hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84", "class_name": "RelatedNodeInfo"}}, "text": "# Relative NDCG@5 (%)\n\n| |ColPali|Idefics2|No Mem.|Full IB|Train|TabF|\n|---|---|---|---|---|---|---|\n|(512)|(64)|Tokens|Loss|Vision|Tuning| |\n\nFigure 4: Relative NDCG@5 performance gain w.r.t. the default ColPali (1024 patches). TabFQuAD fine-tuning measures the performance difference on the TabFQuAD task after the introduction of targeted data in the training set. All other results refer to performance deltas averaged on all ViDoRe tasks.\n\nIdefics2-8B (Lauren\u00e7on et al., 2024), a VLM with a similar architecture and based on a Mistral-7B (Jiang et al., 2023) language backbone and a SigLIP vision encoder paired with a perceiver resampler. The most notable differences with PaliGemma lie in the size of the language model (2B and 7B resp.) and the number of image patches (between 512 and 2048 for PaliGemma, and 64 post-resampling for Idefics212). Our results (Figure 4) suggest language model size has a strong impact on performance, and along with the trained resampler enables more efficient representations for smaller numbers of image embeddings - ColIdefics2 with 64 patches edges out ColPali with 512 patches. Scaling the number of patches of the smaller ColPali model from 512 to 1024, enables largely surpassing the 60-patch ColIdefics2 while being about twice as fast in terms of training and inference latency. These results suggest there are tradeoffs between performance (R1), latencies during online querying (R2) and offline indexation phases (R3), and index memory size.\n\n# Should we fine-tune the vision component?\n\nWe run our contrastive finetuning on a ColPali model in which we also train the vision encoder and the projection layer. Results in Figure 4 show this leads to no significant improvements.\n\n# Do \"query augmentation\" tokens help?\n\nIn ColBERT, special tokens are concatenated to the input query to serve as soft query augmentation buffers. Training without these tokens, we observe no significant performance difference (Figure 4) in the English benchmarks. However, performance on the French tasks seems to improve (Table 5) and inference time.\n\nWith the option of adding 4 sub-image crops of 64 tokens each to the sequence, for a total of 320 tokens.\n\n# Is the Pairwise CE loss best?\n\nTraining with an in-batch negative contrastive loss, instead of the pairwise CE loss that only considers the hardest negative sample, leads to a slight performance degradation (\u22122.4%) on the aggregated benchmark.\n\n# Can the model adapt to new tasks?\n\nContrary to more complex multi-step retrieval pipelines, ColPali can be trained end-to-end, directly optimizing the downstream retrieval task which greatly facilitates fine-tuning to boost performance on specialized domains, multilingual retrieval, or specific visual elements the model struggles with. To demonstrate, we add 1552 samples representing French tables and associated queries to the training set. This represents the only French data in the training set, with all other examples being kept unchanged. We see significant NDCG@5 improvements (Figure 4) and even starker Recall@1 gains (+6.63%) on the TabFQuAD benchmark, with no performance degradation on the rest of the benchmark tasks (+0.34%).\n\n# 7 Conclusions\n\nThrough the conception of a new benchmark ViDoRe, we established the limits of both modern industrial document retrieval pipelines and off-the-shelf image-text contrastive models for visually rich document retrieval. We introduced ColPali, a novel retrieval model that leverages the latest generative Vision Language models to create highly performing multi-vector embeddings purely from visual document features. ColPali largely outperforms the best existing document retrieval methods while enabling faster corpus indexing time and maintaining low querying latencies, suggesting a very high potential for industrial document retrieval applications. We hope to encourage future work by publicly releasing the ViDoRe benchmark and all models and baselines from our study.\n\n# Future Work\n\nFurther performance gains could be obtained by exploring sub-image decomposition (Liu et al., 2023a), optimal image patch resampling strategies (Lauren\u00e7on et al., 2024), or hard-negative mining. Subsequently, our vision is to combine visual retrieval and visually grounded query answering to create RAG systems that purely function from visual features. An interesting line of research could be attempting to generate answers leveraging information stored in the indexed multi-vector patch embeddings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49c440da-96b9-4eaa-a6ae-f8e875b0490d": {"__data__": {"id_": "49c440da-96b9-4eaa-a6ae-f8e875b0490d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69d4d7ff-482f-4aa8-afeb-114aae7cfe25", "node_type": "4", "metadata": {}, "hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0", "class_name": "RelatedNodeInfo"}}, "text": "# Limitations\n\nFocus. In this work, we evaluate models on document retrieval tasks, covering several modalities (figures, text, tables, infographics). We however primarily focus on PDF-type documents, and evaluating systems on image retrieval with documents stemming from web page screenshots or handwritten documents might be an interesting generalization. We also focus on high-resource languages (English and French) and although we have shown the capacity of the ColPali model to generalize to languages outside of its fine-tuning set, it is unclear how the model would perform on languages that are not as represented in the model\u2019s language backbone. Finally, our setup assumes relevant documents exist, but abstention methods for Information Retrieval systems might be interesting to explore in more practical settings in which confidence estimation might be important (Gisserot-Boukhlef et al., 2024).\n\nSupport. This work relies on multi-vector retrieving derived from the ColBERT late interaction mechanism. Although some vector databases support late interaction engines13, many widely used vector retrieval frameworks do not propose native multi-vector support, and some engineering infrastructure efforts may be required to adapt them to work with ColPali (or ColBERT) models.\n\nData. In the creation of ViDoRe, we partially rely on synthetic query generation based on a commercial large language model, which may induce some amount of bias in the generated queries. To compensate for this, we have iterated on the prompting strategy and given real query examples to the models to help ground generation in realistic settings. We have further manually verified all synthetic queries through a lengthy process to validate their relevance and their quality. Our benchmark also includes many benchmark tasks with no synthetic data, and result trends observed between all tasks are correlated, further confirming the coherence of our benchmark design.\n\n# Ethical Considerations\n\nCarbon Footprint. Our work fully leverages prior pretrained models and training is not particularly compute-intensive. Furthermore, we rely on low-rank adapters to further reduce the computational resources needed, both during training and for storage. Overall, a training run represents about 40 hours of Mi250x AMD GPUs. Our experiments, in total, represent 1405 Mi250x GPU hours from highly efficient compute clusters running on low-carbon nuclear energy, representing a total of around 15kg CO2 eq.\n\nImpact. We believe our work could have a strong impact on improving industrial document retrieval systems. Our method is efficient, performs well, and the additional support towards visually rich information from documents could go a long way in unlocking knowledge sources previously difficult to index or query.\n\nResource Release. For transparency, and to foster future work, we release our comprehensive benchmark under open license and host a public leaderboard14. Our models are released under the same usage license as the base model (Gemma Research license for ColPali, Apache2.0 for ColIdefics2) and should be used as intended by the VLM license.\n\n# Acknowledgements\n\nThis work is partially supported by Illuin Technology, and by a grant from ANRT France. This work was performed using HPC resources from the CINES ADASTRA through Grant 2024-AD011015443. We extend our warm thanks to Jonathan Dong, Caio Corro, Victor Pellegrain and Ender Konukoglu for their valuable feedback on the paper.\n\n# References\n\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. 2023. Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. Publisher: arXiv Version Number: 5.\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a Visual Language Model for Few-Shot Learning. Publisher: arXiv Version Number: 2.\n\nAnthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku.\n\n14https://huggingface.co/spaces/vidore/vidore-leaderboard", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0834c725-cb08-4a22-9914-c39947db97f7": {"__data__": {"id_": "0834c725-cb08-4a22-9914-c39947db97f7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1964f7aa-67af-4dcf-ac5b-e38fd8537b77", "node_type": "1", "metadata": {}, "hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha. 2021. DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint. Version Number: 2.\n\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. Publisher: arXiv Version Number: 3.\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv preprint. Version Number: 3.\n\nBurton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422\u2013426. Place: New York, NY, USA Publisher: Association for Computing Machinery.\n\n\u0141ukasz Borchmann, Micha\u0142 Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina Szyndler, and Filip Grali\u0144ski. 2021. DUE: End-to-End Document Understanding Benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv preprint. Version Number: 3.\n\nXi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. 2023. PaLI-3 Vision Language Models: Smaller, Faster, Stronger. arXiv preprint. Version Number: 2.\n\nCohere. 2024. Introducing Rerank 3: A New Foundation Model for Efficient Enterprise Search & Retrieval.\n\nTimoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. 2023. Vision Transformers Need Registers. Publisher: [object Object] Version Number: 2.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Publisher: arXiv Version Number: 2.\n\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX: Exceeding YOLO Series in 2021. arXiv preprint. Version Number: 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1964f7aa-67af-4dcf-ac5b-e38fd8537b77": {"__data__": {"id_": "1964f7aa-67af-4dcf-ac5b-e38fd8537b77", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0834c725-cb08-4a22-9914-c39947db97f7", "node_type": "1", "metadata": {}, "hash": "73a1d174247c6265657797b59c09c64dc8a0dbbe89ce49cf76e1b363e58e5267", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2ba09d9-02c0-4b40-b767-fd4af26b92b4", "node_type": "1", "metadata": {}, "hash": "a33129b68cd64ada00b9243cecd3233f935c166bfedda87a4cb2921a33d65367", "class_name": "RelatedNodeInfo"}}, "text": "2023. Vision Transformers Need Registers. Publisher: [object Object] Version Number: 2.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Publisher: arXiv Version Number: 2.\n\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX: Exceeding YOLO Series in 2021. arXiv preprint. Version Number: 2.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv preprint. Version Number: 4.\n\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, C\u00e9line Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: A simple yet effective abstention mechanism. Preprint, arXiv:2402.12997.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Publisher: arXiv Version Number: 2.\n\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Publisher: arXiv Version Number: 3.", "mimetype": "text/plain", "start_char_idx": 2063, "end_char_idx": 4986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2ba09d9-02c0-4b40-b767-fd4af26b92b4": {"__data__": {"id_": "d2ba09d9-02c0-4b40-b767-fd4af26b92b4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1964f7aa-67af-4dcf-ac5b-e38fd8537b77", "node_type": "1", "metadata": {}, "hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "class_name": "RelatedNodeInfo"}}, "text": "Version Number: 4.\n\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, C\u00e9line Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: A simple yet effective abstention mechanism. Preprint, arXiv:2402.12997.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Publisher: arXiv Version Number: 2.\n\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Publisher: arXiv Version Number: 3.\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix.", "mimetype": "text/plain", "start_char_idx": 4371, "end_char_idx": 5288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d1603c3-1bf2-453e-b9e9-5616896ab5ea": {"__data__": {"id_": "7d1603c3-1bf2-453e-b9e9-5616896ab5ea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e73b2524-cd6a-4cde-bd02-ca033aff2175", "node_type": "1", "metadata": {}, "hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nand William El Sayed. 2023. Mistral 7B. Publisher: arXiv Version Number: 1.\n\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint. Version Number: 3.\n\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.\n\nGeewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2021. OCR-free Document Understanding Transformer. arXiv preprint. Version Number: 5.\n\nAndreas Koukounas, Georgios Mastrapas, Michael G\u00fcnther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Mart\u00ednez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, and Han Xiao. 2024. Jina CLIP: Your CLIP Model Is Also Your Text Retriever. arXiv preprint. Version Number: 1.\n\nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. 2024. What matters when building vision-language models? arXiv preprint ArXiv:2405.02246 [cs].\n\nJinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Y. Zhao. 2023. Rethinking the Role of Token Retrieval in Multi-Vector Retrieval. arXiv preprint. Version Number: 3.\n\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models. Preprint, arXiv:2403.00231.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. 2014. Microsoft COCO: Common Objects in Context. arXiv preprint. Version Number: 3.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv preprint. Version Number: 2.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. Publisher: arXiv Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e73b2524-cd6a-4cde-bd02-ca033aff2175": {"__data__": {"id_": "e73b2524-cd6a-4cde-bd02-ca033aff2175", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d1603c3-1bf2-453e-b9e9-5616896ab5ea", "node_type": "1", "metadata": {}, "hash": "f30400ea5c39a8dda2862c4f5a6ed31adb53965d1dfd780b04bd6e1abb680842", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3316462b-be37-4152-a821-a2024ad0fca3", "node_type": "1", "metadata": {}, "hash": "5b6cd1ec36c79c626e603648645c27d2c8bdf7405d134fa5ee8d442654faaccb", "class_name": "RelatedNodeInfo"}}, "text": "Preprint, arXiv:2403.00231.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. 2014. Microsoft COCO: Common Objects in Context. arXiv preprint. Version Number: 3.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv preprint. Version Number: 2.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. Publisher: arXiv Version Number: 1.\n\nLucas Beyer*, Andreas Steiner*, Andr\u00e9 Susano Pinto*, Alexander Kolesnikov*, Xiao Wang*, Xiaohua Zhai*, Daniel Salz, Maxim Neumann, Ibrahim Al-abdulmohsin, Michael Tschannen, Jeremiah Harmsen, Daniel Keysers, Neil Houlsby, Xi Chen, Emanuele Bugliarello, Thomas Unterthiner, Keran Rong, Matthias Minderer, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Julian Eisenschlos, Manoj Kumar, Matko Bo\u0161njak, Matthias Bauer, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Paul Voigtlaender, Pinelopi Papalampidi, Olivier Henaff, Skanda Kopula, Xi Xiong, Radu Soricut, Model release contributors and general support, Tris Warkentin, Kat Black, Luiz Gustavo Martins, Glenn Cameron, Raj Gundluru, Manvinder Singh, Meg Risdal, Nilay Chauhan, Nate Keating, Nesh Devanathan, Elisa Bandy, Joe Fernandez, Antonia Paterson, Jenny Brennan, Tom Eccles, Pankil Botadra, Ben Bariach, Lav Rai, Minwoo Park, Dustin Luong, Daniel Vlasic, Bo Wu, Wenming Ye, Divyashree Sreepathihalli, Kiranbir Sodhia, Alek Andreev, Armand Joulin, Surya Bhupatiraju, Minh Giang, Joelle Barral, and Zoubin Ghahramani. 2024. PaliGemma.\n\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V Jawahar. 2021. InfographicVQA. arXiv preprint. Version Number: 2.\n\nMinesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2020. DocVQA: A Dataset for VQA on Document Images.\n\nNiklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and Nils Reimers. 2022. MTEB: Massive Text Embedding Benchmark. arXiv preprint. Version Number: 3.\n\nNomic. 2024. Nomic Embed Vision: Expanding The Nomic Latent Space.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. Publisher: arXiv Version Number: 1.\n\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint. Version Number: 1.\n\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109\u2013126. National Institute of Standards and Technology (NIST).\n\nKeshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint. Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 1567, "end_char_idx": 4642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3316462b-be37-4152-a821-a2024ad0fca3": {"__data__": {"id_": "3316462b-be37-4152-a821-a2024ad0fca3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e73b2524-cd6a-4cde-bd02-ca033aff2175", "node_type": "1", "metadata": {}, "hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "class_name": "RelatedNodeInfo"}}, "text": "2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint. Version Number: 1.\n\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109\u2013126. National Institute of Standards and Technology (NIST).\n\nKeshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint. Version Number: 1.\n\nR. Smith. 2007. An Overview of the Tesseract OCR Engine. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2, pages 629\u2013633, Curitiba, Parana, Brazil. IEEE. ISSN: 1520-5363.", "mimetype": "text/plain", "start_char_idx": 4019, "end_char_idx": 4855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2795c181-5047-471d-ba11-0b48062aed3c": {"__data__": {"id_": "2795c181-5047-471d-ba11-0b48062aed3c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9133d5b3-b632-40b6-b387-ac0329d734a4", "node_type": "4", "metadata": {}, "hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6edfd89-c126-44b0-a3a1-72394bb84c8c", "node_type": "1", "metadata": {}, "hash": "275baea2efd06dc7d04371ac8ef8f549db38d8eac1441e281d806a98e93a9ea8", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nKaren Sparck Jones. 1972. A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL. Journal of Documentation, 28(1):11\u201321.\n\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. 2022. Unifying Vision, Text, and Layout for Universal Document Processing. arXiv preprint. Version Number: 3.\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. arXiv preprint. Version Number: 4.\n\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. arXiv preprint. Version Number: 2.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint. Version Number: 2.\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. arXiv preprint. ArXiv:2002.10957 [cs].\n\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. FILIP: Fine-grained Interactive Language-Image Pre-Training. arXiv preprint. Version Number: 1.\n\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. arXiv preprint. Version Number: 3.\n\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid Loss for Language Image Pre-Training. Publisher: [object Object] Version Number: 4.\n\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty. 2023. Retrieving Multimodal Information for Augmented Generation: A Survey. arXiv preprint. Version Number: 3.\n\nFengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. 2022. Towards Complex Document Understanding By Discrete Reasoning. Publisher: arXiv Version Number: 3.\n\n# A Benchmark Datasets\n\n# A.1 Academic Datasets\n\nDocVQA (Mathew et al., 2020) includes collected images from the UCSF Industry Documents Library. Questions and answers were manually annotated.\n\nInfoVQA (Mathew et al., 2021) includes infographics collected from the Internet using the search query \u201cinfographics\u201d. Questions and answers were manually annotated.\n\nTAT-DQA (Zhu et al., 2022) is a large-scale Document VQA dataset that was constructed from publicly available real-world financial reports. It focuses on rich tabular and textual content requiring numerical reasoning. Questions and answers were manually annotated by human experts in finance.\n\narXivQA (Li et al., 2024) is a VQA dataset based on figures extracted from arXiv publications. The questions were generated synthetically using GPT-4 Vision.\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6edfd89-c126-44b0-a3a1-72394bb84c8c": {"__data__": {"id_": "f6edfd89-c126-44b0-a3a1-72394bb84c8c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9133d5b3-b632-40b6-b387-ac0329d734a4", "node_type": "4", "metadata": {}, "hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2795c181-5047-471d-ba11-0b48062aed3c", "node_type": "1", "metadata": {}, "hash": "f5cd5448c9452cbe6dbac1ac03e442b08b61542ba0c8aa8aaaffc80752815b7e", "class_name": "RelatedNodeInfo"}}, "text": "Questions and answers were manually annotated.\n\nInfoVQA (Mathew et al., 2021) includes infographics collected from the Internet using the search query \u201cinfographics\u201d. Questions and answers were manually annotated.\n\nTAT-DQA (Zhu et al., 2022) is a large-scale Document VQA dataset that was constructed from publicly available real-world financial reports. It focuses on rich tabular and textual content requiring numerical reasoning. Questions and answers were manually annotated by human experts in finance.\n\narXivQA (Li et al., 2024) is a VQA dataset based on figures extracted from arXiv publications. The questions were generated synthetically using GPT-4 Vision.\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings. We create additional queries to augment the existing human-annotated ones using the same method described in subsection A.2.\n\n# A.2 Practical Datasets\n\nMethodology. Creating a relevant retrieval dataset close to real use cases is a major challenge as the dataset needs to be both sufficiently large for effective fine-tuning and sufficiently diverse to cover a broad range of modalities (full text, tables, charts, ...), domains (industry, healthcare, ...), and query-document interactions (extractive questions, open-ended questions, ...). Our approach to building this dataset involves several steps: (1) we use a web crawler to collect publicly available documents on various themes and sources, (2) we convert these PDFs into a series of images, one per page, and (3) we generate queries related to each image using a VLM.\n\nWeb-Crawler. We implemented a web crawler to efficiently collect large volumes of documents related to a given topic. The crawler is seeded with a user-defined query (e.g. \"artificial intelligence\") and then uses GPT-3.5 Turbo to brainstorm related topics and subtopics. This query augmentation strategy aims at both broadening and deepening the search. GPT-3.5 Turbo is further used to generate diverse search queries from each subtopic.", "mimetype": "text/plain", "start_char_idx": 2614, "end_char_idx": 4670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "202f20dd-0083-4fed-b098-19c2a7e14b05": {"__data__": {"id_": "202f20dd-0083-4fed-b098-19c2a7e14b05", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "554e003a-18f7-4226-84fe-3021466167a3", "node_type": "4", "metadata": {}, "hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb", "class_name": "RelatedNodeInfo"}}, "text": "# Query Generation and Document Collection\n\nQuery set is then consumed by a pool of parallel workers whose job is to fetch the associated most relevant documents. We use SerpAPI along with a filetype filter (PDF documents only) to programmatically scrape Google Search rankings. Each file is hashed and stored in a Bloom filter (Bloom, 1970) shared among workers to avoid duplicate documents in the final corpus. Unique scraped files are downloaded, and inserted into a SQLite database along with additional metadata.\n\n# Datamix\n\nUsing the web crawler, we collected approximately 1,000 documents for each of the following four seeds: \"energy\", \"government reports\", \"healthcare industry\", and \"artificial intelligence\". These seeds were meticulously hand-picked to align with real-use cases for retrieval models and visually rich pages. We also removed all documents containing any private information. At this stage, we randomly selected 900 files for the training set and 100 files for the test set, ensuring that data leakage into the test set was avoided during subsequent processing steps.\n\n# Query Generation\n\nTo increase the efficiency of our query generation scheme and to limit API calls, we generate at most 3 questions per image. From all the documents collected, we randomly sample 10,000 images per theme and call Claude-3 Sonnet with the following prompt:\n\nRemember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus. Generate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question. Generate at most THREE pairs of questions and answers per page in a dictionary with the following format, answer ONLY this dictionary NOTHING ELSE:\n\n{\n\"questions\": [\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n}\n]\n}\n\nwhere XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n\nNote: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n\n# Human Validation\n\nWe manually validate every single synthetically created query in ViDoRe to ensure quality, query relevance, and consistency with the benchmark objective of evaluating retrieval in practical industrial settings. During this step, we randomly assign document-pair queries to 4 volunteers.\n\n15 https://serpapi.com/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b670c60-7ad0-420b-9ca5-3d0d78cc0186": {"__data__": {"id_": "5b670c60-7ad0-420b-9ca5-3d0d78cc0186", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "817bba4f-a97f-4a48-90a0-de713b5685c6", "node_type": "4", "metadata": {}, "hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073", "class_name": "RelatedNodeInfo"}}, "text": "# B     Implementation details\n\n# B.1     Codebase\n\nThe codebase is written in PyTorch16 and leverages HuggingFace tooling for model implementations and trainers17.\n\n# B.2     Pairwise CE loss\n\nOur in-batch contrastive loss L is defined as the softmaxed cross-entropy of the positive scores\n\nsk+ = LI (dk, qk) w.r.t. to the maximal negative scores sk\u2212 = maxl,l\u0338 =k LI (qk, pl).\n\nFor numerical stability, we reformulate the loss with the softplus function, leading to:\n\nL = bk=11 Xsoftplus sk \u2212 skb\n\n# B.3     Hyperparameters\n\nHyperparameters are tuned on a validation split composed of 2% of the training dataset. We find bi-encoder methods to be more sensible to learning rate variations than late interaction-based models and achieve the best performance for all models with a learning rate of 5e \u2212 5. We experiment with LoRA rank and \u03b1 values and do not notice particular improvements past r = \u03b1 = 32. Per-device batch sizes are kept small due to long sequence lengths that complicate scaling past b = 4. Simulating larger batch sizes for in-batch negative sampling should enable even better results. We find the best results with global batch size b = 32 for 1 epoch on our training set.\n\n# B.4     Embedding size\n\nMinimizing storage footprint can be essential to industrial retrieval systems if databases contain millions of documents. With this criterion in view, we have compared the embedding sizes of the models in our study. As shown in Table 3, ColPali\u2019s embedding size is an order of magnitude larger than BM25 and two orders of magnitude larger than BGE-M3. However, this study is limited to the naive method of storing ColPali\u2019s multi-vector embeddings. In practical scenarios, using cluster centroids can reduce the size of ColPali multi-vector embeddings by up to an order of magnitude (Santhanam et al., 2022) and make it a competitive retrieval system.\n\n# B.5     Latency computations\n\nAll latency computations are done on a NVIDIA L4 GPU. Queries are encoded independently (batch size of 1) to simulate online querying, and pages are encoded with a batch size of 4 for PaliGemma derived models, and 8 for BGE-M3. Reported times include image and text processing time before the model forward pass, as well as query-to-index matching times. We note an interesting feature of ColPali is that all documents have the same sequence length, leading to prior knowledge of runtime and memory consumptions. Query latency experiments are averaged over 1000 queries, and indexing times are measured for a 100 page document. Per page time is obtained by diving total time by 100, corresponding to inverse page throughput.\n\n# B.6     Captioning\n\nExamples of captions generated for visually rich document chunks with Claude-3 Sonnet are shown in Figure 6 and Figure 5. The prompt used for generating the description is the following:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "061e8f8a-fb51-4d61-bc52-5234557555aa": {"__data__": {"id_": "061e8f8a-fb51-4d61-bc52-5234557555aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "506cce11-d72a-405c-a08a-4ef5f19ad9bc", "node_type": "4", "metadata": {}, "hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 5: Example from the \"Energy\" test set.\n\nCaption: The image depicts the hourly energy generation profile, illustrating the contributions of various energy sources over 24 hours. The data is presented as a stacked bar chart, with the x-axis representing the hours of the day from 1 to 2, and the y-axis showing the average hourly generation in MW. The bars are segmented into different colors, each representing a distinct energy source: nuclear, bio, geothermal, solar, wind, hydro, natural gas, and other imports. The chart provides insights into the temporal variations in energy generation across different sources, highlighting the interplay between baseload and intermittent sources throughout the day.\n\n# Figure 6: Example from the \"Government Reports\" test set.\n\nCaption: The image shows a table titled \"System of Record\" which outlines the different types of documents or records maintained across various systems or departments within an organization related to project management and construction. The rows list documents like project plans, budgets, schedules, contracts, purchase orders, invoices, change requests, bid submissions, drawings, manuals, meeting minutes, and reports. The columns indicate the system or department responsible for maintaining each record, such as County Servers, Project View, OnBase, CGI Advantage Financial System, and Purchasing Department. The table uses \"W\" and \"T\" markers to denote which system or department serves as the primary source (writer) or storage location (trailer) for each type of document.\n\n# More similarity maps\n\nIn Figure 7, ColPali assigns a high similarity to all patches with the word \"Kazakhstan\" when given the token <_Kazakhstan>. Moreover, our model seems to exhibit world knowledge capabilities as the patch around the word \"Kashagan\" - an offshore oil field in Kazakhstan - also shows a high similarity score. On the other hand, in Figure 8, we observe that ColPali is also capable of complex image understanding. Not only are the patches containing the word \"formulations\" highly similar to the query token _formula, but so is the upper-left molecule structure.\n\nIt is also interesting to highlight that both similarity maps showcase a few white patches with high similarity scores. This behavior might first seem surprising as the white patches should not carry a meaningful signal from the original images. We believe the vectors associated with these patches share a similar role with the ViT registers (Darcet et al., 2023), i.e. these patches were repurposed for internal computations and stored the global information from the whole image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9895c678-e654-479a-9334-f328d52c3c90": {"__data__": {"id_": "9895c678-e654-479a-9334-f328d52c3c90", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0996ba57-66b0-4db1-b8b3-cd9fa2e3b876", "node_type": "4", "metadata": {}, "hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28", "class_name": "RelatedNodeInfo"}}, "text": "# Historique de production\n\n# Production totale des hydrocarbures liquides\n\n# Kazakhstan (1965-2019)\n\nQuery: \"Quelle partie de la production p\u00e9troli\u00e8re du Kazakhstan provient de champs en mer ?\"\n\n# Ferroelectrics\n\n# Lead Zirconium Titanate\n\nPb(Zr,Ti)O3\n\n1952 Shirane; Pb(Zr,Ti)O3 solid solutions\n\n1955 Jalte coor Berlincourt; Gerson: Complete Study PZT formulations\n\nCurie temperature 170-360\n\nQuery: What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbc872ae-050b-4958-b89a-c9dc31e20af4": {"__data__": {"id_": "dbc872ae-050b-4958-b89a-c9dc31e20af4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "297ac6a3-89db-4283-91b6-8ff124b88fd0", "node_type": "4", "metadata": {}, "hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3c7e4b2-e0be-4da7-bd73-5dc10b360a1b", "node_type": "1", "metadata": {}, "hash": "62816e0b531a77b544cbbfd001f5c8c7767e6aa087dc8bc5a6d4f92170b12b5e", "class_name": "RelatedNodeInfo"}}, "text": "# D. Additional results\n\n# D.1 Other Metrics\n\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy|Gov.|Health.|Avg.| |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Unstructured Text only|BM25|-|26.6|-|-|34.6|45.0|86.0|70.0|68.0|74.0|-|\n| |BGE-M3|-|22.8\u21933.8|-|-|26.1\u21938.5|51.0\u21916.0|81.0\u21935.0|72.0\u21912.0|67.0\u21931.0|77.0\u21913.0|-|\n|Unstructured + OCR|BM25|26.7|28.9|54.0|30.4|50.0|52.0|86.0|77.0|74.0|80.0|55.9|\n| |BGE-M3|28.1\u21911.4|22.9\u21936.0|53.8\u21930.2|55.7\u219125.3|38.6\u219311.4|56.0\u21914.0|82.0\u21934.0|79.0\u21912.0|76.0\u21912.0|83.0\u21913.0|57.5\u21911.6|\n|Unstructured + Captioning|BM25|35.5|30.2|61.5|24.3|49.0|47.0|79.0|76.0|75.0|81.0|55.9|\n| |BGE-M3|29.3\u21936.2|26.0\u21934.2|62.1\u21910.6|58.6\u219134.3|30.6\u219318.4|55.0\u21918.0|80.0\u21911.0|78.0\u21912.0|69.0\u21936.0|83.0\u21912.0|57.2\u21911.3|\n|Contrastive VLMs|Jina-CLIP|19.4|7.3|26.7|12.5|1.6|2.0|11.0|13.0|15.0|17.0|12.6|\n| |Nomic-vision|10.4|6.7|22.1|9.6|1.6|0.0|9.0|9.0|7.0|13.0|8.8|\n| |SigLIP (Vanilla)|34.2|21.3|51.8|46.1|17.9|13.0|50.0|51.0|47.0|65.0|39.7|\n|Ours|(Copied) SigLIP (Vanilla)|34.2|21.3|51.8|46.1|17.9|13.0|50.0|51.0|47.0|65.0|39.7|\n| |BiSigLIP (+fine-tuning)|49.2\u219115.0|23.8\u21912.5|59.0\u21917.2|52.1\u21916.0|20.7\u21912.8|16.0\u21913.0|62.0\u219112.0|61.0\u219110.0|55.0\u21918.0|72.0\u21917.0|47.1\u21917.4|\n| |BiPali (+LLM)|46.4\u2193-2.8|20.0\u2193-3.8|54.6\u2193-4.4|63.2\u219111.1|20.4\u2193-0.4|34.0\u219118.0|59.0\u2193-3.0|45.0\u2193-16.0|57.0\u21912.0|56.0\u2193-16.0|45.6\u2193-1.5|\n| |ColPali (+Late Inter.)|72.4\u219126.0|45.6\u219125.6|74.6\u219120.0|75.4\u219112.1|53.1\u219132.7|55.0\u219121.0|93.0\u219134.0|85.0\u219140.0|85.0\u219128.0|88.0\u219132.0|72.7\u219127.1|\n\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3c7e4b2-e0be-4da7-bd73-5dc10b360a1b": {"__data__": {"id_": "c3c7e4b2-e0be-4da7-bd73-5dc10b360a1b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "297ac6a3-89db-4283-91b6-8ff124b88fd0", "node_type": "4", "metadata": {}, "hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbc872ae-050b-4958-b89a-c9dc31e20af4", "node_type": "1", "metadata": {}, "hash": "00119033c6c21ba634c0f31ec079ed765db6f54b4f9569e4c2947c753f6644b0", "class_name": "RelatedNodeInfo"}}, "text": ")|72.4\u219126.0|45.6\u219125.6|74.6\u219120.0|75.4\u219112.1|53.1\u219132.7|55.0\u219121.0|93.0\u219134.0|85.0\u219140.0|85.0\u219128.0|88.0\u219132.0|72.7\u219127.1|\n\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe. Results are presented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\n\n# D.2 Model Variants\n\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy|Gov.|Health.|Avg.|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|ColSigLIP (PaliGemma)|3.1|3.0|5.1|6.2|2.5|1.0|3.4|3.4|2.3|2.2|3.2|\n|BiSigLIP (PaliGemma)|18.5|14.6|33.4|39.5|16.1|5.2|27.6|32.6|36.6|35.7|26.0|\n|ColSigLIP (Original)|2.6|2.2|2.3|5.7|1.8|1.0|2.6|4.1|1.4|1.5|2.5|\n|ColPali (No Mem. Tokens)|80.4|53.2|82.4|77.4|65.7|63.4|97.0|89.9|93.6|92.4|79.6|\n|ColPali (Best)|79.1|54.4|81.8|83.9|65.8|73.2|96.2|91.0|92.7|94.4|81.3|\n\nTable 5: Evaluation of some \"negative results\" and ablations on ViDoRe; ColPali for reference. Results are presented using NDCG@5 metrics. Text-only metrics are not computed for benchmarks with only visual elements.", "mimetype": "text/plain", "start_char_idx": 1308, "end_char_idx": 2354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77083868-7a42-40f0-bcb1-256f532565ba": {"__data__": {"id_": "77083868-7a42-40f0-bcb1-256f532565ba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "765b95d7-7a8e-473e-9cbe-e435fe78268a", "node_type": "4", "metadata": {}, "hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2", "class_name": "RelatedNodeInfo"}}, "text": "# ViDoRe examples\n\n# Energy\n\n|Query|Response|\n|---|---|\n|What types of accounts or products allow investors to defer paying taxes?| |\n|What is the projected peak electricity demand in California for the year 2030?| |\n|What is the estimated total savings for a PV system in Durham under the net metering (flat rate) billing option over the system\u2019s useful life of 25 years?|Projected 2030 electricity capacities|\n\n# Artificial Intelligence\n\n|Query|Response|\n|---|---|\n|What are some common outcome areas targeted by TAII for different age groups?| |\n|What did the robot monitor to determine when to activate or deactivate the blower motor and blinker?| |\n|What is the key approach used in the PDP architecture?| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d806e310-b5a0-4251-984a-5ac4f109537e": {"__data__": {"id_": "d806e310-b5a0-4251-984a-5ac4f109537e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c478a956-8f9e-492d-a6c1-6ae1cd7e6a9a", "node_type": "4", "metadata": {}, "hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451", "class_name": "RelatedNodeInfo"}}, "text": "# Healthcare Industry\n\n# Query: What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?\n\nFerroelectrics\n\nLojd Circon Um\n\nPblzo Tlal\n\n1952 Shlrano Sufuri\n\n# Query: What government entities are involved in public financing for health care in the US?\n\nUCLA Health System Financing\n\n# Government Reports\n\n# Query: What does the AVPU scale stand for in assessing the level of consciousness of a seriously ill child?\n\n# Query: What are some mandates for the EPA under the Pollution Prevention Act?\n\n# Query: What is the strategy of KPMG Hazem Hassan?\n\n# Query: What is the trust signal score for the consumer industry best-in-class archetype?\n\n# Who we are?\n\nWE\n\nM dats\n\nGMOnsecy\n\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78bffaad-0be5-4871-9c47-4aabb1dd228f": {"__data__": {"id_": "78bffaad-0be5-4871-9c47-4aabb1dd228f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43782e9d-aa09-436f-a65d-7b7a5b82c66e", "node_type": "4", "metadata": {}, "hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc", "class_name": "RelatedNodeInfo"}}, "text": "# Shift\n\n# Query: Selon le graphique, quelle est la capacit\u00e9 d\u2019import et la consommation r\u00e9elle de carburants SAF (biocarburants durables pour l\u2019aviation) pr\u00e9vues en 2050 ?\n\n# Query: Quelle partie de la production p\u00e9troli\u00e8re du Kazakhstan provient de champs en mer ?\n\n# Query: Quels sont les pays ayant la plus grande part des d\u00e9couvertes cumul\u00e9es de p\u00e9trole brut en 2020 (en milliers de barils, hors d\u00e9couvertes cumul\u00e9es) ?\n\n|Lucmolo|Fetall|\n|---|---|\n|20| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"4fdbe99b-a72f-4ea9-9d58-1efbbc0e8af8": {"node_ids": ["53de1f52-55a7-4ee8-acb3-ee46a58ec801"], "metadata": {}}, "7da3d723-b838-417c-bbe8-a24a50b96ea4": {"node_ids": ["ab723b02-84f5-4cd1-b044-da180d75c1de"], "metadata": {}}, "60e079ef-a199-49c3-ac39-40f24a2eccfa": {"node_ids": ["67bb8be7-226b-42ad-990a-f0c7d684cf67"], "metadata": {}}, "128bdb06-8cf0-4df1-b56d-fdd8ffd2769d": {"node_ids": ["96a27d0c-a728-4b25-89e5-dda0dc780acc"], "metadata": {}}, "689b0e3e-8117-4195-8759-dc5bb0960102": {"node_ids": ["7485555b-1e26-4200-be0d-c7adb35c5fa1", "29fed939-f494-4404-9c83-59f8cc3f195a"], "metadata": {}}, "a2b2a7b7-fcec-4af1-b893-a7a5f1b6fb6d": {"node_ids": ["04708a6c-cbd4-40e2-8ea8-cc6581ba3884"], "metadata": {}}, "292d5589-3184-4ff3-b433-03daeaee1cba": {"node_ids": ["9a223960-346f-4cef-a8bc-87e7b0ecc114", "9f24de3e-503b-4b1f-a34c-49acca2618cd"], "metadata": {}}, "b0570d2b-7f87-4bb8-a029-0c83c23aea38": {"node_ids": ["b91907ea-bb44-484d-9ea0-eab8caaecb6a"], "metadata": {}}, "69d4d7ff-482f-4aa8-afeb-114aae7cfe25": {"node_ids": ["49c440da-96b9-4eaa-a6ae-f8e875b0490d"], "metadata": {}}, "4b8f6a82-8d7d-4def-8709-c09dc876bd15": {"node_ids": ["0834c725-cb08-4a22-9914-c39947db97f7", "1964f7aa-67af-4dcf-ac5b-e38fd8537b77", "d2ba09d9-02c0-4b40-b767-fd4af26b92b4"], "metadata": {}}, "7394abf3-6e8c-4750-86fd-81dbee960fa4": {"node_ids": ["7d1603c3-1bf2-453e-b9e9-5616896ab5ea", "e73b2524-cd6a-4cde-bd02-ca033aff2175", "3316462b-be37-4152-a821-a2024ad0fca3"], "metadata": {}}, "9133d5b3-b632-40b6-b387-ac0329d734a4": {"node_ids": ["2795c181-5047-471d-ba11-0b48062aed3c", "f6edfd89-c126-44b0-a3a1-72394bb84c8c"], "metadata": {}}, "554e003a-18f7-4226-84fe-3021466167a3": {"node_ids": ["202f20dd-0083-4fed-b098-19c2a7e14b05"], "metadata": {}}, "817bba4f-a97f-4a48-90a0-de713b5685c6": {"node_ids": ["5b670c60-7ad0-420b-9ca5-3d0d78cc0186"], "metadata": {}}, "506cce11-d72a-405c-a08a-4ef5f19ad9bc": {"node_ids": ["061e8f8a-fb51-4d61-bc52-5234557555aa"], "metadata": {}}, "0996ba57-66b0-4db1-b8b3-cd9fa2e3b876": {"node_ids": ["9895c678-e654-479a-9334-f328d52c3c90"], "metadata": {}}, "297ac6a3-89db-4283-91b6-8ff124b88fd0": {"node_ids": ["dbc872ae-050b-4958-b89a-c9dc31e20af4", "c3c7e4b2-e0be-4da7-bd73-5dc10b360a1b"], "metadata": {}}, "765b95d7-7a8e-473e-9cbe-e435fe78268a": {"node_ids": ["77083868-7a42-40f0-bcb1-256f532565ba"], "metadata": {}}, "c478a956-8f9e-492d-a6c1-6ae1cd7e6a9a": {"node_ids": ["d806e310-b5a0-4251-984a-5ac4f109537e"], "metadata": {}}, "43782e9d-aa09-436f-a65d-7b7a5b82c66e": {"node_ids": ["78bffaad-0be5-4871-9c47-4aabb1dd228f"], "metadata": {}}}}