{"docstore/data": {"065b847f-d5c7-4b5e-946a-e19c0c678276": {"__data__": {"id_": "065b847f-d5c7-4b5e-946a-e19c0c678276", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "647adfc9-c2e6-4a6b-809f-5c987bee7120", "node_type": "4", "metadata": {}, "hash": "bfcc1f49902460d080bfad58b81e30c31320269e8427000e2814958d34de5b35", "class_name": "RelatedNodeInfo"}}, "text": "# Deep Time Series Models: A Comprehensive Survey and Benchmark\n\n# Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, Jianmin Wang\n\n# Abstract\n\nTime series, characterized by a sequence of data points arranged in a discrete-time order, are ubiquitous in real-world applications. Different from other modalities, time series present unique challenges due to their complex and dynamic nature, including the entanglement of nonlinear patterns and time-variant trends. Analyzing time series data is of great significance in real-world scenarios and has been widely studied over centuries. Recent years have witnessed remarkable breakthroughs in the time series community, with techniques shifting from traditional statistical methods to advanced deep learning models. In this paper, we delve into the design of deep time series models across various analysis tasks and review the existing literature from two perspectives: basic modules and model architectures. Further, we develop and release Time Series Library (TSLib) as a fair benchmark of deep time series models for diverse analysis tasks, which implements 24 mainstream models, covers 30 datasets from different domains, and supports five prevalent analysis tasks. Based on TSLib, we thoroughly evaluate 12 advanced deep time series models on different tasks. Empirical results indicate that models with specific structures are well-suited for distinct analytical tasks, which offers insights for research and adoption of deep time series models. Code is available at https://github.com/thuml/Time-Series-Library.\n\n# Index Terms\n\nTime series analysis, deep time series models, survey, benchmark\n\n# 1 INTRODUCTION\n\nTime series refers to a sequence of data points indexed in a discrete-time order, which are omnipresent in real-world applications, such as financial risk assessment, energy sustainability, and weather forecasting. Driven by the increasing availability of vast amounts of time series data across various domains, the community of time series analysis has witnessed tremendous advancements. Compared to image and text data, which have objectively prescribed syntax or intuitive patterns, the semantic information of time series data is primarily derived from the temporal variation. This presents significant challenges in understanding the data, such as identifying sequential dependencies, trends, seasonal patterns, and complicated dynamics. Consequently, analyzing time series data requires sophisticated methods to capture and utilize these complex temporal representations.\n\nGiven the crucial role of time series data in real-world applications, time series analysis has been a longstanding research direction. Time series analysis encompasses the process of analyzing the temporal variation to understand time series data and make accurate predictions and informed decisions. One of the essential cornerstones of time series analysis is discovering the underlying patterns in time series data, which involves the intricate temporal dependencies and variate correlations inherent within the data. By capturing these complex dependencies, time series models can effectively reveal the underlying dynamics, and facilitate various downstream tasks, including forecasting, classification, imputation, and anomaly detection.\n\nTraditional time series methods, such as AutoRegressive Integrated Moving Average (ARIMA), Exponential Smoothing, and Spectral Analysis, have long served as stalwart tools in time series analysis. These models, grounded in statistical methodologies, have been instrumental in discovering patterns, trends, and seasonality within temporal variations. However, their capabilities are hindered due to the inherent limitations of capturing complex nonlinear relationships and long-term dependencies present in real-world time series data. The rigid assumptions of linearity and stationarity that underpin traditional models constrain their adaptability to eventful and evolving data flows.\n\nDeep models have garnered significant attention and achieved remarkable performance across various domains, including natural language processing (NLP), computer vision (CV), and recommendation systems. In recent years, deep learning models have demonstrated their capability to capture the intricate dependencies within time series data, making deep learning models a powerful tool for time series analysis over traditional statistical methods. More recently, Transformer models with attention mechanisms, originally developed for natural language processing tasks, have presented stunning power in processing large-scale data and have also been adapted for learning time series data. These architectures offer the advantage of selectively focusing on different parts of the input sequence, allowing for more nuanced discovery of both temporal and variable dependencies in time series.\n\n# Related Surveys\n\nAlthough various time series models designed for different analysis tasks have emerged in recent years, there is a lack of a comprehensive overview of existing methods, covering both tasks and models. Previous reviews focus exclusively on either a specific model architecture or...\n\n# Footnotes\n\n\u2022 Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Jianmin Wang, and Mingsheng Long are with the School of Software, BNRist, Tsinghua University, Beijing 100084, China. E-mail: wangyuxu22@mails.tsinghua.edu.cn.\n\n\u2022 Yuxuan Wang, Haixu Wu and Jiaxiang Dong contributed equally to this work.\n\n\u2022 Corresponding author: Mingsheng Long, mingsheng@tsinghua.edu.cn.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29f51ced-9aed-4dfe-93b6-ca58679f7f64": {"__data__": {"id_": "29f51ced-9aed-4dfe-93b6-ca58679f7f64", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12b41c0e-ed75-4138-8753-69a3b62e6e92", "node_type": "4", "metadata": {}, "hash": "607f9cd734896d72aca21332bf2193291aeaa3369a7d5365c8a8bfed801115da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f4009d5-cc2b-452c-934f-c7569f1aed73", "node_type": "1", "metadata": {}, "hash": "7fc356da9e40dde4c738955742399c3207efc2f5e1ba5de8c2bde049dbe42d40", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 2\n\n# TABLE 1\n\nComparison between our work and other related surveys. We present a comprehensive review of tasks and models, with a benchmark provided.\n\n|Survey| | |Analysis Task| | | | |Model Architecture| | |Benchmark|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|Fawaz et al. (2019) [8]| |\u2713|\u2713|\u2713|\u2713| | | | | | |\n|Braei et al. (2020) [9]| | | | |\u2713|\u2713|\u2713| | | | |\n|Torres et al. (2021) [10]|\u2713| |\u2713|\u2713| | | | | | | |\n|Garc\u00eda et al. (2021) [11]| |\u2713|\u2713|\u2713| | | | | | | |\n|Wen et al. (2022) [12]|\u2713|\u2713| |\u2713| | | | | | | |\n|Jin et al. (2023) [13]|\u2713|\u2713|\u2713| | | | | | | | |\n|Shao et al. (2023) [14]|\u2713| |\u2713|\u2713|\u2713|\u2713| | | | | |\n|Qiu et al. (2024) [15]|\u2713| |\u2713|\u2713|\u2713|\u2713| | | | | |\n|Our Survey|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713| | | | |\n\nan analysis task. For example, [8], [9], [10], [11] reviews deep learning methods for specific time series analysis tasks while failing to include advanced architecture such as Transformer. Several surveys [12], [13] provide up-to-date reviews for time series analysis focusing on specific deep learning architectures (i.e., Graph Neural Network and Transformer). Recently, BasicTS [14] and TFB [15] introduce forecasting benchmarks that enable an unbiased evaluation of existing approaches but do not provide an overview of the architectural design of those deep models.\n\nIn this survey, we provide a comprehensive review of deep time series models for researchers and practitioners, starting from the basic modules to modern architectures. To foster practical applications, a time series benchmark is offered for a fair evaluation and identifying the effective scope of existing models. Our survey is organized as follows. Section 2 provides the background concepts of time series analysis. Section 3 introduces the basic modules that are widely utilized in prevalent deep time series models. Section 4 reviews the existing deep time series models in terms of the architecture design. Section 5 introduces the proposed open-source benchmark\u2014Time Series Library (TSLib)\u2014and presents extensive experimental comparison with detailed analysis. Section 6 provides a brief discussion of future research directions while Section 7 summarizes this survey.\n\n# 2 PRELIMINARIES\n\n# 2.1 Time Series\n\nTime series is a sequence of T observations ordered by time, which can be denoted as X = {x1, x2, ..., xT } \u2208 RT \u00d7C , where xt \u2208 RC represents the observed values at time point t and C is the number of variables. Since time series data are physical measurements obtained from sensors, systems are often recorded with multiple variables. Consequently, real-world time series usually recorded in a multivariate form. Theoretical studies [25], [26] have shown that when there are two or more non-stationary series, a linear combination of them can be stationary. This co-integration property helps in uncovering and modeling long-term relationships among non-stationary series. Therefore, the essence of time series analysis is to capture and utilize the temporal dependencies and inter-variable correlations within the observations.\n\nTemporal Dependency Given the sequential nature inherent in the observations, one evident technological paradigm is to capture the temporal dependence of a set of historical data. The basic idea of temporal dependencies is the intricate correlations between time points or sub-series. Traditional statistical models have laid the groundwork for modeling temporal dependencies. Prominent models include ARIMA (Autoregressive Integrated Moving Average) [1] have been extensively studied for capturing complex temporal patterns in the time series modality. Owing to their simplicity and interpretability, these statistical methods remain popular for tasks where the underlying temporal dynamics do not exhibit high complexity. Considering the high-dimensionality and non-stationarity of real-world time series, the research focus shifted towards deep learning for time series analysis.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f4009d5-cc2b-452c-934f-c7569f1aed73": {"__data__": {"id_": "5f4009d5-cc2b-452c-934f-c7569f1aed73", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12b41c0e-ed75-4138-8753-69a3b62e6e92", "node_type": "4", "metadata": {}, "hash": "607f9cd734896d72aca21332bf2193291aeaa3369a7d5365c8a8bfed801115da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29f51ced-9aed-4dfe-93b6-ca58679f7f64", "node_type": "1", "metadata": {}, "hash": "26e1fa254c7432f1670a6b5b6e565e26517cedc2d6ecbf67e66427cdc253773f", "class_name": "RelatedNodeInfo"}}, "text": "This co-integration property helps in uncovering and modeling long-term relationships among non-stationary series. Therefore, the essence of time series analysis is to capture and utilize the temporal dependencies and inter-variable correlations within the observations.\n\nTemporal Dependency Given the sequential nature inherent in the observations, one evident technological paradigm is to capture the temporal dependence of a set of historical data. The basic idea of temporal dependencies is the intricate correlations between time points or sub-series. Traditional statistical models have laid the groundwork for modeling temporal dependencies. Prominent models include ARIMA (Autoregressive Integrated Moving Average) [1] have been extensively studied for capturing complex temporal patterns in the time series modality. Owing to their simplicity and interpretability, these statistical methods remain popular for tasks where the underlying temporal dynamics do not exhibit high complexity. Considering the high-dimensionality and non-stationarity of real-world time series, the research focus shifted towards deep learning for time series analysis. These advanced methods are designed to handle more complex temporal dynamics and offer greater flexibility in capturing the temporal dependency of time series data.\n\nVariate Correlation In addition to capturing temporal dependencies, understanding the variate correlations within high-dimensionality plays a pivotal role in analyzing multivariate time series. These correlations refer to the complex interactions and associations among different variables changing across the time. They provide valuable insights into the underlying dynamics and dependencies among the measurements, enabling a more comprehensive understanding of the latent process. Traditional approaches, such as Vector Autoregressive (VAR) models [27], extend the concept of autoregression to multiple variables and can capture the relationships between multiple quantities as they evolve over time. Technically, VAR represents each variable as a linear combination of its lagged values and the lagged values of all other variables in the model, which results in an inability to capture complex and non-linear relationships. Recently, advanced deep models, such as Graph Neural Networks [28] and Transformers [29], [30], have also been introduced for variate correlation modeling.\n\n# 2.2 Time Series Analysis Tasks\n\nBased on the understanding of underlying patterns and trends within time series data, time series analysis encompasses various downstream applications, including forecasting [31], [32], imputation [33], [34], [35], classification [8], [36], and anomaly detection [5], [37], each serving distinct purposes in diverse application domains.\n\nWe illustrate representative time series analysis tasks in Figure 1. Forecasting is a fundamental task in time.", "mimetype": "text/plain", "start_char_idx": 2824, "end_char_idx": 5714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c24eb29c-41d9-40f3-b229-83a1601d4d41": {"__data__": {"id_": "c24eb29c-41d9-40f3-b229-83a1601d4d41", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2e35d18-03bf-473a-b3ce-b15c038338a9", "node_type": "4", "metadata": {}, "hash": "b26b621570eeff5449636676bc4a0c54f4cf49039b7cdc081d215ce7911eba55", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 3 BASIC MODULES\n\nTime series modeling approaches have evolved significantly, transitioning from traditional statistical models to sophisticated deep learning models. Despite these advancements, many classical tools and analytical algorithms remain widely used and continue to serve as foundational design principles in modern deep models.\n\n# 3.1 Stationarization\n\nAs a foundational concept in time series analysis, stationarity refers to the property of a time series where its statistical properties remain constant over time. A stationary time series has a constant mean and variance, which simplifies statistical analysis and makes it easier to capture the underlying patterns and behavior within a time series. Since many statistics-based time series analysis methods take stationarity as a basic assumption, stationarization of time series data has become an essential module. There are ways of transforming non-stationary time series into stationary. Traditional time series models stationarize the time series through differencing or log-transformation. In recent deep learning approaches, data normalization takes the role of stationarization in a simple but effective way, which standardizes the value distribution of observations while maintaining the intrinsic variations and further helps mitigate the distribution shift between the source and target domains.\n\nThe deep adaptive input normalization (DAIN) layer was proposed to adaptively stationarize time series data according to their original distribution. RevIN introduces reversible instance normalization to time series data, which is an effective normalization-and-denormalization method with learnable affine transforms to make the model bypass the non-stationary inputs. Non-Stationary Transformer (Stationary for short in the following) proposes a simpler but more effective series stationarization technique that improves the predictive capability of non-stationary series without extra parameters. Specifically, for a sequence with T time stamps and C variates X = {X1, X2, ..., XT } \u2208 RT \u00d7C , the outline of Stationary can be summarized as:\n\n\u03bcx = 1/T \u03a3 Xi, \u03c3\u00b2 = 1/T \u03a3 (Xi \u2212 \u03bcx)\u00b2,Y' = p\u03c3x (X \u2212 \u03bcx)\u00b2/(Y' + \u03bcx)\u00b2 + \u03f5\n\nwhere \u03f5 is in a small value for numerical stability. \u03bcx, \u03c3\u00b2 \u2208 R1\u00d7C are the variate-specific mean and variance. To recover the distribution and non-stationarity of the original series, a de-normalization module is further used to augment the model output Y' with mean and variance statistics of inputs. The idea of stationarization and the above-mentioned techniques have been widely used in subsequent deep time series models. Recent SAN rethinks the nature of non-stationary data and tries to split it into non-overlap equally-sized slices and perform normalization on each slice. Specifically, based on the evolving trends of statistical properties, SAN introduces a statistics prediction module to predict the distributions of future slices.\n\n# 3.2 Decomposition\n\nDecomposition, as a conventional approach in time series analysis, can disentangle time series into several components with categorized patterns, and works primarily useful for exploring complex series variations. In the previous work, diverse decomposition paradigms are explored.\n\n# 3.2.1 Seasonal-Trend Decomposition\n\nSeasonal-trend decomposition is one of the most common practices to make raw data more predictable, which...", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d456ee2b-d445-49c6-8858-238652abc5d5": {"__data__": {"id_": "d456ee2b-d445-49c6-8858-238652abc5d5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eefce1db-0425-4101-89d5-462df76903d5", "node_type": "4", "metadata": {}, "hash": "7c08d51d256035a14a3531ae4566d50c7e6edc8ac1c9f7d79f5f40c7011954c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baea2fc5-c1f5-41f2-b6c4-1bb7f54e93c5", "node_type": "1", "metadata": {}, "hash": "b0482959b541d16831c426844d238166f2abdfeb36ae840d44d93b61f64f805d", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 4\n\ncan separate the series into several different components: trend, seasonal, cyclical, and irregular, namely\n\nX = T + C + S + I,\n\nwhere the trend component T represents the overall long-term pattern of the data over time, the cyclical component C reflects repeated but non-periodic fluctuations within data, the seasonal component S indicates the repetitive patterns over a fixed period, and the irregular component I is the residuals or remainder of the time series after the other components have been removed.\n\nThe trend-seasonality decomposition can be achieved by using mathematical tools such as filters or exponential smoothing [49], [50]. Previous statistical approaches mainly adopt the trend-seasonality decomposition as data preprocessing [51]. In deep models, Autoformer [22] firstly introduces the idea of decomposition to deep learning architecture and proposes a series decomposition block as a basic module to extract the seasonal and trend-cyclical parts of deep features and input series, whose computation process can be formalized as:\n\nXT = AvgPool (Padding(X)),\n\nXS = X \u2212 XT.\n\nThe series decomposition block is concisely implemented based on a temporally average pooling layer with padding operation to keep the sequence length unchanged. This design can capture trends XT, and the remainder is taken as the seasonal part XS. The proposed series decomposition block has been widely used in the follow-up [6], [52], [53], [54], [55] as a native building block of deep models to disentangle the underlying patterns of deep features.\n\n# 3.2.2 Basis Expansion\n\nBasis expansion is a mathematical method used to represent a function or a set of data points in terms of a new set of pre-defined functions. These new functions form a basis for a function space, meaning any function in that space can be expressed as a linear combination of these basis functions. In the context of time series analysis, basis expansion is used to reveal complex non-linear temporal relationships by decomposing the time series into a combination of basic variations, which also enhances interpretability. As a representative model, N-BEATS [56] presents hierarchical decomposition to time series by utilizing a fully connected layer to produce expansion coefficients for both backward and forward forecasts. For l-th blocks in the proposed hierarchical architecture, the operation can be as follows:\n\nXl = Xl\u22121 \u2212 X\u02c6l\u22121\n\nX\u02c6l,\u02c6l = Block(Xl), Yl\n\n\u02c6l\u22121 is the backcast results which restrict the block where X to approximate the input signal Xl\u22121, then Xl removes the portion of well-estimated signal X\u02c6l\u22121 from X\u02c6l\u22121, therefore providing a hierarchical decomposition. Y\u02c6l is the partial forecast based on the decomposed input Xl and the final forecast Y\u02c6l is the sum of all partial forecasts.\n\nSubsequently, N-HiTs [57] redefine the N-BEATS by incorporating subsampling layers before the fully connected blocks, which enhances the input decomposition via multi-frequency data sampling and future predictor via multi-scale interpolation. DEPTS [58] puts forward a novel decoupled formulation for periodic time series by introducing the periodic state as a hidden variable and then develops a deep expansion module on top of residual learning to conduct layer-by-layer expansions between observed signals and hidden periodic states. Similarly, DEWP [59] is also a stack-by-stack expansion model to handle multivariate time series data, where each stack consists of a variable expansion block to capture dependencies among multiple variables and a time expansion block to learn temporal dependencies.\n\n# 3.2.3 Matrix Factorization\n\nThe above-mentioned two decomposition methods are proposed for univariate series or applied to multivariate series in a variate-independent way. Here, we discuss a factorization-based decomposition for multivariate series. Specifically, many multivariate time series data in real-world scenarios can also be referred to as high-dimensional data. They can be formalized in the form of a matrix, whose rows correspond to variate and columns correspond to time points. Since variables in multivariate time series tend to be highly correlated, it can be possibly reduced to a more compact space. Matrix factorization methods [60] work by decomposing the high-dimensional series data into the product of two matrices in a lower-dimensional latent space. For a multivariate time series X \u2208 RT \u00d7C, as shown in Figure 2 the matrix can be approximated by the multiplications of two lower rank embedding matrix, X \u2248 F Xb, in which F \u2208 Rk\u00d7C, Xb \u2208 RT \u00d7k and k is a hyperparameter.\n\nBesides the estimation, there are regularizers to avoid overfitting problems in factorization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baea2fc5-c1f5-41f2-b6c4-1bb7f54e93c5": {"__data__": {"id_": "baea2fc5-c1f5-41f2-b6c4-1bb7f54e93c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eefce1db-0425-4101-89d5-462df76903d5", "node_type": "4", "metadata": {}, "hash": "7c08d51d256035a14a3531ae4566d50c7e6edc8ac1c9f7d79f5f40c7011954c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d456ee2b-d445-49c6-8858-238652abc5d5", "node_type": "1", "metadata": {}, "hash": "1d1dab5124af2d8f9c2ad37e26e34ec0d7eee86f16b0dae150e57d77af523141", "class_name": "RelatedNodeInfo"}}, "text": "Specifically, many multivariate time series data in real-world scenarios can also be referred to as high-dimensional data. They can be formalized in the form of a matrix, whose rows correspond to variate and columns correspond to time points. Since variables in multivariate time series tend to be highly correlated, it can be possibly reduced to a more compact space. Matrix factorization methods [60] work by decomposing the high-dimensional series data into the product of two matrices in a lower-dimensional latent space. For a multivariate time series X \u2208 RT \u00d7C, as shown in Figure 2 the matrix can be approximated by the multiplications of two lower rank embedding matrix, X \u2248 F Xb, in which F \u2208 Rk\u00d7C, Xb \u2208 RT \u00d7k and k is a hyperparameter.\n\nBesides the estimation, there are regularizers to avoid overfitting problems in factorization. Going beyond the canonical design that takes the squared Frobenius norm as regularizers, Temporal regularized matrix factorization (TRMF) [61] designs an autoregressive-based temporal regularizer to describe temporal dependencies among latent temporal embeddings. Further, [62] extended TRMF with a new spatial autoregressive regularizer to estimate low-rank latent factors by simultaneously learning the spatial and temporal autocorrelations. NoTMF [63] integrates the vector autoregressive process with differencing operations into the classical low-rank matrix factorization framework to better model real-world time series data with trend and seasonality. Eliminating the need for tuning regularization parameters, BTF [64] is a fully Bayesian model that integrates the probabilistic matrix factorization and vector autoregressive process into a single probabilistic graphical model. Instead of using an autoregressive-based temporal regularization,", "mimetype": "text/plain", "start_char_idx": 3932, "end_char_idx": 5727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5df8a21a-7f9b-4377-9bfb-96bf9b162416": {"__data__": {"id_": "5df8a21a-7f9b-4377-9bfb-96bf9b162416", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1271b560-90d9-4728-8aca-459ac3f3a978", "node_type": "4", "metadata": {}, "hash": "76a25d979cb8180363a121e30f6e83206d7e5874992179d4648e24f7211da38b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d35fd88-d6c8-4280-a0c5-e51815e859de", "node_type": "1", "metadata": {}, "hash": "c4da3b2d46edec6f7b9a663676c07303967672de1bf0f8a3d631c27821b94f65", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 5\n\n# DeepGLO\n\nDeepGLO [65] utilizes a temporal convolution network for regularization to capture non-linear dependencies. LSTM-GL-ReMF [66] contains an LSTM-based temporal regularizer to learn complex long-term and short-term non-linear temporal correlations and a Graph Laplacian spatial regularizer [67] to capture spatial correlations.\n\n# 3.3 Fourier Analysis\n\nFourier analysis [68], [69] can convert a physical signal into the Fourier domain to highlight the inherent frequency properties of the original data and has been a well-acknowledged analysis tool in extensive areas. Since time series are usually recorded as a sequence of discrete time points by sampling the original continuous signals, Fourier analysis has become one of the mainstream tools in time series modeling and has been demonstrated favorable effectiveness and efficiency [70], [71]. Introducing the Fourier domain not only augments the representation of the original series but also provides a global view since the frequency spectrum distribution, which can indicate essential periodic properties of time series. In practice, Fast Fourier Transform (FFT) [72] and Wavelet Transform (WT) [73] as the basic algorithms connecting the discrete temporal domain to the frequency domain, have gained increasing popularity in the modular design of deep time series models [74], [75], [76], [77], [78], [79], [80], [81]. Existing approaches can be roughly divided into two categories: time-domain and frequency-domain modeling.\n\n# 3.3.1 Time-Domain Modeling\n\nThe fundamental principle behind the Fourier transform is that sequential data can be decomposed and represented by a series of periodic signals. Consequently, it can be used to identify potentially dominant periods and their corresponding frequencies in the data by analyzing the highest amplitude components. As a typical practice, TimesNet [3] employs the Fast Fourier Transform (FFT) to extract the most significant frequencies with the highest amplitude values, subsequently reshaping the 1D time series data into a 2D space based on the identified periods for better representation learning. Following TimesNet, PDF [82] posits that frequencies with larger values facilitate a more discernible distinction between long-term and short-term relationships.\n\nIn addition to exploiting the information of the sequence obtained by the Fourier Transformer, some works attempt to perform efficient computation through the Fast Fourier Transformer. Auto-correlation is a fundamental concept in time series analysis that measures the dependence between observations at different time points within a sequence of data. The Wiener-Khinchin theorem [83] provides a mathematical relationship between the auto-correlation function and the power spectral density (PSD) of a stationary random process, where the auto-correlation function represents the inverse Fourier transform of the PSD. Taking the data as a real discrete-time process, Autoformer [22] proposes an Auto-Correlation mechanism with an efficient Fast Fourier Transforms to capture the series-wise correlation.\n\nThe frequency-domain representation provides information about the amplitudes and phases, where low-frequency components correspond to slower variations or trends in the signal, and high-frequency components capture fine details or rapid variations. A significant body of work has focused on leveraging frequency-domain information to enhance the model\u2019s capability in capturing temporal dependencies. FiLM [84] introduces Frequency Enhanced Layers (FEL) which combine Fourier analysis with low-rank approximation to keep the part of the representation related to low-frequency Fourier components and the top eigenspace to effectively reduce the noise and boost the training speed. FITS [85] integrates a low-pass filter (LPF) to eliminate high-frequency components above a specified cutoff frequency, thereby compressing the model size while preserving essential information. From an opposite idea, FEDformer [86] posits that retaining only low-frequency components is insufficient for time series modeling, as it may dismiss important fluctuations in the data. Based on the above considerations, to capture the global view of time series, FEDformer represents the series by randomly selecting a constant number of Fourier components, including both high-frequency and low-frequency components.\n\n# 3.3.2 Frequency-Domain Modeling\n\nBuilding on time-frequency analysis in signal processing, several approaches have been developed to study time series simultaneously in both the time and frequency domains. ATFN [87] comprises an augmented sequence-to-sequence model that learns the trending features of complex non-stationary time series, along with a frequency-domain block designed to capture dynamic and intricate periodic patterns. TFAD [88] introduces a time-frequency analysis-based model that employs temporal convolutional networks to learn both time-domain and frequency-domain representations.\n\nSome works have developed specialized deep learning architecture to process the frequency domain of time series. STFNet [75] applies Short-Time Fourier Transform to input signals and applies filtering, convolution, and pooling operations directly in the frequency domain.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d35fd88-d6c8-4280-a0c5-e51815e859de": {"__data__": {"id_": "0d35fd88-d6c8-4280-a0c5-e51815e859de", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1271b560-90d9-4728-8aca-459ac3f3a978", "node_type": "4", "metadata": {}, "hash": "76a25d979cb8180363a121e30f6e83206d7e5874992179d4648e24f7211da38b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5df8a21a-7f9b-4377-9bfb-96bf9b162416", "node_type": "1", "metadata": {}, "hash": "1b4b73d5e8c0549d82cb9750ace716f19e46184a6151184e8e630241fd274adb", "class_name": "RelatedNodeInfo"}}, "text": "Based on the above considerations, to capture the global view of time series, FEDformer represents the series by randomly selecting a constant number of Fourier components, including both high-frequency and low-frequency components.\n\n# 3.3.2 Frequency-Domain Modeling\n\nBuilding on time-frequency analysis in signal processing, several approaches have been developed to study time series simultaneously in both the time and frequency domains. ATFN [87] comprises an augmented sequence-to-sequence model that learns the trending features of complex non-stationary time series, along with a frequency-domain block designed to capture dynamic and intricate periodic patterns. TFAD [88] introduces a time-frequency analysis-based model that employs temporal convolutional networks to learn both time-domain and frequency-domain representations.\n\nSome works have developed specialized deep learning architecture to process the frequency domain of time series. STFNet [75] applies Short-Time Fourier Transform to input signals and applies filtering, convolution, and pooling operations directly in the frequency domain. StemGNN [28] combines Graph Fourier Transform (GFT) and Discrete Fourier Transform to model both inter-series correlations and temporal dependencies. EV-FGN [89] uses a 2D discrete Fourier transform on the spatial-temporal plane of the embeddings and performs graph convolutions for capturing the spatial-temporal dependencies simultaneously in the frequency domain. FreTS [90] leverages Discrete Fourier Transform (DFT) to transform the data into the frequency domain spectrum and introduces frequency domain MLPs designed for complex numbers with separated modeling for the real parts and the imaginary parts. FCVAE [91] integrates both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) concurrently. Recent TSLANet [92] propose a lightweight Adaptive Spectral Block (ASB) to replace the self-attention mechanism, which is achieved via Fourier-based multiplications by global and local filters. FourierDiffusion [93] explores extending the score-based SDE formulation of diffusion to complex-valued data and therefore implements time series diffusion in the frequency domain.\n\n# 4 MODEL ARCHITECTURES\n\nAs we have discussed in Section 2, the time series model needs to unearth the intrinsic temporal dependencies and", "mimetype": "text/plain", "start_char_idx": 4242, "end_char_idx": 6627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d33e49d5-09d7-4aa2-8475-9df4452c7a71": {"__data__": {"id_": "d33e49d5-09d7-4aa2-8475-9df4452c7a71", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e212b53-3c61-4de3-8729-495b3c2d4102", "node_type": "4", "metadata": {}, "hash": "66d3c5eb8a06dd6fdfff50b2e4b3e30c44345f33e74426324f123183dd4739be", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 6\n\n|MLP-based|CNN-based|RNN-based|GNN-based|Transformer-based|TSLib Implementation| |\n|---|---|---|---|---|---|---|\n|DLinear|DeepGLO|Autoformer|NSTransformer|TiDE| | |\n|BRITS|LogSparse|Informer|FEDformer|FreTS| | |\n|LSTNet|TFT|N-BEATS|TimeGrad|Pyraformer|Koopa|TimeMixer|\n|2018|2019|2020|2021|2022|2023|2024|\n|DCRNN|MTGNN|GDN|SCINet|Crossformer|iTransformer| |\n|STGCN|ASTGCN|AGCRN| |PatchTST|Mamba| |\n| |Graph WaveNet|StemGNN| |MICN| | |\n| | |THOC| |TimesNet| | |\n\nFig. 3. An overview of representative time series models in chronological order. We mark models with different colors based on their architectures.\n\nIn this section, we provide a technical review of the existing deep time series models. As we have presented in Figure 3, existing works can be classified into five categories based on their backbone architecture, namely MLP-based, RNN-based, CNN-based, GNN-based, and Transformer-based.\n\n# 4.1 Multi-Layer Perceptrons\n\nAs a representation of traditional statistical time series models, the Auto-regressive (AR) model assumes that the model output depends linearly on its own historical values. Inspired by the remarkable performance of auto-regressive models, Multi-Layer Perceptrons (MLP) have become a popular architecture for modeling time series data.\n\nAs a representative work of linear-based models, N-BEATS [24] is a pure MLP-based deep time series model without any time-series-specific knowledge to capture the temporal patterns in time series. Specifically, as described in Equ. (4) N-BEATS consists of deep stacks of fully-connected layers with two residual branches in each layer, one is for the backcast prediction and the other one is the forecast branch. Extending the idea of neural basis expansion analysis, N-HiTs [57] use multi-rate signal sampling and hierarchical interpolation and N-BEATSx [94] incorporate exogenous variables to enhance the prediction.\n\nRecent research by DLinear [52], also referred to as LTSF-Linear, challenges the effectiveness of complicated deep architecture in temporal modeling. It argues a simple linear regression in the raw space that achieves remarkable performance in both modeling and efficiency. As illustrated in Figure 4, prevalent MLP-based deep time series models consist of simple linear layers primarily designed for forecasting tasks. Also lightweight but effective, FITS [85] advocates time series analysis can be treated as interpolation exercises within the complex frequency domain and further introduces a complex-valued linear layer to learn amplitude scaling and phase shift in the frequency domain. Inspired by MLP-Mixer [95] in computer vision, several works have attempted to utilize MLPs to model both temporal and variate dependencies. TSMixer [96] contains interleaving time-mixing and feature-mixing MLPs to extract information from different perspectives. To better model the global dependencies in time series data, FreTS [90] investigates the learned patterns of frequency-domain MLPs which are operated on both inter-series and intra-series scales to capture channel-wise and time-wise dependencies in multivariate data.\n\nRecent works have moved beyond using simple linear layers over discrete time points. TimeMixer suggests that time series exhibit distinct patterns in different sampling scales and proposes an MLP-based multiscale mixing architecture. TiDE [97] incorporates exogenous variables to enhance the time series prediction. Based on Koopman theory and Dynamic Mode Decomposition (DMD) [98], which is a dominant approach for analyzing complicated dynamical systems, Koopa [99] hierarchically disentangles dynamics through an end-to-end predictive training framework and can utilize real-time incoming series for online development.\n\n# 4.2 Recurrent Neural Networks\n\nRecurrent Neural Networks (RNNs) are specifically designed to model sequential data [100], [101], [102], such as natural.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edab247d-1dcf-4124-b515-daf5c16a9c58": {"__data__": {"id_": "edab247d-1dcf-4124-b515-daf5c16a9c58", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5040f2d4-717e-4a69-a249-147390e2c170", "node_type": "4", "metadata": {}, "hash": "83d80ec72c46a220718205cde66bfe3e679ddbbb945a13532b24012a8819e883", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "116c6535-8578-4b48-9088-ab73d523d52e", "node_type": "1", "metadata": {}, "hash": "ee029efb875e20c861d7e708f9ef47d1027e66dae839f2a4e630700d0a0cc271", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 7\n\n# Historical Series\n\n# Future\n\n# Generative Outputs\n\nRNN Cell\nRNN Cell\nRNN Cell\nRNN Cell\nRNN Cell\nRNN Cell\n# Deterministic Output\n\nFig. 5. Illustration of RNN-based model in the forecasting task.\n\nlanguage processing [103] and audio modeling [104]. Since time series are also serial in nature, RNNs have emerged as a popular choice for analyzing time series data [105]. Existing RNN-based deep time series models focus on combating the gradient vanishing problem caused by the vanilla recurrent structure and modeling the mutual correlation among multivariate variables. Previous works [106], [107], [108], [109], [110] use variants of RNN to model temporal dependencies.\n\nLSTNet [111] combines the recurrent structure with the convolutional layer to capture both the short-term local dependency between variables and long-term patterns for time series. Moreover, a novel recurrent-skip component based on the periodic pattern is introduced to alleviate gradient vanishing in modeling long-term dependencies. Similarly, DA-RNN [112] combines the recurrent unit with a dual-stage attention mechanism to adaptively extract relevant series at each time step. Beyond deterministic forecasts, DeepAR [113] proposes an auto-regressive recurrent network model to predict the probability distribution of further time points. Technologically, it learns not only the seasonal behavior with time series but dependencies on given covariates across time series, allowing the model to make predictions even when there is little or no historical data.\n\nAlso based on Markovian state representation, the State Space Model (SSM) [114] is another classical mathematical framework that captures the probabilistic dependence between observed measurements in stochastic dynamical systems. Concretely, a single-input single-output (SISO) linear state space model is defined as follows:\n\ndx(t) = Ax(t) + Bu(t),\n\ndt\n\ny(t) = Cx(t) + Du(t),\n\nwhere u(t), x(t), y(t) are input signal, latent state, and output signal respectively. The system is characterized by the matrices A \u2208 RN \u00d7N , B \u2208 RN \u00d71, C \u2208 R1\u00d7N , D \u2208 R1\u00d71 can be learned by the deep neural network.\n\nSSMs have proven their effectiveness and efficiency in processing well-structured time series data, but traditional approaches have to refit each time series sample separately and therefore cannot infer shared patterns from a dataset of similar time series. With the rise of deep learning models, modern SSMs are often implemented in a recurrent manner. By adapting and propagating a deterministic hidden state, RNNs are able to represent long-term dependencies in continuous data which offer an alternative to classical state space models. Therefore, some work [115], [116] have attempted to fuse classical state space models with deep neural networks. Representative like Deep State Spaces Model (DSSM) [117], using a recurrent neural network (RNN) to parametrize a particular linear SSM, takes advantage of incorporating structural assumptions and learning complex patterns. Structured State Space sequence model (S4) [118] introduces a new parameterization for the SSM by conditioning matrix A with a low-rank correction, allowing it to be diagonalized stably, which empowers the model with better long-term modeling capacity. Similar to S4, LS4 [119] is a generative model with latent space evolution following a state space ordinary differential equations (ODE).\n\nRecent work on Mamba [120] has emerged as a powerful method for modeling long-context sequential data while scaling linearly with sequence length. Utilizing a simple selection mechanism that parameterizes the SSM parameters based on the input, Mamba can discern the importance of information in a manner similar to the attention mechanism, posing a potentially effective way to sequential modeling.\n\n# 4.3 Convolutional Neural Networks\n\nSince the semantic information of time series is mainly hidden in the temporal variation, Convolutional neural networks (CNN) [18], [121] have become a competitive backbone for their ability to capture local features and pattern recognition. By leveraging convolutions and hierarchical feature extraction, CNNs have shown remarkable success in various computer vision tasks, such as image classification [122], segmentation [123] and object detection [124].\n\nConsidering the temporal continuity of time series data, previous works [125], [126], [127] apply one-dimensional CNN (1D CNN) to capture the local patterns of time series data. Recent SCINet [128] applies normal convolutions with a hierarchical downsample-convolve-interact architecture to capture dynamic temporal dependencies at different temporal resolutions of time series data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "116c6535-8578-4b48-9088-ab73d523d52e": {"__data__": {"id_": "116c6535-8578-4b48-9088-ab73d523d52e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5040f2d4-717e-4a69-a249-147390e2c170", "node_type": "4", "metadata": {}, "hash": "83d80ec72c46a220718205cde66bfe3e679ddbbb945a13532b24012a8819e883", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edab247d-1dcf-4124-b515-daf5c16a9c58", "node_type": "1", "metadata": {}, "hash": "82fc455cd6a225486590d33c0b0ba119b9d79726be80b0e37f6b2baed3fe0027", "class_name": "RelatedNodeInfo"}}, "text": "# 4.3 Convolutional Neural Networks\n\nSince the semantic information of time series is mainly hidden in the temporal variation, Convolutional neural networks (CNN) [18], [121] have become a competitive backbone for their ability to capture local features and pattern recognition. By leveraging convolutions and hierarchical feature extraction, CNNs have shown remarkable success in various computer vision tasks, such as image classification [122], segmentation [123] and object detection [124].\n\nConsidering the temporal continuity of time series data, previous works [125], [126], [127] apply one-dimensional CNN (1D CNN) to capture the local patterns of time series data. Recent SCINet [128] applies normal convolutions with a hierarchical downsample-convolve-interact architecture to capture dynamic temporal dependencies at different temporal resolutions of time series data. Inspired by the idea of masked convolution [129], Wavenet [130] introduces causal convolution and dilated causal convolution to model long-range temporal causality. Similar to Wavenet, Temporal Convolutional Networks (TCN) [131] uses a stack of dilated convolutional kernels with progressively enlarged dilation factors to achieve a large receptive field. However, the limited receptive field of TCN makes it difficult for them to capture global relationships in time series data. Based on TCN, MICN [132] is a local-global convolution network that combines different convolution kernels to model temporal correlation from a local and global perspective. ModernTCN [133] boosts the traditional TCN to capture cross-time and cross-variable dependency by DWConv and ConvFFN separately. Considering that DWConv is proposed to learn temporal information, it is operated variate-independently to learn the temporal dependency of each univariate time series.\n\nBeyond 1D space, motivated by the periodicity properties of time series data, TimesNet [3] transforms the 1D time series X1D data into a set of 2D tensors X2D = {X2D, ..., X2D}1 in each TimesBlock based on the estimated period lengths, where the inter-period variations are presented in tensor columns and inner-period ones are shown in tensor rows. Here k is a hyperparameter, corresponding to multiple 1D-to-2D transformations with different periods. Then it applies", "mimetype": "text/plain", "start_char_idx": 3891, "end_char_idx": 6193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d4c0faa-ece9-488f-ba42-a88e23a29076": {"__data__": {"id_": "2d4c0faa-ece9-488f-ba42-a88e23a29076", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08a2bbe8-5a1c-4333-99d6-6b2c07ceab91", "node_type": "4", "metadata": {}, "hash": "1c38e71427d5c92b8e1340d9a464cdf6e3b4077c4793ee33d4326b05f0800913", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 1D Space\n\n# 1D Series\n\n# 2D Space\n\ndependency, alongside a recurrent neural network to capture temporal dynamics. Similarly, STGCN [139] integrates graph convolutional networks to model the spatial dependencies among traffic sensors with temporal convolutions to capture the temporal dependencies in the traffic time series data. Graph WaveNet [140] combines graph convolution with dilated casual convolution and learns an adaptive dependency matrix through node embedding, enabling the model to automatically capture hidden spatial dependencies in spatial-temporal graph data. Similarly, AGCRN [141] enhances the traditional graph convolutional network with node adaptive parameter learning and data-adaptive graph generation modules, allowing for the automatic capture of spatial and temporal correlations without a pre-defined graph structure. MTGNN [142] introduces a graph learning layer to adaptively learn the graph adjacency matrix, thereby capturing hidden relationships among multivariate time series data. STFGNN [143] employs a Spatial-Temporal Fusion Graph Neural Network with a generated temporal graph to learn localized spatial-temporal heterogeneity and global spatial-temporal homogeneity. StemGNN [28] leverages the advantages of both the Graph Fourier Transform (GFT) and the Discrete Fourier Transform (DFT), modeling multivariate time series in the spectral domain.\n\n# 4.5 Transformers\n\nIn the view of the great success in the field of natural language processing [16], [144], [145], [146], [147] and computer vision [19], [148], [149], [150], Transformers have also emerged as a powerful backbone for time series analysis. Benefiting from the self-attention mechanism [151], Transformer-based models can capture long-term temporal dependencies and complex multivariate correlations. As overviewed in Figure 8, existing Transformer-based time series models can be categorized based on the granularity of representation used in the attention mechanism, namely point-wise, patch-wise, and series-wise approaches.\n\n# 4.5.1 Point-wise Dependency\n\nDue to the serial nature of time series, most existing Transformer-based works use a point-wise representation of time series data and apply attention mechanisms to capture the correlations among different time points. Among these point-wise modeling approaches, Data Embedding is a crucial component that maps the value of time series data to a high-dimensional representation. Given time series X \u2208 RT \u00d7C with corresponding time stamp information Xmark \u2208 RT \u00d7D, where C is the variate number and D is the types of time stamps, the embedding module can be summarized as follow:\n\nHt = Projection(Xt) + PE(Xt) + TE(Xtmark)\n\nwhere Ht \u2208 RT \u00d7dmodel and dmodel is the dimension of the embedded representation, value projection Projection : C 7\u2192 Rdmodel and timestamp embedding TE : RD 7\u2192 Rdmodel are implemented by channel-dimension linear layers, and PE(\u00b7) denotes the absolute position embedding to preserve the sequential context of input series.\n\nThe core goal of GNN architecture is to model the underlying topological relations in multivariate data, therefore existing GNN-based works can be roughly divided into two categories based on whether graph structure is part of the input into the model. DCRNN [138] models the spatial dependency of traffic as a diffusion process on a directed graph and uses diffusion convolution to capture the spatial dependency of traffic.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c99364df-7b1f-4e9f-bd5a-7e117c434a74": {"__data__": {"id_": "c99364df-7b1f-4e9f-bd5a-7e117c434a74", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f69f847-5fba-4a5c-b825-83a53aa31d7d", "node_type": "4", "metadata": {}, "hash": "672a7cd4f4c06623a4b3425c7c97f11d550af807b4053fb364bae4e5ae6491b5", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# TABLE 2\n\n# Model cards of Transformer-based deep time series models with architectural details.\n\n|Category|Method|Architecture|Embedding|Attention Mechanism|\n|---|---|---|---|---|\n|Vanilla|Transformer|Enc-Dec|Standard|FullAttention(Q, K, V) = Softmax( QK\u22ba)\u221ad|\n|LogSparse [152]|Dec-only|Standard|Qb, K = CausualCov(H)b|FullAttention(b, K, V) = Softmax( \u221ad ) Qb K\u22ba|\n|Point-wise|Informer [21]|Enc-Dec|Standard|ProbSparse-Attention(Q, K, V) = Softmax( QK\u22ba)V\u221ad|\n| |Pyraformer [153]|Enc-Dec|Standard|Pyramid-Attention(Q, K, V) = Masked(Softmax( QK\u22ba)V)\u221ad|\n| |Autoformer [22]|Enc-Dec|Standard|Auto-Correlation(Q, K, V) = Pk=1 Roll(V, \u03c4) Ri|\n|Patch-wise|Crossformer [29]|Enc-Dec|Patch-Wise|FullAttention(Q, K, V) = Softmax( QK\u22ba)\u221ad|\n| |PatchTST [23]|Enc-only|Patch-Wise|FullAttention(Q, K, V) = Softmax( QK\u22ba)\u221ad|\n|Variate-wise|iTransformer [30]|Enc-only|Variate-Wise|FullAttention(Q, K, V) = Softmax( QK\u22ba)\u221ad|\n\nchance to contain the dominant information in self-attention.\n\nRepresentative Work Based on the proposed sparsity measurement, it further designs a ProbSparse self-attention only using top queries with the biggest measurement results, which can reduce the quadratic complexity in time and memory. Pyraformer [153] constructs a multi-resolution C-ary tree and develops a Pyramidal Attention Mechanism, in which every node can only attend to its neighboring, adjacent, and children nodes. With the calculated mask for attention, Pyraformer can capture both short- and long-temporal dependencies with linear time and space complexity.\n\n# 4.5.2 Patch-wise Dependency\n\nPatch-wise Token PatchTST\u201a Crossformer\u201a TimeXer Patch-based architectures play a crucial role in the Transformer models for both Natural Language Processing (NLP) [16] and Computer Vision (CV) [19]. Since point-wise representations are insufficient to capture local semantic information in temporal data, several studies [23], [154], [155] have been devoted to exploring patch-level temporal dependencies within time series data.\n\nPioneer work Autoformer [22] proposes an Auto-Correlation Mechanism, which captures the series-wise dependencies of time series to replace canonical point-wise self-attention. Based on the stochastic process theory [156], Auto-Correlation utilizes the Fast Fourier Transform to discover the time-delay similarities between different sub-series. A time delay module is further proposed to aggregate the similar sub-series from underlying periods instead of the relation between scattered points, which firstly explores the sub-series level modeling in Transformer-based models.\n\nGiven that the canonical attention approach leads to quadratic computational complexity, numerous efficient Transformers [21], [22], [86], [153] have been proposed to mitigate the complexity caused by point-wise modeling, which is summarized in Table 2. LogSparse [152] proposes Convolutional Self-Attention to replace canonical attention by employing causal convolutions to produce queries and keys in the self-attention layer. Informer [21] introduces a Query Sparsity Measurement, where a larger value indicates a higher number of patches split, and Pi denotes the i-th patch with sequence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01586bee-35d3-4c3e-a451-1f73944b245c": {"__data__": {"id_": "01586bee-35d3-4c3e-a451-1f73944b245c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e240ef1-84e1-4611-943e-0fab87df0e42", "node_type": "4", "metadata": {}, "hash": "2d84b6ae6864b6a10acfe5abafdd314d4c7c5a53cbb47083ac6e419f0a20647e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff6e763a-f82e-4d33-8496-0596bd304fc5", "node_type": "1", "metadata": {}, "hash": "126a9db7b5740534c746ab8bf491d761f79ed61e69d2bbb5b28340187375d9d1", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 10\n\nlength P. The patches are mapped to the latent space through and a learnable position embedding Wpos \u2208 RP 7\u2192 RN.\n\na temporal linear projection PatchEmbed : Rdmodel\u00d7dmodel\n\nBased on the vanilla attention mechanism, PatchTST [23] learns the patch-wise dependencies. Going beyond PatchTST, recent Pathformer [158] proposes a multi-scale Transformer-based model with adaptive pathways. Based on the patch division of different scales, the adaptive pathways select the patch sizes with the top K weights generated by the router to capture multi-scale characteristics.\n\nThe success of PatchTST also benefits from channel-independence design, where each temporal patch-level token only contains information from a single series. In addition to capturing the patch-level temporal dependencies within one single series, recent approaches [54], [157] have endeavored to capture interdependencies among patches from different variables over time. Crossformer [29] introduces a Two-Stage Attention layer containing a Cross-Time Stage and a Cross-Dimension Stage to efficiently capture the cross-time and cross-variate dependencies between each patch token. For the obtained embedded vector H \u2208 RN \u00d7C\u00d7dmodel, the overall attention stage can be described as follow:\n\nZtime = MSAtime (H, H, H)\n\ndim R, Ztime, Z ,time\n\nB = MSA1\n\nZdim = MSA2dim Ztime, B, B\n\nwhere R \u2208 RN\u00d7C\u00d7dmodel is a learnable vector array used as a router to gather information from all dimensions and then distribute the gathered information.\n\n# 4.5.3 Series-wise Dependency\n\nFurther expanding the receptive field, there are also some works that attempt to use the tokenization of the whole time series to capture inter-series dependencies. iTransformer [30] introduce VariateEmbed to multivariate data, and for i-th variable X(i), it can be simply formulated as follows:\n\nH(i) = VariateEmbed(X(i))\n\nwhere VariateEmbed : RT \u2192 Rdmodel is instantiated as trainable linear projector. Based on the global representations of each series, iTransformer utilizes the vanilla Transformer without any architectural modifications to capture mutual correlations in multivariate time series data. Similarly, TimeXer [159] focuses on forecasting with exogenous variables and utilizes patch-level and series-level representations for endogenous and exogenous variables, respectively. Additionally, an endogenous global token is introduced to TimeXer, which serves as a bridge in-between and therefore captures intra-endogenous temporal dependencies and exogenous-to-endogenous correlations jointly.\n\n# 5 TIME SERIES LIBRARY\n\nTime series analysis has emerged as an important research area, attracting significant attention from both academia and industry. Recently, extensive exploration of deep learning based methods for time series analysis has resulted in significant advances. However, the issue of fair benchmarking poses a pressing challenge in this domain. The absence of fair, rational, and comprehensive benchmarks can lead to biased comparisons between different methods and hinder accurate evaluation of their effectiveness, potentially inflating domain advances or hindering practical applications. This presents a substantial obstacle to understanding advances and fostering robust development within the field.\n\nIn the domain of time series analysis, several benchmarks have been proposed, such as DGCRN [160], LibCity [161], DL-Traff [162], TS-bench [163], and BasicTS [14]. More specifically, Autoformer [22] proposed a standard long-term forecasting benchmark covering different practical applications. Further, to verify the generality of different time series analysis models, TimesNet [3] builds a more comprehensive model generalization benchmark covering five mainstream time series analysis tasks. However, these benchmarks typically have some limitations. One issue with current time series benchmarks is their limited coverage of time series analysis tasks and specific domains, which limits their practical applications. Moreover, these benchmarks often fail to provide detailed discussions and comprehensive summaries of task types, model architectures, and specific baseline methods. As a result, they do not effectively guide the design of more efficient time series analysis methods or drive further development in the field.\n\nTo effectively address these issues, we introduce and implement Time Series Library (TSLib), a benchmark for fair and comprehensive comparing and evaluating the performance of deep time series models across various time series analysis tasks. As shown in Figure 9, TSLib encompasses a unified model experiment pipeline, standardized evaluation protocols, extensive and diverse real-world datasets, mainstream and advanced time series analysis models, and unified experimental validation and analysis process.\n\nIn our Time Series Library, we meticulously followed the official codes and implemented 24 widely used and advanced deep time series analysis models. These models are derived from four canonical deep learning architectures. Users can choose from these models based on their specific practical usage scenarios. The code is available at https://github.com/thuml/Time-Series-Library.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff6e763a-f82e-4d33-8496-0596bd304fc5": {"__data__": {"id_": "ff6e763a-f82e-4d33-8496-0596bd304fc5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e240ef1-84e1-4611-943e-0fab87df0e42", "node_type": "4", "metadata": {}, "hash": "2d84b6ae6864b6a10acfe5abafdd314d4c7c5a53cbb47083ac6e419f0a20647e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01586bee-35d3-4c3e-a451-1f73944b245c", "node_type": "1", "metadata": {}, "hash": "f1b2ab53c37ff0f60710bec823833b2d11f9067488cff0f45fb605cf387d9528", "class_name": "RelatedNodeInfo"}}, "text": "As a result, they do not effectively guide the design of more efficient time series analysis methods or drive further development in the field.\n\nTo effectively address these issues, we introduce and implement Time Series Library (TSLib), a benchmark for fair and comprehensive comparing and evaluating the performance of deep time series models across various time series analysis tasks. As shown in Figure 9, TSLib encompasses a unified model experiment pipeline, standardized evaluation protocols, extensive and diverse real-world datasets, mainstream and advanced time series analysis models, and unified experimental validation and analysis process.\n\nIn our Time Series Library, we meticulously followed the official codes and implemented 24 widely used and advanced deep time series analysis models. These models are derived from four canonical deep learning architectures. Users can choose from these models based on their specific practical usage scenarios. The code is available at https://github.com/thuml/Time-Series-Library.\n\nAs follows, we will provide a detailed description of our TSLib, including the design and implementation principles (Section 5.1), the evaluation protocols and metrics (Section 5.2), the dataset descriptions (Section 5.3), and the main results of models with different architectures (Section 5.4).\n\n# 5.1 Design and Implementation Principle\n\nTSLib is designed based on the well-established factory pattern and implements a unified interface between data and model objects, thus enabling a clear separation between deep model creation and usage, promoting modularity and flexibility. By loading different data and model objects and combining specific task heads during model training, TSLib enables different datasets and models to be shared and extended, allowing easy switching between various time series analysis tasks. These design and implementation principles provide enhanced flexibility and scalability for our TSLib. Furthermore, as illustrated in Figure 9, TSLib introduces a unified experimental pipeline covering the overall process of the model training and evaluation, which includes data source, data processing, model training and analysis, and model performance evaluation.", "mimetype": "text/plain", "start_char_idx": 4238, "end_char_idx": 6465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "852b9889-56c5-48ea-80fc-f40c85da03ea": {"__data__": {"id_": "852b9889-56c5-48ea-80fc-f40c85da03ea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05888620-a24b-45ce-9a64-b4328b9fb53b", "node_type": "4", "metadata": {}, "hash": "8ca46c37fc83c0c67c0a15b89e47c6d91119f27f1d2f4e127b130d509664e912", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "607beb3c-c513-4d3a-9e3f-ab7f7536155c", "node_type": "1", "metadata": {}, "hash": "44ca02a9e4b0db24f891bdd31919653ef3f77118eaf411f65d21f2c119d5b2b8", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# Performance Evaluation\n\n|Data Sources|Layers|Time Series Analysis Tasks|\n|---|---|---|\n|HealthCare|Normalization|Classification|\n|Electricity|Design Principles|- Accuracy, F1-Score \u2026|\n|Weather|Decomposition|- Mean, Std\u2026|\n|Finance|- Trend, Seasonal, Remainder\u2026|Imputation|\n|Traf\ufb01c|Fourier Analysis|- MSE, MAE \u2026|\n|e-Market|- FFT, DFT\u2026|Forecasting|\n| |Models|- MSE, MAE, SMAPE \u2026|\n| |Transformer, CNN, RNN, MLP|Anomaly Detection|\n| |iTransformer, SCINeT, LSTNet, DLinear|- Accuracy, F1-Score \u2026|\n| |PatchTST, TCN, DeepAR, N-HiTS| |\n| |FedFormer, MICN, DA-RNN, N-BEATS| |\n| |Autoformer, TimesNet, Mamba, TiDE| |\n\nFig. 9. Architecture and experiment pipeline of Time Series Library. Left: Unified training and evaluation process. Right: Overall Architecture.\n\n# Data Source\n\nOur TSLib provides extensive support for a wide range of diverse and multi-type datasets in a variety of formats, including \".csv\", \".npz\", \".txt\", etc. As shown in Figure 9 and Table 3, TSLib currently supports more than 30 datasets with different sampled frequencies across four mainstream time series analysis tasks, all derived from real-world scenarios in domains such as energy, transportation, economics, weather, and medicine, etc. Moreover, TSLib excels in scalability, allowing for the effortless integration of new data sources of different data types.\n\n# Data Processing\n\nData processing plays a pivotal role in guaranteeing stable training within the realm of time series analysis. Within the Time Series Library, a multitude of data processing steps are conducted, including time window splitting, data batch generation, etc. Subsequently, the raw data is partitioned into separate sets for training, validation, and testing purposes, enabling streamlined model training and equitable comparisons. These steps serve as indispensable prerequisites for attaining precise and dependable results across a range of diverse time series analysis tasks.\n\nMoreover, our TSLib provides additional support for numerous crucial and effective data processing strategies based on different model design principles [39], [40], [41], [45] to enhance model performance and training efficiency. We encapsulate these various design strategies within our basic data processing layer, encompassing techniques such as data normalization, time-frequency decomposition, Fourier analysis, and more. When utilizing TSLib, users have the flexibility to select these strategies to improve training effect based on their specific requirements and objectives.\n\n# Model Training and Analysis\n\nAfter the data processing phase, the raw time series data is transformed into the desired format for model training. Model training forms the crux of the entire experiment pipeline, where we fine-tune the model parameters based on the input to predict the output with minimal error. Our primary goal during model training is to obtain the best possible trainable parameters that result in a significant improvement in model performance. Each model has its own unique design and training objective. The model analysis procedure is to determine the optimal model parameters by comparing the correlation between the training and validation losses. Our TSLib includes complete log printing and result storage functions record and evaluate the training process. By employing rational model analysis techniques, we can efficiently obtain models with superior performance and stronger generalization.\n\n# Performance Evaluation\n\nModel evaluation is a crucial step in verifying the effectiveness and generalization of trained time series models. It involves model prediction and performance evaluation, providing insights into the efficacy of the trained model. In TSLib, we provide evaluation support for four mainstream time series analysis tasks: classification, imputation, forecasting (long-term or short-term), and anomaly detection. Each task comes with its specific evaluation metric, enabling a comprehensive assessment of the performance of models. These metrics play a crucial role in determining the effectiveness of the trained model and its suitability for the intended task.\n\n# 5.2 Evaluation Protocols\n\nIn order to conduct a fair and comprehensive model performance verification, our Time Series Library is designed to provide standardized evaluation protocols for four mainstream time series analysis tasks following [3]. The primary goal of these standardized and unified evaluation protocols is to quantify the effectiveness of different time series analysis methods with varying architectures. Additionally, they provide valuable insights into the strengths and limitations of different methods across diverse time series analysis tasks. By establishing these standardized evaluation protocols, we aim to promote fair comparisons between different methods and improve our understanding of their performance in various time series analysis scenarios.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "607beb3c-c513-4d3a-9e3f-ab7f7536155c": {"__data__": {"id_": "607beb3c-c513-4d3a-9e3f-ab7f7536155c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05888620-a24b-45ce-9a64-b4328b9fb53b", "node_type": "4", "metadata": {}, "hash": "8ca46c37fc83c0c67c0a15b89e47c6d91119f27f1d2f4e127b130d509664e912", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "852b9889-56c5-48ea-80fc-f40c85da03ea", "node_type": "1", "metadata": {}, "hash": "c58cc5755f7361081161d50b1a0ffc1f496718c5ca370affd2d5b88faa7ca33d", "class_name": "RelatedNodeInfo"}}, "text": "In TSLib, we provide evaluation support for four mainstream time series analysis tasks: classification, imputation, forecasting (long-term or short-term), and anomaly detection. Each task comes with its specific evaluation metric, enabling a comprehensive assessment of the performance of models. These metrics play a crucial role in determining the effectiveness of the trained model and its suitability for the intended task.\n\n# 5.2 Evaluation Protocols\n\nIn order to conduct a fair and comprehensive model performance verification, our Time Series Library is designed to provide standardized evaluation protocols for four mainstream time series analysis tasks following [3]. The primary goal of these standardized and unified evaluation protocols is to quantify the effectiveness of different time series analysis methods with varying architectures. Additionally, they provide valuable insights into the strengths and limitations of different methods across diverse time series analysis tasks. By establishing these standardized evaluation protocols, we aim to promote fair comparisons between different methods and improve our understanding of their performance in various time series analysis scenarios.\n\nFor long-term forecasting and imputations, we rely on Mean Square Error (MSE) and Mean Absolute Error (MAE) as the primary evaluation metrics. These metrics help us accurately assess the accuracy of our predictions and imputations. For short-term forecasting, we use the Symmetric Mean Absolute Percentage Error (SMAPE) and Mean Absolute Scaled Error (MASE) as metrics, which focus on absolute errors and reduce the impact of outliers, providing reliable.", "mimetype": "text/plain", "start_char_idx": 3781, "end_char_idx": 5445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98449be0-2d90-4f31-a2e0-b4c315b87f4a": {"__data__": {"id_": "98449be0-2d90-4f31-a2e0-b4c315b87f4a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "165b8571-561f-49fa-a5d3-7f1bda24f647", "node_type": "4", "metadata": {}, "hash": "e9138944f105971b7de388ddd87b1788c2befa5394b4a1f0f4361f1d4ab9b7a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05fb2e3e-a7df-4d2d-85e0-6d8c8cbc4fbf", "node_type": "1", "metadata": {}, "hash": "85b331091d63ea0cdc50cfbd762ecf92295ff7231b65fd69d8e0a4cbc8cd56db", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 12\n\n# TABLE 3\n\n# Summary of supported experimental datasets in Time Series Library.\n\n|Task|Dataset|Dimension|Length|Domain|Size|\n|---|---|---|---|---|---|\n|Classification|EthanolConcentration|3|1,751|Alcohol Industry|20.3 MB|\n| |FaceDetection|144|62|Face (250 Hz)|789.1 MB|\n| |Handwriting|3|152|Motion|3.9 MB|\n| |Heartbeat|61|405|Health (0.061 secs)|87.8 MB|\n| |JapaneseVowels|12|29|Voice|1.1 MB|\n| |PEMS-SF|963|144|Transportation (1 day)|420.1 MB|\n| |SelfRegulationSCP1|6|896|Health (256 Hz)|17.8 MB|\n| |SelfRegulationSCP2|7|1,152|Health (256 Hz)|17.7 MB|\n| |SpokenArabicDigits|13|93|Voice (11025 Hz)|37.6 MB|\n| |UWaveGestureLibrary|3|315|Gesture|3.4 MB|\n|Imputation|ETTh1, ETTh2|7|17,420|Electricity (1 hour)|10.4 MB|\n| |ETTm1, ETTm2|7|69,680|Electricity (15 mins)|2.6 MB|\n| |Electricity|321|26,304|Electricity (1 hour)|95.6 MB|\n| |Weather|21|52,696|Environment (10 mins)|7.2 MB|\n| |ETTh1, ETTh2|7|17,420|Electricity (1 hour)|10.4 MB|\n| |ETTm1, ETTm2|7|69,680|Electricity (15 mins)|2.6 MB|\n| |Electricity|321|26,304|Electricity (1 hour)|95.6 MB|\n|Long-term Forecasting|Weather|21|52,696|Environment (10 mins)|7.2 MB|\n| |Traffic|862|17,544|Transportation (1 hour)|136.5 MB|\n| |Exchange|8|7,588|Economic (1 day)|623 KB|\n| |ILI|7|966|Health (1 week)|66 KB|\n| |M4-Yearly|1|6|Demographic| |\n| |M4-Quarterly|1|8|Finance| |\n|Short-term Forecasting|M4-Monthly|1|18|Industry|589.5 MB|\n| |M4-Weakly|1|13|Macro| |\n| |M4-Daily|1|14|Micro| |\n| |M4-Hourly|1|48|Other| |\n| |SMD|38|100|Industry (1 min)|436.4 MB|\n| |MSL|55|100|Industry (1 min)|58.2 MB|\n|Anomaly Detection|SMAP|25|100|Industry (1 min)|113.0 MB|\n| |SwaT|51|100|Industry (1 min)|903.2 MB|\n| |PSM|25|100|Industry (1 min)|107.1 MB|\n\nEvaluations of forecast accuracy across different datasets and methodologies. In the case of time series classification tasks, we utilize Accuracy as the evaluation metric. Accuracy measures the overall prediction performance by calculating the ratio of correctly classified samples to the total number of samples. For anomaly detection, we employ the F1 score to validate the identification of abnormal values. The F1 score represents a balanced combination of precision and recall, offering a comprehensive assessment of a classifier\u2019s performance, especially when dealing with imbalanced classes in the context of anomaly detection.\n\n# 5.3 Datasets\n\nTSLib includes a variety of mainstream datasets across different numbers of samples and categories, a richness of tasks, and a diversity of domains. In this section, we will focus on introducing representative datasets from various time series analysis tasks included in the Time Series Library.\n\nClassification Time series classification aims to assign a label or category to a time series based on its temporal features. To evaluate this capability, we selected ten multi-variate datasets from the UEA Time Series Classification Archive [164], supported in our TSLib. These datasets cover a range of practical tasks, including gesture, action, audio recognition, and medical diagnosis through heartbeat monitoring. We pre-processed the datasets according to the descriptions provided in [165]. Detailed dataset descriptions are shown in Table 3.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05fb2e3e-a7df-4d2d-85e0-6d8c8cbc4fbf": {"__data__": {"id_": "05fb2e3e-a7df-4d2d-85e0-6d8c8cbc4fbf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "165b8571-561f-49fa-a5d3-7f1bda24f647", "node_type": "4", "metadata": {}, "hash": "e9138944f105971b7de388ddd87b1788c2befa5394b4a1f0f4361f1d4ab9b7a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98449be0-2d90-4f31-a2e0-b4c315b87f4a", "node_type": "1", "metadata": {}, "hash": "27dc6e9a31c9d63494b5a69da08b106f1305e0466df0310057bfdeac18e2421a", "class_name": "RelatedNodeInfo"}}, "text": "# 5.3 Datasets\n\nTSLib includes a variety of mainstream datasets across different numbers of samples and categories, a richness of tasks, and a diversity of domains. In this section, we will focus on introducing representative datasets from various time series analysis tasks included in the Time Series Library.\n\nClassification Time series classification aims to assign a label or category to a time series based on its temporal features. To evaluate this capability, we selected ten multi-variate datasets from the UEA Time Series Classification Archive [164], supported in our TSLib. These datasets cover a range of practical tasks, including gesture, action, audio recognition, and medical diagnosis through heartbeat monitoring. We pre-processed the datasets according to the descriptions provided in [165]. Detailed dataset descriptions are shown in Table 3.\n\nImputation Due to glitches, the collected time series data may contain partially missing values, posing a challenge for time series analysis. Therefore, time series imputation is a crucial task in real-world applications, aiming to fill in missing values within a time series based on contextual observations from the data. For our benchmark, we selected Electricity Transformer Temperature (ETT) [21], Electricity, and Weather to evaluate the performance of time series imputation tasks with different missing ratios.\n\nForecasting Time series forecasting is an essential task in time series analysis, and it has been widely explored in academic and industry domains. By leveraging historical patterns and trends, the model can predict future values or trends of a time series. Time series forecasting can be broadly divided into two types: long-term forecasting and short-term forecasting. For the long-term time series forecasting task, a wide range of datasets are included in our benchmark, including Electricity Transformer Temperature (ETT), Electricity, Weather, Traffic, Exchange, and Illness (ILI). For the short-term forecasting task, we selected the M4 dataset [166], which comprises six sub-datasets with varying sampling frequencies and domains.", "mimetype": "text/plain", "start_char_idx": 2402, "end_char_idx": 4525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b467249-165b-413e-89c7-7517258708f8": {"__data__": {"id_": "2b467249-165b-413e-89c7-7517258708f8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a71ee94d-9a42-4a52-a668-e4654d1fc481", "node_type": "4", "metadata": {}, "hash": "c2b0ce0acb9769180a61b53996edf1383a47a4b8587521aa0eb0177c7e427ca6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d1346be-639b-4e1d-9840-36b11a9b09d1", "node_type": "1", "metadata": {}, "hash": "2063ede2582d7b5fe6c9de2403b669244e5d3569b9bbc002c0421b4e8a41c7aa", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# Long-term Forecasting (Unified hyperparameter)\n\n# Long-term Forecasting (Hyperparameter searching)\n\n# Short-term Forecasting\n\n| |Average MSE|Average MSE|Average MSE|\n|---|---|---|---|\n|StationaryFEDformer|0.589|1.803|0.490|\n|N-BEATS|0.559|0.436|0.430|\n|StationaryFEDformer|0.459|0.394|0.386|\n|N-BEATS|0.436|0.429|0.427|\n|StationaryFEDformer|0.414|1.130|0.403|\n|N-BEATS|0.352|1.075|0.381|\n|StationaryFEDformer|0.332|1.030|1.000|\n|N-BEATS|0.373|0.992|0.371|\n| |0.317| |0.945|\n| |0.313| |0.922|\n| |0.305|0.871|0.888|\n| |0.890| | |\n\n# StationaryFEDformer\n\n# N-BEATS\n\n# Imputation\n\n# Classification\n\n# Anomaly Detection\n\n| |Average Accuracy (%)|Average MSE|\n|---|---|---|\n|StationaryFEDformer|85.24|0.117|\n|N-BEATS|73.67|0.119|\n| |73.17|0.118|\n| |72.50|0.112|\n| |72.66|71.71|\n| |83.45|0.050|\n| |71.07| |\n| |70.67| |\n| |70.71|82.79|\n| |82.46|0.077|\n| |0.069|0.065|\n| |0.062|0.057|\n| |0.059|0.050|\n\n# Fig. 10. Comparison of model performance in Time Series Library.\n\nFull results are averaged from a diverse set of datasets supported by TSLib across four mainstream analysis tasks. In the classification and anomaly detection tasks, higher Accuracy or F1-score indicates better forecasting performance. Conversely, lower average MSE or MAE demonstrate superior model effectiveness for the other tasks. The top three models for each time analysis task are highlighted on a leaderboard, and the optimal model for each time series analysis task is indicated.\n\n# Long-term Forecasting\n\n# (Average MSE)\n\n|Long-term Forecasting (*)|Anomaly Detection|\n|---|---|\n|(Average MSE)|(Average F1-Score)|\n|0.342|86.24|\n|0.305|82.46|\n|0.386| |\n|0.118|0.922|\n|Short-term Forecasting|Imputation|\n|(Average OWA)|(Average MSE)|\n|0.871|0.050|\n|Classi\ufb01cation| |\n|(Average Accuracy)| |\n\n# Fig. 11. Overall performance of models with different deep architectures.\n\n# Anomaly Detection\n\nAnomaly detection involves identifying unusual or abnormal patterns in a time series. These anomalies can indicate critical events, faults, or outliers that require attention or further investigation. There are some mainstream anomaly detection datasets supported in TSLib, such as Server Machine Dataset (SMD) [167], Mars Science Laboratory rover (MSL) [168], Soil Moisture Active Passive satellite (SMAP) [168], Secure Water Treatment (SWaT) [169], and Pooled Server Metrics (PSM) [170] which are collected from a variety of industrial scenarios.\n\n# 5.4 Main Results\n\nTo examine the strengths and limitations of various methods in mainstream time series analysis tasks, we select twelve representative models from our TSLib. These models encompass four popular deep model architectures and address tasks, including long-term and short-term forecasting, classification, imputation, and anomaly detection.\n\n# Baselines\n\nTo conduct a fair comparative analysis and thoroughly explore the effectiveness of different model architectures in various time series analysis tasks, we conduct comparative comparison experiments using state-of-the-art models designed based on different deep architectures. As shown in Figure 11, we select several advanced and representative Transformer-based models: iTransformer [30], PatchTST [23], Autoformer [22], Non-Stationary Transformer (Stationary) [41], and FEDformer [86] to verify the performance. Additionally, we consider TimesNet [3] and SCINet [128] as the CNN-based models to compare. For RNN-based models, we included the novel and effective Mamba [120].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d1346be-639b-4e1d-9840-36b11a9b09d1": {"__data__": {"id_": "7d1346be-639b-4e1d-9840-36b11a9b09d1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a71ee94d-9a42-4a52-a668-e4654d1fc481", "node_type": "4", "metadata": {}, "hash": "c2b0ce0acb9769180a61b53996edf1383a47a4b8587521aa0eb0177c7e427ca6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b467249-165b-413e-89c7-7517258708f8", "node_type": "1", "metadata": {}, "hash": "f87f2132fad07f59cb4affb97d2f6f0bd6e9889117ddc5bff697d7bfa4330e2f", "class_name": "RelatedNodeInfo"}}, "text": "These models encompass four popular deep model architectures and address tasks, including long-term and short-term forecasting, classification, imputation, and anomaly detection.\n\n# Baselines\n\nTo conduct a fair comparative analysis and thoroughly explore the effectiveness of different model architectures in various time series analysis tasks, we conduct comparative comparison experiments using state-of-the-art models designed based on different deep architectures. As shown in Figure 11, we select several advanced and representative Transformer-based models: iTransformer [30], PatchTST [23], Autoformer [22], Non-Stationary Transformer (Stationary) [41], and FEDformer [86] to verify the performance. Additionally, we consider TimesNet [3] and SCINet [128] as the CNN-based models to compare. For RNN-based models, we included the novel and effective Mamba [120]. Finally, we include DLinear [52], N-BEATS [24], and TiDE [97] as representative MLP-based models for analysis. It is important to mention that TiDE [97] is designed to be dependent on specific timestamps, and cannot be easily adapted to part tasks without timestamps, such as short-term forecasting, anomaly detection, and classification.\n\n# Unified Experimental Settings\n\nFor the long-term forecasting task, we conducted two experimental settings to ensure a fair and comprehensive comparison: unified hyperparameter and hyperparameter searching. For the unified hyperparameter, we conducted experiments using a range of standardized hyperparameters set across different datasets. This allows us to accurately evaluate the relative performance of time series models with different deep architectures while keeping other factors constant. As for the \u201chyperparameter searching\u201d scenario, we conducted separate hyperparameter searches for different model architectures and time series datasets. This approach enabled us to identify the best performance of different time series analysis models. By employing above both settings, we obtain a comprehensive understanding of the forecasting performance of different time series models. For the remaining tasks, we maintained the standard experimental settings as outlined in [3] to validate the performance of different time series models.", "mimetype": "text/plain", "start_char_idx": 2666, "end_char_idx": 4920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a283d5aa-48c6-4da9-ba86-4605d13b9484": {"__data__": {"id_": "a283d5aa-48c6-4da9-ba86-4605d13b9484", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97149929-3cc1-409a-a111-5a20b6a9ed80", "node_type": "4", "metadata": {}, "hash": "7107477183d8e19f2a9dee85f3e1be45b7939ea710972337aca3b36688cff1e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04477231-c1f4-46f8-ad28-380cd775e0e4", "node_type": "1", "metadata": {}, "hash": "b6bc9d9c1491e012cc554e4f7cfddc30836928b725ef77f49ff53db2000743bf", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# Overall Results\n\nBased on the overall results of the models across different architectures in Figure 11, we surprisingly find that the MLP-based models, which are generally simpler and have lower computational overhead, perform well on the time series forecasting task. However, these models appear to be less effective in other types of tasks, which requires the model to learn more informative representations. On the contrary, the CNN-based models demonstrate more comprehensive capabilities and excel in classification, imputation, and anomaly detection tasks. The RNN-based models, while performing well on anomaly detection tasks, show limited effectiveness compared to other model architectures. In contrast, the Transformer-based models have demonstrated highly competitive performance across various time series analysis tasks. This can be attributed to the powerful data modeling capabilities inherent in the transformer architecture, which contribute to its overall and consistently superior performance across diverse time series analysis tasks. It further shows that Transformer-based models hold significant research value and application potential in the field of time series analysis and have emerged as a particularly promising option in the time series domain.\n\nAs illustrated in Figure 10, we have also included more detailed results and a top three performance leaderboard for four representative time series analysis tasks. These results clearly show that the Transformer-based models, namely iTransformer [30] and PatchTST [23], exhibit superior forecasting capabilities compared to other models for both long-term and short-term forecasting tasks. This further proves that it is of great significance and value to explore different modeling methods of temporal tokens in time series. Additionally, TimesNet [3] shows a more comprehensive and effective performance covering time series classification, imputation, and anomaly detection tasks. It has pioneered a milestone in the general time series analysis model.\n\nWe believe that TSLib can provide useful start code, valuable insights on model properties and model selection guidance for future research and real-world applications.\n\n# 6 FUTURE DIRECTIONS\n\nIn this section, we provide a discussion on the promising directions for time series analysis.\n\n# 6.1 Time Series Pre-training\n\nA pretraining-finetuning learning paradigm is a two-stage approach commonly used in nature language processing (NLP) [171], [16], [172], [173], [173] and computer vision (CV) [174], [175], [176], [177]. Pre-training establishes the basis of the abilities of Large Models through unsupervised learning [178], [179]. Fine-tuning can improve the performance of the pre-trained model on a specific task or domain.\n\nDue to the limited availability of labeled datasets, self-supervised pre-training [175] has garnered significant attention and has been extensively investigated in the domains of natural language modeling and computer vision. Self-supervised pre-training paradigm significantly reduces labeling expenses and benefits for diverse downstream tasks. Notably, recent research efforts have introduced several self-supervised pre-training methods tailored for time series data, which can be primarily classified into contrastive learning [180] and masked time series modeling [16], [181].\n\nContrastive learning refers to learning the representations of data by contrasting between similar and dissimilar pairs, where similar sample pairs are learned to be close to each other and dissimilar pairs are far apart [180], [182], [183]. Although SimCLR [182] has demonstrated remarkable success in the domain of computer vision, directly applying SimCLR to the field of time series data yields unsatisfactory results due to the insufficient modeling of temporal dependencies. CPC [184] introduced contrastive predictive coding, which utilizes model-predicted features as positive samples and randomly-sampled features as negative samples, to obtain time series representations that are advantageous for downstream tasks. TimeCLR [185] proposes a DTW data augmentation to generate phase shift and amplitude-change phenomena which can preserve time series structure and feature information. TS-TCC [186] employs efficient data augmentations designed for time-series data, and learns discriminative representations from the proposed Temporal Contrasting module and Contextual Contrasting module. TS2Vec [187] employs a hierarchical contrastive learning method and defines the contrastive loss from both instance-wise and patch-wise perspectives across different augmented context views, resulting in a robust contextual representation for each timestamp. Furthermore, LaST [188] takes the idea of variational inference theory [189] and proposes seasonal-trend representations learning and disentanglement mechanisms. CoST [190] proposes a contrastive learning framework to learn disentangled seasonal-trend representations for long sequence time series data. TF-C [191] develop frequency-based contrastive augmentation to leverage rich spectral information and explore time-frequency consistency.\n\nMasked modeling is a reconstruction-based method, which can predict a masked token in a sequence based on the context unmasked part [16], [181], [192].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04477231-c1f4-46f8-ad28-380cd775e0e4": {"__data__": {"id_": "04477231-c1f4-46f8-ad28-380cd775e0e4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97149929-3cc1-409a-a111-5a20b6a9ed80", "node_type": "4", "metadata": {}, "hash": "7107477183d8e19f2a9dee85f3e1be45b7939ea710972337aca3b36688cff1e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a283d5aa-48c6-4da9-ba86-4605d13b9484", "node_type": "1", "metadata": {}, "hash": "d58032e4ee8e586e3079e22a7653c18a28ea239a22435290e3493c02d9d37241", "class_name": "RelatedNodeInfo"}}, "text": "TS-TCC [186] employs efficient data augmentations designed for time-series data, and learns discriminative representations from the proposed Temporal Contrasting module and Contextual Contrasting module. TS2Vec [187] employs a hierarchical contrastive learning method and defines the contrastive loss from both instance-wise and patch-wise perspectives across different augmented context views, resulting in a robust contextual representation for each timestamp. Furthermore, LaST [188] takes the idea of variational inference theory [189] and proposes seasonal-trend representations learning and disentanglement mechanisms. CoST [190] proposes a contrastive learning framework to learn disentangled seasonal-trend representations for long sequence time series data. TF-C [191] develop frequency-based contrastive augmentation to leverage rich spectral information and explore time-frequency consistency.\n\nMasked modeling is a reconstruction-based method, which can predict a masked token in a sequence based on the context unmasked part [16], [181], [192]. TST [193] follows the pre-training paradigm proposed in BERT [16] and proposes a pre-training framework for multivariate time series. Further, PatchTST [23] segments the time series data into multiple non-overlapping patches and proposes a patch-level masked modeling approach. SimMTM [194] relates masked modeling to manifold learning and presents a neighborhood aggregation design for reconstruction based on the similarities learned in series-wise representation space. HiMTM [195] proposes a novel hierarchical masked time series pre-training framework to effectively capture the multi-scale characteristics of time series data. TimeSiam [196] constructs an asymmetric masking reconstruction task to capture intrinsic temporal correlations between randomly sampled past and current subseries and learn internal time-dependent representations based on Siamese networks.\n\n# 6.2 Large Time Series Models\n\nWith the advent of Large Language Models (LLMs), the utilization of large-scale models to tackle time series downstream tasks has gained significant attention as the direction of future research. Current researches present the following two possible roadmaps to large time series models.\n\n# 6.2.1 Time Series Foundation Models\n\nRecent advancements in deep learning, particularly with the emergence of foundation models (FMs), have demonstrated significant progress in natural language processing.", "mimetype": "text/plain", "start_char_idx": 4332, "end_char_idx": 6792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f36b5f6f-020e-4270-a036-247fc1022486": {"__data__": {"id_": "f36b5f6f-020e-4270-a036-247fc1022486", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dce5a1e-d154-4670-8609-36a570e8224b", "node_type": "4", "metadata": {}, "hash": "a3bc1fd7bd98ac1bf060844e2b3a1a2c691883ac6663d8146172339d953291e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0a07b23-c30f-4178-a2a5-06121ec85127", "node_type": "1", "metadata": {}, "hash": "08ed79cc1c4752fcc40708fefc847facc8e267b235ade0ab6439f955f443ffd3", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 6.2.1 Generalized Time Series Foundation Models\n\n(NLP) and computer vision (CV) domains [179], [197], [198]. Different from prior deep models, foundation models are pre-trained on massive amounts of data, which enables them to have a wide range of general knowledge learned from diverse domains. Given their success in capturing contextual information and semantic understanding, it is promising to explore a generalized time series foundation model that can effectively learn complex temporal dependencies and capture the underlying dynamics inherent in time series data. Early attempts such as TimeGPT [199], Lag-LlaMa [200], and Timer [155] focus solely on univariate time series data. Nevertheless, in real-world forecasting scenarios, it is crucial to involve additional information that is related to the temporal variation of the target time series and must be taken into account, such as weather conditions or holidays. MOIRAI [201] tries to flatten multivariate time series into a single sequence containing all variate, but its generalization capabilities to other downstream analysis tasks are under-explored. In addition to modeling inter-series dependencies, modeling the relationship between time series and external factors in other can achieve a better understanding of time series data. These external factors may be in the form of other modalities, such as text data or calendar data, and thus multimodal learning is a future trend in the development of a multi-modal time series foundation model.\n\n# 6.2.2 Adaptation of Large Language Models\n\nLarge Language Models (LLMs) have made significant strides in solving various natural language processing tasks. Exemplified by the success of models like GPTs [17], [146] and LLaMA [202], [203], LLMs have proven adept at generalizing to unseen tasks by simply following provided prompts. Therefore, it has become a promising future research direction to unleash the power of LLMs in the field of time series. Here are two paradigms for adapting LLMs.\n\n# Fine-Tuning Pre-trained Language Models\n\nBased on the similar sequential nature, fine-tuning pre-trained language models to equip them with time series analysis capabilities has become a promising research topic. When applying LLMs to time series, it is essential to tokenize the time series before feeding it to a pre-trained model. Thus, adapting an LLM for time series included two key components: time series tokenization and efficient fine-tuning methods for time series analysis tasks. LLM4TS [204] proposes a two-stage fine-tuning strategy, including the time-series alignment stage to align LLMs with the nuances of time series data, and the fine-tuning stage for downstream tasks. LLMTime [205] treats time series forecasting as next-token prediction in text and attempts to encode time series data as a string of numerical digits. Recent Chronos [206] introduces a pre-trained probabilistic time series model based on existing Transformer-based language model architectures. Technologically, Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains the model on these tokenized time series via the cross-entropy loss. Benefiting from the generative capability of LLMs, most of the existing research focuses on time series forecasting tasks. GPT4TS [207] propose a unified framework for diverse time series analysis tasks by using a pre-trained GPT-2 model and fine-tuning the positional embeddings and the parameters of the layer normalization for each analysis task.\n\n# Prompting Large Language Models\n\nRecent Large Language Models exhibit the strong abilities of in-context learning [146] and instruction following [208]. Therefore, the paradigm of leveraging natural language instructions or task examples to guide the model in addressing novel tasks has emerged as a groundbreaking approach [209], [210], [211], which has become a potential solution for time series analysis tasks [212]. Recent literature, such as PromptCast [212], UniTime [213], and TimeLLM [214] focus on investigating a prompt template to enable LLMs to perform the forecasting task. There are other works represented by Autotimes [55], [215], that attempt to design soft prompts for time series data. However, existing prompting approaches are tailored for forecasting, and how to empower LLMs to other time series tasks besides forecasting is relatively unexplored.\n\n# 6.3 Practical Applications\n\n# 6.3.1 Handling Extremely Long Series\n\nDeep time series models have demonstrated remarkable performance across a wide range of downstream tasks, their applicability to longer time series data is often limited by scalability and high computational complexity. In industrial time series analysis, high-frequency sampling results in lengthy historical data, impeding the practical implementation of advanced deep models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0a07b23-c30f-4178-a2a5-06121ec85127": {"__data__": {"id_": "e0a07b23-c30f-4178-a2a5-06121ec85127", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dce5a1e-d154-4670-8609-36a570e8224b", "node_type": "4", "metadata": {}, "hash": "a3bc1fd7bd98ac1bf060844e2b3a1a2c691883ac6663d8146172339d953291e3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f36b5f6f-020e-4270-a036-247fc1022486", "node_type": "1", "metadata": {}, "hash": "c5bf5f7cbf747f3f800b393ef22d6c1f0e10b40600363c10ccad128b4a6944d2", "class_name": "RelatedNodeInfo"}}, "text": "Recent literature, such as PromptCast [212], UniTime [213], and TimeLLM [214] focus on investigating a prompt template to enable LLMs to perform the forecasting task. There are other works represented by Autotimes [55], [215], that attempt to design soft prompts for time series data. However, existing prompting approaches are tailored for forecasting, and how to empower LLMs to other time series tasks besides forecasting is relatively unexplored.\n\n# 6.3 Practical Applications\n\n# 6.3.1 Handling Extremely Long Series\n\nDeep time series models have demonstrated remarkable performance across a wide range of downstream tasks, their applicability to longer time series data is often limited by scalability and high computational complexity. In industrial time series analysis, high-frequency sampling results in lengthy historical data, impeding the practical implementation of advanced deep models. Existing methods usually include patching techniques to enable them to handle long sequences, and when the input length becomes longer, the patch length can be increased accordingly to reduce the computational complexity. However, model performance is closely tied to patch length; hence, solely increasing patch size to reduce complexity may compromise capabilities. Therefore, addressing the limitations of deep models in handling longer time series could be a promising topic.\n\n# 6.3.2 Utilizing Exogenous Variables\n\nSince variations within the time series are often influenced by external factors, it is crucial to include exogenous variables in the analysis to gain a more comprehensive understanding of these factors. Exogenous variables, which are widely discussed in time series prediction tasks, are included in the model for uniform training in modeling time series data, without requiring separate analysis. However, in practical applications, different from multivariate time series analysis, the main variables and covariates usually occupy different positions. Given the crucial role played by exogenous variables in real-world applications, it is essential to explore a unified framework for modeling the relationships between the endogenous and exogenous variants, which allows for a more comprehensive understanding of interrelations and causality among different variants, leading to better and more reliable model performance, as well as interpretability.\n\n# 6.3.3 Processing Heterogeneous Data\n\nIn the field of time series analysis, there is still an unexplored area related to the modeling of heterogeneous time series data. Heterogeneous time series data encompasses a wide range of diverse characteristics, such as varying sampling rates, irregularities, and different length scales. These diverse features make it challenging to develop models that can effectively capture the underlying patterns and relationships.", "mimetype": "text/plain", "start_char_idx": 4049, "end_char_idx": 6889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "961de226-256a-46a1-ad35-49d680567653": {"__data__": {"id_": "961de226-256a-46a1-ad35-49d680567653", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "836d23df-e4a4-48da-8924-544b586ed923", "node_type": "4", "metadata": {}, "hash": "50598629cfcc324c9a3cd5fb7d24068a40da415c26bd7bc68d0b068817fc55ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "586dc347-a24a-4b30-a587-f026fa90caba", "node_type": "1", "metadata": {}, "hash": "1be436630f6cb2c70dbbf62a82d8ca60bef9207c0178b1d6d94432f7f333a87b", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# 6\n\nwithin the data. Moreover, the need for fixed-size inputs in current deep learning models limits their ability to handle the dynamic nature of heterogeneous time series data.\n\nAddressing these challenges requires innovative approaches that can adapt to the unique nature of each individual time series while still capturing the overarching patterns across multiple series. This may involve developing new techniques for feature extraction, incorporating domain knowledge into model design, or exploring alternative architectures that are better suited to handle variable-length inputs.\n\nAs researchers continue to explore this unexplored area in time series analysis, there is potential for significant advances in areas such as finance, healthcare, and environmental monitoring. By improving our ability to model and analyze heterogeneous time series data, we can gain deeper insights into complex systems and make more informed decisions based on predictive analytics. Overall, further research in this area holds great promise for advancing our understanding of temporal data dynamics and enhancing the capabilities of time series modeling in real-world applications.\n\n# 7 CONCLUSION\n\nIn this survey, we provide a systematic review of deep models in time series analysis and introduce Time-Series Library (TSLib) as a fair benchmark for deep time series models across various analysis tasks. Compared with previous reviews that focus on a specific analysis task or model architecture, this paper provides a comprehensive survey and overview of existing deep models for time series analysis, ranging from forecasting, classification, imputation, and anomaly detection. We first present a detailed review of the universal modules that are widely used among time series models, including normalization, decomposition, and Fourier analysis. Next, we summarize existing deep time series models from the perspective of backbone architecture. Based on the review of existing literature, we introduce a practical open-source library, Time Series Library (TSLib), which has included representative deep time series models that can be a fair evaluation benchmark in the field of time series analysis. Finally, we discuss future research directions for deep time series models based on the recent development of the AI community and the practical application needs of time series analysis in real-world scenarios.\n\n# REFERENCES\n\n1. G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series analysis: forecasting and control. John Wiley & Sons, 2015.\n2. J. D. Hamilton, Time series analysis. Princeton university press, 2020.\n3. H. Wu, T. Hu, Y. Liu, H. Zhou, J. Wang, and M. Long, \u201cTimesnet: Temporal 2d-variation modeling for general time series analysis,\u201d in ICLR, 2023.\n4. K. Jin, J. Wi, E. Lee, S. Kang, S. Kim, and Y. Kim, \u201cTrafficbert: Pre-trained model with large-scale data for long-range traffic flow forecasting,\u201d Expert Systems with Applications, vol. 186, p. 115738, 2021.\n5. J. Xu, H. Wu, J. Wang, and M. Long, \u201cAnomaly transformer: Time series anomaly detection with association discrepancy,\u201d in ICLR, 2022.\n6. H. Wu, H. Zhou, M. Long, and J. Wang, \u201cInterpretable weather forecasting for worldwide stations with a unified deep model,\u201d Nature Machine Intelligence, pp. 1\u201310, 2023.\n7. L. H. Koopmans, The spectral analysis of time series. Elsevier, 1995.\n8. H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, \u201cDeep learning for time series classification: a review,\u201d Data mining and knowledge discovery, vol. 33, no. 4, pp. 917\u2013963, 2019.\n9. M. Braei and S. Wagner, \u201cAnomaly detection in univariate time-series: A survey on the state-of-the-art,\u201d arXiv preprint arXiv:2004.00433, 2020.\n10. J. F. Torres, D. Hadjout, A. Sebaa, F. Mart\u00ednez- Alvarez, and A. Troncoso, \u201cDeep learning for time series forecasting: a survey,\u201d Big Data, vol. 9, no. 1, pp. 3\u201321, 2021.\n11. A. Bl\u00e1zquez-Garc\u00eda, A. Conde, U. Mori, and J. A. Lozano, \u201cA review on outlier/anomaly detection in time series data,\u201d Computing Surveys, vol. 54, no. 3, pp. 1\u201333, 2021.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "586dc347-a24a-4b30-a587-f026fa90caba": {"__data__": {"id_": "586dc347-a24a-4b30-a587-f026fa90caba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "836d23df-e4a4-48da-8924-544b586ed923", "node_type": "4", "metadata": {}, "hash": "50598629cfcc324c9a3cd5fb7d24068a40da415c26bd7bc68d0b068817fc55ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "961de226-256a-46a1-ad35-49d680567653", "node_type": "1", "metadata": {}, "hash": "7de4fa0f8609ccd0111b44dbc377b073e1bc5fe5e802e341e798794873eb63cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bdd3c28-6823-450c-ad2e-27a9a7e667d0", "node_type": "1", "metadata": {}, "hash": "b880dffe7d51cfb75890ee43480202e9537ed10de59c3f339b11d050f778947f", "class_name": "RelatedNodeInfo"}}, "text": "917\u2013963, 2019.\n9. M. Braei and S. Wagner, \u201cAnomaly detection in univariate time-series: A survey on the state-of-the-art,\u201d arXiv preprint arXiv:2004.00433, 2020.\n10. J. F. Torres, D. Hadjout, A. Sebaa, F. Mart\u00ednez- Alvarez, and A. Troncoso, \u201cDeep learning for time series forecasting: a survey,\u201d Big Data, vol. 9, no. 1, pp. 3\u201321, 2021.\n11. A. Bl\u00e1zquez-Garc\u00eda, A. Conde, U. Mori, and J. A. Lozano, \u201cA review on outlier/anomaly detection in time series data,\u201d Computing Surveys, vol. 54, no. 3, pp. 1\u201333, 2021.\n12. Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun, \u201cTransformers in time series: A survey,\u201d arXiv preprint arXiv:2202.07125, 2022.\n13. M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb, I. King, and S. Pan, \u201cA survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection,\u201d arXiv preprint arXiv:2307.03759, 2023.\n14. Z. Shao, F. Wang, Y. Xu, W. Wei, C. Yu, Z. Zhang, D. Yao, G. Jin, X. Cao, G. Cong et al., \u201cExploring progress in multivariate time series forecasting: Comprehensive benchmarking and heterogeneity analysis,\u201d arXiv preprint arXiv:2310.06119, 2023.\n15. X. Qiu, J. Hu, L. Zhou, X. Wu, J. Du, B. Zhang, C. Guo, A. Zhou, C. S. Jensen, Z. Sheng et al., \u201cTfb: Towards comprehensive and fair benchmarking of time series forecasting methods,\u201d arXiv preprint arXiv:2403.20150, 2024.\n16. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n17. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., \u201cGpt-4 technical report,\u201d arXiv preprint arXiv:2303.08774, 2023.\n18. K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in CVPR, 2016.\n19. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021.\n20. S. Zhang, L. Yao, A. Sun, and Y. Tay, \u201cDeep learning based recommender system: A survey and new perspectives,\u201d Computing surveys, vol. 52, no. 1, pp. 1\u201338, 2019.\n21. H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang, \u201cInformer: Beyond efficient transformer for long sequence time-series forecasting,\u201d in AAAI, 2021.\n22. H. Wu, J. Xu, J. Wang, and M. Long, \u201cAutoformer: Decomposition transformers with auto-correlation for long-term series forecasting,\u201d in NeurIPS, 2021.\n23. Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam, \u201cA time series is worth 64 words: Long-term forecasting with transformers,\u201d in ICLR, 2023.\n24.", "mimetype": "text/plain", "start_char_idx": 3649, "end_char_idx": 6403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bdd3c28-6823-450c-ad2e-27a9a7e667d0": {"__data__": {"id_": "1bdd3c28-6823-450c-ad2e-27a9a7e667d0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "836d23df-e4a4-48da-8924-544b586ed923", "node_type": "4", "metadata": {}, "hash": "50598629cfcc324c9a3cd5fb7d24068a40da415c26bd7bc68d0b068817fc55ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "586dc347-a24a-4b30-a587-f026fa90caba", "node_type": "1", "metadata": {}, "hash": "1be436630f6cb2c70dbbf62a82d8ca60bef9207c0178b1d6d94432f7f333a87b", "class_name": "RelatedNodeInfo"}}, "text": "52, no. 1, pp. 1\u201338, 2019.\n21. H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang, \u201cInformer: Beyond efficient transformer for long sequence time-series forecasting,\u201d in AAAI, 2021.\n22. H. Wu, J. Xu, J. Wang, and M. Long, \u201cAutoformer: Decomposition transformers with auto-correlation for long-term series forecasting,\u201d in NeurIPS, 2021.\n23. Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam, \u201cA time series is worth 64 words: Long-term forecasting with transformers,\u201d in ICLR, 2023.\n24. B. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio, \u201cN-BEATS: neural basis expansion analysis for interpretable time series forecasting,\u201d in ICLR, 2020.\n25. C. W. Granger, \u201cSome properties of time series data and their use in econometric model specification,\u201d Journal of econometrics, vol. 16, no. 1, pp. 121\u2013130, 1981.\n26. R. F. Engle and C. W. Granger, \u201cCo-integration and error correction: representation, estimation, and testing,\u201d Econometrica: journal of the Econometric Society, pp. 251\u2013276, 1987.\n27. J. H. Stock and M. W. Watson, \u201cVector autoregressions,\u201d Journal of Economic perspectives, vol. 15, no. 4, pp. 101\u2013115, 2001.\n28. D. Cao, Y. Wang, J. Duan, C. Zhang, X. Zhu, C. Huang, Y. Tong, B. Xu, J. Bai, J. Tong et al., \u201cSpectral temporal graph neural network for multivariate time-series forecasting,\u201d in NeurIPS, 2020.\n29. Y. Zhang and J. Yan, \u201cCrossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting,\u201d in ICLR, 2023.\n30. Y. Liu, T. Hu, H. Zhang, H. Wu, S. Wang, L. Ma, and M. Long, \u201citransformer: Inverted transformers are effective for time series forecasting,\u201d arXiv preprint arXiv:2310.06625, 2023.\n31. A. S. Weigend, Time series prediction: forecasting the future and understanding the past. Routledge, 2018.\n32. G. P. Zhang, \u201cTime series forecasting using a hybrid arima and neural network model,\u201d Neurocomputing, vol. 50, pp. 159\u2013175, 2003.", "mimetype": "text/plain", "start_char_idx": 5897, "end_char_idx": 7814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83d10d2e-a96b-4675-be01-9986d65617e5": {"__data__": {"id_": "83d10d2e-a96b-4675-be01-9986d65617e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "704e614a-8cd1-41ab-827c-3a91cc8062d7", "node_type": "4", "metadata": {}, "hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bed53cd-e14c-4188-9963-816778ce12e2", "node_type": "1", "metadata": {}, "hash": "a635630396efb22b2c52c3762f609b28883978b3e52d720c1dffed7a3e6a8c83", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# References\n\n1. C. Fang and C. Wang, \u201cTime series data imputation: A survey on deep learning approaches,\u201d arXiv preprint arXiv:2011.11347, 2020.\n2. Y. Luo, X. Cai, Y. Zhang, J. Xu et al., \u201cMultivariate time series imputation with generative adversarial networks,\u201d in NeurIPS, 2018.\n3. S. Moritz and T. Bartz-Beielstein, \u201cimputets: time series missing value imputation in r.\u201d R Journal, vol. 9, no. 1, p. 207, 2017.\n4. B. Zhao, H. Lu, S. Chen, J. Liu, and D. Wu, \u201cConvolutional neural networks for time series classification,\u201d Journal of Systems Engineering and Electronics, vol. 28, no. 1, pp. 162\u2013169, 2017.\n5. N. Laptev, S. Amizadeh, and I. Flint, \u201cGeneric and scalable framework for automated time-series anomaly detection,\u201d in SIGKDD, 2015.\n6. D. Ulyanov, A. Vedaldi, and V. Lempitsky, \u201cInstance normalization: The missing ingredient for fast stylization,\u201d arXiv preprint arXiv:1607.08022, 2016.\n7. N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosifidis, \u201cDeep adaptive input normalization for time series forecasting,\u201d IEEE transactions on neural networks and learning systems, vol. 31, no. 9, pp. 3760\u20133765, 2019.\n8. T. Kim, J. Kim, Y. Tae, C. Park, J.-H. Choi, and J. Choo, \u201cReversible instance normalization for accurate time-series forecasting against distribution shift,\u201d in ICLR, 2021.\n9. Y. Liu, H. Wu, J. Wang, and M. Long, \u201cNon-stationary transformers: Exploring the stationarity in time series forecasting,\u201d in NeurIPS, 2022.\n10. J. Gao, W. Hu, and Y. Chen, \u201cClient: Cross-variable linear integrated enhanced transformer for multivariate long-term time series forecasting,\u201d arXiv preprint arXiv:2305.18838, 2023.\n11. H. Wang, Y. Mo, N. Yin, H. Dai, B. Li, S. Fan, and S. Mo, \u201cDance of channel and sequence: An efficient attention-based approach for multivariate time series forecasting,\u201d arXiv preprint arXiv:2312.06220, 2023.\n12. Z. Liu, M. Cheng, Z. Li, Z. Huang, Q. Liu, Y. Xie, and E. Chen, \u201cAdaptive normalization for non-stationary time series forecasting: A temporal slice perspective,\u201d in NeurIPS, 2024.\n13. R. B. Cleveland, W. S. Cleveland, J. E. McRae, and I. Terpenning, \u201cStl: A seasonal-trend decomposition,\u201d J. Off. Stat, vol. 6, no. 1, pp. 3\u201373, 1990.\n14. O. D. Anderson, \u201cTime-series. 2nd edn.\u201d 1976.\n15. C. RB, \u201cStl: A seasonal-trend decomposition procedure based on loess,\u201d J Off Stat, vol. 6, pp. 3\u201373, 1990.\n16. E. B. Dagum and S. Bianconcini, Seasonal adjustment methods and real time trend-cycle estimation. Springer, 2016.\n17. Q. Wen, J. Gao, X. Song, L. Sun, H. Xu, and S. Zhu, \u201cRobuststl: A robust seasonal-trend decomposition algorithm for long time series,\u201d in AAAI, 2019.\n18. A. M. De Livera, R. J. Hyndman, and R. D. Snyder, \u201cForecasting time series with complex seasonal patterns using exponential smoothing,\u201d Journal of the American statistical association, vol. 106, no. 496, pp. 1513\u20131527, 2011.\n19. S. J. Taylor and B. Letham, \u201cForecasting at scale,\u201d The American Statistician, vol. 72, no.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bed53cd-e14c-4188-9963-816778ce12e2": {"__data__": {"id_": "7bed53cd-e14c-4188-9963-816778ce12e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "704e614a-8cd1-41ab-827c-3a91cc8062d7", "node_type": "4", "metadata": {}, "hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83d10d2e-a96b-4675-be01-9986d65617e5", "node_type": "1", "metadata": {}, "hash": "fe9d67ecb715a2253218ff3fc302532903b36f90563f1b0111d6335dcb33218e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5970011-ba6c-4b7d-9bd6-c742c47c6afe", "node_type": "1", "metadata": {}, "hash": "84430e8ca9c5eb206ae5ca18ba6aaddcf3c32910f906a028f5d9bd5e67a852bc", "class_name": "RelatedNodeInfo"}}, "text": "16. E. B. Dagum and S. Bianconcini, Seasonal adjustment methods and real time trend-cycle estimation. Springer, 2016.\n17. Q. Wen, J. Gao, X. Song, L. Sun, H. Xu, and S. Zhu, \u201cRobuststl: A robust seasonal-trend decomposition algorithm for long time series,\u201d in AAAI, 2019.\n18. A. M. De Livera, R. J. Hyndman, and R. D. Snyder, \u201cForecasting time series with complex seasonal patterns using exponential smoothing,\u201d Journal of the American statistical association, vol. 106, no. 496, pp. 1513\u20131527, 2011.\n19. S. J. Taylor and B. Letham, \u201cForecasting at scale,\u201d The American Statistician, vol. 72, no. 1, pp. 37\u201345, 2018.\n20. A. Zeng, M. Chen, L. Zhang, and Q. Xu, \u201cAre transformers effective for time series forecasting?\u201d in AAAI, 2023.\n21. H. Cao, Z. Huang, T. Yao, J. Wang, H. He, and Y. Wang, \u201cInparfomer: evolutionary decomposition transformers with interactive parallel attention for long-term time series forecasting,\u201d in AAAI, 2023.\n22. D. Du, B. Su, and Z. Wei, \u201cPreformer: predictive transformer with multi-scale segment-wise correlations for long-term time series forecasting,\u201d in ICASSP, 2023.\n23. D. Cao, F. Jia, S. O. Arik, T. Pfister, Y. Zheng, W. Ye, and Y. Liu, \u201cTempo: Prompt-based generative pre-trained transformer for time series forecasting,\u201d arXiv preprint arXiv:2310.04948, 2023.\n24. B. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio, \u201cN-beats: Neural basis expansion analysis for interpretable time series forecasting,\u201d arXiv preprint arXiv:1905.10437, 2019.\n25. C. Challu, K. G. Olivares, B. N. Oreshkin, F. G. Ramirez, M. M. Canseco, and A. Dubrawski, \u201cNhits: Neural hierarchical interpolation for time series forecasting,\u201d in AAAI, 2023.\n26. W. Fan, S. Zheng, X. Yi, W. Cao, Y. Fu, J. Bian, and T.-Y. Liu, \u201cDepts: Deep expansion learning for periodic time series forecasting,\u201d in ICLR, 2022.\n27. W. Fan, Y. Fu, S. Zheng, J. Bian, Y. Zhou, and H. Xiong, \u201cDewp: Deep expansion learning for wind power forecasting,\u201d ACM Transactions on Knowledge Discovery from Data, vol. 18, no. 3, pp. 1\u201321, 2024.\n28. L. Xiong, X. Chen, T.-K. Huang, J. Schneider, and J. G. Carbonell, \u201cTemporal collaborative filtering with bayesian probabilistic tensor factorization,\u201d in SIAM-SDM, 2010.\n29. H.-F. Yu, N. Rao, and I. S. Dhillon, \u201cTemporal regularized matrix factorization for high-dimensional time series prediction,\u201d in NeurIPS, 2016.\n30. K. Takeuchi, H. Kashima, and N. Ueda, \u201cAutoregressive tensor factorization for spatio-temporal predictions,\u201d in ICDM, 2017.\n31. X. Chen, C. Zhang, X.-L. Zhao, N. Saunier, and L. Sun, \u201cNonstationary temporal matrix factorization for multivariate time series forecasting,\u201d arXiv preprint arXiv:2203.10651, 2022.\n32. X. Chen and L. Sun, \u201cBayesian temporal factorization for multi-dimensional time series prediction,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 4659\u20134673, 2021.\n33. R. Sen, H.-F. Yu, and I. S. Dhillon, \u201cThink globally, act locally: A deep neural network approach to high-dimensional time series forecasting,\u201d in NeurIPS, 2019.\n34. J.-M. Yang, Z.-R.", "mimetype": "text/plain", "start_char_idx": 2437, "end_char_idx": 5489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5970011-ba6c-4b7d-9bd6-c742c47c6afe": {"__data__": {"id_": "a5970011-ba6c-4b7d-9bd6-c742c47c6afe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "704e614a-8cd1-41ab-827c-3a91cc8062d7", "node_type": "4", "metadata": {}, "hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bed53cd-e14c-4188-9963-816778ce12e2", "node_type": "1", "metadata": {}, "hash": "a635630396efb22b2c52c3762f609b28883978b3e52d720c1dffed7a3e6a8c83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f07e17b-19cc-4939-b664-1f871b0592cb", "node_type": "1", "metadata": {}, "hash": "03358c7197d100c55c6a208c8fa7ac4cf3ef5e181fb9dddcb89cd828023bdedf", "class_name": "RelatedNodeInfo"}}, "text": "31. X. Chen, C. Zhang, X.-L. Zhao, N. Saunier, and L. Sun, \u201cNonstationary temporal matrix factorization for multivariate time series forecasting,\u201d arXiv preprint arXiv:2203.10651, 2022.\n32. X. Chen and L. Sun, \u201cBayesian temporal factorization for multi-dimensional time series prediction,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 4659\u20134673, 2021.\n33. R. Sen, H.-F. Yu, and I. S. Dhillon, \u201cThink globally, act locally: A deep neural network approach to high-dimensional time series forecasting,\u201d in NeurIPS, 2019.\n34. J.-M. Yang, Z.-R. Peng, and L. Lin, \u201cReal-time spatiotemporal prediction and imputation of traffic status based on lstm and graph laplacian regularized matrix factorization,\u201d Transportation Research Part C: Emerging Technologies, vol. 129, p. 103228, 2021.\n35. D. Cai, X. He, J. Han, and T. S. Huang, \u201cGraph regularized nonnegative matrix factorization for data representation,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 33, no. 8, pp. 1548\u20131560, 2010.\n36. I. N. Sneddon, Fourier transforms. Courier Corporation, 1995.\n37. J. W. Goodman, Introduction to Fourier optics. Roberts and Company publishers, 2005.\n38. P. Bloomfield, Fourier analysis of time series: an introduction. John Wiley & Sons, 2004.\n39. K. Samiee, P. Kovacs, and M. Gabbouj, \u201cEpileptic seizure classification of eeg time-series using rational discrete short-time fourier transform,\u201d IEEE transactions on Biomedical Engineering, vol. 62, no. 2, pp. 541\u2013552, 2014.\n40. E. O. Brigham, The fast Fourier transform and its applications. Prentice-Hall, Inc., 1988.\n41. Y. Meyer, Wavelets and operators: volume 1. Cambridge university press, 1992, no. 37.\n42. J. Wang, Z. Wang, J. Li, and J. Wu, \u201cMultilevel wavelet decomposition network for interpretable time series analysis,\u201d in SIGKDD, 2018.\n43. S. Yao, A. Piao, W. Jiang, Y. Zhao, H. Shao, S. Liu, D. Liu, J. Li, T. Wang, S. Hu et al., \u201cStfnets: Learning sensing signals from the time-frequency perspective with short-time fourier neural networks,\u201d in The World Wide Web Conference, 2019, pp. 2192\u20132202.\n44. M. Rhif, A. Ben Abbes, I. R. Farah, B. Mart\u00ednez, and Y. Sang, \u201cWavelet transform application for/in non-stationary time-series analysis: A review,\u201d Applied Sciences, vol. 9, no. 7, p. 1345, 2019.\n45. L. Minhao, A. Zeng, L. Qiuxia, R. Gao, M. Li, J. Qin, and Q. Xu, \u201cT-wavenet: A tree-structured wavelet neural network for time series signal analysis,\u201d in ICLR, 2021.\n46. L. Yang and S. Hong, \u201cUnsupervised time-series representation learning with iterative bilinear temporal-spectral fusion,\u201d in ICML, 2022.\n47. J. Wang, C. Yang, X. Jiang, and J. Wu, \u201cWhen: A wavelet-dtw hybrid attention network for heterogeneous time series analysis,\u201d in SIGKDD, 2023.\n48. F. Yang, X. Li, M. Wang, H. Zang, W. Pang, and M. Wang, \u201cWaveform: Graph enhanced wavelet learning for long sequence forecasting of multivariate time series,\u201d in AAAI, 2023.\n49. K. Yi, Q. Zhang, L. Cao, S. Wang, G. Long, L. Hu, H. He, Z. Niu, W. Fan, and H. Xiong, \u201cA survey on deep learning based time series analysis with frequency transformation,\u201d arXiv preprint arXiv:2302.02173, 2023.\n50.", "mimetype": "text/plain", "start_char_idx": 4911, "end_char_idx": 8075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f07e17b-19cc-4939-b664-1f871b0592cb": {"__data__": {"id_": "8f07e17b-19cc-4939-b664-1f871b0592cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "704e614a-8cd1-41ab-827c-3a91cc8062d7", "node_type": "4", "metadata": {}, "hash": "ea5c4a0239e2ad228aa9529b02ff2f3ce2b6eb8c4be6e52ca60649f9e8783cdd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5970011-ba6c-4b7d-9bd6-c742c47c6afe", "node_type": "1", "metadata": {}, "hash": "84430e8ca9c5eb206ae5ca18ba6aaddcf3c32910f906a028f5d9bd5e67a852bc", "class_name": "RelatedNodeInfo"}}, "text": "47. J. Wang, C. Yang, X. Jiang, and J. Wu, \u201cWhen: A wavelet-dtw hybrid attention network for heterogeneous time series analysis,\u201d in SIGKDD, 2023.\n48. F. Yang, X. Li, M. Wang, H. Zang, W. Pang, and M. Wang, \u201cWaveform: Graph enhanced wavelet learning for long sequence forecasting of multivariate time series,\u201d in AAAI, 2023.\n49. K. Yi, Q. Zhang, L. Cao, S. Wang, G. Long, L. Hu, H. He, Z. Niu, W. Fan, and H. Xiong, \u201cA survey on deep learning based time series analysis with frequency transformation,\u201d arXiv preprint arXiv:2302.02173, 2023.\n50. T. Dai, B. Wu, P. Liu, N. Li, J. Bao, Y. Jiang, and S.-T. Xia, \u201cPeriodicity decoupling framework for long-term series forecasting,\u201d in ICLR, 2023.\n51. N. Wiener, \u201cGeneralized harmonic analysis,\u201d Acta mathematica, vol. 55, no. 1, pp. 117\u2013258, 1930.\n52. T. Zhou, Z. Ma, Q. Wen, L. Sun, T. Yao, W. Yin, R. Jin et al., \u201cFilm: Frequency improved legendre memory model for long-term time series forecasting,\u201d in NeurIPS, 2022.\n53. Z. Xu, A. Zeng, and Q. Xu, \u201cFits: Modeling time series with 10k parameters,\u201d arXiv preprint arXiv:2307.03756, 2023.", "mimetype": "text/plain", "start_char_idx": 7531, "end_char_idx": 8616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca5cc118-1846-4ab1-87d4-32afdc29c35e": {"__data__": {"id_": "ca5cc118-1846-4ab1-87d4-32afdc29c35e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf", "node_type": "4", "metadata": {}, "hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a50ba741-b326-41e1-940a-2ec88539b1ea", "node_type": "1", "metadata": {}, "hash": "32003f6816b0332ac11db23ced60a7f2512351249a771367c76602b1b735a91c", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# References\n\n1. T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, and R. Jin, \u201cFedformer: Frequency enhanced decomposed transformer for long-term series forecasting,\u201d in ICML, 2022.\n2. Z. Yang, W. Yan, X. Huang, and L. Mei, \u201cAdaptive temporal-frequency network for time-series forecasting,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 4, pp. 1576\u20131587, 2020.\n3. C. Zhang, T. Zhou, Q. Wen, and L. Sun, \u201cTfad: A decomposition time series anomaly detection architecture with time-frequency analysis,\u201d in CIKM, 2022.\n4. K. Yi, Q. Zhang, L. Hu, H. He, N. An, L. Cao, and Z. Niu, \u201cEdge-varying fourier graph networks for multivariate time series forecasting,\u201d arXiv preprint arXiv:2210.03093, 2022.\n5. K. Yi, Q. Zhang, W. Fan, S. Wang, P. Wang, H. He, N. An, D. Lian, L. Cao, and Z. Niu, \u201cFrequency-domain mlps are more effective learners in time series forecasting,\u201d in NeurIPS, 2024.\n6. Z. Wang, C. Pei, M. Ma, X. Wang, Z. Li, D. Pei, S. Rajmohan, D. Zhang, Q. Lin, H. Zhang et al., \u201cRevisiting vae for unsupervised time series anomaly detection: A frequency perspective,\u201d in WWW, 2024.\n7. E. Eldele, M. Ragab, Z. Chen, M. Wu, and X. Li, \u201cTslanet: Rethinking transformers for time series representation learning,\u201d in ICML, 2024.\n8. J. Crabb\u00e9, N. Huynh, J. Stanczuk, and M. van der Schaar, \u201cTime series diffusion in the frequency domain,\u201d in ICML, 2024.\n9. K. G. Olivares, C. Challu, G. Marcjasz, R. Weron, and A. Dubrawski, \u201cNeural basis expansion analysis with exogenous variables: Forecasting electricity prices with nbeatsx,\u201d International Journal of Forecasting, vol. 39, no. 2, pp. 884\u2013900, 2023.\n10. I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al., \u201cMlp-mixer: An all-mlp architecture for vision,\u201d in NeurIPS, 2021.\n11. S.-A. Chen, C.-L. Li, N. Yoder, S. O. Arik, and T. Pfister, \u201cTsmixer: An all-mlp architecture for time series forecasting,\u201d arXiv preprint arXiv:2303.06053, 2023.\n12. A. Das, W. Kong, A. Leach, R. Sen, and R. Yu, \u201cLong-term forecasting with tide: Time-series dense encoder,\u201d arXiv preprint arXiv:2304.08424, 2023.\n13. P. J. Schmid, \u201cDynamic mode decomposition of numerical and experimental data,\u201d Journal of fluid mechanics, vol. 656, pp. 5\u201328, 2010.\n14. Y. Liu, C. Li, J. Wang, and M. Long, \u201cKoopa: Learning non-stationary time series dynamics with koopman predictors,\u201d arXiv preprint arXiv:2305.18803, 2023.\n15. S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n16. I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in NeurIPS, 2014.\n17.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a50ba741-b326-41e1-940a-2ec88539b1ea": {"__data__": {"id_": "a50ba741-b326-41e1-940a-2ec88539b1ea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf", "node_type": "4", "metadata": {}, "hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca5cc118-1846-4ab1-87d4-32afdc29c35e", "node_type": "1", "metadata": {}, "hash": "5d16368df214b5fd9f2af91d1e571cb086710ec9d2e3e70ba15769f0bba85d4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ca2c115-01e9-4c06-8074-61c72c562ba0", "node_type": "1", "metadata": {}, "hash": "30403ea75e12d401eedc9ffc86fa6c78b7f9996ae49838dcaccc79f72d313bba", "class_name": "RelatedNodeInfo"}}, "text": "13. P. J. Schmid, \u201cDynamic mode decomposition of numerical and experimental data,\u201d Journal of fluid mechanics, vol. 656, pp. 5\u201328, 2010.\n14. Y. Liu, C. Li, J. Wang, and M. Long, \u201cKoopa: Learning non-stationary time series dynamics with koopman predictors,\u201d arXiv preprint arXiv:2305.18803, 2023.\n15. S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n16. I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in NeurIPS, 2014.\n17. K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder\u2013decoder for statistical machine translation,\u201d in EMNLP, 2014.\n18. R. Nallapati, B. Zhou, C. Gulcehre, and B. Xiang, \u201cAbstractive text summarization using sequence-to-sequence rnns and beyond,\u201d in SIGNLL, 2016.\n19. S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo, A. Courville, and Y. Bengio, \u201cSamplernn: An unconditional end-to-end neural audio generation model,\u201d arXiv preprint arXiv:1612.07837, 2016.\n20. A. Tealab, \u201cTime series forecasting using artificial neural networks methodologies: A systematic review,\u201d Future Computing and Informatics Journal, vol. 3, no. 2, pp. 334\u2013340, 2018.\n21. W. Cao, D. Wang, J. Li, H. Zhou, L. Li, and Y. Li, \u201cBrits: Bidirectional recurrent imputation for time series,\u201d in NeurIPS, 2018.\n22. J. Yoon, W. R. Zame, and M. van der Schaar, \u201cEstimating missing data in temporal data streams using multi-directional recurrent neural networks,\u201d IEEE Transactions on Biomedical Engineering, vol. 66, no. 5, pp. 1477\u20131490, 2018.\n23. Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, \u201cRecurrent neural networks for multivariate time series with missing values,\u201d Scientific reports, vol. 8, no. 1, p. 6085, 2018.\n24. I. M. Baytas, C. Xiao, X. Zhang, F. Wang, A. K. Jain, and J. Zhou, \u201cPatient subtyping via time-aware lstm networks,\u201d in SIGKDD, 2017.\n25. L. Shen, Z. Li, and J. Kwok, \u201cTimeseries anomaly detection using temporal hierarchical one-class network,\u201d in NeurIPS, 2020.\n26. G. Lai, W.-C. Chang, Y. Yang, and H. Liu, \u201cModeling long-and short-term temporal patterns with deep neural networks,\u201d in SIGIR, 2018.\n27. Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G. Cottrell, \u201cA dual-stage attention-based recurrent neural network for time series prediction,\u201d arXiv preprint arXiv:1704.02971, 2017.\n28. D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski, \u201cDeepar: Probabilistic forecasting with autoregressive recurrent networks,\u201d International Journal of Forecasting, vol. 36, no. 3, pp. 1181\u20131191, 2020.\n29. A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9, \u201cCombining recurrent, convolutional, and continuous-time models with linear state space layers,\u201d in NeurIPS, 2021.\n30. M. Fraccaro, S. K. S\u00f8nderby, U. Paquet, and O. Winther, \u201cSequential neural models with stochastic layers,\u201d in NeurIPS, 2016.\n31.", "mimetype": "text/plain", "start_char_idx": 2217, "end_char_idx": 5181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ca2c115-01e9-4c06-8074-61c72c562ba0": {"__data__": {"id_": "4ca2c115-01e9-4c06-8074-61c72c562ba0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf", "node_type": "4", "metadata": {}, "hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a50ba741-b326-41e1-940a-2ec88539b1ea", "node_type": "1", "metadata": {}, "hash": "32003f6816b0332ac11db23ced60a7f2512351249a771367c76602b1b735a91c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09446021-cdbf-42a5-9946-3cfb4ba01b18", "node_type": "1", "metadata": {}, "hash": "3360f6122cbecb9382621beff68f3b03e69b7296e7db0fcf3aceb2edd2ee4ea6", "class_name": "RelatedNodeInfo"}}, "text": "28. D. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski, \u201cDeepar: Probabilistic forecasting with autoregressive recurrent networks,\u201d International Journal of Forecasting, vol. 36, no. 3, pp. 1181\u20131191, 2020.\n29. A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9, \u201cCombining recurrent, convolutional, and continuous-time models with linear state space layers,\u201d in NeurIPS, 2021.\n30. M. Fraccaro, S. K. S\u00f8nderby, U. Paquet, and O. Winther, \u201cSequential neural models with stochastic layers,\u201d in NeurIPS, 2016.\n31. R. Krishnan, U. Shalit, and D. Sontag, \u201cStructured inference networks for nonlinear state space models,\u201d in AAAI, 2017.\n32. S. S. Rangapuram, M. W. Seeger, J. Gasthaus, L. Stella, Y. Wang, and T. Januschowski, \u201cDeep state space models for time series forecasting,\u201d in NeurIPS, 2018.\n33. A. Gu, K. Goel, and C. R\u00e9, \u201cEfficiently modeling long sequences with structured state spaces,\u201d in ICLR, 2022.\n34. L. Zhou, M. Poli, W. Xu, S. Massaroli, and S. Ermon, \u201cDeep latent state space models for time-series generation,\u201d in ICML, 2023.\n35. A. Gu and T. Dao, \u201cMamba: Linear-time sequence modeling with selective state spaces,\u201d arXiv preprint arXiv:2312.00752, 2023.\n36. J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, G. Wang, J. Cai et al., \u201cRecent advances in convolutional neural networks,\u201d Pattern recognition, vol. 77, pp. 354\u2013377, 2018.\n37. M. Tan and Q. Le, \u201cEfficientnet: Rethinking model scaling for convolutional neural networks,\u201d in ICML, 2019.\n38. A. Kendall, V. Badrinarayanan, and R. Cipolla, \u201cBayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding,\u201d arXiv preprint arXiv:1511.02680, 2015.\n39. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \u201cYou only look once: Unified, real-time object detection,\u201d in CVPR, 2016.\n40. A. Le Guennec, S. Malinowski, and R. Tavenard, \u201cData augmentation for time series classification using convolutional neural networks,\u201d in ECML/PKDD workshop, 2016.\n41. Z. Cui, W. Chen, and Y. Chen, \u201cMulti-scale convolutional neural networks for time series classification,\u201d arXiv preprint arXiv:1603.06995, 2016.\n42. H. Ismail Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt, J. Weber, G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean, \u201cInceptiontime: Finding alexnet for time series classification,\u201d Data Mining and Knowledge Discovery, vol. 34, no. 6, pp. 1936\u20131962, 2020.\n43. M. Liu, A. Zeng, M. Chen, Z. Xu, Q. Lai, L. Ma, and Q. Xu, \u201cScinet: Time series modeling and forecasting with sample convolution and interaction,\u201d in NeurIPS, 2022.\n44. A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu, \u201cPixel recurrent neural networks,\u201d in ICML, 2016.\n45. A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d arXiv preprint arXiv:1609.03499, 2016.\n46.", "mimetype": "text/plain", "start_char_idx": 4650, "end_char_idx": 7576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09446021-cdbf-42a5-9946-3cfb4ba01b18": {"__data__": {"id_": "09446021-cdbf-42a5-9946-3cfb4ba01b18", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf", "node_type": "4", "metadata": {}, "hash": "db4c98fd7cbb6255c5b0d5b05eb6a69e8a1412e32ccfc13846e57beee94a9563", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ca2c115-01e9-4c06-8074-61c72c562ba0", "node_type": "1", "metadata": {}, "hash": "30403ea75e12d401eedc9ffc86fa6c78b7f9996ae49838dcaccc79f72d313bba", "class_name": "RelatedNodeInfo"}}, "text": "43. M. Liu, A. Zeng, M. Chen, Z. Xu, Q. Lai, L. Ma, and Q. Xu, \u201cScinet: Time series modeling and forecasting with sample convolution and interaction,\u201d in NeurIPS, 2022.\n44. A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu, \u201cPixel recurrent neural networks,\u201d in ICML, 2016.\n45. A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d arXiv preprint arXiv:1609.03499, 2016.\n46. S. Bai, J. Z. Kolter, and V. Koltun, \u201cAn empirical evaluation of generic convolutional and recurrent networks for sequence modeling,\u201d arXiv preprint arXiv:1803.01271, 2018.\n47. H. Wang, J. Peng, F. Huang, J. Wang, J. Chen, and Y. Xiao, \u201cMicn: Multi-scale local and global context modeling for long-term series forecasting,\u201d in ICLR, 2022.\n48. D. Luo and X. Wang, \u201cModerntcn: A modern pure convolution structure for general time series analysis,\u201d in ICLR, 2024.\n49. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \u201cGoing deeper with convolutions,\u201d in CVPR, 2015.\n50. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking the inception architecture for computer vision,\u201d in CVPR, 2016.\n51. F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, \u201cThe graph neural network model,\u201d IEEE transactions on neural networks, vol. 20, no. 1, pp. 61\u201380, 2008.\n52. T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d in ICLR, 2017.", "mimetype": "text/plain", "start_char_idx": 7088, "end_char_idx": 8631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2299c14e-0913-4401-8f0d-f3340bb219ce": {"__data__": {"id_": "2299c14e-0913-4401-8f0d-f3340bb219ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f", "node_type": "4", "metadata": {}, "hash": "da7cc8991c75a07ec89201fbfb9fe7d27bb7c31fe080c469555575080a3cdf40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc538e16-bfe1-4776-badb-f6ea1bb120d6", "node_type": "1", "metadata": {}, "hash": "0d0c4ad1019bd6531002ed16b1ac5e3fb06fd08dee32faa3e41f7a4a36608729", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# References\n\n1. Y. Li, R. Yu, C. Shahabi, and Y. Liu, \u201cDiffusion convolutional recurrent neural network: Data-driven traffic forecasting,\u201d in ICLR, 2018.\n2. B. Yu, H. Yin, and Z. Zhu, \u201cSpatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting,\u201d in IJCAI, 2018.\n3. Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, \u201cGraph wavenet for deep spatial-temporal graph modeling,\u201d in IJCAI, 2019.\n4. L. Bai, L. Yao, C. Li, X. Wang, and C. Wang, \u201cAdaptive graph convolutional recurrent network for traffic forecasting,\u201d in NeurIPS, 2020.\n5. Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang, \u201cConnecting the dots: Multivariate time series forecasting with graph neural networks,\u201d in SIGKDD, 2020.\n6. M. Li and Z. Zhu, \u201cSpatial-temporal fusion graph neural networks for traffic flow forecasting,\u201d in AAAI, 2021.\n7. J. Lu, D. Batra, D. Parikh, and S. Lee, \u201cVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,\u201d in NeurIPS, 2019.\n8. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, \u201cXlnet: Generalized autoregressive pretraining for language understanding,\u201d in NeurIPS, 2019.\n9. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage models are few-shot learners,\u201d in NeurIPS, 2020.\n10. W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., \u201cA survey of large language models,\u201d arXiv preprint arXiv:2303.18223, 2023.\n11. H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J \u00e9gou, \u201cTraining data-efficient image transformers & distillation through attention,\u201d in ICML, 2021.\n12. Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in ICCV, 2021.\n13. W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, \u201cPyramid vision transformer: A versatile backbone for dense prediction without convolutions,\u201d in ICCV, 2021.\n14. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in NeurIPS, 2017.\n15. S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan, \u201cEnhancing the locality and breaking the memory bottleneck of transformer on time series forecasting,\u201d in NeurIPS, 2019.\n16. S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and S. Dustdar, \u201cPyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting,\u201d in ICLR, 2021.\n17. T. Zhou, P. Niu, L. Sun, R. Jin et al., \u201cOne fits all: Power general time series analysis by pretrained lm,\u201d in NeurIPS, 2023.\n18.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc538e16-bfe1-4776-badb-f6ea1bb120d6": {"__data__": {"id_": "bc538e16-bfe1-4776-badb-f6ea1bb120d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f", "node_type": "4", "metadata": {}, "hash": "da7cc8991c75a07ec89201fbfb9fe7d27bb7c31fe080c469555575080a3cdf40", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2299c14e-0913-4401-8f0d-f3340bb219ce", "node_type": "1", "metadata": {}, "hash": "a899597338be4c8bf6daab246a79072b3a35397bdab2d4eacdd371d5156a710a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8ae56e8-f265-4804-a3de-1cd660e4d613", "node_type": "1", "metadata": {}, "hash": "ba1bb88693546facd4d3fe17c622c29f6d6e61ab2a35b9e59c4de5518b2a8d21", "class_name": "RelatedNodeInfo"}}, "text": "15. S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan, \u201cEnhancing the locality and breaking the memory bottleneck of transformer on time series forecasting,\u201d in NeurIPS, 2019.\n16. S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and S. Dustdar, \u201cPyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting,\u201d in ICLR, 2021.\n17. T. Zhou, P. Niu, L. Sun, R. Jin et al., \u201cOne fits all: Power general time series analysis by pretrained lm,\u201d in NeurIPS, 2023.\n18. Y. Liu, H. Zhang, C. Li, X. Huang, J. Wang, and M. Long, \u201cTimer: Transformers for time series analysis at scale,\u201d arXiv preprint arXiv:2402.02368, 2024.\n19. S. Papoulis, Probability, Random Variables and Stochastic Processes by Athanasios. Boston: McGraw-Hill, 2002.\n20. W. Xue, T. Zhou, Q. Wen, J. Gao, B. Ding, and R. Jin, \u201cCard: Channel aligned robust blend transformer for time series forecasting,\u201d in ICLR, 2024.\n21. P. Chen, Y. Zhang, Y. Cheng, Y. Shu, Y. Wang, Q. Wen, B. Yang, and C. Guo, \u201cMulti-scale transformers with adaptive pathways for time series forecasting,\u201d in ICLR, 2023.\n22. Y. Wang, H. Wu, J. Dong, Y. Liu, Y. Qiu, H. Zhang, J. Wang, and M. Long, \u201cTimexer: Empowering transformers for time series forecasting with exogenous variables,\u201d arXiv preprint arXiv:2402.19072, 2024.\n23. F. Li, J. Feng, H. Yan, G. Jin, F. Yang, F. Sun, D. Jin, and Y. Li, \u201cDynamic graph convolutional recurrent network for traffic prediction: Benchmark and solution,\u201d ACM Transactions on Knowledge Discovery from Data, vol. 17, no. 1, pp. 1\u201321, 2023.\n24. J. Wang, J. Jiang, W. Jiang, C. Li, and W. X. Zhao, \u201cLibcity: An open library for traffic prediction,\u201d in SIGSPATIAL, 2021.\n25. R. Jiang, D. Yin, Z. Wang, Y. Wang, J. Deng, H. Liu, Z. Cai, J. Deng, X. Song, and R. Shibasaki, \u201cDl-traff: Survey and benchmark of deep learning models for urban traffic prediction,\u201d in CIKM, 2021.\n26. Y. Hao, X. Qin, Y. Chen, Y. Li, X. Sun, Y. Tao, X. Zhang, and X. Du, \u201cTs-benchmark: A benchmark for time series databases,\u201d in ICDE, 2021.\n27. A. J. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. G. Bostrom, P. Southam, and E. J. Keogh, \u201cThe uea multivariate time series classification archive, 2018,\u201d arXiv preprint arXiv:1811.00075, 2018.\n28. G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, \u201cA transformer-based framework for multivariate time series representation learning,\u201d KDD, 2021.\n29. S. Makridakis, E. Spiliotis, and V. Assimakopoulos, \u201cThe m4 competition: 100,000 time series and 61 forecasting methods,\u201d International Journal of Forecasting, vol. 36, no. 1, pp. 54\u201374, 2020.\n30. Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, \u201cRobust anomaly detection for multivariate time series through stochastic recurrent neural network,\u201d in SIGKDD, 2019.\n31. K. Hundman, V. Constantinou, C. Laporte, I. Colwell, and T. Soderstrom, \u201cDetecting spacecraft anomalies using lstms and nonparametric dynamic thresholding,\u201d in SIGKDD, 2018.\n32.", "mimetype": "text/plain", "start_char_idx": 2305, "end_char_idx": 5269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8ae56e8-f265-4804-a3de-1cd660e4d613": {"__data__": {"id_": "e8ae56e8-f265-4804-a3de-1cd660e4d613", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f", "node_type": "4", "metadata": {}, "hash": "da7cc8991c75a07ec89201fbfb9fe7d27bb7c31fe080c469555575080a3cdf40", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc538e16-bfe1-4776-badb-f6ea1bb120d6", "node_type": "1", "metadata": {}, "hash": "0d0c4ad1019bd6531002ed16b1ac5e3fb06fd08dee32faa3e41f7a4a36608729", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ccbcb06-5b9f-4e36-9c14-64db839c5284", "node_type": "1", "metadata": {}, "hash": "8a9a2dcd78139919080a709e56ba7cff536f47fdf1315219d950f5c3c5b0c828", "class_name": "RelatedNodeInfo"}}, "text": "29. S. Makridakis, E. Spiliotis, and V. Assimakopoulos, \u201cThe m4 competition: 100,000 time series and 61 forecasting methods,\u201d International Journal of Forecasting, vol. 36, no. 1, pp. 54\u201374, 2020.\n30. Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, \u201cRobust anomaly detection for multivariate time series through stochastic recurrent neural network,\u201d in SIGKDD, 2019.\n31. K. Hundman, V. Constantinou, C. Laporte, I. Colwell, and T. Soderstrom, \u201cDetecting spacecraft anomalies using lstms and nonparametric dynamic thresholding,\u201d in SIGKDD, 2018.\n32. A. P. Mathur and N. O. Tippenhauer, \u201cSwat: A water treatment testbed for research and training on ics security,\u201d in CySWater, 2016.\n33. A. Abdulaal, Z. Liu, and T. Lancewicki, \u201cPractical approach to asynchronous multivariate time series anomaly detection and localization,\u201d KDD, 2021.\n34. X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, \u201cPre-trained models for natural language processing: A survey,\u201d Science China Technological Sciences, vol. 63, no. 10, pp. 1872\u20131897, 2020.\n35. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d Journal of machine learning research, vol. 21, no. 140, pp. 1\u201367, 2020.\n36. P. P. Ray, \u201cChatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope,\u201d Internet of Things and Cyber-Physical Systems, 2023.\n37. K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in CVPR, 2020.\n38. X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, \u201cSelf-supervised learning: Generative or contrastive,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 1, pp. 857\u2013876, 2021.\n39. Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, \u201cSimmim: A simple framework for masked image modeling,\u201d in CVPR, 2022.\n40. K. He, X. Chen, S. Xie, Y. Li, P. Doll ar, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in CVPR, 2022.\n41. A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., \u201cImproving language understanding by generative pre-training,\u201d OpenAI, 2018.\n42. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., \u201cLanguage models are unsupervised multitask learners,\u201d OpenAI, vol. 1, no. 8, p. 9, 2019.\n43. A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, \u201cA survey on contrastive self-supervised learning,\u201d Technologies, vol. 9, no. 1, p. 2, 2020.\n44. K. He, X. Chen, S. Xie, Y. Li, P. Doll ar, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in CVPR, 2022.\n45. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in ICML, 2020.\n46.", "mimetype": "text/plain", "start_char_idx": 4717, "end_char_idx": 7570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ccbcb06-5b9f-4e36-9c14-64db839c5284": {"__data__": {"id_": "4ccbcb06-5b9f-4e36-9c14-64db839c5284", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f", "node_type": "4", "metadata": {}, "hash": "da7cc8991c75a07ec89201fbfb9fe7d27bb7c31fe080c469555575080a3cdf40", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8ae56e8-f265-4804-a3de-1cd660e4d613", "node_type": "1", "metadata": {}, "hash": "ba1bb88693546facd4d3fe17c622c29f6d6e61ab2a35b9e59c4de5518b2a8d21", "class_name": "RelatedNodeInfo"}}, "text": "1, no. 8, p. 9, 2019.\n43. A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, \u201cA survey on contrastive self-supervised learning,\u201d Technologies, vol. 9, no. 1, p. 2, 2020.\n44. K. He, X. Chen, S. Xie, Y. Li, P. Doll ar, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in CVPR, 2022.\n45. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in ICML, 2020.\n46. Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, \u201cUnsupervised feature learning via non-parametric instance discrimination,\u201d in CVPR, 2018.\n47. A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018.\n48. X. Yang, Z. Zhang, and R. Cui, \u201cTimeclr: A self-supervised contrastive learning framework for univariate time series representation,\u201d Knowledge-Based Systems, vol. 245, p. 108606, 2022.\n49. E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh, X. Li, and C. Guan, \u201cTime-series representation learning via temporal and contextual contrasting,\u201d in IJCAI, 2021.\n50. Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, and B. Xu, \u201cTs2vec: Towards universal representation of time series,\u201d in AAAI, 2022.\n51. Z. Wang, X. Xu, W. Zhang, G. Trajcevski, T. Zhong, and F. Zhou, \u201cLearning latent seasonal-trend representations for time series forecasting,\u201d in NeurIPS, 2022.\n52. D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d arXiv preprint arXiv:1312.6114, 2013.", "mimetype": "text/plain", "start_char_idx": 7109, "end_char_idx": 8614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19fa79e3-812a-4960-8c8c-657318a02e9b": {"__data__": {"id_": "19fa79e3-812a-4960-8c8c-657318a02e9b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93a9d023-c5bc-49c5-adda-57b508974fb6", "node_type": "4", "metadata": {}, "hash": "4fc4d9d7d641097fb5421bef21b578919d5b5baacbf3fdff162713e41e70a960", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac978d6a-561f-440e-8ab4-87c053b5ae84", "node_type": "1", "metadata": {}, "hash": "41afc01a5db430e6194c244627ae0bf97ca7309273b389eabdc08a0df5365cd3", "class_name": "RelatedNodeInfo"}}, "text": "# IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X\n\n# References\n\n1. G. Woo, C. Liu, D. Sahoo, A. Kumar, and S. Hoi, \u201cCoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting,\u201d in ICLR, 2022.\n2. X. Zhang, Z. Zhao, T. Tsiligkaridis, and M. Zitnik, \u201cSelf-supervised contrastive pre-training for time series via time-frequency consistency,\u201d in NeurIPS, 2022.\n3. A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, \u201cData2vec: A general framework for self-supervised learning in speech, vision and language,\u201d in ICML, 2022.\n4. G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, \u201cA transformer-based framework for multivariate time series representation learning,\u201d in SIGKDD, 2021.\n5. J. Dong, H. Wu, H. Zhang, L. Zhang, J. Wang, and M. Long, \u201cSimmtm: A simple pre-training framework for masked time-series modeling,\u201d arXiv preprint arXiv:2302.00861, 2023.\n6. S. Zhao, M. Jin, Z. Hou, C. Yang, Z. Li, Q. Wen, and Y. Wang, \u201cHimtm: Hierarchical multi-scale masked time series modeling for long-term forecasting,\u201d arXiv preprint arXiv:2401.05012, 2024.\n7. J. Dong, H. Wu, Y. Wang, Y. Qiu, L. Zhang, J. Wang, and M. Long, \u201cTimesiam: A pre-training framework for siamese time-series modeling,\u201d arXiv preprint arXiv:2402.02475, 2024.\n8. R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., \u201cOn the opportunities and risks of foundation models,\u201d arXiv preprint arXiv:2108.07258, 2021.\n9. A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., \u201cSegment anything,\u201d arXiv preprint arXiv:2304.02643, 2023.\n10. A. Garza and M. Mergenthaler-Canseco, \u201cTimegpt-1,\u201d arXiv preprint arXiv:2310.03589, 2023.\n11. K. Rasul, A. Ashok, A. R. Williams, A. Khorasani, G. Adamopoulos, R. Bhagwatkar, M. Bilo\u0161, H. Ghonia, N. V. Hassen, A. Schneider et al., \u201cLag-llama: Towards foundation models for time series forecasting,\u201d arXiv preprint arXiv:2310.08278, 2023.\n12. G. Woo, C. Liu, A. Kumar, C. Xiong, S. Savarese, and D. Sahoo, \u201cUnified training of universal time series forecasting transformers,\u201d arXiv preprint arXiv:2402.02592, 2024.\n13. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023.\n14. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023.\n15. C. Chang, W.-C.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac978d6a-561f-440e-8ab4-87c053b5ae84": {"__data__": {"id_": "ac978d6a-561f-440e-8ab4-87c053b5ae84", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93a9d023-c5bc-49c5-adda-57b508974fb6", "node_type": "4", "metadata": {}, "hash": "4fc4d9d7d641097fb5421bef21b578919d5b5baacbf3fdff162713e41e70a960", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19fa79e3-812a-4960-8c8c-657318a02e9b", "node_type": "1", "metadata": {}, "hash": "ce230988cd5a4e4631f05280b0486912926a5a9f6319cd6e50e61a1a7946858e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b03f6dd5-1a57-4596-abd2-5dde0ff157b3", "node_type": "1", "metadata": {}, "hash": "81620294339a3d98091ab73662f68eda71b3c4098577a895c772f37624f3b0b1", "class_name": "RelatedNodeInfo"}}, "text": "13. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023.\n14. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d arXiv preprint arXiv:2307.09288, 2023.\n15. C. Chang, W.-C. Peng, and T.-F. Chen, \u201cLlm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms,\u201d arXiv preprint arXiv:2308.08469, 2023.\n16. S. Q. Nate Gruver, Marc Finzi and A. G. Wilson, \u201cLarge Language Models Are Zero Shot Time Series Forecasters,\u201d in NeurIPS, 2023.\n17. A. F. Ansari, L. Stella, C. Turkmen, X. Zhang, P. Mercado, H. Shen, O. Shchur, S. S. Rangapuram, S. P. Arango, S. Kapoor et al., \u201cChronos: Learning the language of time series,\u201d arXiv preprint arXiv:2403.07815, 2024.\n18. T. Zhou, P. Niu, L. Sun, R. Jin et al., \u201cOne fits all: Power general time series analysis by pretrained lm,\u201d in NeurIPS, 2024.\n19. J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, \u201cFinetuned language models are zero-shot learners,\u201d in ICLR, 2021.\n20. A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, \u201cZero-shot text-to-image generation,\u201d in ICML, 2021.\n21. V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., \u201cMultitask prompted training enables zero-shot task generalization,\u201d arXiv preprint arXiv:2110.08207, 2021.\n22. M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, \u201cVisual prompt tuning,\u201d in European Conference on Computer Vision. Springer, 2022, pp. 709\u2013727.\n23. H. Xue and F. D. Salim, \u201cPrompt-based time series forecasting: A new task and dataset,\u201d arXiv preprint arXiv:2210.08964, 2022.\n24. X. Liu, J. Hu, Y. Li, S. Diao, Y. Liang, B. Hooi, and R. Zimmermann, \u201cUnitime: A language-empowered unified model for cross-domain time series forecasting,\u201d arXiv preprint arXiv:2310.09751, 2023.\n25. M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-Y. Chen, Y. Liang, Y.-F. Li, S. Pan et al., \u201cTime-llm: Time series forecasting by reprogramming large language models,\u201d arXiv preprint arXiv:2310.01728, 2023.\n26. Y. Liu, G. Qin, X. Huang, J. Wang, and M. Long, \u201cAutotimes: Autoregressive time series forecasters via large language models,\u201d arXiv preprint arXiv:2402.02370, 2024.\n\n# Author Information\n\nYuxuan Wang received the BE degree from Beihang University in 2022.", "mimetype": "text/plain", "start_char_idx": 2249, "end_char_idx": 4874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b03f6dd5-1a57-4596-abd2-5dde0ff157b3": {"__data__": {"id_": "b03f6dd5-1a57-4596-abd2-5dde0ff157b3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93a9d023-c5bc-49c5-adda-57b508974fb6", "node_type": "4", "metadata": {}, "hash": "4fc4d9d7d641097fb5421bef21b578919d5b5baacbf3fdff162713e41e70a960", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac978d6a-561f-440e-8ab4-87c053b5ae84", "node_type": "1", "metadata": {}, "hash": "41afc01a5db430e6194c244627ae0bf97ca7309273b389eabdc08a0df5365cd3", "class_name": "RelatedNodeInfo"}}, "text": "25. M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-Y. Chen, Y. Liang, Y.-F. Li, S. Pan et al., \u201cTime-llm: Time series forecasting by reprogramming large language models,\u201d arXiv preprint arXiv:2310.01728, 2023.\n26. Y. Liu, G. Qin, X. Huang, J. Wang, and M. Long, \u201cAutotimes: Autoregressive time series forecasters via large language models,\u201d arXiv preprint arXiv:2402.02370, 2024.\n\n# Author Information\n\nYuxuan Wang received the BE degree from Beihang University in 2022. She is now working towards the PhD degree in computer software at Tsinghua University. Her research interests include machine learning and time series analysis.\n\nHaixu Wu received the BE degree in software engineering from Tsinghua University in 2020. He is working towards the PhD degree in computer software at Tsinghua University. His research interests include scientific machine learning and spatiotemporal learning.\n\nJiaxiang Dong received the ME degree in computer science and technology from Nankai University in 2018. He is currently working toward the PhD degree in computer software at Tsinghua University. His research interests include machine learning and time series pre-training.\n\nYong Liu received the BE degree in software engineering from Tsinghua University in 2021. He is working towards the PhD degree in computer software at Tsinghua University. His research interests include time series analysis and large time series models.\n\nMingsheng Long received the BE and PhD degrees from Tsinghua University in 2008 and 2014 respectively. He was a visiting researcher with UC Berkeley from 2014 to 2015. He is currently a tenured associate professor with the School of Software, Tsinghua University. He serves as an associate editor of IEEE Transactions on Pattern Analysis and Machine Intelligence and Artificial Intelligence Journal, and as area chairs of major machine learning conferences, including ICML, NeurIPS, and ICLR. His research is dedicated to machine learning theory, algorithms, and models, with special interests in transfer learning and domain adaptation, deep learning and foundation models, scientific learning, and world models.\n\nJianmin Wang received the BE degree from Peking University, China, in 1990, and the ME and PhD degrees in computer software from Tsinghua University, China, in 1992 and 1995, respectively. He is a full professor with the School of Software, Tsinghua University. His research interests include Big Data management systems and large-scale data analytics. He led to developing a product data and lifecycle management system, which has been deployed in hundreds of enterprises in China. He is leading the development of the Tsinghua DataWay Big Data platform in the National Engineering Lab for Big Data Software.", "mimetype": "text/plain", "start_char_idx": 4395, "end_char_idx": 7151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec8577e8-581f-413b-9861-fd8595c05118": {"__data__": {"id_": "ec8577e8-581f-413b-9861-fd8595c05118", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27bfdc15-3f59-4992-8659-4b532ba28f77", "node_type": "4", "metadata": {}, "hash": "0867c77b35eeb1450cb63a129da73e5e971535d9a32cf3922811b9e1c130905b", "class_name": "RelatedNodeInfo"}}, "text": "# Driver Positioning and Incentive Budgeting with an Escrow Mechanism for Ridesharing Platforms\n\n# Hao Yi Ong, Daniel Freund, Davide Crapis\n\n# May 3, 2021\n\n# Abstract\n\nDrivers on the Lyft rideshare platform do not always know where the areas of supply shortage are in real time. This lack of information hurts both riders trying to find a ride and drivers trying to determine how to maximize their earnings opportunity. Lyft\u2019s Personal Power Zone (PPZ) product helps the company to maintain high levels of service on the platform by influencing the spatial distribution of drivers in real time via monetary incentives that encourage them to reposition their vehicles. The underlying system that powers the product has two main components: (1) a novel \u201cescrow mechanism\u201d that tracks available incentive budgets tied to locations within a city in real time, and (2) an algorithm that solves the stochastic driver positioning problem to maximize short-run revenue from riders\u2019 fares. The optimization problem is a multiagent dynamic program that is too complicated to solve optimally for our large-scale application. Our approach is to decompose it into two subproblems. The first determines the set of drivers to incentivize and where to incentivize them to position themselves. The second determines how to fund each incentive using the escrow budget. By formulating it as two convex programs, we are able to use commercial solvers that find the optimal solution in a matter of seconds. Rolled out to all 320 cities in which Lyft operates in a little over a year, the system now generates millions of bonuses that incentivize hundreds of thousands of active drivers to optimally position themselves in anticipation of ride requests every week. Together, the PPZ product and its underlying algorithms represent a paradigm shift in how Lyft drivers drive and generate earnings on the platform. Its direct business impact has been a 0.5% increase in incremental bookings, amounting to tens of millions of dollars per year. In addition, the product has brought about significant improvements to the driver and rider experience on the platform. These include statistically significant reductions in pick-up times and ride cancellations. Finally, internal surveys reveal that the vast majority of drivers prefer PPZs over the legacy system.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39cc66d7-de5b-4722-9063-3359169b9c5d": {"__data__": {"id_": "39cc66d7-de5b-4722-9063-3359169b9c5d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3888170d-5508-4326-85f4-e9f7a8471b80", "node_type": "4", "metadata": {}, "hash": "bc5f7514f2b00f508731dec7d32efeeb0af0fee031e3b0196fcae238a676937e", "class_name": "RelatedNodeInfo"}}, "text": "# Contents\n\n# 1 Introduction\n\n4\n\n# 2 Problem Background\n\n6\n\n# 2.1 Cherry-picking of Long/Short Rides\n\n7\n\n# 2.2 Spatiotemporal Volatility\n\n7\n\n# 2.3 Spatial Volatility\n\n8\n\n# 2.4 Coordination\n\n8\n\n# 2.5 Limitations in the Legacy System\n\n8\n\n# 3 Product Description\n\n9\n\n# 4 Implementation Challenges\n\n9\n\n# 4.1 Repositioning\n\n9\n\n# 5 Stochastic Model\n\n11\n\n# 5.1 Supply\n\n11\n\n# 5.2 Demand\n\n11\n\n# 6 Optimization Approach\n\n12\n\n# 6.1 Budgeting Via a Location-based Escrow Mechanism\n\n12\n\n# 6.2 Problem Decomposition and Certainty Equivalent Approximation\n\n13\n\n# 6.3 Sequential Algorithm\n\n14\n\n# 6.4 Driver Positioning\n\n14\n\n# 6.5 Incentive Computation\n\n16\n\n# 7 Numerical Experiments\n\n16\n\n# 7.1 Data\n\n16\n\n# 7.2 Benchmarking Simulation\n\n16\n\n# 8 Live Experiments\n\n18\n\n# 8.1 Causal Inference Framework\n\n18\n\n# 8.2 Live Experiments: Results\n\n21\n\n# 9 Broader Impact\n\n22\n\n# A Optimization Approach\n\n26\n\n# A.1 The Model\n\n26\n\n# A.1.1 Order dispatch\n\n26\n\n# A.1.2 Price modifiers and elasticity\n\n27\n\n# A.1.3 Driver allocation\n\n27\n\n# A.1.4 Supply dynamics\n\n27\n\n# A.1.5 Escrow budgeting\n\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b15be2e4-6383-4a0f-99bf-b4306fa700da": {"__data__": {"id_": "b15be2e4-6383-4a0f-99bf-b4306fa700da", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d40fbb26-d7a8-4d3e-a2d0-3e472a150254", "node_type": "4", "metadata": {}, "hash": "0f6868dcdca25630ae0aa6ce9340e0730e4a4e12ec47273cc4c8b04f2d88cdaa", "class_name": "RelatedNodeInfo"}}, "text": "# A. Driver Positioning\n\n# A.2 Driver Positioning\n\n28\n\n# A.3 Incentive Computation\n\n29\n\n# A.4 Practical Implementation Details\n\n29\n\n# A.4.1 Convexification\n\n30\n\n# A.4.2 Allocation vectorization\n\n30\n\n# A.4.3 Location pruning\n\n30\n\n# B. Perturbation and Sensitivity Analysis\n\n# B.1 The Perturbed Problem\n\n31\n\n# B.2 Local Sensitivity Analysis\n\n32\n\n# B.3 Global Sensitivity Analysis\n\n33\n\n# C. Video of PPZs Served in Production\n\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13bd854f-0455-49ad-bac9-241c7e7d128b": {"__data__": {"id_": "13bd854f-0455-49ad-bac9-241c7e7d128b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1117bc87-6ef8-452e-9754-9671a2ebb9ed", "node_type": "4", "metadata": {}, "hash": "2bc924de312e4431c0d51e17a97656a382e9f2cc7bf1b9e3c2b7ec95b295ffc5", "class_name": "RelatedNodeInfo"}}, "text": "# 1 Introduction\n\nLyft, Inc. develops and operates a mobile application (app), offering a ridesharing platform, as well as motorized scooter and bicycle sharing services. The company is based in San Francisco, California and operates in 320 cities in the United States and Canada. The biggest and most mature part of its business is ride sharing, a marketplace in which riders are matched in real time to drivers who drive them to their destinations. Our work seeks to incentivize the improved spatial positioning of drivers, so that drivers might better meet rider demand. The work was conducted with Lyft\u2019s Driver Positioning team, a cross-functional team of product managers and designers, software engineers, and research scientists that work as a unit to tackle problems related to the supply side of the market and develop products to improve this side of the marketplace.\n\nIn this paper, we describe the development of Lyft\u2019s Personalized Power Zone (PPZ) product. PPZ is an innovation that Lyft introduced to improve the drivers\u2019 experiences during peak demand periods. Historically, ride-hailing platforms have focused on dynamic pricing to match rider demand to available driver supply, charging higher prices during periods of heightened demand. Lyft calls its dynamic-pricing product \u201cPrime Time\u201d (PT) and Uber calls its equivalent \u201cSurge.\u201d These are multiplicative modifiers on top of the base time-and-distance fare, thereby reactively suppressing rider demand through marked-up fares. For drivers, both platforms have traditionally used heatmaps, which show the magnitude of elevated ride fares as colors on city maps as a visual aid to indicate locations with elevated demand; see the left subfigure in Figure 1. Since the driver\u2019s PT bonus in the legacy system was proportional to the ride\u2019s base fare and the PT multiplier, heatmaps serve as an incentive that drivers can use to reposition themselves toward areas with high PT multipliers [LFK18].\n\nGenerally, dynamic pricing provides a high earning potential for drivers, especially around persistent heatmap \u201chotspots,\u201d which are zones on the city map with elevated prices. In a commission-based platform system, drivers benefit from the rides\u2019 higher prices (i.e., higher $/minute spent driving a passenger). In addition, greater demand can give rise to greater driver utilization (i.e., minutes spent driving passengers/total minutes driving), as long as the time drivers spend per ride does not increase due to increased pick-up times, often referred to as estimated time of arrival (ETA) [CKW17]. Nonetheless, prior to the implementation of PPZs, the driver experience in peak demand periods was never optimized for drivers. Although some evidence exists showing that drivers react to the heatmap [LFK18], the predominant wisdom among drivers has been that actively chasing heatmap hotspots is not a good strategy to maximize earnings [Gri17]. In large part, this is motivated by the unpredictable and fast-paced updates of PT levels set by dynamic pricing algorithms that adapt rapidly to changes in the marketplace (see Figure 2). The goal of PPZs was to replace the driver\u2019s PT heatmap and adapt the drivers\u2019 compensation during PT in a manner that simultaneously incentivizes improved driver positioning within a city and the drivers\u2019 experience by more directly rewarding the positioning effort.\n\nReplacing the driver PT product required a significant shift to Lyft\u2019s ride-hailing platform design. For example, it involved deviating from a fixed in-ride, on-trip commission-based model that pays the driver a constant fraction of the rider\u2019s PT surcharge. This change was", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47b05167-cf4e-4077-b313-7558c7a7c684": {"__data__": {"id_": "47b05167-cf4e-4077-b313-7558c7a7c684", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb0da17b-71fb-4820-8b80-188bccf29b5e", "node_type": "4", "metadata": {}, "hash": "53b2b2a6bc67e013b6a0dd1b7b43699b39f8bb737d785f93115a34db580d781d", "class_name": "RelatedNodeInfo"}}, "text": "# 2-47 PM\n\n# Iya Drive mode\n\n# PRIME TIME\n\n# 25757 MORE\n\nEarn a ride bonus\n\nEnter the zone earn a bonus on your next ride\n\n# Navigate\n\nFigure 1: PT adds a predetermined percentage of the base fare onto the driver\u2019s total fare (left), whereas drivers accrue a Personal Power Zone bonus when they enter the corresponding zones (right).\n\nnecessary to improve upon the shortcomings of the driver PT experience, which we outline in additional detail below. However, beyond this design change, it also required significant algorithmic innovations. Our paper highlights these algorithmic innovations and the results they produced. The two key algorithmic developments that we describe are as follows.\n\n- A robust spatial budget-tracking mechanism that provides real-time demand signals, which we discuss in the Budgeting Via a Location-Based Escrow Mechanism subsection.\n- An asymptotically optimal two-stage algorithm that produces driver relocation incentives to maximize market efficiency, which we describe in the Problem Decomposition and Certainty Equivalent Approximation subsection.\n\nBeyond the change to Lyft\u2019s platform, the PPZ development also deviated significantly from the ideas traditionally considered in the academic study of the gig economy. In these works, the focus has usually been on the platform taking a constant proportion of each trip\u2019s fare, including the PT portion. Only recently have Garg and Nazerzadeh [GN21] considered a stylized model that exposes fundamental limitations that arise with a multiplicative PT bonus due to the fast-paced changes in the levels of dynamic rider prices often observed in the market. (Note that Garg and Nazerzadeh was originally written in 2019 and was updated in 2021.) To the best of our knowledge, all prior work had either explicitly (e.g., [BJR15], [BCS16], [CDL17]) or implicitly (e.g., [MFP19]) considered models in which the drivers\u2019 bonus was a constant proportion of the rider\u2019s PT. In contrast to Garg and Nazerzadeh [GN21],\n\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7721da31-35e7-42ac-8614-fb1787dd6512": {"__data__": {"id_": "7721da31-35e7-42ac-8614-fb1787dd6512", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0435041d-c9ef-4eb1-9b39-a907d02384fc", "node_type": "4", "metadata": {}, "hash": "2b9938b9900280880603f0a03c59d3b78ad5d7da3e60e932a41142448355d7ed", "class_name": "RelatedNodeInfo"}}, "text": "# Prime Time markup over time\n\n|Time|Location|Location|Location|\n|---|---|---|---|\n|18:45:00|160|160|160|\n|19:00:00|140|140|140|\n|19:15:00|120|120|120|\n|19:30:00|100|100|100|\n|19:45:00| | | |\n|20:00:00| | | |\n|20:15:00| | | |\n|20:30:00| | | |\n\nFigure 2: PT bonus multiplier values plotted for three nearby locations illustrate how spatially and temporally volatile they can be over a short time span.\n\nWe describe a real-world implementation of a system that considers this concern and others, which we describe in the Problem Background section.\n\n# 2 Problem Background\n\nWe structured the remainder of this paper as follows. In the Problem Background and Product Description sections, we discuss the shortcomings of the legacy system and the PPZ product design. We describe the key technical challenges to overcome when implementing PPZs in Technical Implementation Challenges. In the Stochastic Model and Optimization Approach sections, respectively, we describe the theoretical model that motivated our algorithmic approach and the approach. In Implementation, Numerical Experiments, and Live Experiments, we outline the challenges that arose in measuring the business impact of the new system and the causal inference approach we developed to overcome them. We conclude with a discussion of the impacts these innovations enabled in the Broader Impact section. In the appendices, we provide a formal description of the technical challenge and our optimization model. The algorithms we present are currently deployed at Lyft in the 320 cities in which we operate. Their cumulative impact has included an increase in yearly bookings by tens of millions of dollars and a reduction in driver cancellation rates by 13%. Every week, these algorithms generate millions of PPZs that help hundreds of thousands active drivers decide where to drive.\n\nWe begin by discussing the limitations of the legacy driver PT system. The papers most similar to ours focus on the problem of setting prices to incentivize drivers to relocate toward high-earning opportunities. The key component the literature misses, to the best of our", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a01e21aa-0562-46d6-b271-efd856f04e28": {"__data__": {"id_": "a01e21aa-0562-46d6-b271-efd856f04e28", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e21f8e32-57bd-4673-bbf1-868381c1b8fe", "node_type": "4", "metadata": {}, "hash": "ba7103e4ea89e7935bf70fc6dc645ea175d4aa8aa55da2871f7f1e431a2285ea", "class_name": "RelatedNodeInfo"}}, "text": "knowledge, is the significant and unpredictable spatial and temporal volatility of rider PT. To keep supply and demand in balance in a fast-paced market such as ride-hailing, the platform frequently updates its prices; for example. Uber updates its prices every two minutes [LFK18]. Consider the time series plot of the PT bonus multiplier levels in three busy nearby locations (i.e., within a mile apart) in the greater San Francisco Bay Area, as we show in Figure 2. Although the price-setting algorithm enforces some level of spatiotemporal smoothness, market conditions and thus PT bonus multiplier values change rapidly across time and space. In addition, the direction and magnitude of the changes are unpredictable. This holds true for platform designers with full information, and even more so for drivers with less information. Thus, common assumptions around drivers being rational agents with full information fail to capture the uncertainty inherent in the driver\u2019s decision-making process. Below, we describe these complications and how we designed PPZs to overcome them.\n\n# 2.1 Cherry-picking of Long/Short Rides\n\nGarg and Nazerzadeh [GN21] show that drivers may cherry-pick long rides during PT periods and short rides during non-PT periods to maximize their earnings using the legacy system. They consider a two-state Markovian system in which the state characterizes whether the current rider price is either low or high (i.e., the dynamic pricing is abstracted to being only a binary state). A driver receives a stream of ride requests of different trip lengths; the state of the system determines the rate at which the driver receives requests and the pay per minute and per mile of driving. The driver\u2019s policy is to pick the set of requests to accept in each state. The main insight of Garg and Nazerzadeh is that by making the bonus for PT rides additive, rather than proportional to the trip length, the platform induces the driver to accept a larger set of trips. Intuitively, this is because accepting a short trip during surge periods may provide only a small earnings boost if the bonus is proportional to the (short) trip; yet, at the end of the trip, the surge period with its elevated opportunities may be over. Furthermore, they show (in their Theorem 3) that a bonus that is an affine function of the trip length is the best the platform can do to incentivize drivers to accept as many trips as possible. Although this constitutes a significant conceptual contribution toward an understanding of the fundamental limitations that arise under the legacy model, their stylized model does not give rise to a practical algorithm to overcome these shortcomings. Beyond implementing a product that capitalizes on their main insights, PPZ also addresses the following concerns that are not captured in the Garg and Nazerzadeh [GN21] model.\n\n# 2.2 Spatiotemporal Volatility\n\nAs we show in Figure 2, PT is not only volatile in time but also in space. In particular, because different regions within a city experience different supply-demand imbalances, the platform prices rides differently across locations. Such spatial differences in prices are widely captured in the literature; examples include Bimpikis et al. [BCS16], Afeche et al. [ALM18], and Ma et al. [MFP19]. However, the literature does not account for the interplay between spatial and temporal volatility. Specifically, between the time a driver sees prices induced by", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cf826e0-c43c-4361-9807-04c1451cc7dc": {"__data__": {"id_": "7cf826e0-c43c-4361-9807-04c1451cc7dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20899be0-7bb6-432e-aca4-523e402f5c06", "node_type": "4", "metadata": {}, "hash": "bff813d3ff236542431706732e5a6567d603b210e0b0006156f1f0a205d5c26a", "class_name": "RelatedNodeInfo"}}, "text": "# 2.3 Spatial Volatility\n\nAbove we illustrated how temporal volatility discourages drivers from spatially repositioning. However, spatial price volatility also affects drivers who are already in an area where prices are high. Recall that under the legacy system, drivers earn a direct cut of the fare paid by the rider. In that world, a driver may encounter situations in which he/she is idling in a high-PT location and yet is dispatched to a nearby location with lower (or even no) PT. Ironically, the high PT at the driver\u2019s location may dissuade a customer in that location from requesting a ride. Thus, the request-suppressing effect of PT may cause the driver to experience a smaller PT bonus and a longer ETA to serve the ride.\n\n# 2.4 Coordination\n\nFinally, many papers in the ridesharing context, including Ma et al. [MFP19], Castro et al. [CBL18], Afeche et al. [ALM18], Yang et al. [YIF18], and Bimpikis et al. [BCS16], assume different forms of spatial equilibria arising from drivers selfishly optimizing their own earnings. In practice, it is difficult to imagine how such equilibria would emerge in a dynamic system without agents having the ability to observe the actions of other drivers. Consider a group of drivers trying to reposition toward two different locations with PT. Even if they coordinated on the number of drivers to reposition to each location, they would need to make this decision based only on the observed PT levels. Because these would roughly correspond to relative supply shortfall, and not to absolute supply shortfall (see Price modifiers and elasticity in Appendix A: Optimization Approach), successful coordination is extremely unlikely.\n\n# 2.5 Limitations in the Legacy System\n\nIn summary, despite providing drivers with higher earnings opportunities, the legacy system had misalignments between (1) the drivers\u2019 earnings maximization behaviors, (2) the behaviors the platform incentivizes (e.g., repositioning toward PT), and (3) the driver behaviors that would maximize platform metrics, such as number of rides served. Cumulatively, these led to both a poorer driver experience and platform inefficiencies. Noticeably, these issues could not have been addressed through mere algorithmic improvements to dynamic pricing. For example, they could not be resolved through a better forecast of riders\u2019 willingness to pay, or a change in the optimization problem underlying the price-setting process. Instead, a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e215ca7-35ce-4ce7-b961-21430d8669e0": {"__data__": {"id_": "5e215ca7-35ce-4ce7-b961-21430d8669e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d257f9ff-57fa-4031-b2f2-0656b3345edd", "node_type": "4", "metadata": {}, "hash": "2244d8f11f76fe87664863a0a49d695b72c53e3dd513cc7ade779cc01ab13249", "class_name": "RelatedNodeInfo"}}, "text": "# 3 Product Description\n\nThe goal of the PPZ project was to optimize the PT experience for drivers by channeling the riders\u2019 PT payments to drivers in a way that would resolve the limitations above and thereby improve the platform\u2019s market metrics (e.g., bookings and driver hours); see the Live Experiments: Results subsection below. From the driver\u2019s perspective, a PPZ incentive consists of a delimited geographic area associated with a fixed, visible bonus. The driver qualifies for the bonus when entering the area and receives it upon completing his/her next ride request; the right side of Figure 1 shows the PPZ driver interface, including the bonus the driver accrues upon entering the purple/pink PPZ zone. By providing a clear reward for an unambiguous action, PPZ removes the uncertainty in the legacy system and thus creates an effective nudge for drivers to reposition. Beyond removing the issues surrounding uncertainty, globally optimizing the incentives given to drivers across a city also allows the PPZ product to globally coordinate the drivers\u2019 locations. In addition, as PPZ replaces the proportional PT bonus of the legacy system, the driver may also receive an after-ride adjustment to ensure that the gap between the rider\u2019s fare and the driver\u2019s earnings is not excessively large. The additive upfront bonus experience, in conjunction with such after-ride adjustments based on time and distance, are similar to the affine bonuses suggested by Garg and Nazerzadeh [GN21] and thus partially address the issues of drivers strategically rejecting trips to boost their earnings or cherry-picking their rides.\n\n# 4 Implementation Challenges\n\nA successful implementation of PPZs had to address two key technical challenges. First, PPZ would need to incentivize a better positioning of drivers than the legacy system. Second, the total amount of money spent on the PPZ incentives should match its budget (i.e., the PT paid by riders). In the following subsections, we describe these challenges and our initial attempts to overcome them.\n\n# 4.1 Repositioning\n\nWithin any city in which Lyft operates, a set of drivers, referred to as open drivers or open supply, is available for dispatch. PPZs encourage open drivers to reposition to a different location by offering a monetary incentive at that location. The previous approach to incentive-based repositioning was based on the marginal value of supply in all spatial units of the city. Value can refer to rider request conversions or to the revenue they represent. This concept, which we refer to as the local sensitivity approach, was to incentivize drivers to reposition from one location to another if a significant differential in the marginal value of supply between the two locations existed.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2766, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c198dd33-1aa4-438b-b73f-2137bfdd6bd1": {"__data__": {"id_": "c198dd33-1aa4-438b-b73f-2137bfdd6bd1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93c933e3-1f25-45ef-9d39-ad622f69e126", "node_type": "4", "metadata": {}, "hash": "c1cb0a2998f39629240082e28a0f2dfe58c9a7b5ccf18e8e1956e094834c5a57", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 3\n\nThe graph shows the marginal value of supply evaluated at some location, as the highly nonlinear, step function-like empirically-fitted (solid) and theoretical (dashed) curves illustrate.\n\nUnfortunately, the local nature of the approach did not account for the highly interactive nature of incentivizing a large number of drivers to reposition within a city. Implicitly, the local-sensitivity approach assumes that the local approximation (i.e., the gradient of our value function with respect to supply) provides a good indication of the market state even when it is perturbed by incentivizing many drivers to move simultaneously. This was a poor assumption. In practice, we observed that although the gradient evaluated at the current market state is accurate in the case of a small change in supply, incentivizing a large number of drivers to move breaks the locality assumption. The marginal value of supply is highly nonlinear and drops steeply beyond a specified supply count. Figure 3 shows an example of a location at which the nonlinearity is exhibited in both the empirically observed (solid line) and theoretically computed marginal values (dashed line). The steep drop in value appears when the supply count matches the number of ride requests at the location.\n\nIn Appendix B: Perturbation and Sensitivity Analysis, we prove that not only is the local sensitivity analysis largely invalid, but the decisions also made based on this approach can hurt market conditions.\n\nBecause PT is based on a multiplicative price markup on top of the eventual time-and-distance-based trip fare, the source of our budget is dynamic and is only fully realized after a ride has been completed. Specifically, we cannot know the value of the ride fare that funds the PPZ bonus until much later when the ride has been completed, or at least requested. To encourage a driver to reposition toward better ride opportunities, we must offer a PPZ bonus before the ride, which is intended to fund the PPZ bonus. This chicken-or-egg problem was at the source of PPZ\u2019s budget-control challenge.\n\nIn the initial marginal-value-of-supply approach, the bonuses were paid as an affine function of the marginal value. Unfortunately, these bonus values often grossly overestimated", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e08fec4b-a71b-4982-8e5b-013525c5e38f": {"__data__": {"id_": "e08fec4b-a71b-4982-8e5b-013525c5e38f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d3592d4-6aa8-4700-8c69-77087c11060c", "node_type": "4", "metadata": {}, "hash": "a1baa116df2c1a2e8d2afa44d98d19b9d9b505fd083eec1b78e775ceb3c1dd5c", "class_name": "RelatedNodeInfo"}}, "text": "The available funds, leading to overspending of up to 15 times our budget at times. This was in large part because the marginal value is a poor measure of the incremental revenue we can derive from the market. The spending problem became so severe that market degradation forced us to roll back the PPZ product from a major city in which we had already launched it [Smi18]. If we were to eventually scale the product to all 320 cities, we had to develop a much more robust way to control spending.\n\n# 5 Stochastic Model\n\nIn this section, we describe a stochastic model that motivated our eventual optimization approach. The model is based on a multistage stochastic process on a discrete network of locations. Below, we outline the steps in each stage. We begin by describing what is assumed to be known at the beginning of the first stage. Initially, we know for each location i the current number of idle drivers and the expected demand. In addition, we know for any two locations i and j (1) whether we can dispatch a driver from i to serve demand in j, and (2) the response probability of a driver in i to relocate to j (i.e., the probability that a driver will relocate from i to j when a PPZ incentivizes that driver to do so). Finally, for each location we have a current budget estimate. We provide details in the Budgeting Via a Location-Based Escrow Mechanism subsection below.\n\n# 5.1 Supply\n\nBased on the above information, we must determine the set of drivers to whom we will provide an incentive to relocate, and the destination to which we want these drivers to relocate. In doing so, we are constrained to not overspend the budget. Each driver who is given an incentive makes a stochastic decision whether to reposition based on that driver\u2019s present location and the incentivized destination.\n\n# 5.2 Demand\n\nAfter the drivers relocate, the platform sets the PT price markups based on the new driver locations and the forecast demand. The prices are set as part of an optimization problem to maximize a metric, which we refer to as no-PT bookings. This metric captures the time-and-distance fare of all serviced ride requests but does not include the PT markups. The no-PT bookings metric is one of Lyft\u2019s most important metrics because it captures revenue without rewarding markets with mismatched supply and demand. For example, when including PT, bookings may be higher in the short run due to extreme PT markups; however, these are often perceived as detrimental to rider retention in the long run. Thus, no-PT bookings is the main metric we aim to maximize with PPZs. The constraints of the underlying optimization problem ensure that the expected number of ride requests, suppressed by the PT price markups, does not deplete the supply beyond a reserve level that ensures that ETAs remain acceptably low [CKW17]. After the platform sets its prices, we model the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1c6c617-50a5-43be-afee-8fbbf0172971": {"__data__": {"id_": "f1c6c617-50a5-43be-afee-8fbbf0172971", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6882b8ec-6e55-45c7-b489-7ac2ba1f3488", "node_type": "4", "metadata": {}, "hash": "91c37095f8789fcbe637a916e774ba7004016c4d3a1cbadfa5104b72f10ccaa3", "class_name": "RelatedNodeInfo"}}, "text": "# 6 Optimization Approach\n\nIn this section, we provide a high-level overview of the methods we use to solve the problem described in the Stochastic Model section above. As part of the solution, we also describe a novel escrow mechanism used to generate budgetary signals for our algorithm. We provide a technically rigorous and detailed discussion of our approach in Appendix A: Optimization Approach.\n\n# 6.1 Budgeting Via a Location-based Escrow Mechanism\n\nOur second key challenge was to spend a highly dynamic budget before we collect it. Recall the budget materializes only upon ride completion via rider PT payments. This dynamic budget is also highly location dependent because demand levels can vary significantly across a city. Given the delay between the time at which we must spend and the time at which the budget is realized in our accounts, we would like to accurately predict when and where the budget income may materialize; however, due to the spatiotemporal volatility of PT, doing so is impossible. To circumvent this conundrum, our key insight was that fast, real-time data based on upfront fares at ride dispatch can substitute for good predictions. Intuitively, we treat the PT paid by riders as part of our budget at the time and place of the ride request and dispatch despite the risk of the ride still being canceled or the PT amount changing because the rider changes the origin or destination. Upon completion of the trip, we then correct for the difference in amount that was realized. We note that although ride requests are a lagging indicator for future demand, we know empirically that rider demand (and the elasticity of demand) is generally well-correlated in the same location within a reasonably short time duration. Thus, as long as we can spend the budget accumulating from upfront fares quickly, the location of incoming PT across the city at any point in time will provide a good indication of where we should provide PPZs.\n\nTo track how much to spend and therefore how much money to offer per incentive, we create a set of virtual accounts associated with the locations from which ride requests originate. We call these virtual accounts a city\u2019s \u201clocal accounts.\u201d The escrow mechanism updates in response to events such as when a driver receives a PPZ or accepts a ride request and when a rider cancels a ride request or is dropped off at the ride destination. The occurrence of these events correspond to when and where we can account for the estimated and actual financial transactions, which determine the portions of the PT income earmarked for PPZ spend and the PPZ spend itself. As these events occur, we actively update the balance of the location-based escrow accounts in real time.\n\nPT income is attributed to the local account corresponding to the location at which the ride request originates. Based on the repositioning optimization, PPZ expenditure is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e960c757-0e62-4490-a7b3-25340fe87a69": {"__data__": {"id_": "e960c757-0e62-4490-a7b3-25340fe87a69", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bae8eb49-a2c5-46f1-b4ca-52407bc5c4a1", "node_type": "4", "metadata": {}, "hash": "336c0f82b5e9dab9b8b4bd6385f9778ff5ee13690ff9f2d680c83399a1b83581", "class_name": "RelatedNodeInfo"}}, "text": "# PPZ creation\n\nWhen driver is assigned PPZ, the Bonus accrual accounts corresponding to the PPZ destination decreased the contribution of the account towards the cost of the incentive:\n\n|Rider intent|Ride request|\n|---|---|\n|app-open; high request intent generate expected PT budget that added to the accounts that correspond to ride request origins:|While the driver accrues money in their expected balances the tagged payout accounts balances decrease as the expected payout increases due to the increased probability of PT income:|\n\n|Ride accept|Ride completed|\n|---|---|\n|PPZ bonus \"locked-in\" expenditure for tagged accounts:|After the rider is dropped off; the \"fully realized\" PPZ payout and PT budget are substituted for the estimates that previously adjusted the balances in the accounts:|\n\nFigure 4: A sample timeline illustrates how riders generate escrow account balances that allow PPZs to be generated and served to encourage drivers to reposition. Note that these sample events illustrate how the escrow accounts are updated for expected incoming budget and payouts and eventually \u201crealized\u201d as finalized account balances.\n\nAttributed to a set of local accounts corresponding to the PPZ destination. A subtlety is that PT income is attributed to a single account whereas PPZ expenditure can possibly be attributed to multiple accounts near the PPZ destination. The reason for this is that the driver who reaches a PPZ destination may subsequently be dispatched to pick up riders in locations close to the destination as well as those in the destination location. Thus, pooling multiple account balances to motivate the driver to reposition is appropriate. Although the full set of events causing updates to the account balances is too numerous to list, Figure 4 gives an intuitive illustration of how riders and drivers typically interact with the platform and consequently induce escrow account balance changes.\n\nAs we explain above, we want to avoid accumulating money in the accounts (as opposed to spending it in real time); to avoid this, the PPZ bonus amounts offered are set to equal approximately the ratio of the available balance in the accounts to the number of drivers who are expected to reposition to the locations tied to the local escrow accounts, given the PPZ incentive. They thus form a set of \u201caccount clearance targets\u201d that functions as a budgetary reference for an amount to spend at each time step of our PPZ allocation and bonus computation. Its purpose is to clear our accounts and avoid accumulating money. Such target spend signals from the escrow mechanism provide a safe way for PPZs to spend incoming PT money quickly without overspending. In addition, they serve as principled, location-based accounting signals for PPZ to incentivize drivers to reposition.\n\n# 6.2 Problem Decomposition and Certainty Equivalent Approximation\n\nThe problem described in the Stochastic Model section is a Markov decision process (MDP) that can theoretically be solved optimally; however, such an approach is impractical due", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3052, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "493ebafb-758c-4f4c-a642-c4c4168463dd": {"__data__": {"id_": "493ebafb-758c-4f4c-a642-c4c4168463dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "707987a8-0c39-47b7-94d1-9663fa24b198", "node_type": "4", "metadata": {}, "hash": "ff1f1c77793e5ac82a915f9aa2fd45b7bf4ac4661201102b943b866837389c5d", "class_name": "RelatedNodeInfo"}}, "text": "# 6.3 Sequential Algorithm\n\nFigure 5 illustrates how the escrow mechanism\u2019s account balances are updated (A) and entered into the pair of subproblems (B and C), which are then solved to obtain the PPZ allocations sent to drivers at each time step (D). First, in (A), PT income and PPZ payouts are distributed to and contributed from various escrow accounts depending on driver and rider events and the locations at which they occur. The escrow account updates occur in real time and the account balances are entered as parameters to the PPZ optimization subroutines. Then, in (B), the algorithm optimizes for the relocation of drivers to match the locations at which future rider requests are expected to originate while ignoring the exact payouts needed to incentivize drivers to reposition. In (C), the incentive computation subroutine produces actual bonus payout values for the PPZ to be given to each driver. Finally, in (D), PPZs are created to incentivize a subset of the idle drivers to reposition and better serve predicted rider requests. To maintain some level of parity between PT income and driver bonuses based on where the ride requests originate, we enforce an equal-split payout among all drivers guided to the same PPZ destination.\n\n# 6.4 Driver Positioning\n\nOur driver positioning subproblem is to maximize the expected revenue-weighted rider request \u201cconversions\u201d in the entire city. The key decision variable is the set of PPZ allocation fractions Aij \u2208 [0, 1] from some location i with open drivers to another location j. These allocation fractions tell us how many drivers we want to incentivize in any one location to reposition to another location (in a single decision period). More explicitly, for some driver", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c8018b0-29ea-431f-b59a-0ca241227a2a": {"__data__": {"id_": "4c8018b0-29ea-431f-b59a-0ca241227a2a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ef385b5-8499-4b8f-bf7c-44100a4762a6", "node_type": "4", "metadata": {}, "hash": "c25c8edbaaaf68044b891f828b4e416096ea5b17ed047129a0fcacd379051e44", "class_name": "RelatedNodeInfo"}}, "text": "# Incentivize drivers reposition\n\nDetermine incentives for each PPZ. Escrow updates accounts as real-time events are processed to better match predicted rider demand within allocation neighborhood: allocation and corresponding Escrow account contributions for each PPZ.\n\nPPZs offered to some drivers and shown as pink/purple zone that is navigated to.\n\nFigure 5: The graphic shows a systemic view of the driver positioning and incentive computation subroutines, which are executed for a city to improve the spatial distribution of drivers and match rider demand.\n\nIn location i, we choose some destination out of all locations available j weighted by Aij (including the \u201cnull location\u201d in which we do not serve any PPZ).\n\nThe PPZ allocation fractions are set based on constraints on the service level, the dynamics of how drivers reposition vis-a-vis PPZ incentive allocations, and budgetary restrictions arising from the available escrow account balances. The service level constraint ensures that demand (subject to pricing) does not exceed supply in the location of the spatial neighborhood to which drivers may be dispatched. That is, in each location\u2019s \u201cdispatch neighborhood,\u201d the market is balanced by simultaneously inducing PT and accounting for the drivers incentivized to reposition.\n\nA notable aspect of the budgetary constraints is that although local account balances are aggregated and tagged to discrete locations, they are accessible to PPZ offers that guide drivers to any destination location within a spatial \u201ccontribution radius\u201d around the local account\u2019s location tag. This design choice for the escrow mechanism is motivated by the fact that upon reaching a specific PPZ destination, a driver can subsequently be dispatched to not only the destination location\u2019s riders but also to riders in nearby locations. The exact details are provided in Appendix A: Optimization Approach. Here, we simply highlight that this property allows us to make flexible allocation decisions; for example, we can serve forecast demand located near but not at the locations of existing demand.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2df8e644-8c2a-46bd-b3be-1234f4651f62": {"__data__": {"id_": "2df8e644-8c2a-46bd-b3be-1234f4651f62", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b75f3b15-933c-4b74-8b49-c7f9042e2149", "node_type": "4", "metadata": {}, "hash": "500892f44e905c113626d96caabfb15317bfe274102c290a6c4de6768bd8f9a8", "class_name": "RelatedNodeInfo"}}, "text": "# 6.5 Incentive Computation\n\nGiven the allocations from the driver positioning subroutine, the incentive computation subproblem attempts to set the monetary payout so that every driver sent to the same location is offered the same expected bonus. The rationale is as follows. The balances at each location represent the sum of the PT markups that each rider will pay for trips of different lengths. Although we cannot know which ride request will be dispatched to a driver at some location ahead of time, one fair approach is to average the expected PT income over all possible trips to which that driver can be dispatched from that location. This results in an even split of the expected PT income among drivers. One decision variable is thus the set of per-PPZ bonus values unique to each destination location. Another decision variable is the set of contribution fractions indicating the portion of the available balance in each escrow account to be set aside for every PPZ offer ending at some destination location. These decisions are constrained by simple lower and upper bounds on the range of permissible bonus values. An important constraint is that the expected expenditure resulting from drivers who respond to the PPZ instructions should not exceed the available budget.\n\n# 7 Numerical Experiments\n\nWe carry out numerical simulation using previous data in back-tests to exercise our algorithm in a safe nonproduction environment. The back-tests also enable us to plan for required computation resources and understand how the model handles specific scenarios, for example, how it handles peak and nonpeak commute periods or cities that clearly divide their downtown and suburban areas and those that do not.\n\n# 7.1 Data\n\nWe work with three months of data from cities (anonymized for confidentiality) that are representative of the diversity of the topographies of the cities in which we operate. We collect data for problem parameters from production logs. As a proxy for the account balances for which we do not have historical data, we assume that the system will always clear the account balances at each decision period. As a result, the balances available at each decision period come from the historical PT transactions from the previous period.\n\n# 7.2 Benchmarking Simulation\n\nWe consider a driver positioning and incentive budgeting strategy compared against a null allocation benchmark. The null allocation benchmark represents the legacy market conditions with the driver PT product without PPZ allocations. Rather than a comprehensive rollout of the PPZ model over a long time horizon, we consider only the results of our single-period model, as shown in Figure 6. Thus, our simulation cannot account for the cumulative impact of PPZs incentivizing the drivers\u2019 repositioning in previous time steps and will be biased toward returning higher-impact estimates.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62d127c4-9513-43cb-9ac2-fa1938efa088": {"__data__": {"id_": "62d127c4-9513-43cb-9ac2-fa1938efa088", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a81c6658-672d-41b7-8961-12fd3ce171d0", "node_type": "4", "metadata": {}, "hash": "008e36d9d29f826e701b4fc47b5107d83107bbab67761f687ea117d1aa53ecfa", "class_name": "RelatedNodeInfo"}}, "text": "# Rider sessions\n\n# Driver shifts\n\n# Conversions\n\n# Account balances\n\n# No-PT bookings\n\n# City topography\n\n# Simulation loop\n\n- Populate rider and drivers in the city\n- Compute problem parameters and solve mathematical programs\n- Simulate riders requesting for rides and drivers repositioning given PT price markups and PPZ allocations (skip for no-PPZ scenario), respectively; based on trained ML behavior models\n- Compute updated driver locations and supply state at next time step\n- Compute market metrics\n\nFigure 6: Initial market conditions are sampled and propagated for one time step to obtain performance metrics such as rider conversions and no-PT bookings.\n\nWe conduct back-test simulations for four cities, using two strategies that optimize for different objectives, namely, the rider request conversions and no-PT bookings. For each simulation, we give a relative version of the incremental objective value or return. We do not directly use the objective value from the optimization. Instead, we take the allocation results and enter them into a rudimentary simulator to obtain the performance metrics. The simulator employs a model fitted from historical data (e.g., idle behavior and dispatch likelihood) to determine the response of the drivers to the allocation guidance and the simulated return.\n\nTable 1 shows that the performance of the algorithm and hence product can vary significantly as the city changes. Empirically, cities that are less homogeneous in population density across the region tend to enjoy greater metric improvements (e.g., cities A and C in our simulations). Some examples are cities with sharp concentrations of demand at residential and office buildings at various times of the day. This often means that there are greater opportunities in incentivizing drivers to move between locations to improve the market balance. This observation is consistent with our goal of rewarding drivers that reposition themselves with PPZs.\n\nTo illustrate the above, Figure 7 plots the relative incremental gain results for City A based on the no-PT bookings maximizing algorithm. The distribution plot (left subfigure) shows a right skew in the incremental gains. This skew is due to our uniformly random sampling of periods causing our distributions to be dominated by lull periods, as opposed to demand peaks with greater repositioning opportunities (see Figure 8). We remark that the significant performance gains on the y-axis when the budget is extremely small can be explained by scenarios in which the driver count is low, such that only one PPZ with, for example, $3 can yield relative gains in rider request conversion of a few percentage points.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "deb131cd-aae1-4857-b406-f279b1f2d3db": {"__data__": {"id_": "deb131cd-aae1-4857-b406-f279b1f2d3db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8cf0a5a-bf22-4d0f-92f3-f3de3bdb9fc1", "node_type": "4", "metadata": {}, "hash": "ceef9ffaba1320ba0ea39b985df66ebdc0c9f673ac17ce3a5303690c6145fe1f", "class_name": "RelatedNodeInfo"}}, "text": "|Objective| |City|Conversion| | |No-PT bookings|\n|---|---|---|---|---|---|---|\n| | | |Mean|Median|Mean|Median|\n| |Conversion|A|2.350%|1.504%|1.151%|0.622%|\n| | |B|0.758%|0.092%|0.222%|0.022%|\n| | |C|4.254%|2.009%|0.364%|0.075%|\n| | |D|0.905%|0.114%|0.426%|0.021%|\n| |No-PT|A|0.913%|0.499%|1.902%|1.179%|\n| | |B|0.205%|0.021%|0.705%|0.097%|\n| | |C|0.426%|0.064%|3.601%|1.450%|\n| | |D|0.346%|0.002%|0.466%|0.003%|\n\n# 8 Live Experiments\n\nWe ran a series of experiments with the twin goals of estimating the marketplace outcomes of our product and determining whether to launch it to all Lyft markets. In particular, we were interested in estimating the effect of our incentive-based supply positioning system on conversion, that is, the probability that if a potential rider opens the Lyft app, that opening becomes a Lyft ride. In the context of our marketplace, naive estimators suffer from network interaction or interference bias, which can result in estimates that differ significantly from actual effects, sometimes by an order of magnitude [Cha16]. We briefly introduce the techniques we used to circumvent this problem before presenting the results of our experiment.\n\n# 8.1 Causal Inference Framework\n\nThe most widely used causal inference technique to circumvent the problem of statistical interference in ridesharing platforms and marketplaces, including Lyft, is time-split or switch-back tests [XLG+18]. Treatment is randomly assigned by time interval (rather than user) so that all the users in the market in a given time interval belong to only one variant and the effect of interference is minimized. Unfortunately, we could not use switchbacks because (1) our product involved a major change to the user interface and switching back and forth between two substantially different experiences for the same user was not feasible, and (2) we wanted to measure the long-term effects of continuously exposing users to our product. For these reasons, we based our inferences on a driver-split experiment in conjunction with a model that we built and validated to correct for most of the interference bias.\n\nWe wanted to estimate how a change in the distribution of supply would affect the aggregate number of rides that can be dispatched on the platform. At the micro-scale, this is a function of the effect of local supply on the probability that a rider opening the Lyft app becomes an actual ride. Our model obtains this effect from experimental data by (1)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74cb8228-db5f-4d76-9fc6-053c154844b6": {"__data__": {"id_": "74cb8228-db5f-4d76-9fc6-053c154844b6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0e94bd9-aefb-47aa-8195-61507e4b77b2", "node_type": "4", "metadata": {}, "hash": "9d34861c4eb5ce80836251ace57c2acdf448742f7d5edba5fca5732cb9672902", "class_name": "RelatedNodeInfo"}}, "text": "# 3.0 Rider request conversion gain (%)\n\n|No-PT bookings gain (%)|Gain (%)|Rider request conversion gain (%)|\n|---|---|---|\n|0.0|10|82|\n|0.5| |9|\n|1.0| | |\n|1.5| | |\n|2.0| | |\n|2.5| | |\n\nFigure 7: In the distribution (left) and joint (right) plots of the incremental gain in no-PT bookings and rider request conversions, the plots are normalized such that the area under the curves totals 1. The joint plot of the incremental gains (right subfigure) show that the gain in no-PT bookings (the objective) is about twice that of the rider conversion gain and that optimizing for either metric still improves the other.\n\nHeuristically estimating the counter-factual supply that would occur in a scenario with 100% treatment drivers or with 100% control drivers, and (2) using a machine learning model to map the counter-factual supply to incremental conversion (or, indeed, any market-level metric such as revenue, bookings, or ETAs). We then aggregate these micro-scale estimates to obtain the macro-level effect across the platform.\n\nSuppose we run a 50-50 driver-split test in which we assign half the drivers the old PT experience and half the drivers the new PPZ experience. For most drivers on our platform, the local supply will include both treatment and control drivers. We compute the counter-factual supply by rescaling the number of drivers in a group as if there were only drivers from that group. For example, if a rider had 5 nearby treatment drivers and 3 nearby control drivers, we compute 100% control supply as 3 \u00d7 2 = 6 and 100% treatment supply as 5 \u00d7 2 = 10. We make an additional adjustment that takes into account the way rides are matched and the riders\u2019 request elasticity with respect to supply.\n\nWe then combine our estimates of counter-factual supply with a conversion model. This is a machine learning model that was trained offline and estimates the probability that a rider requests a ride, given the observed local supply, observed local demand, plus other context features such as time and location. It is a flexible model that we can use to estimate the direct impact of the experiment by comparing the factual scenario to the 100% control counter-factual and the potential impact of rolling out the product by comparing the 100% treatment counter-factual to the 100% control counter-factual. In both cases, the incremental", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5baa6e4-1345-4cc4-81e5-c3c92629f018": {"__data__": {"id_": "c5baa6e4-1345-4cc4-81e5-c3c92629f018", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29defb66-e9ea-4784-be99-3eeba7203251", "node_type": "4", "metadata": {}, "hash": "3f7e90c98f442de69726611fe0dc7598d36ccf0eff35c90b4062243f0d1c7ca5", "class_name": "RelatedNodeInfo"}}, "text": "# 2\n\n# 8\n\n# 1\n\n# L\n\n# 1\n\n# Figure 8:\n\nThe right skew in the distribution of gains can be explained because randomly sampling timestamps uniformly overwhelmingly returns timestamps with no-PT rides and therefore no escrow budget.\n\nConversion estimates are interpreted as causal, because they hold constant all the variables, other than local supply, that affect rider conversion.\n\nAs we note above, the goal of this approach is to remove most of the interference bias and produce estimates that are close to a bias-free, but in our case, infeasible experimental design. Some amount of bias is unavoidable, unfortunately. For example, when testing PPZ and the legacy system in a 50-50 driver-split test, the drivers in the legacy system observe prices that are set for all riders; that is, they depend partly on the positioning of the treatment drivers. To the extent that drivers chase the surge, which expert drivers recommend against as we mention above, this causes some interference bias in our results. Fortunately, our validation experiments showed that this residual bias is small compared to the interference bias for which our framework successfully corrects.\n\nWe validated our framework with a mixed time-split and driver-split design using a driver positioning incentive as treatment, which has similar marketplace effects to our PPZs but\n\n# 20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9ecbf34-069f-415c-b2f6-225934450f8b": {"__data__": {"id_": "a9ecbf34-069f-415c-b2f6-225934450f8b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96498834-119d-4552-a4ce-fcb1e47fb499", "node_type": "4", "metadata": {}, "hash": "369cfbe7a347f35eb4b4581c31e4903d6fccf9ca35e216108efd3d4b157daaa4", "class_name": "RelatedNodeInfo"}}, "text": "# 8.2 Live Experiments: Results\n\n# Bookings\n\nWe implemented and tested the version of the PPZ model that optimizes for no-PT bookings in selected cities. Experimental results show that the performance improvement brought by the PPZ algorithm is consistent in all cities, with gains in the bookings averaging at slightly higher than 0.5.\n\n# Other marketplace improvements\n\nWe observed a reduction in ride pick-up times and, correspondingly, drivers spending more time in their cars with riders. We also observed improvements on a number of driver engagement metrics.\n\n- We saw a small but significant increase in active drivers for the drivers with PPZ versus the ones with the legacy system during the test period (+0.82%).\n- Drivers drove more hours with PPZ in both nonpeak and peak hours (+0.53%), with the biggest lift concentrated during peak hours (+1.43%).\n- ETAs (-1.1%) and driver cancel rates (-12.5%) were down significantly (thus validating PPZ\u2019s effectiveness on addressing the problem of drivers cherry-picking rides).\n\n# Driver earnings and driver sentiment\n\nDriver earnings were constant in aggregate. This is expected because we are only shifting the budget allocation and increased earnings would lead to increased supply, thus lowering driver utilization, and thereby decreasing earnings, again, in equilibrium [HHK21]. However, the distribution of earnings changed in a way that reduces the inequality induced by differences in experience levels (i.e., less experienced drivers are more productive with PPZ). In particular, median hourly driver earnings increased by $0.20 with PPZ, and by up to $0.50 during some weeks. Surveys sent to drivers who have experienced the product also show a preference for the PPZ product over PT across different cities. Some responses to the new system include the following quotes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e22977f4-3c85-4a7f-96ef-304cdfe0cc68": {"__data__": {"id_": "e22977f4-3c85-4a7f-96ef-304cdfe0cc68", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97291e4a-0512-49d5-bdc6-ff4fb59f6ee4", "node_type": "4", "metadata": {}, "hash": "36c5aab85b7647309e9d9f8d5a67892475c6370b94ab98d69db89a496fd7e3ee", "class_name": "RelatedNodeInfo"}}, "text": "submitted by anonymous drivers in these surveys:\n\n\u201cBefore, I used to drive looking for my next pick up. Now I can stay in the area waiting for my next pick up and the good thing is I\u2019m waiting and earning money, thanks!\u201d\n\n\u201cLA is so big, nice to not waste time!\u201d\n\n\u201cIt\u2019s a great incentive and makes me want to go to areas to earn more on my rides.\u201d\n\nIn addition, we also received positive reviews from operations teams working in our major markets.\n\n\u201cPPZs increased utilization and hours. I think this was communicated well, intuitive for drivers. Performing well thus far\u2014all driver metrics look positive, and have not seen significant adverse sentiment in-market.\u201d\n\nGiven these promising results, the proposed algorithm and product have been successfully deployed on Lyft\u2019s ridesharing platform across all 320 cities. See Appendix C: Video of PPZs Served in Production for an illustration of the product being served \u201clive\u201d in the San Francisco Bay Area.\n\n# 9 Broader Impact\n\nThe introduction of mathematical optimization and, more generally, operations research techniques in this application has paved the way for more sophisticated analysis and decision making in incentive budgeting and generation. That Lyft already uses operations research techniques in many of its core applications, including order dispatch and trip pricing, is no surprise. That said, incentive budgeting and generation for drivers has previously been accomplished using heuristics. PPZ represents the first time that dynamic decision making in real time has been automated with mathematically principled algorithms.\n\nBeyond the PPZ product, the escrow mechanism we built is being developed to provide complementary real-time incentives that can access the same hundreds of millions of dollars of yearly budget. In particular, the escrow mechanism allows us to funnel part of the budget to other incentives that can act in tandem with PPZ to achieve even better market results. For example, as we can see from our live-test results, PPZ does not increase the participation rates of drivers in the short term; that is, it is not designed to encourage offline drivers to come online to drive on the platform, nor does it significantly impact them to do so. One direction that we are actively exploring is adapting weekly, manually managed incentives, which already exist for drivers (e.g., ride streaks), to encourage more drivers to come onto the platform when Lyft experiences an unexpected driver supply shortage.\n\nWe are currently working on some of these applications, stretching our system to account for different types of supply and testing new products that work in concert with PPZs. These new applications will help engage online drivers and signal earnings opportunities to offline drivers, with the broader goal of increasing market balance and efficiency on the platform.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c2a9ae0-c729-4393-bb80-d14743f9a417": {"__data__": {"id_": "5c2a9ae0-c729-4393-bb80-d14743f9a417", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eea54be3-50a9-4b54-a365-925aadb4a7e5", "node_type": "4", "metadata": {}, "hash": "a4f0a635e1b1b17fbb10bc4ae7ba18e10794eb10d45221ce23e37288fc98a924", "class_name": "RelatedNodeInfo"}}, "text": "# Acknowledgments\n\nWe thank David Shmoys, Siddarth Patil, and Chris Sholley for their feedback on this paper and prior drafts. PPZ is still a work-in-progress, and the work would not have been possible without the support of many people and various teams at Lyft. The core group of researchers who helped develop the PPZ algorithm, the escrow mechanism, and its dependencies include Ido Bright, Cameron Bruggeman, Carolyn Cotterman, Benedict Kuester, Michael Rotkowitz, Lei Tang, and Michael Yoshizawa. The product has benefitted greatly from the feedback of our colleagues at Lyft and Lyft\u2019s users. In particular, we thank the following people for their contributions to PPZ: Ben Dear, Eduardo Apolinario, Matt Green, Dan Barragan, Praveen Athmanathan, Richard Zhao, Gaurav Gupta, Seth Melnick, Bryan Jung, David Linder, Efferman Ezell, Vijay Narasiman, Eli Schachar, Jia Yan, Ramon Iglesias, Varun Krishnan, Charlene Zhou, Akshay Balwally, San Tan, Udi Milo, Jose Abelenda, Jeremy Karp, Derek Salama, Adriel Frederick, John Fremlin, and Garrett J. van Ryzin.\n\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42267297-7d4b-45d5-a248-3dac71197e42": {"__data__": {"id_": "42267297-7d4b-45d5-a248-3dac71197e42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85fb42c9-7d17-4f14-bec1-af85400a4069", "node_type": "4", "metadata": {}, "hash": "fc1f746d0081fd18b00ef65e941e75e7373f3f3d8d0697af1978143c7236093f", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n- [ALM18] Philipp Af`eche, Zhe Liu, and Costis Maglaras. Ride-hailing networks with strategic drivers: The impact of platform control capabilities on performance. Rotman School of Management Working Paper, 2018.\n- [BCS16] Kostas Bimpikis, Ozan Candogan, and Daniela Saban. Spatial pricing in ride-sharing networks. Operations Research, 2016.\n- [BDLY19] Anton Braverman, JG Dai, Xin Liu, and Lei Ying. Empty-car routing in ridesharing systems. Operations Research, 67(5):1437\u20131452, 2019.\n- [BFL16] Siddhartha Banerjee, Daniel Freund, and Thodoris Lykouris. Pricing and optimization in shared vehicle systems: An approximation framework. arXiv preprint arXiv:1608.06819, 2016.\n- [BJR15] Siddhartha Banerjee, Ramesh Johari, and Carlos Riquelme. Pricing in ride-sharing platforms: A queueing-theoretic approach. In Proceedings of the Sixteenth ACM Conference on Economics and Computation, pages 639\u2013639. ACM, 2015.\n- [CBL18] Francisco Castro, Omar Besbes, and Ilan Lobel. Surge pricing and its spatial supply response. Columbia Business School Research Paper, 2018.\n- [CDL17] Gerard P Cachon, Kaitlin M Daniels, and Ruben Lobel. The role of surge pricing on a service platform with self-scheduling capacity. Manufacturing & Service Operations Management, 19(3):368\u2013384, 2017.\n- [Cha16] Nicholas Chamandy. Experimentation in a ridesharing marketplace: interference across a network, Sep 2016.\n- [CKW17] Juan Camilo Castillo, Dan Knoepfle, and Glen Weyl. Surge pricing solves the wild goose chase. In Proceedings of the 2017 ACM Conference on Economics and Computation, 2017.\n- [GN21] Nikhil Garg and Hamid Nazerzadeh. Driver surge pricing. https://gargnikhil.com/files/papers/garg_driversurge.pdf, 2021.\n- [Gri17] Gridwise. Why you should never chase surges (and what to do instead). https://gridwise.io/why-you-should-never-chase-surges-and-what-to-do-instead, aug 2017.\n- [HHK21] Jonathan V Hall, John J Horton, and Daniel T Knoepfle. Pricing in designed markets: The case of ride-sharing. 2021.\n- [LFK18] Alice Lu, Peter I Frazier, and Oren Kislev. Surge pricing moves uber\u2019s driver-partners. In Proceedings of the 2018 ACM Conference on Economics and Computation, 2018.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c719ce5d-e93f-4d86-9f05-02ad026b6c91": {"__data__": {"id_": "c719ce5d-e93f-4d86-9f05-02ad026b6c91", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db618e43-6a7f-4e87-82a8-37232cd32937", "node_type": "4", "metadata": {}, "hash": "0ba35ac21622deb20aa5f08605be002263efcb1ba1840c18bee7fb0f5da9512a", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n- [MFP19] Hongyao Ma, Fei Fang, and David C Parkes. Spatio-temporal pricing for ridesharing platforms. Proceedings of the 2018 ACM Conference on Economics and Computation, 2019.\n- [OW16] Erhun Ozkan and Amy R Ward. Dynamic matching for real-time ridesharing. 2016.\n- [Smi18] Mark Smithivas. Lyft\u2019s new prime time experiment review. https://therideshareguy.com/what-its-like-to-be-part-of-lyfts-new-prime-time-experiment/, 2018.\n- [TVR05] Kalyan T Talluri and Garrett J Van Ryzin. The theory and practice of revenue management, volume 68. Springer Science & Business Media, 2005.\n- [XLG+18] Zhe Xu, Zhixin Li, Qingwen Guan, Dingshui Zhang, Qiang Li, Junxiao Nan, Chunyang Liu, Wei Bian, and Jieping Ye. Large-scale order dispatch in on-demand ride-hailing platforms: A learning and planning approach. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 905\u2013913, 2018.\n- [YIF18] Pu Yang, Krishnamurthy Iyer, and Peter Frazier. Mean field equilibria for resource competition in spatial settings. Stochastic Systems, 8(4):307\u2013334, 2018.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9650164d-b5a6-439b-97b9-2a1fa6670990": {"__data__": {"id_": "9650164d-b5a6-439b-97b9-2a1fa6670990", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "352aeac5-eaca-435e-9d74-354004736df1", "node_type": "4", "metadata": {}, "hash": "8eecd4e1791050bbc6ca43c2290d2e8d6c755cc1518848f57f09a91f22fe1f8e", "class_name": "RelatedNodeInfo"}}, "text": "# A Optimization Approach\n\nThe PPZ algorithm has two main subroutines: the driver positioning and incentive computation subproblems. We first define the notation and provide details of our model, which is independent of the methods for the allocation of PPZs or the evaluation of the platform efficiency. We then describe the optimization problems that we solve to generate PPZ allocations and the dollar bonus values offered for each PPZ. We refer the reader to the main text for a high-level overview of the system. We highlight that, to focus on the core challenges of the PPZ problem, we leave out an important component of the PPZ system. In particular, we assume that we have access to generic problem parameters that are used across many different teams at Lyft. This includes, among others, forecasts and estimates of supply, demand, and other important market conditions. Below, all defined quantities are given as forecasts or estimates of the market conditions unless otherwise stated.\n\n# A.1 The Model\n\n# A.1.1 Order dispatch.\n\nRidesharing platforms typically compute dispatches across a city or region, and Lyft is no exception. A balanced market has an adequate supply of drivers for the riders requesting ride dispatches (i.e., demand) at the geographic neighborhood level around any location. We call this the dispatch neighborhood. Consider a contiguous spatial region with n nonoverlapping, discrete spatial clusters or locations. We denote the \u201clocal\u201d demand and idle supply as (dreq)1:n and (s)1:n, respectively, for all locations i = 1, . . . , n. The \u201cneighborhood\u201d demand and idle supply around all locations i = 1, . . . , n are denoted as (dN req)1:n and (sN)1:n, respectively. To obtain the neighborhood quantities from the local ones, we introduce the dispatch neighborhood matrix M \u2208 {0, 1}n\u00d7n, where each entry Mij indicates whether locations i and j are within the same dispatch neighborhood. Using this matrix, we select and add up req the demand and supply within the neighborhood of each location (i.e., dN = M dreq and sN = M s). In practice, the neighborhood matrix depends on region-specific dispatch parameters and prevailing traffic conditions. It is also very sparse. We also, by ride prices and incentives, try to maintain a level of available drivers (r)1:n in the dispatch neighborhood of each location i = 1, . . . , n to avoid a wild goose chase scenario in future periods. As [CKW17] explain, a platform depleted of idle drivers is often required to match drivers with riders who are far from where the driver is positioned. These chases occupy drivers and reduce the rate of rides served and earnings, which exacerbate the problem. Through PT and PPZ, we simultaneously modulate demand and incentivize drivers to position themselves to maintain a healthy level of available drivers in the marketplace. We express the market-balancing condition as:\n\nreq = M dreq M s \u2212 r = sN \u2212 r.\n\ndN\n\n(1)\n\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3207532-03e6-4e42-b890-2627f679299f": {"__data__": {"id_": "d3207532-03e6-4e42-b890-2627f679299f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a3a1a13-78a1-4a26-964f-294ff14386fc", "node_type": "4", "metadata": {}, "hash": "755f15c2b1cb118af8652c3091b44d3420a10f5470890018b3da6c9a94333f6b", "class_name": "RelatedNodeInfo"}}, "text": "# A.1.2 Price modifiers and elasticity.\n\nThe PT service determines the price modifiers (x)1:n to be applied to rides at each location i = 1, . . . , n. We model the aggregate rider response to the price modification at each location n \u2192 [0, 1]n using a conversion function, which is given by y : R+. It gives the probability that a rider (unit demand) will request a ride at some location, given some PT value. Denoting the number of riders on the application determining whether to request a ride as d \u2208 R+n, the expected PT-modulated demand is dreq = d \u25e6 y (x), where \u25e6 denotes the Hadamard (elementwise) multiplication of vectors. Based on observed data, conversion exponentially decays with increasing prices.\n\n# A.1.3 Driver allocation.\n\nThe PPZ system determines the driver allocations A \u2208 Rn\u00d7n, where Aij is the fraction of drivers in location i to whom we serve incentives to encourage them to move to location j. Naturally, the driver allocation fractions originating from each location can sum at most to unity. We limit the repositioning incentives of drivers to within a geographic neighborhood of their original location to avoid placing the incentives too far away from the driver or at a location that cannot be reached (e.g., construction zones). We encode undesirable allocations by restricting the appropriate elements to zero in Amax \u2208 {0, 1}n\u00d7n. Our driver allocation constraint is\n\nA1 1, 0 \u2264 A \u2264 Amax. (2)\n\n# A.1.4 Supply dynamics.\n\nWe model the evolution of supply dynamics through the probability of a driver transitioning to a specific location k at the next period Pk,ij, given a PPZ to go from the driver\u2019s current location i to the location j. Drivers without PPZs stay open with probability P0. Suppose (s0)1:n is the pre-PPZ allocation driver count at locations i = 1, . . . , n. Then, the expected supply evolves as\n\ns = [1T (P1 ...\u25e6 A) s0] + P0 (I - diag (A1)) s0. (3)\n\nTo unpack this equation, recognize that Pk \u25e6 A gives the probability that each allocation will result in a transition to location k. Therefore, (Pk \u25e6 A) s0 gives the vector of drivers that, in expectation because of how we sample PPZ allocations for each driver, end up in location k from each origin location 1, . . . , n. Summing the elements of this vector produces the expected number of drivers that end up in location k. Next, recognize that A1 gives the total fraction of allocated drivers from each origin such that I - diag (A1) gives the unallocated fraction of drivers at each location. It now follows that (I - diag (A1)) s0 is the vector of driver counts that were unallocated at each location; multiplying it by P0 gives the n\u00d7n probability that they stay on the platform. Defining a response probability matrix Pc \u2208 R+.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "217b31f2-2d1f-404a-b7fb-53e3ea1b35d9": {"__data__": {"id_": "217b31f2-2d1f-404a-b7fb-53e3ea1b35d9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bb116f6-1482-47cf-8c0b-5ff01e9de263", "node_type": "4", "metadata": {}, "hash": "56d382e2c45de695d2add60ad035339984f420aa95b7ce31a1d0f18ffd596ebe", "class_name": "RelatedNodeInfo"}}, "text": "# Current Page\n\n# A.1.5 Escrow budgeting\n\nThe escrow mechanism uses real-time PT and PPZ financial line items at ride accept to track the available budget to spend (e)1:n at each location i = 1, . . . , n. For any PPZ, the escrow mechanism requires a set of contributing accounts and contribution fractions that funds it. This set of contributions can be encoded by C \u2208 Rn\u00d7n, where Cij is the fraction of budget contribution from location i\u2019s escrow account to any PPZ with the destination location j. Similar to the allocation matrix constraint, we restrict invalid contributions to some location from a local account that is too far away from zero in Cmax \u2208 {0, 1}n\u00d7n. The contribution constraints\n\n0 \u2264 C \u2264 Cmax\n\nC1 1, (4)\n\nencode the fact that the (valid) budget contribution fractions must sum at most to unity. We require that the total amount of money allocated for each PPZ destination location cannot exceed the total available from the local balances Ce, even if we offered the minimum allowable bonus amount bmin to every responding driver (Pc \u25e6 A) s0. The contributions are thus governed by\n\nbmin (Pc \u25e6 A) s0 \u2264 Ce. (5)\n\nFor any valid allocation \u02dc, the final bonus values shown to drivers are governed by\n\n((         \u02dc)s0)b \u2264 Ce, bmin \u2264 b \u2264 bmax\n\ndiag(Pc \u25e6 A) (6)\n\nsuch that the total payout for all complied allocations in each destination location is not more than the budget available in the escrow account balances. Further, all bonus values are subject to lower and upper bounds bmin and bmax, respectively.\n\n# A.2 Driver Positioning\n\nThe driver positioning objective is to maximize the aspects of profit that PPZs can directly impact. Specifically, we maximize the expected no-PT bookings fT(d \u25e6 y (x)), where (f)1:n are the time-and-distance fares we expect to collect at locations i = 1, . . . , n. Our key decision variable is the allocation A and the free variables are the PT values x, the open supply that does not already have a PPZ s and thus can be offered an incentive to reposition, and the escrow contribution C. Note that \u00af is a parameter that gives the count of s drivers in each location that already have a PPZ and thus cannot receive an incentive to be repositioned. The positioning problem is to maximize the expected bookings subject to the market-balancing condition [Equation (1)], supply dynamics [Equation (3)], driver allocation constraints [Constraints (2)], and escrow budgeting constraints [Constraints (4) and\n\n28", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e5f0992-86e9-47bb-8f10-28411a84b36a": {"__data__": {"id_": "4e5f0992-86e9-47bb-8f10-28411a84b36a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32d5fd23-84ed-4d13-9938-cf101daf425a", "node_type": "4", "metadata": {}, "hash": "37f5c0757cf5d92a42a16bf5e4fa2f73dbd44e84de767f5d1187d698fbbe0e5f", "class_name": "RelatedNodeInfo"}}, "text": "# A.3 Incentive Computation\n\nFor each location i, the escrow mechanism provides a bonus target btgt that an open driver should ideally receive by averaging out the available account balances surrounding i. Recall that this is the chosen fair approach of splitting the marked-up income by averaging it out over all possible trips for which drivers can be dispatched from each destination. Given the optimal PPZ allocation A?, the incentive computation subproblem attempts to find escrow account contributions C to match the bonus values b with the ideal bonus target btgt such that \u2016b \u2212 btgt\u20162 is minimized. The bonus targets are set such that the available budget is fully spent. The solution is subject to the bonus value constraints [Constraints (6)] and the contribution simplex [Constraints (4)]. Note that simply setting the final bonus payout values to the ideal bonus target will not necessarily yield a feasible solution under the budget constraints. In particular, the minimum and maximum bonus values sometimes restrict us from using the ideal budget target as solutions. The problem is\n\nminimize \u2016b \u2212 btgt\u201622\n\nsubject to\n\n- diag ((Pc \u25e6 A?) s) b \u2264 CT e\n- bmin \u2264 b \u2264 bmax\n- C1 = 1, 0 \u2264 C \u2264 Cmax\n\nwhich yields the optimal bonus payouts b? and escrow contribution C?. In practice, we apply an `1 regularizer on the contributions C such that the set of accounts contributing to any PPZ is not excessively large.\n\n# A.4 Practical Implementation Details\n\nWe now describe some necessary steps that allow the optimization to run efficiently at Lyft\u2019s scale.\n\n29", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "633523f4-9853-41c8-9fc9-318476d688c6": {"__data__": {"id_": "633523f4-9853-41c8-9fc9-318476d688c6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "369fd348-471b-4887-ad19-f02ddfeb684e", "node_type": "4", "metadata": {}, "hash": "5058351fbaf00008888cedd8cafb88828cff1095981ca6d7f681f28dad8a46dc", "class_name": "RelatedNodeInfo"}}, "text": "# A.4.1 Convexification\n\nAs presented, the PPZ optimization problem is nonconvex, which greatly complicates its solution and, for all practical purposes, makes its computation potentially unfeasibly long. The nonconvexity is the result of the conversion function that appears in the adjusted market-balancing condition [Equation (1)] and the objective function.\n\nFortunately, a change-of-variable convexifies the problem. Since f is strictly monotone, we can let y = y (x) (in a slight abuse of notation) and recover x by inverting the conversion function. Observe that the new variable has domain y \u2208 [0, 1] and that each element yi is strictly decreasing in xi for i = 1, . . . , n. With this change-of-variable, Equation (1) and the objective function become affine. Our trick here resembles the standard revenue management technique to optimize over quantiles rather than prices [TVR05].\n\n# A.4.2 Allocation vectorization\n\nEven in a city of modest size, there are easily more than 10,000 locations to consider, which yields more than 100,000,000 allocation variables. The authors are not aware of solvers that can solve such large problem instances in a matter of seconds, which is a requirement for our real-time application. Luckily, because allocation neighborhoods are generally far smaller than the size of the entire region and there are operationally undesirable allocation pairs, Amax and therefore any valid allocation is extremely sparse. We vectorize the allocation matrix by constructing an allocation vector where each element corresponds to a valid allocation pair as indicated by a nonzero entry in Amax.\n\n# A.4.3 Location pruning\n\nWe consider only locations that are affected by demand and supply changes to ensure solution efficiency. Specifically, we only consider the union of two types of locations. The first type comprises locations with nonzero demand, and the locations within their dispatch neighborhood. The second type comprises locations with nonzero PPZ-assignable supply that have an allocation neighborhood that has any overlap with the dispatch neighborhood of locations with nonzero demand, and the locations within their allocation neighborhood. These form our active set of locations that we consider in the mathematical program. Combining location pruning with allocation vectorization, we are able to drastically reduce the number of decision variables, while provably preserving the optimal solution. This reduces the number of variables to under 100,000, even for our largest markets. Using the FICO Xpress quadratic program commercial solver on an AWS C5n instance featuring four 3.0 GHz Intel Xeon Platinum processors and 21 GiB of memory, we are able to consistently solve the problems in seconds and deliver fresh PPZ incentives every minute.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29d50115-8781-438f-ae4a-9f6513f02337": {"__data__": {"id_": "29d50115-8781-438f-ae4a-9f6513f02337", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0803b89a-0adf-4dab-9b17-68b99acfcf02", "node_type": "4", "metadata": {}, "hash": "8c539d4b071a43857cdba2034e6857e393f016a785071d1f70269fe71917de9a", "class_name": "RelatedNodeInfo"}}, "text": "# Perturbation and Sensitivity Analysis\n\nIn this appendix we provide analytical sensitivity results to demonstrate that the local sensitivity approach fails to accurately represent market conditions.\n\nConsider Problem (7), the driver positioning problem, except that drivers are fixed in their original locations and the only variable is the conversion quantile y. Our problem is to maximize the expected sum of trip fares by setting appropriate multiplicative price modifiers n, which affect the conversion of rider app-opens into requests y. Solving this problem x \u2208 R+ thus gives us the maximum revenue or bookings we can achieve from the market given the supply distribution that we currently have. This simplified market-optimizing problem is\n\nminimize       fobj (x) = \u2212f T (d \u25e6 y)                                       (9)\n\nsubject to M (d \u25e6 y)  M s0 \u2212 r.\n\nSuppose Problem (9) is feasible. Because we have a linear program, strong duality holds. Here, the optimal dual variables provide insights on the sensitivity of the optimal value with respect to the perturbations of the constraints. In particular, they tell us how the optimal market revenue varies with respect to changes in the market-balancing conditions and, in turn, how demand, pricing, and supply may influence the revenue.\n\n# The Perturbed Problem\n\nWe consider the following perturbed version of the original Problem (9):\n\nminimize       fobj (y) = \u2212f T (d \u25e6 y)                                        (10)\n\nsubject to M (d \u25e6 y) \u2212 (M s0 \u2212 r)  q\n\nwith variables y. The problem coincides with the original optimization problem when q = 0. When q > 0, we have relaxed the constraint; when q < 0, we have tightened it. Thus, the perturbed problem results from the original problem by tightening or relaxing the right side of the adjusted market-balancing inequality by q.\n\nFor the sake of clear exposition in this section, we will rewrite the optimization Problem (10) as\n\nminimize       fobj (y)                                              (11)\n\nsubject to f mkt (y)  q,\n\nwhere f mkt is the left side of the perturbed problem\u2019s adjusted market-balancing condition.\n\nWe define p? (q) as the optimal value of the perturbed Problem (11)\n\np? (q) = inf {f obj (y) \u2223 y \u2208 D, f mkt (y)  q} ,\u2223                                 (12)\n\nwhere D is the domain of the optimization problem (i.e., the set of the points that are defined on the objective and constraint functions). We can have p? (q) = \u221e correspond to the perturbations of the constraints that results in infeasibility. Note that p? (0) = p optimal value of the unperturbed problem. Roughly speaking, the function p? : Rn?, the\u2192 R", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22e377cd-03d0-4146-9341-916287373d16": {"__data__": {"id_": "22e377cd-03d0-4146-9341-916287373d16", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2dd0f2c-3d39-414c-a7e0-a4cf7b3d0776", "node_type": "4", "metadata": {}, "hash": "29dfbb4b10b1098486716af3301433abaa4e4497a9cd433703505680adc1dc0e", "class_name": "RelatedNodeInfo"}}, "text": "# B.2 Local Sensitivity Analysis\n\nLet (\u03bb?, \u03bd?) be optimal for the dual of the unperturbed optimization Problem (9), where \u03bb is the dual variable that corresponds to the market-balancing constraint and \u03bd is the dual variable that corresponds to the price modifier constraint. Suppose now that p? (q) is differentiable at q = 0. Then, provided that strong duality holds, the optimal dual variable \u03bb? equals the gradient of p? at q = 0 with respect to q, that is,\n\n\u03bbi ?= \u2212\u2202p\u2202qi? (0). (13)\n\nThis means that if the above conditions hold, the optimal Lagrange multipliers are exactly the local sensitivities of the optimal value with respect to constraint perturbations. In economics, \u03bb and \u03bd are often referred to as the natural or equilibrium prices or \u201cshadow prices\u201d of their corresponding constraints. In the case of the market-balancing conditions, these can be thought of as the price that we should accord to (marginal) supply in the dispatch neighborhoods. That is, \u03bbi ? is the marginal value of supply in PPZ\u2019s context.\n\nLocally, this interpretation is symmetric: decreasing the supply count in the ith location\u2019s dispatch neighborhood by a small amount (i.e., taking qi small and negative) yields a change in the optimal market revenue objective value \u2212p? of approximately \u2212\u03bbi ?q; increasing the supply count in the ith location\u2019s dispatch neighborhood by a small amount (i.e., taking qi small and positive) yields an increase of approximately \u03bbi ?q. Thus, Equation (13) gives us precisely the marginal value of supply to use in a local sensitivity approach.\n\nThis local sensitivity result gives us a quantitative measure of how active a market-balancing constraint is at the optimum y?. If f imkt < 0, then the constraint is inactive, and it follows that the supply count can be decreased or increased a little without affecting the optimal market revenue objective value. By complementary slackness, the associated Lagrange multiplier \u03bbi? must be zero. However, now suppose that f imkt = 0, that is, the market-balancing constraint for the ith location\u2019s dispatch neighborhood is active at the optimum. The ith optimal Lagrange multiplier tells us how active the constraint is: if \u03bbi? is small, it means that the supply count can be decreased or increased a little without much effect on the optimal market revenue objective value; if \u03bbi? is large, it means that if the supply count is decreased or increased a little a bit, the effect on the optimal market revenue objective value will be great.\n\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25aebe7f-057b-4038-828a-5c4e1853ad4c": {"__data__": {"id_": "25aebe7f-057b-4038-828a-5c4e1853ad4c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6224201b-4efd-42a0-9cef-0946e4fe84e2", "node_type": "4", "metadata": {}, "hash": "c4009c09aa3b9f6a9d0db40cd0da90a91f3a69b0401cca67af8915d4dfb85dc9", "class_name": "RelatedNodeInfo"}}, "text": "# B.3 Global Sensitivity Analysis\n\nThis analysis motivates a natural approach to allocate PPZs based on \u03bb?: PPZs should incentivize drivers to reposition from locations i with small \u03bbi? to locations j with large \u03bbj ?. Based on the above, one would expect this to increase the market revenue. Unfortunately, as we will demonstrate, the local sensitivity analysis does not generalize beyond the point at which the marginal value of supply was evaluated.\n\nTo examine how the local sensitivity results break down, consider Equation (12). For all q, we will show that\n\np? (q) \u2265 p? (0) \u2212 \u03bb?T q. (14)\n\nTo establish this inequality, suppose y is feasible for the perturbed problem. Then we have, by strong duality,\n\np? (0) = g (\u03bb?, \u03bd?) \u2264 f obj (y) + \u03bb?T f mkt (y) + \u03bd?T f bal (y) \u2264 f obj (y) + \u03bb?T q + \u03bd?T 0 = f obj (y) + \u03bb?T q.\n\nHere, g is the Lagrange dual function and the first inequality follows from its definition. Recall also that \u03bb? 0 by definition of the Lagrange dual problem. We thus have\n\nf obj (y) \u2265 p? (0) \u2212 \u03bb?T q,\n\nwhich leads to Inequality (14). Recall that p? (q) is the negative of the perturbed optimal market revenue objective value.\n\nFor clarity, we can rewrite the inequality constraint as\n\n\u2212p? (q) \u2264 \u03bb?T q \u2212 p? (0)\n\nand make the two following observations:\n\n1. Suppose \u03bbi? is large and we decrease the supply count in the ith location\u2019s dispatch neighborhood; that is, we tighten the ith constraint and choose qi < 0. Then the optimal market revenue objective value of the objective \u2212p? (q) will decrease greatly.\n2. Suppose \u03bbi? is small and we increase the supply count in the ith location\u2019s dispatch neighborhood; that is, we loosen the ith constraint and choose qi > 0. Then the optimal market revenue objective value of the objective \u2212p? (q) will not increase too much.\n\nSo far, these observations align with the results obtained from the local sensitivity analysis. The inequality established and the two observations above provide an upper bound on \u2212p? (q), the optimal market revenue objective value of the perturbed version of the original.\n\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b464a67-3915-4958-ac68-2b7171839759": {"__data__": {"id_": "9b464a67-3915-4958-ac68-2b7171839759", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44b5c628-e4cf-4b05-a77a-3b6353959098", "node_type": "4", "metadata": {}, "hash": "4c5cb864d6f613f120deec2700bd6ccc6c7341aefd42caf31d6e4c5161b8a880", "class_name": "RelatedNodeInfo"}}, "text": "# Market-Optimizing Problem\n\nNotice, however, that it establishes no lower bound of the perturbed optimal market revenue objective. We thus see that the results are not symmetric with respect to increasing or decreasing the supply count in the market-balancing constraint.\n\nTo illustrate this more clearly, consider the case where \u03bbi is large and we slightly increase the supply count in the ith location\u2019s dispatch neighborhood; that is, loosen the ith constraint slightly and choose qi > 0 for some small q. In this case, our inequality is not helpful in establishing any conclusion about how the perturbed optimal market revenue objective changes; it certainly does not imply that the perturbed optimal market revenue objective \u2212p? (q) increases considerably. In other words, a large optimal dual variable value for some location\u2019s dispatch neighborhood does not imply that adding more drivers to it will improve the market conditions.\n\nRecall the rule of incentivizing drivers to move from locations i with small \u03bbi to locations j with large \u03bbj established from our local sensitivity results. Unlike what our local sensitivity results suggest, our global sensitivity result tells us that a large \u201cshadow price\u201d or \u201cmarginal value of supply\u201d for some location does not imply that incentivizing drivers to reposition to that location will yield market revenue gains. Given that incentivizing drivers to reposition away from a location i may, even with small \u03bbi, yield large market revenue drops, simply using a static value of supply defined by an optimal Lagrange multiplier can hurt us because of this asymmetry. Recalling Figure 3, this asymmetry is apparent in both the analytical and the empirical estimates. Thus, a static set of shadow prices is a poor way to coordinate how supply is managed. If at all useful, it might be for visualizing snapshots of and providing spatial intuition for how severe the supply shortage is across a city. To actually coordinate supply management tools, we needed a fundamentally different approach, as we have discussed in this paper.\n\n# C. Video of PPZs Served in Production\n\nAn videographic illustration of actual PPZ incentives being served in production for the San Francisco Bay Area can be found at https://youtu.be/mlxY-qxlb6w. The data for this video were obtained from the period of April 20\u201321, 2019. The video plays back the PPZs created for and earned by actual drivers and provides some intuition about when and where repositioning opportunities are. Figure 9 is a snapshot of the video, with the colored arcs representing the origin and destination of the PPZ directions. The PPZ origin is represented by the end of the arc in a lighter hue and the destination the one in a darker hue. The purple arcs indicate PPZs that were eventually satisfied and earned by drivers, whereas the teal arcs indicate PPZs that were not satisfied. The highlighted boxes indicate the location buckets used in our algorithm, with red indicating areas of higher actual PT and yellow areas of lower PT. When there is no PT, there is no highlight. The rate of PPZs generated is indicated in a time-series plot at the bottom of the video, providing a sense of the seasonality of when the best opportunities to allocate PPZs are (typically, morning and evening commutes to and from the residential and downtown areas of the city).\n\n34", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16b66d2f-9b16-4dde-a651-5187d3229110": {"__data__": {"id_": "16b66d2f-9b16-4dde-a651-5187d3229110", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1619346-c4a4-4190-bacb-f7a0411ad965", "node_type": "4", "metadata": {}, "hash": "febaf131872b92b5408d0a0f7d63821b65e48e564e3f1e0b37302973cc5a432f", "class_name": "RelatedNodeInfo"}}, "text": "# San Francisco\n\n# Poliao\n\n# Jnnct suns\n\n# ct Hill\n\n# Sunsct\n\n# Distrct\n\n# Itatl\n\nFigure 9: This snapshot of the video illustrates PPZs actually served in the San Francisco Bay Area. Note that the arc colors are used to indicate whether the drivers complied with and earned the incentives.\n\n35", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98e94121-f62c-42a6-a810-61d59ea5db9e": {"__data__": {"id_": "98e94121-f62c-42a6-a810-61d59ea5db9e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee3dba33-cbd5-4d14-a0bb-f1925fe3cc21", "node_type": "4", "metadata": {}, "hash": "fbe44af67d87f75f6b106a9776507fff79e758333c74c99b56a6b0c5bb79bcd6", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# JIAN LI AND XIANGDONG YE\n\n# ABSTRACT\n\nWe give a summary on the recent development of chaos theory in topological dynamics, focusing on Li-Yorke chaos, Devaney chaos, distributional chaos, positive topological entropy, weakly mixing sets and so on, and their relationships.\n\n# 1. INTRODUCTION\n\nTopological dynamics is a branch of the theory of dynamical systems in which qualitative, asymptotic properties of dynamical systems are studied, where a dynamical system is a phase (or state) space X endowed with an evolution map T from X to itself. In this survey, we require that the phase space X is a compact metric space and the evolution map T : X &rarr; X is continuous.\n\nThe mathematical term chaos was first introduced by Li and Yorke in 1975 [69], where the authors established a simple criterion on the existence of chaos for interval maps, known as \u201cperiod three implies chaos\u201d. Since then, the study of chaos theory has played a big role in dynamical systems, even in nonlinear science.\n\nIn common usage, \u201cchaos\u201d means \u201ca state of disorder\u201d. However, in chaos theory, the term is defined more precisely. Various alternative, but closely related definitions of chaos have been proposed after Li-Yorke chaos.\n\nAlthough there is still no definitive, universally accepted mathematical definition of chaos (in our opinion it is also impossible), most definitions of chaos are based on one of the following aspects:\n\n1. complex trajectory behavior of points, such as Li-Yorke chaos and distributional chaos;\n2. sensitivity dependence on initial conditions, such as Devaney chaos, Auslander-Yorke chaos and Li-Yorke sensitivity;\n3. fast growth of different orbits of length n, such as having positive topological entropy, or its variants;\n4. strong recurrence property, such as weakly mixing property and weakly mixing sets.\n\nSince there are so many papers dealing with the chaos theory in topological dynamics, we are not able to give a survey on all of them. We only can select those which we are familiar with, and are closely related to our interest, knowledge and ability. As we said before it is impossible to give a universe definition of chaos which covers all the features of the complex behaviors of a dynamical system, it is thus important to know the relationships among the various definitions of chaos. So we will focus on Li-Yorke chaos, Devaney chaos, distributional chaos, positive topological entropy, weakly mixing sets, sensitivity and so on, and their relationships in this survey. See [14, 85] for related surveys.\n\nDate: March 24, 2015.\n\n2010 Mathematics Subject Classification. 54H20, 37B05, 37B40.\n\nKey words and phrases. Li-Yorke chaos, Devaney chaos, sensitive dependence on initial conditions, distributional chaos, weak mixing, topological entropy, Furstenberg family.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7358e19d-b7ba-42a6-9cea-158701321ce5": {"__data__": {"id_": "7358e19d-b7ba-42a6-9cea-158701321ce5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4c59342-ca6f-4b8d-87da-db70670598cf", "node_type": "4", "metadata": {}, "hash": "d37229e418afe3582ee15b1282bb60c9bee27913757842cf25bfaec60d546519", "class_name": "RelatedNodeInfo"}}, "text": "# J. LI AND X. YE\n\nNow we outline the development of chaos theory for general topological dynamical systems brie\ufb02y. The notions of topological entropy and weak mixing were introduced by Adler et al [1] in 1965 and Furstenberg [25] in 1967 respectively. After the mathematical term of chaos by Li-Yorke appearing in 1975, Devaney [19] de\ufb01ned a kind of chaos, known as Devaney chaos today, in 1989 based on the notion of sensitivity introduced by Guckenheimer [35]. The implication among them has attracted a lot of attention. In 1991 Iwanik [50] showed that weak mixing implies Li-Yorke chaos. A breakthrough concerning the relationships among positive entropy, Li-Yorke chaos and Devaney chaos came in 2002. In that year, it was shown by Huang and Ye [46] that Devaney chaos implies Li-Yorke one by proving that a non-periodic transitive system with a periodic point is Li-Yorke chaotic; Blanchard, Glasner, Kolyada and Maass [15] proved that positive entropy also implies Li-Yorke chaos (we remark that the authors obtained this result using ergodic method, and for a combinatorial proof see [53]). Moreover, the result also holds for so\ufb01c group actions by Kerr and Li [54, Corollary 8.4].\n\nIn 1991, Xiong and Yang [109] showed that in a weakly mixing system there are considerably many points in the domain whose orbits display highly erratic time dependence. It is known that a dynamical system with positive entropy may not contain any weakly mixing subsystem. Capturing a common feature of positive entropy and weak mixing, Blanchard and Huang [17] in 2008 de\ufb01ned the notion of weakly mixing set and showed that positive entropy implies the existence of weakly mixing sets which also implies Li-Yorke chaos. A further discussion along the line will be appeared in a forthcoming paper by Huang, Li, Ye and Zhou [41].\n\nDistributional chaos was introduced in 1994 by Schweizer and Sm\u00b4dotlessital [92], and there are at least three versions of distributional chaos in the literature (DC1, DC2 and DC3). It is known that positive entropy does not imply DC1 chaos [86] and Sm\u00b4dotlessital conjectured that positive entropy implies DC2 chaos. Observing that DC2 chaos is equivalent to so called mean Li-Yorke chaos, recently Downarowicz [21] proved that positive entropy indeed implies DC2 chaos. An alternative proof can be found in [40] by Huang, Li and Ye. We remark that both proofs use ergodic theory heavily and there is no combinatorial proof at this moment.\n\nThis survey will be organized as follows. In Section 2 we provide basic de\ufb01nitions in topological dynamics. Li-Yorke chaos, sensitivity and chaos in transitive systems will be discussed in Sections 3-5. In Section 6 we review the results on distributional chaos. In the following two sections we focus on weakly mixing sets and chaos in the induced spaces.\n\n# 2. PRELIMINARIES\n\nIn this section, we provide some basic notations, de\ufb01nitions and results which will be used later in this survey.\n\nDenote by N (Z+, Z, respectively) the set of all positive integers (non-negative integers, integers, respectively). The cardinality of a set A is usually denoted by |A|.\n\nLet X be a compact metric space. A subset A of X is called a perfect set if it is a closed set with no isolated points; a Cantor set if it is homeomorphic to the standard middle third Cantor set; a Mycielski set if it is a union of countably many Cantor sets. For convenience we restate here a version of Mycielski\u2019s theorem ([79, Theorem 1]) which we shall use.\n\n# Theorem 2.1 (Mycielski Theorem)\n\nLet X be a perfect compact metric space. If R is a dense G\u03b4 subset of Xn, then there exists a dense Mycielski set K proper subset X such that for any distinct n points x1, x2, ..., xn element K, the tuple (x1, x2, ..., xn) is in R.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70e3ccf5-6ee2-4db4-b4d6-0ec245e3c9aa": {"__data__": {"id_": "70e3ccf5-6ee2-4db4-b4d6-0ec245e3c9aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b056c15-0557-4d5c-af51-5d23a349df35", "node_type": "4", "metadata": {}, "hash": "e84ef09a542dc0165fe37ab7c6fe773b1552158eb77fb4f6705f7052ae481b18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64b92cd4-420b-4128-b5e9-296c29676736", "node_type": "1", "metadata": {}, "hash": "3aa260254a0a0a05123cc571a14fd54efeb1f0b6ac0d588dc7de7830c1406be0", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# 2.1. Topological dynamics\n\nBy a (topological) dynamical system, we mean a pair (X, T), where X is a compact metric space and T : X &rarr; X is a continuous map. The metric on X is usually denoted by d. One can think of X as a state space for some system, and T as the evolution law of some discrete autonomous dynamics on X: if x is a point in X, denoting the current state of a system, then T x can be interpreted as the state of the same system after one unit of time has elapsed. For every non-negative integer n, we can define the iterates Tn : X &rarr; X as T0 = id the identity map on X and Tn+1 = Tn &bull; T. One of the main topics of study in dynamical systems is the asymptotic behaviour of Tn as n &rarr; \u221e.\n\nNote that we always assume that the state space X is not empty. If the state space X contains only one point, then we say that the dynamical system on X is trivial, because in this case the unique map on X is the identity map.\n\nFor any n &ge; 2, the n-th fold product of (X, T) is denoted by (Xn, T(n)), where Xn = X \u00d7 X \u00d7 &hellip; \u00d7 X (n-times) and T(n) = T \u00d7 T \u00d7 &hellip; \u00d7 T (n-times). We set the diagonal of Xn as \u0394n = { (x, x, &hellip;, x) \u2208 Xn : x \u2208 X }, and set \u0394(n) = { (x1, x2, &hellip;, xn) \u2208 Xn : there exist 1 &le; i < j &le; n such that xi = xj }.\n\nThe orbit of a point x in X is the set Orb(x, T) = { x, T x, T2 x, &hellip; }; the \u03c9-limit set of x, denoted by \u03c9(x, T), is the limit set of the orbit of x, that is\n\n\u03c9(x, T) = \u2229 { Ti x : i &ge; n }n=1\u221e.\n\nA point x \u2208 X is called recurrent if there exists an increasing sequence { pi } in \u2115 such that limi &rarr; \u221e Tpi x = x. Clearly, x is recurrent if and only if x \u2208 \u03c9(x, T).\n\nIf Y is a non-empty closed invariant (i.e. T Y \u2286 Y) subset of X, then (Y, T) is also a dynamical system, we call it as a subsystem of (X, T). A dynamical system (X, T) is called minimal if it contains no proper subsystems. A subset A of X is minimal if (A, T) forms a minimal subsystem of (X, T). It is easy to see that a non-empty closed invariant set A \u2282 X is minimal if and only if the orbit of every point of A is dense in A. A point x \u2208 X is called minimal or almost periodic if it belongs to a minimal set.\n\nA dynamical system (X, T) is called transitive if for every two non-empty open subsets U and V of X there is a positive integer n such that Tn U \u2229 V \u2260 \u2205; totally transitive if (X, Tn) is transitive for all n \u2208 \u2115; weakly mixing if the product system (X \u00d7 X, T \u00d7 T) is transitive; strongly mixing if for every two non-empty open subsets U and V of X there is N > 0 such that Tn U \u2229 V \u2260 \u2205 for all n &ge; N. Any point with dense orbit is called a transitive point. In a transitive system the set of all transitive points is a dense G\u03b4 subset of X and we denote it by Trans(X, T). For more details related to transitivity see [57].\n\nLet (X, T) be a dynamical system. A pair (x, y) of points in X is called asymptotic if limn &rarr; \u221e d(Tn x, Tn y) = 0; proximal if lim inf d(Tn x, Tn y) = 0; distal if lim inf d(Tn x, Tn y) > 0. The system (X, T) is called proximal if any two points in X form a proximal pair; distal if any two distinct points in X form a distal pair.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64b92cd4-420b-4128-b5e9-296c29676736": {"__data__": {"id_": "64b92cd4-420b-4128-b5e9-296c29676736", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b056c15-0557-4d5c-af51-5d23a349df35", "node_type": "4", "metadata": {}, "hash": "e84ef09a542dc0165fe37ab7c6fe773b1552158eb77fb4f6705f7052ae481b18", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70e3ccf5-6ee2-4db4-b4d6-0ec245e3c9aa", "node_type": "1", "metadata": {}, "hash": "242a2bab053cba454ac8d069063015d2f816de47e8f163cc4c1b5d6d4622ebc0", "class_name": "RelatedNodeInfo"}}, "text": "In a transitive system the set of all transitive points is a dense G\u03b4 subset of X and we denote it by Trans(X, T). For more details related to transitivity see [57].\n\nLet (X, T) be a dynamical system. A pair (x, y) of points in X is called asymptotic if limn &rarr; \u221e d(Tn x, Tn y) = 0; proximal if lim inf d(Tn x, Tn y) = 0; distal if lim inf d(Tn x, Tn y) > 0. The system (X, T) is called proximal if any two points in X form a proximal pair; distal if any two distinct points in X form a distal pair.\n\nLet (X, T) and (Y, S) be two dynamical systems. If there is a continuous surjection \u03c0 : X &rarr; Y which intertwines the actions (i.e., \u03c0 &bull; T = S &bull; \u03c0), then we say that \u03c0 is a factor map, (Y, S) is a factor of (X, T) or (X, T) is an extension of (Y, S). The factor map \u03c0 is almost one-to-one if there exists a residual subset G of X such that \u03c0\u22121(\u03c0(x)) = { x } for any x \u2208 G.\n\nIn 1965, Adler, Konheim and McAndrew introduced topological entropy in topological dynamics [1]. Let CX be the set of finite open covers of X. Given two open covers U and V, their join is the cover\n\n{ U \u2229 V : U \u2208 U, V \u2208 V }.", "mimetype": "text/plain", "start_char_idx": 2698, "end_char_idx": 3814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a083281a-7373-4f63-9f88-5c562a1b7bde": {"__data__": {"id_": "a083281a-7373-4f63-9f88-5c562a1b7bde", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bcfc07a-a4e6-4e06-b39d-548fdde1ebeb", "node_type": "4", "metadata": {}, "hash": "1edb2752005f9ed77109ffa291db1940e596d6ae3634c5e11c8af92df3e22eb4", "class_name": "RelatedNodeInfo"}}, "text": "# 4\n\n# J. LI AND X. YE\n\nWe define N(U) as the minimum cardinality of subcovers of U. The topological entropy of T with respect to U is\n\nhtop(T, U) = limN\u2192\u221e N log N\ni=0\n\nThe topological entropy of (X, T) is defined by\n\nhtop(T) = supU\u2208C htop(T, U).\n\nWe refer the reader to [103, 28] for more information on topological entropy, and [31] for a survey of local entropy theory.\n\n# 2.2. Furstenberg family\n\nThe idea of using families of subsets of Z+ when dealing with dynamical properties of maps was first used by Gottschalk and Hedlund [34], and then further developed by Furstenberg [26]. For a systematic study and recent results, see [2, 43, 47, 59, 93, 108]. Here we recall some basic facts related to Furstenberg families.\n\nA Furstenberg family (or just family) F is a collection of subsets of Z+ which is upwards hereditary, that is, if F1 \u2208 F and F1 \u2282 F2 then F2 \u2208 F. A family F is proper if N \u2208 F and N \u2209 F. If F1 and F2 are families, then we define\n\nF1 \u2229 F2 = {F1 \u2229 F2 : F1 \u2208 F1, F2 \u2208 F2}\n\nIf F is a family, the dual family of F, denoted by \u03baF, is the family\n\n{F \u2282 N : F \u2229 F' \u2260 \u2205, F' \u2208 F}\n\nWe denote by Finf the family of infinite subsets of N. The dual family of Finf is just the collection of co-finite subsets of Z+, denoted by Fcf. A family F is called full if it is proper and F \u2229 \u03baF \u2282 Finf.\n\nFor a sequence {pi}i=1\u221e in N, define the finite sums of {pi}i=1\u221e as\n\nFS{pi}i=1\u221e = \u2211i \u2208 \u03b1 pi : \u03b1 is a non-empty finite subset of N.\n\nA subset F of Z+ is called an IP-set if there exists a sequence {pi}i=1\u221e in N such that FS{pi}i=1\u221e \u2282 F. We denote by Fip the family of all IP-sets.\n\nA subset F of Z+ is called thick if it contains arbitrarily long runs of positive integers, i.e., for every n \u2208 N there exists some a \u2208 Z+ such that {a, a + 1, ..., a + n} \u2282 F; syndetic if it has bounded gaps, i.e., there is N \u2208 N such that [n, n + N] \u2229 F \u2260 \u2205 for every n \u2208 Z+. The families of all thick sets and syndetic sets are denoted by Ft and Fs, respectively. It is easy to see that \u03baFs = Ft.\n\nLet (X, T) be a dynamical system. For x \u2208 X and a non-empty subset U of X, we define the entering time set of x into U as\n\nN(x, U) = {n \u2208 Z+ : Tnx \u2208 U}\n\nIf U is a neighborhood of x, then we usually call N(x, U) the return time set of x into U. Clearly, a point x is recurrent if and only if for every open neighborhood U of x, the return time set N(x, U) is infinite. In general, we can define recurrence with respect to a Furstenberg family.\n\n# Definition 2.2.\n\nLet (X, T) be a dynamical system and F be a Furstenberg family. A point x \u2208 X is said to be F-recurrent if for every open neighborhood U of x, N(x, U) \u2208 F.\n\nIt is well known that the following lemma holds (see, e.g., [2, 26]).\n\n# Lemma 2.3.\n\nLet (X, T) be a dynamical system and x \u2208 X. Then", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "128d3dcd-ab94-4e35-9bb8-92e05593d61b": {"__data__": {"id_": "128d3dcd-ab94-4e35-9bb8-92e05593d61b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1611ebf1-c46c-464a-9bed-27811fcdfd80", "node_type": "4", "metadata": {}, "hash": "d5ba69d898c3dd37a17c52d212586ec7cd3b4332e16fb9943fdf279e210037e6", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# 5\n\n(1) x is a minimal point if and only if it is an F-recurrent point;\n\n(2) x is a recurrent point if and only if it is an Fip-recurrent point.\n\nFor two non-empty subsets U and V of X, we define the hitting time set of U and V as\n\nN(U,V) = {n \u2208 \u2124\u207a : T\u207fU \u2229 V \u2260 \u2205}.\n\nIf U is a non-empty open subset of X, then we usually call N(U,U) the return time set of U. Clearly, a dynamical system (X, T) is transitive (resp. strongly mixing) if and only if for any two non-empty open subsets U, V of X, the hitting time set N(U,V) is infinite (resp. co-finite).\n\n# Definition 2.4.\n\nLet (X, T) be a dynamical system and F be a family. We say that (X, T) is\n\n(1) F-transitive if for any two non-empty open subsets U, V of X, N(U,V) \u2208 F;\n\n(2) F-mixing if the product system (X \u00d7 X, T \u00d7 T) is F-transitive.\n\n# Theorem 2.5 ([25]).\n\nLet (X, T) be a dynamical system. Then (X, T) is weakly mixing if and only if it is F-transitive.\n\n# 3. LI-YORKE CHAOS\n\nThe mathematical terminology \u201cchaos\u201d was first introduced in 1975 by Li and Yorke to describe the complex behavior of trajectories. It turns out that it is the common feature of all known definitions of chaos.\n\n# 3.1. Li-Yorke chaos and its relation with topological entropy.\n\nFollowing the idea in [69], we usually define the Li-Yorke chaos as follows.\n\n# Definition 3.1.\n\nLet (X, T) be a dynamical system. A pair (x, y) \u2208 X \u00d7 X is called scrambled if\n\nlim inf d(T\u207fx, T\u207fy) = 0 as n \u2192 \u221e and lim sup d(T\u207fx, T\u207fy) > 0 as n \u2192 \u221e,\n\nthat is (x, y) is proximal but not asymptotic. A subset C of X is called scrambled if any two distinct points x, y \u2208 C form a scrambled pair. The dynamical system (X, T) is called Li-Yorke chaotic if there is an uncountable scrambled set in X.\n\nIt is worth to notice that the terminology \u201cscrambled set\u201d was introduced in 1983 by Sm\u00edtal in [95]. Note that we should assume that a scrambled set contains at least two points. We will keep this convention throughout this paper.\n\nIn [69], Li and Yorke showed that\n\n# Theorem 3.2.\n\nIf a continuous map f : [0, 1] \u2192 [0, 1] has a periodic point of period 3, then it is Li-Yorke chaotic.\n\n# Definition 3.3.\n\nLet (X, T) be a dynamical system. For a given positive number \u03b4 > 0, a pair (x, y) \u2208 X \u00d7 X is called \u03b4-scrambled, if\n\nlim inf d(T\u207fx, T\u207fy) = 0 as n \u2192 \u221e and lim sup d(T\u207fx, T\u207fy) > \u03b4 as n \u2192 \u221e.\n\nA subset C of X is \u03b4-scrambled if any two distinct points x, y in C form a \u03b4-scrambled pair. The dynamical system (X, T) is called Li-Yorke \u03b4-chaotic, if there exists an uncountable \u03b4-scrambled set in X.\n\nNote that the Auslander-Floyd system [7] is Li-Yorke chaotic, but for any \u03b4 > 0 there is no uncountable \u03b4-scrambled sets.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "630cf3fc-75fe-4726-bbfe-fb9acaea9405": {"__data__": {"id_": "630cf3fc-75fe-4726-bbfe-fb9acaea9405", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "764e6d7b-d79b-4603-8048-7d2ec800bb78", "node_type": "4", "metadata": {}, "hash": "a42cdde41b843812bf8c8af30b37b84a6de6e3b886927316c324233f37f5bd66", "class_name": "RelatedNodeInfo"}}, "text": "# 6\n\n# J. LI AND X. YE\n\nIn [51] Jankov\u00b4a and Sm\u00b4dotlessital showed that if a continuous map f : [0, 1] &rarr; [0, 1] has positive topological entropy then there exists a perfect \u03b4-scrambled set for some \u03b4 > 0. A natural question is that whether there exists a Li-Yorke chaotic map with zero topological entropy. This was shown, independently, by Xiong [106] and Sm\u00b4dotlessital [96] in 1986.\n\n# Theorem 3.4.\n\nThere exists a continuous map f : [0, 1] &rarr; [0, 1], which is Li-Yorke chaotic but has zero topological entropy.\n\nIn 1991, using Mycielski Theorem 2.1, Iwanik [50] showed that every weakly mixing system is Li-Yorke chaotic.\n\n# Theorem 3.5 ([50]).\n\nIf a non-trivial dynamical system (X, T) is weakly mixing, then there exists a dense Mycielski \u03b4-scrambled subset of X for some \u03b4 > 0.\n\nAs already mentioned above, for interval maps positive topological entropy implies Li-Yorke chaos. It used to be a long-standing open problem whether this also holds for general topological dynamical systems. In 2002, Blanchard, Glasner, Kolyada and Maass gave a positive answer by using ergodic theory method.\n\n# Theorem 3.6 ([15]).\n\nIf a dynamical system (X, T) has positive topological entropy, then there exists a Mycielski \u03b4-scrambled set for some \u03b4 > 0.\n\nIn fact we know from the proof of Theorem 3.6 that if (X, T) has an ergodic invariant measure which is not measurable distal then same conclusion holds. In 2007, Kerr and Li [53] gave a new proof of Theorem 3.6 by using combinatorial method. First we recall the definition of IE-tuples.\n\n# Definition 3.7.\n\nLet (X, T) be a topological dynamical system and k \u2265 2. For a tuple &tilde; = (A1, . . . , Ak) of subsets of X, we say that a subset J of Z+ is an independence set for &tilde; if for any non-empty finite subset I of J, we have \u2229i \u2208 I T-iAs(i) \u2260 \u2205 for any s \u2208 {1, . . . , k}I. A tuple &tilde; = (x1, . . . , xk) \u2208 Xk is called an IE-tuple if for every product neighborhood U1 \u00d7 ... \u00d7 Uk of &tilde; the tuple (U1, . . . , Uk) has an independence set of positive density.\n\nThe following theorem characterizes positive topological entropy and has many applications. It was first proved by Huang and Ye using the notion of interpolating sets [49], and we state it here using the notion of independence by Kerr and Li [53]. Recall for a dynamical system (X, T), a tuple (x1, . . . , xk) \u2208 Xk is an entropy tuple if xi \u2260 xj for i \u2260 j and for any disjoint closed neighborhoods Vi of x, the open cover {V1c, . . . , Vkc} has positive entropy. When k = 2 we call it an entropy pair. A subset A of X is an entropy set if any tuple of points in A with pairwise different coordinates is an entropy tuple.\n\n# Theorem 3.8 ([49, 53]).\n\nLet (X, T) be a dynamical system. Then a tuple on X is an entropy tuple if and only if it is a non-diagonal IE-tuple. In particular, the system (X, T) has zero topological entropy if and only if every IE-pair is diagonal (i.e. all of its entries are equal).\n\nBy developing some deep combinatorial tools Kerr and Li showed\n\n# Theorem 3.9 ([53]).\n\nLet (X, T) be a dynamical system. Suppose that k \u2265 2 and &tilde; = (x1, . . . , xk) is a non-diagonal IE-tuple. For each 1 \u2264 j \u2264 k, let Aj be a neighborhood of x. Then there exists a Cantor set Zj \u2282 Aj for each j = 1, . . . , k such that the following hold:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1171fade-0afd-42a9-878c-52269c23e0b4": {"__data__": {"id_": "1171fade-0afd-42a9-878c-52269c23e0b4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14481d3d-e5d7-49f3-95a6-46d8add21e05", "node_type": "4", "metadata": {}, "hash": "bd86976017bbc6716eb3835a19378a59a44277f5abd00a0d62e99abdea53b997", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# 3.2. Completely scrambled systems and invariant scrambled sets.\n\nDefinition 3.10. We say that a dynamical system (X, T) is completely scrambled if the whole space X is scrambled.\n\nIt should be noticed that for any \u03b4 > 0, the whole space can not be \u03b4-scrambled [18].\n\nRecall that a dynamical system (X, T) is proximal if any two points in X form a proximal pair. Clearly, every completely scrambled system is proximal. We have the following characterization of proximal systems.\n\n# Theorem 3.11 ([42, 6]).\n\nA dynamical system (X, T) is proximal if and only if it has a fixed point which is the unique minimal point in X.\n\nIn 1997, Mai [73] showed that there are some completely scrambled systems on non-compact spaces.\n\n# Theorem 3.12.\n\nLet X be a metric space uniformly homeomorphic to the n-dimensional open cube In = (0, 1)n, n \u2265 2. Then there exists a homeomorphism f: X &rarr; X such that the whole space X is a scrambled set of X.\n\nIn 2001, Huang and Ye [45] showed that on some compact spaces there are also some completely scrambled systems.\n\n# Theorem 3.13.\n\nThere are \u201cmany\u201d compacta admitting completely scrambled homeomorphisms, which include some countable compacta, the Cantor set and continua of arbitrary dimension.\n\nHuang and Ye also mentioned in [45] that an example of completely scrambled transitive homeomorphism is a consequence of construction of uniformly rigid proximal systems by Katznelson and Weiss [52]. Recall that a dynamical system (X, T) is uniformly rigid if\n\nlim inf sup d(Tn(x), x) = 0.\n\nLater in [46], they showed that every almost equicontinuous but not minimal system has a completely scrambled factor. These examples are not weakly mixing, so the existence of completely scrambled weakly mixing homeomorphism is left open in [45]. Recently, For\u00fds et al. in [24] constructed two kinds of completely scrambled systems which are weakly mixing, proximal and uniformly rigid. The first possible approach is derived from results of Akin and Glasner [4] by a combination of abstract arguments. The second method is obtained by modifying the construction of Katznelson and Weiss from [52]. More precisely, we have the following result.\n\n# Theorem 3.14 ([24]).\n\nThere are completely scrambled systems which are weakly mixing, proximal and uniformly rigid.\n\nOn the other hand, Blanchard, Host and Ruette [16] proved that any positive topological entropy system can not be completely scrambled by showing that there are \u201cmany\u201d non-diagonal asymptotic pairs in any dynamical system with positive topological entropy.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42c24cbf-224a-40b8-80c9-0dc8f85d3cbd": {"__data__": {"id_": "42c24cbf-224a-40b8-80c9-0dc8f85d3cbd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d2c2755-b053-4a4a-8be0-84cb5dd32d35", "node_type": "4", "metadata": {}, "hash": "5204cff796020bebda61f7e60e96c5b1f787b65e156435ac18e60f69f882ee8d", "class_name": "RelatedNodeInfo"}}, "text": "# 8\n\n# J. LI AND X. YE\n\n# Theorem 3.15 ([16]).\n\nLet (X, T) be a dynamical system and \u00b5 be an ergodic invariant measure on (X, T) with positive entropy. Then for \u00b5-a.e. x \u2208 X there exists a point y \u2208 X \\{x} such that (x, y) is asymptotic.\n\nRecently in [44], Huang, Xu and Yi generalized Theorem 3.15 to positive entropy G-systems for certain countable, discrete, infinite left-orderable amenable groups G. We remark that the following question remains open.\n\n# Problem 1.\n\nLet T be a homeomorphism on a compact metric space X. If for any two distinct points x, y \u2208 X, (x, y) is either Li-Yorke scrambled for T or Li-Yorke scrambled for T\u22121, does T have zero topological entropy?\n\nFor n \u2265 1, TnS is also a scrambled set of (X, T). It is interesting to consider whether a scrambled set may be invariant under T. Since the space X is compact, if (x, f(x)) is scrambled for some x \u2208 X, then there is a fixed point in X. It is shown in [22] that\n\n# Theorem 3.16.\n\nLet f: [0, 1] \u2192 [0, 1] be a continuous map. Then f has positive topological entropy if and only if fn has an uncountable invariant scrambled set for some n > 0.\n\n# In 2009, Yuan and Ldieresisu proved that\n\n# Theorem 3.17 ([112]).\n\nLet (X, T) be a non-trivial transitive system. If (X, T) has a fixed point, then there exists a dense Mycielski subset K of X such that K is an invariant scrambled set.\n\n# In 2010, Balibrea, Guirao and Oprocha studied invariant \u03b4-scrambled sets and showed that\n\n# Theorem 3.18 ([9]).\n\nLet (X, T) be a non-trivial strongly mixing system. If (X, T) has a fixed point, then there exist \u03b4 > 0 and a dense Mycielski subset S of X such that S is an invariant \u03b4-scrambled set.\n\nThey also conjectured in [9] that there exists a weakly mixing system which has a fixed point but without invariant \u03b4-scrambled sets. The authors in [24] found that the existence of invariant \u03b4-scrambled sets is relative to the property of uniform rigidity. It is shown in [24] that a necessary condition for a dynamical system possessing invariant \u03b4-scrambled sets for some \u03b4 > 0 is not uniformly rigid, and this condition (with a fixed point) is also sufficient for transitive systems.\n\n# Theorem 3.19 ([24]).\n\nLet (X, T) be a non-trivial transitive system. Then (X, T) contains a dense Mycielski invariant \u03b4-scrambled set for some \u03b4 > 0 if and only if it has a fixed point and is not uniformly rigid.\n\nCombining Theorems 3.14 and 3.19, the above mentioned conjecture from [9] has an affirmative answer. This is because by Theorem 3.14 there exist weakly mixing, proximal and uniformly rigid systems which have a fixed point, but by Theorem 3.19 they do not have any invariant \u03b4-scrambled set.\n\nAs far as we know, whenever a dynamical system has been shown to be Li-Yorke chaotic the proof implies the existence of a Cantor or Mycielski scrambled set. This naturally arises the following problem:\n\n# Problem 2.\n\nIf a dynamical system is Li-Yorke chaotic, does there exist a Cantor scrambled set?\n\nAlthough we do not know the answer to Problem 2, there are severe restrictions on the Li-Yorke chaotic dynamical systems without a Cantor scrambled set, if they exist (see [18]). On the other hand, we have", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e749d08-9f11-4380-bbe4-13ccc6732bd1": {"__data__": {"id_": "2e749d08-9f11-4380-bbe4-13ccc6732bd1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2cf2a93-94b2-4c3a-b686-7f93daced2ce", "node_type": "4", "metadata": {}, "hash": "b54bc1ade45acf6c59489b276e2570de8fc8df4a20bea0ae9b86383dc92ff6a8", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# Theorem 3.20 ([18]).\n\nIf a dynamical system (X, T) is Li-Yorke \u03b4-chaotic for some \u03b4 > 0, then it does have some Cantor \u03b4-scrambled set. The key point is that the collection of \u03b4-scrambled pairs is a G\u03b4 subset of X \u00d7 X, then one can apply the Mycielski Theorem to get a Cantor \u03b4-scrambled set. But in general the collection of scrambled pairs is a Borel set of X \u00d7 X but may be not G\u03b4. By [99, Example 3.6] there exists a dynamical system such that the collection of scrambled pairs is dense in X \u00d7 X but not residual.\n\n# 4. SENSITIVE DEPENDENCE ON INITIAL CONDITIONS\n\nWe say that a dynamical system (X, T) has sensitive dependence on initial conditions (or just sensitive) if there exists some \u03b4 > 0 such that for each x \u2208 X and each \u03b5 > 0 there is y \u2208 X with d(x, y) < \u03b5 and n \u2208 N such that d(Tnx, Tny) > \u03b4. The initial idea goes back at least to Lorentz [72] and the phrase\u2014sensitive dependence on initial conditions\u2014was used by Ruelle [88] to indicate some exponential rate of divergence of orbits of nearby points. As far as we know the first to formulate the sensitivity was Guckenheimer, [35], in his study on maps of the interval (he required the condition to hold for a set of positive Lebesgue measure). The precise expression of sensitivity in the above form was introduced by Auslander and Yorke in [8].\n\n# 4.1. Equicontinuity and Sensitivity.\n\nThe opposite side of sensitivity is the notion of equicontinuity.\n\n# Definition 4.1.\n\nA dynamical system (X, T) is called equicontinuous if for every \u03b5 > 0 there exists some \u03b4 > 0 such that whenever x, y \u2208 X with d(x, y) < \u03b4, d(Tnx, Tny) < \u03b5 for n = 0, 1, 2, . . ., that is the family of maps {Tn : n \u2208 Z+} is uniformly equicontinuous.\n\nEquicontinuous systems have simple dynamical behaviors. It is well known that a dynamical system (X, T) with T being surjective is equicontinuous if and only if there exists a compatible metric \u03c1 on X such that T acts on X as an isometry, i.e., \u03c1(Tx, Ty) = \u03c1(x, y) for any x, y \u2208 X. See [74] for the structure of equicontinuous systems.\n\nWe have the following dichotomy result for minimal systems.\n\n# Theorem 4.2 ([8]).\n\nA minimal system is either equicontinuous or sensitive.\n\nEquicontinuity can be localized in an obvious way.\n\n# Definition 4.3.\n\nLet (X, T) be a dynamical system. A point x \u2208 X is called an equicontinuous point if for any \u03b5 > 0, there exists some \u03b4 > 0 such that d(x, y) < \u03b4 implies d(Tnx, Tny) < \u03b5 for all n \u2208 N. A transitive system is called almost equicontinuous if there exists some equicontinuous point.\n\nWe have the following dichotomy result for transitive systems.\n\n# Theorem 4.4 ([3, 29]).\n\nLet (X, T) be a transitive system. Then either\n\n1. (X, T) is almost equicontinuous, in this case the collection of equicontinuous points coincides with the collection of transitive points; or\n2. (X, T) is sensitive.\n\nIt is interesting that almost equicontinuity is closely related to the uniform rigid property which was introduced by Glasner and Maon in [32] as a topological analogue of rigidity in ergodic theory.\n\n# Theorem 4.5 ([29]).\n\nLet (X, T) be a transitive system. Then it is uniformly rigid if and only if it is a factor of an almost equicontinuous system. In particular, every almost equicontinuous system is uniformly rigid.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af7f5101-cf9f-4f08-b5fe-189709625aa0": {"__data__": {"id_": "af7f5101-cf9f-4f08-b5fe-189709625aa0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c23edf9-18ca-4874-aecc-203868bdb030", "node_type": "4", "metadata": {}, "hash": "18d61403ba1629fa2ed53b54a3b1ce4dd95a5a9520e007672d7251594a5a918a", "class_name": "RelatedNodeInfo"}}, "text": "# 10 J. LI AND X. YE\n\nIt is shown in [32] that every uniformly rigid system has zero topological entropy. Then by Theorem 4.5 every almost equicontinuous system also has zero topological entropy.\n\n# 4.2. n-sensitivity and sensitive sets\n\nAmong other things, Xiong [107] introduced a new notion called n-sensitivity, which says roughly that in each non-empty open subset there are n distinct points whose trajectories are apart from (at least for one common moment) a given positive constant pairwise.\n\n# Definition 4.6\n\nLet (X, T) be a dynamical system and n \u2265 2. The system (X, T) is called n-sensitive, if there exists some \u03b4 > 0 such that for any x \u2208 X and \u03b5 > 0 there are x1, x2, ..., xn \u2208 B(x, \u03b5) and k \u2208 N satisfying\n\n1 \u2264 i < j \u2264 n, d(Tkxi, Tkxj) > \u03b4.\n\n# Proposition 4.7\n\n([107]) If a dynamical system (X, T) is weakly mixing, then it is n-sensitive for all n \u2265 2. Moreover, if a dynamical system (X, T) on a locally connected space X is sensitive, then it is n-sensitive for all n \u2265 2.\n\nIn [94], Shao, Ye and Zhang studied the properties of n-sensitivity for minimal systems, and showed that n-sensitivity and (n + 1)-sensitivity are essentially different.\n\n# Theorem 4.8\n\n([94]) For every n \u2265 2, there exists a minimal system (X, T) which is n-sensitive but not (n + 1)-sensitive.\n\nRecently, using ideas and results from local entropy theory, Ye and Zhang [111] and Huang, Lu and Ye [42] developed a theory of sensitive sets, which measures the \u201cdegree\u201d of sensitivity both in the topological and the measure-theoretical setting.\n\n# Definition 4.9\n\nLet (X, T) be a dynamical system. A subset A of X is sensitive if for any n \u2265 2, any n distinct points x1, x2, ..., xn in A, any neighborhood Ui of xi, i = 1, 2, ..., n, and any non-empty open subset U of X there exist k \u2208 N and y \u2208 U such that Tk(y) \u2208 Ui for i = 1, 2, ..., n.\n\nIt is shown in [111] that a transitive system is n-sensitive if and only if there exists a sensitive set with cardinality n. Moreover, a dynamical system is weakly mixing if and only if the whole space X is sensitive.\n\n# Theorem 4.10\n\n([111]) If a dynamical system is transitive, then every entropy set is also a sensitive set. This implies that if a transitive system has positive topological entropy, then there exists an uncountable sensitive set.\n\nThe number of minimal subsets is related to the cardinality of sensitive sets. For example, it was shown in [111] that if a transitive system has a dense set of minimal points but is not minimal, then there exists an infinite sensitive set. Moreover, if there are uncountable pairwise disjoint minimal subsets, then there exists an uncountable sensitive set.\n\nIn 2011, Huang, Lu and Ye got a fine structure of sensitive sets in minimal systems.\n\n# Theorem 4.11\n\n([42]) Let (X, T) be a minimal dynamical system and \u03c0 : (X, T) \u2192 (Y, S) be the factor map to the maximal equicontinuous factor of (X, T). Then\n\n1. each sensitive set of (X, T) is contained in some \u03c0-1(y) for some y \u2208 Y;\n2. for each y \u2208 Y, \u03c0-1(y) is a sensitive set of (X, T).\n\nConsequently, (X, T) is n-sensitive, not (n + 1)-sensitive, if and only if maxy \u2208 Y #(\u03c0-1(y)) = n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67fb1692-142e-448a-bbd1-3ecfea6d98ef": {"__data__": {"id_": "67fb1692-142e-448a-bbd1-3ecfea6d98ef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c75fc2b8-5705-45ea-98b8-be05e7d95cba", "node_type": "4", "metadata": {}, "hash": "ae947dfe014190bca29ef0b6940c911cad0ee6ca46cbb119349acc760c7b0e5e", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# 4.3. Li-Yorke sensitivity\n\nA concept that combines sensitivity and Li-Yorke \u03b4-scrambled pairs was proposed by Akin and Kolyada in [6], which is called Li-Yorke sensitivity.\n\nDefinition 4.12. A dynamical system (X, T) is called Li-Yorke sensitive if there exists some \u03b4 > 0 such that for any x \u2208 X and \u03b5 > 0, there is y \u2208 X satisfying d(x, y) < \u03b5 such that\n\nlim infn\u2192\u221ed(Tnx, Tny) = 0 and lim supn\u2192\u221ed(Tnx, Tny) > \u03b4.\n\nLet (X, T) be a dynamical system and x \u2208 X. The proximal cell of x is the set {y \u2208 X : (x, y) is proximal}. First, it was proved in [55] that for a weakly mixing (X, T) the set of points x at which the proximal cell is residual in X is itself residual in X. Later it was proved by Furstenberg in [26] that the proximal cell of every point is residual, provided that the system is minimal and weakly mixing. Finally, Akin and Kolyada proved in [6] that in any weakly mixing system the proximal cell of every point is residual. Note that the authors in [43] got more about the structure of the proximal cells of F-mixing systems, where F is a Furstenberg family.\n\nIn [6], Akin and Kolyada also proved that\n\nTheorem 4.13 ([6]). If a non-trivial dynamical system (X, T) is weakly mixing then it is Li-Yorke sensitive.\n\nSee a survey paper [56] for more results about Li-Yorke sensitivity and its relation to other concepts of chaos. But the following question remains open.\n\nProblem 3. Are all Li-Yorke sensitive systems Li-Yorke chaotic?\n\n# 4.4. Mean equicontinuity and mean sensitivity\n\nDefinition 4.14. A dynamical system (X, T) is called mean equicontinuous if for every \u03b5 > 0, there exists some \u03b4 > 0 such that whenever x, y \u2208 X with d(x, y) < \u03b4,\n\nlim supn\u2192\u221e (1/n) \u2211i=0n-1 d(Tix, Tiy) < \u03b5.\n\nThis definition is equivalent to the notion of mean-L-stability which was first introduced by Fomin [23]. It is an open question if every ergodic invariant measure on a mean-L-stable system has discrete spectrum [91]. The authors in [65] firstly gave an affirmative answer to this question. See [27] for another approach to this question.\n\nTheorem 4.15 ([65]). If a dynamical system (X, T) is mean equicontinuous, then every ergodic invariant measure on (X, T) has discrete spectrum and hence the topological entropy of (X, T) is zero.\n\nSimilarly, we can define the local version and the opposite side of mean equicontinuity.\n\nDefinition 4.16. Let (X, T) be a dynamical system. A point x \u2208 X is called mean equicontinuous if for every \u03b5 > 0, there exists some \u03b4 > 0 such that for every y \u2208 X with d(x, y) < \u03b4,\n\nlim supn\u2192\u221e (1/n) \u2211i=0n-1 d(Tix, Tiy) < \u03b5.\n\nA transitive system is called almost mean equicontinuous if there is at least one mean equicontinuous point.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac651916-78bc-4d6b-a755-57bae36cadeb": {"__data__": {"id_": "ac651916-78bc-4d6b-a755-57bae36cadeb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1d2f0b-658c-468d-a97c-6a7b415fb786", "node_type": "4", "metadata": {}, "hash": "a8c198a8ead11b65ae60f71aafba73c4f826423a5372f93b1e77b089946c9a38", "class_name": "RelatedNodeInfo"}}, "text": "# 12\n\n# J. LI AND X. YE\n\n# Definition 4.17.\n\nA dynamical system (X, T) is called mean sensitive if there exists some \u03b4 > 0 such that for every x element X and every neighborhood U of x, there exists y element U and n element N such that\n\nlim supn\u2192\u221e 1/(n\u22121) \u2211i=0n d(Tix, Ty) > \u03b4.\n\nWe have the following dichotomy for transitive systems and minimal systems.\n\n# Theorem 4.18 ([65]).\n\nIf a dynamical system (X, T) is transitive, then (X, T) is either almost mean equicontinuous or mean sensitive. In particular, if (X, T) is a minimal system, then (X, T) is either mean equicontinuous or mean sensitive.\n\nRecall that an almost equicontinuous system is uniformly rigid and thus has zero topological entropy. The following Theorem 4.19 shows that an almost mean equicontinuous system behaves quite differently.\n\n# Theorem 4.19 ([65]).\n\nIn the full shift (\u03a32, \u03c3), every minimal subshift (Y, \u03c3) is contained in an almost mean equicontinuous subshift (X, \u03c3).\n\nSince it is well-known that there are many minimal subshifts of (\u03a32, \u03c3) with positive topological entropy, an immediate corollary of Theorem 4.19 is the following result.\n\n# Corollary 4.20.\n\nThere exist many almost mean equicontinuous systems which have positive topological entropy.\n\nGlobally speaking a mean equicontinuous system is \u2018simple\u2019, since it is a Banach proximal extension of an equicontinuous system and each of its ergodic measures has discrete spectrum. Unfortunately, the local version does not behave so well, as Theorem 4.19 shows. We will introduce the notion of Banach mean equicontinuity, whose local version has the better behavior that we are looking for.\n\n# Definition.\n\nLet (X, T) be a dynamical system. We say that (X, T) is Banach mean equicontinuous if for every \u03b5 > 0, there exists some \u03b4 > 0 such that whenever x, y element X with d(x, y) < \u03b4,\n\nlim supM\u2192\u221e 1/(M\u2212N) \u2211i=NM\u22121 d(Tix, Ty) < \u03b5.\n\nA point x element X is called Banach mean equicontinuous if for every some \u03b4 > 0 such that for every y element B(x, \u03b4),\n\nlim supM\u2192\u221e 1/(M\u2212N) \u2211i=NM\u22121 d(Tix, Ty) < \u03b5.\n\nWe say that a transitive system (X, T) is almost Banach mean equicontinuous if there exists a transitive point which is Banach mean equicontinuous.\n\nA dynamical system (X, T) is Banach mean sensitive if there exists some \u03b4 > 0 such that for every x element X and every \u03b5 > 0 there is y element B(x, \u03b5) satisfying\n\nlim supM\u2192\u221e 1/(M\u2212N) \u2211i=NM\u22121 d(Tix, Ty) > \u03b4.\n\nWe also have the following dichotomy for transitive systems and minimal systems.\n\n# Theorem 4.21 ([65]).\n\nIf a dynamical system (X, T) is transitive, then (X, T) is either almost Banach mean equicontinuous or Banach mean sensitive. If (X, T) is a minimal system, then (X, T) is either Banach mean equicontinuous or Banach mean sensitive.\n\n# Theorem 4.22 ([65]).\n\nLet (X, T) be a transitive system. If the topological entropy of (X, T) is positive, then (X, T) is Banach mean sensitive.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b70d2504-2dfa-4724-aad2-ccf9db730211": {"__data__": {"id_": "b70d2504-2dfa-4724-aad2-ccf9db730211", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "03d3d6e6-6e07-4e3b-b145-6eb8f9a9c6b1", "node_type": "4", "metadata": {}, "hash": "eab12bcd2edcb0aa2a1af8c262b5c7ff7012c2266eeb72e3fb520bee71796be5", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# 4.5. Other generalization of sensitivity\n\nCombining Theorems 4.21 and 4.22, we have the following corollary.\n\n# Corollary 4.23.\n\nIf (X, T) is almost Banach mean equicontinuous then the topological entropy of (X, T) is zero.\n\nBy Corollary 4.20, there are many almost mean equicontinuous systems which have positive entropy. Then by Corollary 4.23 they are not almost Banach mean equicontinuous. But the following question is still open.\n\n# Problem 4.\n\nIs there a minimal system which is mean equicontinuous but not Banach mean equicontinuous?\n\nNote that sensitivity can be generalized in other ways, see [76, 104, 38] for example.\n\nLet (X, T) be a dynamical system. For \u03b4 > 0 and a non-empty open subset U, let\n\nN(\u03b4, U) = {n \u2208 N : \u2203x, y \u2208 U with d(T^n x, T^n y) > \u03b4} = {n \u2208 N : diam(T^n(U)) > \u03b4}.\n\nLet F be a Furstenberg family. According to Moothathu [76], we say that a dynamical system (X, T) is F-sensitive if there exists some \u03b4 > 0 such that for any non-empty open subset U of X, N(\u03b4, U) \u2208 F. F-sensitivity for some special families were discussed in [102, 27, 71, 67].\n\nA dynamical system (X, T) is called multi-sensitive if there exists some \u03b4 > 0 such that for any finite open non-empty subsets U1, ..., Un of X,\n\n\u2229i=1nN(\u03b4, Ui) \u2260 \u2205.\n\nIn [38] among other things, Huang, Kolyada and Zhang proved that for a minimal system thick sensitivity is equivalent to multi-sensitivity. Moreover, they showed the following dichotomy for minimal systems.\n\n# Theorem 4.24.\n\nLet (X, T) be a minimal system. Then (X, T) is multi-sensitive if and only if it is not an almost one-to-one extension of its maximal equicontinuous factor.\n\nResults similar to Theorem 4.24 for other families will be appeared in [110] by Ye and Yu.\n\n# 5. CHAOS IN TRANSITIVE SYSTEMS\n\nIn this section, we discuss various kinds of chaos in transitive systems.\n\n# 5.1. Auslander-Yorke chaos.\n\nIn [8] Auslander and Yorke defined a kind of chaos as \u201ctopological transitivity plus pointwise instability\u201d. This leads to the following definition of Auslander-Yorke chaos.\n\n# Definition 5.1.\n\nA dynamical system (X, T) is called Auslander-Yorke chaotic if it both transitive and sensitive.\n\nDue to Ruelle and Takens\u2019 work on turbulence [89], Auslander-Yorke chaos was also called Ruelle-Takens chaos (see [107] for example). It should be noticed that there is no implication relation between Li-Yorke chaos and Auslander-Yorke chaos. For example, any non-equicontinuous distal minimal system is sensitive, so it is Auslander-Yorke chaotic, but it has no Li-Yorke scrambled pairs. On the other hand, there are non-periodic transitive systems with a fixed point that are not sensitive [4]; by Theorem 5.6 they are Li-Yorke chaotic.\n\nThe work of Wiggins [105] leads to the following definition.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6cf53ea-7f80-42e5-93ae-0373ca5ea885": {"__data__": {"id_": "d6cf53ea-7f80-42e5-93ae-0373ca5ea885", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8176ef21-d338-4cf8-808c-bcc3235b67dd", "node_type": "4", "metadata": {}, "hash": "5b8733b0b0513872b5eb23220c39062796be433407b833535822dfd063b1bc1e", "class_name": "RelatedNodeInfo"}}, "text": "# 14\n\n# J. LI AND X. YE\n\n# Definition 5.2.\n\nA dynamical system (X, T) is called Wiggins chaotic if there exists a subsystem (Y, T) of (X, T) such that (Y, T) is both transitive and sensitive.\n\nIn [90], Ruette investigated transitive and sensitive subsystems for interval maps. She showed that\n\n# Theorem 5.3.\n\n1. For a transitive map f : [0, 1] &rarr; [0, 1], if it is Wiggins chaotic then it is also Li-Yorke chaotic.\n2. There exists a continuous map f : [0, 1] &rarr; [0, 1] of zero topological entropy which is Wiggins chaotic.\n3. There exists a Li-Yorke chaotic continuous map f : [0, 1] &rarr; [0, 1] which is not Wiggins chaotic.\n\n# 5.2. Devaney chaos.\n\nIn his book [19], Devaney proposed a new kind of chaos, which is usually called Devaney chaos.\n\n# Definition 5.4.\n\nA dynamical system (X, T) is called Devaney chaotic if it satisfies the following three properties:\n\n1. (X, T) is transitive;\n2. (X, T) has sensitive dependence on initial conditions;\n3. the set of periodic points of (X, T) is dense in X.\n\nSensitive dependence is widely understood as the central idea in Devaney chaos, but it is implied by transitivity and density of periodic points, see [11] or [29]. In 1993, S. Li [68] showed that\n\n# Theorem 5.5.\n\nLet f : [0, 1] &rarr; [0, 1] be a continuous map. Then f has positive topological entropy if and only if there exists a Devaney chaotic subsystem.\n\nIn 1996, Akin, Auslander and Berg [3] discussed in details when a transitive map is sensitive. An important question is that: does Devaney chaos imply Li-Yorke chaos? In 2002, Huang and Ye gave a positive answer to this long-standing open problem.\n\n# Theorem 5.6 ([46]).\n\nLet (X, T) be a non-periodic transitive system. If there exists a periodic point, then it is Li-Yorke chaos. Particularly, Devaney chaos implies Li-Yorke chaos.\n\nA key result in the proof of Theorem 5.6 is called Huang-Ye equivalences in [6]. To state this result, we need some notations.\n\nFor every \u03b5 > 0, let\n\nV\u03b5 = { (x, y) \u2208 X \u00d7 X : d(x, y) \u2264 \u03b5 } and\n\nAsym\u03b5(T) = \u221e \u2229 { T-kV\u03b5 }\n\nn=0 k=n\n\nFor every x \u2208 X, let Asym\u03b5(T)(x) = { y \u2208 X : (x, y) \u2208 Asym\u03b5(T) }.\n\n# Theorem 5.7 (Huang-Ye Equivalences).\n\nFor a dynamical system (X, T) the following conditions are equivalent.\n\n1. (X, T) is sensitive.\n2. There exists \u03b5 > 0 such that Asym\u03b5(T) is a first category subset of X \u00d7 X.\n3. There exists \u03b5 > 0 such that for every x \u2208 X, Asym\u03b5(T)(x) is a first category subset of X.\n4. There exists \u03b5 > 0 such that for every x \u2208 X, x \u2208 X \\ Asym\u03b5(T)(x).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9fe81c5-10bb-489e-92aa-cc2e38368695": {"__data__": {"id_": "c9fe81c5-10bb-489e-92aa-cc2e38368695", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "714e515a-097e-413a-95e7-7ff121ca2a03", "node_type": "4", "metadata": {}, "hash": "8892921da6c0dd547348c07d828bdbd84a05a120e440d029f8ebba881ef7bd0f", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# 5.3. Multivariant Li-Yorke chaos\n\nThe scrambled set in Li-Yorke chaos only compares the trajectories of two points. It is natural to consider the trajectories of finite points. In 2005, Xiong introduced the following multivariant chaos in the sense of Li-Yorke.\n\n# Definition 5.9 ([107])\n\nLet (X, T) be a dynamical system and n \u2265 2. A tuple (x1, x2, ..., xn) \u2208 Xn is called n-scrambled if\n\nlim inf1 \u2264 i < j \u2264 n d(fk(x), fk(xj)) = 0 as k \u2192 \u221e maxi\n\nand lim sup1 \u2264 i < j \u2264 n d(fk(x), fk(xj)) > \u03b4 > 0 as k \u2192 \u221e mini\n\nA subset C of X is called n-scrambled if any pairwise distinct n points in C form an n-scrambled tuple. The dynamical system (X, T) is called Li-Yorke n-chaotic if there exists an uncountable n-scrambled set.\n\nSimilarly, if the separated constant \u03b4 is uniform for all pairwise distinct n-tuples in C, we can define n-\u03b4-scrambled sets and Li-Yorke n-\u03b4-chaos.\n\n# Theorem 5.10 ([107])\n\nLet (X, T) be a non-trivial transitive system. If (X, T) has a fixed point, then there exists a dense Mycielski subset C of X such that C is n-scrambled for all n \u2265 2.\n\nIn 2011, Li proved that there are no 3-scrambled tuples for an interval map with zero topological entropy.\n\n# Theorem 5.11 ([58])\n\nLet f: [0, 1] \u2192 [0, 1] be a continuous map. If f has zero topological entropy, then there are no 3-scrambled tuples.\n\nNote that there exists a continuous map f: [0, 1] \u2192 [0, 1], which is Li-Yorke chaotic but has zero topological entropy (see Theorem 3.4). Then we have the following result, which shows that Li-Yorke 2-chaos and 3-chaos are essentially different.\n\n# Corollary 5.12\n\nThere exists a Li-Yorke 2-chaotic system which has no 3-scrambled tuples.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd06978f-fa42-4655-9b44-9f34a02bbdb7": {"__data__": {"id_": "dd06978f-fa42-4655-9b44-9f34a02bbdb7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "799966c7-e2d0-4e58-8eed-8b7ebdee35ac", "node_type": "4", "metadata": {}, "hash": "0d3f07df09f4097e59329a0ae986d0260cb1155f3a256a75a7dd343ea399121e", "class_name": "RelatedNodeInfo"}}, "text": "# 5.4. Uniform chaos\n\nRecall that a dynamical system (X, T) is scattering if for any minimal system (Y, S) the product system (X \u00d7 Y, T \u00d7 S) is transitive. It is not hard to show that every weakly mixing system is scattering. In [46], Huang and Ye also proved that scattering implies Li-Yorke chaos.\n\n# Theorem 5.13\n\nIf a non-trivial dynamical system (X, T) is scattering, then there is a dense Mycielski scrambled set.\n\nSince every system has a minimal subsystem, for a scattering system (X, T) there exists a subsystem (Y, S) of (X, T) such that (X \u00d7 Y, T \u00d7 T) is transitive. In 2010, Akin et al. showed that a dynamical system satisfying this property is more complicated than Li-Yorke chaos, and proposed the concept of uniform chaos [5].\n\n# Definition 5.14\n\nLet (X, T) be a dynamical system. A subset K of X is said to be:\n\n1. uniformly recurrent if for every x element K;\n2. recurrent if every finite subset of K is uniformly recurrent;\n3. uniformly proximal if for every \u03b5 > 0 there exists an n element N with diam(T nK) < \u03b5;\n4. proximal if every finite subset of K is uniformly proximal.\n\n# Definition 5.15\n\nLet (X, T) be a dynamical system. A subset K proper subset X is called a uniformly chaotic set if there are Cantor sets C1 proper subset C2 proper subset period centered such that:\n\n1. for each N element N, CN is uniformly recurrent;\n2. for each N element N, CN is uniformly proximal;\n3. K := \u221ei=1 Cn is a recurrent subset of X and also a proximal subset of X.\n\nThe system (X, T) is called (densely) uniformly chaotic if it has a (dense) uniformly chaotic subset of X.\n\nBy the definition, any uniformly chaotic set is scrambled, then uniform chaos implies Li-Yorke chaos. The following is the main result of [5].\n\n# Theorem 5.16\n\nLet (X, T) be a non-trivial dynamical system. If there exists a subsystem (Y, T) of (X, T) such that (X \u00d7 Y, T \u00d7 T) is transitive, then (X, T) is densely uniformly chaotic.\n\nAs a corollary, we can show that many transitive systems are uniformly chaotic. Note that a dynamical system is weakly scattering if its product with any minimal equicontinuous system is transitive.\n\n# Corollary 5.17\n\nIf (X, T) is a dynamical system without isolated points and satisfies one of the following properties, then it is densely uniformly chaotic:\n\n1. (X, T) is transitive and has a fixed point;\n2. (X, T) is totally transitive with a periodic point;\n3. (X, T) is scattering;\n4. (X, T) is weakly scattering with an equicontinuous minimal subsystem;\n5. (X, T) is weakly mixing.\n\nFurthermore, if (X, T) is transitive and has a periodic point of period p, then there is a closed Tp-invariant subset X0 of X, such that (X0, Tp) is densely uniformly chaotic and X = p-1T X0; In particular, (X, T) is uniformly chaotic.\n\n# Remark 5.18\n\nBy Corollary 5.17, Devaney chaos implies uniform chaos.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6548d328-f710-450b-b483-8d8c90af5341": {"__data__": {"id_": "6548d328-f710-450b-b483-8d8c90af5341", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7dcd424-2e3f-4d7c-a2e9-ad7bdd79e735", "node_type": "4", "metadata": {}, "hash": "75b869ebe60f99debcba265031d6b24889095f89d93f1eecc8ba4bff9cd759ba", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# 6. VARIOUS TYPES OF DISTRIBUTIONAL CHAOS AND ITS GENERALIZATION\n\n# 6.1. Distributional chaos\n\nIn [92], Schweizer and Sm\u00b4dotlessital used ideas from probability theory to develop a new definition of chaos, so called distributional chaos. Let (X, T) be a dynamical system. For a pair of points x, y in X and a positive integer n, we define a function \u03a6xy on the real line by\n\n\u03a6xy(t) = n n 1#braceleftbig0 \u2264 i \u2264 n \u2212 1 : d(Tix, Tiy) < tbracerightbig,i\n\nwhere #(periodcentered) denotes the number of elements of a set. Clearly, the function \u03a6xy is non-decreasing, has minimal value 0 (since \u03a6xy(t) = 0 for all t \u2264 0), has maximum value 1/n (since \u03a6xy(t) = 1 for all t greater than the diameter of X), and is left continuous. Then \u03a6xy is a distribution function whose value at t may be interpreted as the probability that the distance between the initial segment of length n of the trajectories of x and y is less than t.\n\nWe are interested in the asymptotic behavior of the function \u03a6xy as n gets large. To this end, we consider the functions \u03a6xy and \u03a6xy defined by\n\n\u03a6xy(t) = lim sup \u03a6xy(t)n and \u03a6xy(t) = lim inf \u03a6xy(t)n\n\nThen \u03a6xy and \u03a6xy are distribution functions with \u03a6xy(t) \u2264 \u03a6xy(t) for all t. It follows that \u03a6xy is an asymptotic measure of how close x and y can come together, while \u03a6xy is an asymptotic measure of their maximum separation. We shall refer to \u03a6xy as the upper distribution, and to \u03a6xy as the lower distribution of x and y.\n\n# Definition 6.1\n\nLet (X, T) be a dynamical system. A pair (x, y) \u2208 X \u00d7 X is called distributionally scrambled if\n\n1. for every t > 0, \u03a6xy(t) = 1, and\n2. there exists some \u03b4 > 0 such that \u03a6xy(\u03b4) = 0.\n\nA subset C of X is called distributionally scrambled if any two distinct points in C form a distributionally scrambled pair. The dynamical system (X, T) is called distributionally chaotic if there exists an uncountable distributionally scrambled set.\n\nSimilarly, if the separated constant \u03b4 is uniform for all non-diagonal pairs in C, we can define distributionally \u03b4-scrambled sets and distributional \u03b4-chaos.\n\nAccording to the definitions, it is clear that any distributionally scrambled pair is scrambled, and then distributional chaos is stronger than Li-Yorke chaos. In [92], Schweizer and Sm\u00b4dotlessital showed that\n\n# Theorem 6.2 ([92])\n\nLet f : [0, 1] \u2192 [0, 1] be a continuous map. Then f is distributionally chaotic if and only if it has positive topological entropy.\n\nIn 1998, Liao and Fan [70] constructed a minimal system with zero topological entropy which is distributionally chaotic. In 2006, Oprocha [80] showed that weak mixing and Devaney chaos do not imply distributional chaos.\n\nIn Section 5.3, Li-Yorke chaos is extended to a multivariant version. In fact, it is clear that any kind of chaos defined by scrambled pairs can be extended to multivariant version.\n\nIn [100], Tan and Fu showed that distributionally n-chaos and (n + 1)-chaos are essentially different.\n\n# Theorem 6.3 ([100])\n\nFor every n \u2265 2, there exists a transitive system which is distributionally n-chaotic but without any distributionally (n + 1)-scrambled tuples.\n\nIn 2013, Li and Oprocha showed that", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bddccd67-d3b9-4138-be50-242894ee5e52": {"__data__": {"id_": "bddccd67-d3b9-4138-be50-242894ee5e52", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0da184e-5c74-4c9e-83d3-143fadc4ca2f", "node_type": "4", "metadata": {}, "hash": "ba0dce2584d5816e99ba2e036f6502d2f98ac0c826d3b1cfc2624547b3c374dc", "class_name": "RelatedNodeInfo"}}, "text": "# Theorem 6.4 ([62])\n\nFor every n \u2265 2, there exists a dynamical system which is distributionally n-chaotic but not Li-Yorke (n + 1)-chaotic.\n\nNote that the example constructed in the proof of Theorem 6.4 contains (n + 1)-scrambled tuples. Then, a natural question is as follows.\n\n# Problem 5\n\nIs there a dynamical system (X, T) with an uncountable distributionally 2-scrambled set but without any 3-scrambled tuples?\n\nRecently, Dolecaronzelov\u00e1 made some progress on Problem 5, but the original problem is still open. She showed that\n\n# Theorem 6.5 ([20])\n\nThere exists a dynamical system X with an infinite extremal distributionally scrambled set but without any scrambled triple.\n\n# Theorem 6.6 ([20])\n\nThere exists an invariant Mycielski (not closed) set X in the full shift with an uncountable extremal distributionally 2-scrambled set but without any 3-scrambled tuple.\n\n# 6.2. The three versions of distributional chaos\n\nPresently, we have at least three different definitions of distributionally scrambled pair (see [98] and [10]). The original distributionally scrambled pair is said to be distributionally scrambled of type 1.\n\n# Definition 6.7\n\nA pair (x, y) \u2208 X \u00d7 X is called distributionally scrambled of type 2 if\n\n1. for any t > 0, \u03a6xy(t) = 1, and\n2. there exists some \u03b4 > 0 such that \u03a6xy(\u03b4) < 1.\n\nA pair (x, y) \u2208 X \u00d7 X is called distributionally scrambled of type 3 if there exists an interval [a, b] \u2282 (0, \u221e) such that \u03a6xy(t) < \u03a6xy(t) for all t \u2208 [a, b].\n\nA subset C of X is called distributionally scrambled of type i (i = 1, 2, 3) if any two distinct points in C form a distributionally scrambled pair of type i. The dynamical system (X, T) is called distributionally chaotic of type i (DCi for short) if there exists an uncountable distributionally scrambled set of type i.\n\nIt should be noticed that in [98] and [10] DCi chaos only requires the existence of one distributionally scrambled pair of type i. It is not hard to see that any distributionally scrambled pair of type 1 or 2 is topological conjugacy invariant [98]. But a distributionally scrambled pair of type 3 may not be topological conjugacy invariant [10]. It is also not hard to construct a dynamical system that has a distributionally scrambled pair of type 2, but no distributionally scrambled pairs of type 1. A really interesting example constructed in [101] shows that there exists a minimal subshift which is distributionally chaotic of type 2, while it does not contain any distributionally scrambled pair of type 1.\n\n# 6.3. The relation between distributional chaos and positive topological entropy\n\nIt is known that in general there is no implication relation between DC1 and positive topological entropy (see [70] and [86]). In [97] Sm\u00eddotlessital conjectured that positive topological entropy does imply DC2. Oprocha showed that this conjecture is true for uniformly positive entropy minimal systems [81]. Finally, Downarowicz solved this problem by proving this conjecture in the general case [21].\n\n# Theorem 6.8 ([21])\n\nIf a dynamical system (X, T) has positive topological entropy, then there exists a Cantor distributional \u03b4-scrambled set of type 2 for some \u03b4 > 0.\n\nIt was observed in [21] that a pair (x, y) is DC2-scrambled if and only if it is mean scrambled in the Li-Yorke sense, that is\n\n1\n\nlim infN\u2192\u221e (1/N) \u2211k=1N d(Tkx, Tky) = 0", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55fe216f-fb51-494f-8392-f785fb0b31aa": {"__data__": {"id_": "55fe216f-fb51-494f-8392-f785fb0b31aa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e347ce2d-af15-4fa3-942b-d6cccb26d7c2", "node_type": "4", "metadata": {}, "hash": "f7116a3864a257d3857b2c2e97f4b8554e9662b4080bed8be20a9eed7950a063", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\nand\n\nlim sup\nNarrowright\u221e Nk=\n1 \u22111dparenleftbigT kx, T ky) > 0.\n\nRecently, Huang, Li and Ye showed that positive topological entropy implies a multivariant version of mean Li-Yorke chaos.\n\n# Theorem 6.9 ([40]).\n\nIf a dynamical system (X, T) has positive topological entropy, then there exists a Mycielski multivariant mean Li-Yorke (\u03b4n)-scrambled subset K of X, that is for every n greaterequal 2 and every n pairwise distinct points x1, . . . , xn element K, we have\n\n1 \u22111lessequali< jlessequaln dparenleftbigT kx, T kx) = 0\nN\nlim infNarrowright\u221e Nk=1 max i j\n\nlim supNarrowright\u221e Nk=\n1 \u221111lessequali< jminlessequaln dparenleftbigT kxi, T kx j) >\u03b4n > 0.\n\nWe remark that the key tool in the proof of Theorem 6.9 is the excellent partition constructed in [16, Lemma 4], which is different from the one used in [21]. So among other things for n = 2 the authors also obtained a new proof of Theorem 6.8.\n\nNote that in [16] Blanchard, Host and Ruette showed that the closure of the set of asymptotic pairs contains the set of entropy pairs. Kerr and Li [53] proved that the intersection of the set of scrambled pairs with the set of entropy pairs is dense in the set of entropy pairs. We extended the above mentioned results to the following result.\n\n# Theorem 6.10 ([40]).\n\nIf a dynamical system has positive topological entropy, then for any n greaterequal 2,\n\n1. the intersection of the set of asymptotic n-tuples with the set of topological entropy n-tuples is dense in the set of topological entropy n-tuples;\n2. the intersection of the set of mean n-scrambled tuples with the set of topological entropy n-tuples is dense in the set of topological entropy n-tuples.\n\n# 6.4. Chaos via Furstenberg families.\n\nIn Section 2, we have shown that there is a powerful connection between topological dynamics and Furstenberg families. In 2007, Xiong, Ldieresisu and Tan introduced the notion of chaos via Furstenberg families. It turned out that the Li-Yorke chaos and some version of distributional chaos can be described as chaos in Furstenberg families sense.\n\n# Definition 6.11 ([108, 101]).\n\nLet (X, T) be a dynamical system. Let F1 and F2 be two Furstenberg families. A pair (x, y) element X \u00d7 X is called (F1, F2)-scrambled if it satisfies\n\n1. for any t > 0, braceleftn element Z+ : d(T nx, T ny) < tbraceright element F1, and\n2. there exists some \u03b4 > 0 such that braceleftn element Z+ : d(T nx, T ny) >\u03b4 braceright element F2.\n\nA subset C of X is called (F1, F2)-scrambled if any two distinct points in C form a (F1, F2)-scrambled pair. The dynamical system (X, T) is called (F1, F2)-chaotic if there is an uncountable (F1, F2)-scrambled set in X.\n\nSimilarly, if the separated constant \u03b4 is uniform for all non-diagonal pairs in C, we can define (F1, F2)-\u03b4-scrambled sets and (F1, F2)-\u03b4-chaos.\n\nLet F be a subset of Z+. The upper density of F is defined as\n\nd(F) = lim supn 1#(F intersection braceleft0, 1, . . ., n \u2212 1braceright).\n\nnarrowright\u221e\n\nFor any a element [0, 1], let\n\nM(a) = braceleftF propersubset Z+ : F is in\ufb01nite and d(F) greaterequal abraceright.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a7cf802-657f-4793-8802-4bf4961a98a1": {"__data__": {"id_": "8a7cf802-657f-4793-8802-4bf4961a98a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65fe5291-1962-4b33-b64a-866c643c33fa", "node_type": "4", "metadata": {}, "hash": "6fca58c92e57eac4fcf8fb97aa94736f0ccf7510fd41482c8733839b31de9d2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16f77f09-cd24-4d9c-aac5-b5fa31ba1fdd", "node_type": "1", "metadata": {}, "hash": "c164dade06980564004b7695c40055d8425867e2ee69ec4d723f1f451320a1fc", "class_name": "RelatedNodeInfo"}}, "text": "# 20\n\n# J. LI AND X. YE\n\nClearly, every M(a) is a Furstenberg family. The following proposition can be easily verified by the definitions, which shows that Li-Yorke chaos and distributional chaos can be characterized via Furstenberg families.\n\n# Proposition 6.12.\n\nLet (X, T) be a dynamical system and (x, y) element X \u00d7 X. Then\n\n1. (x, y) is scrambled if and only if it is (M(0), M(0))-scrambled.\n2. (x, y) is distributionally scrambled of type 1 if and only if it is (M(1), M(1))-scrambled.\n3. (x, y) is distributionally scrambled of type 2 if and only if it is (M(1), M(t))-scrambled with some t > 0.\n\nA Furstenberg family F is said to be compatible with the dynamical system (X, T), if for every non-empty open subset U of X the set {x element X : {n element Z+ : T^n x element U} element F} is a G\u03b4 subset of X.\n\n# Lemma 6.13 ([108]).\n\nIf F = M(t) for some t element [0, 1], then F is compatible with any dynamical system (X, T).\n\n# Theorem 6.14 ([108, 101]).\n\nLet (X, T) be a dynamical system. Suppose that there exists a fixed point p element X such that the set \u221e=1 T^\u2212i(p) is dense in X, and a non-empty closed invariant subset A of X disjoint with the point p such that \u221e=1 T^\u2212i A is dense in X. Then for every two Furstenberg families F1 and F2 compatible with the system (X \u00d7 X, T \u00d7 T), there exists a dense Mycielski (F1, F2)-\u03b4-scrambled set for some \u03b4 > 0. In particular, the dynamical system (X, T) is (M(1), M(1))-\u03b4-chaotic for some \u03b4 > 0.\n\nThere are two important Furstenberg families: the collections of all syndetic sets and all sets with Banach density one. They are so specific that we use terminologies: syndetically scrambled sets and Banach scrambled sets, when studying chaos via those two Furstenberg families.\n\nRecall that a subset F of Z+ is syndetic if there is N element N such that [n, n + N] \u2229 F \u2260 \u2205 for every n element Z+. Let (X, T) be a dynamical system. A pair of points (x, y) element X\u00b2 is called syndetically proximal if for every \u03b5 > 0 the set {n element Z+ : d(T^n x, T^n y) < \u03b5} is syndetic. If (X, T) is proximal, that is every pair in X\u00b2 is proximal, then every pair in X\u00b2 is also syndetically proximal [77]. Moothathu studied the syndetically proximal relation, and identified certain sufficient conditions for the syndetically proximal cell of each point to be small [77]. A subset S of X is called syndetically scrambled if for any two distinct points x, y element S, (x, y) is syndetically proximal but not asymptotic. In [78] Moothathu and Oprocha systematically studied the syndetically proximal relation and the possible existence of syndetically scrambled sets for many kinds of dynamical systems, including various classes of transitive subshifts, interval maps, and topologically Anosov maps.\n\nA subset F of Z+ is said to have Banach density one if for every \u03bb < 1 there exists N \u2265 1 such that #(F \u2229 I) \u2265 \u03bb #(I) for every subinterval I of Z+ where #(I) denotes the number of elements of I. A pair of points (x, y) element X with #(I) \u2265 N, where #(I) denotes every \u03b5 > 0 the set {n element Z+ : d(T^n x, T^n y) < \u03b5} has Banach density one. Clearly, every set with Banach density one is syndetic, then a Banach proximal pair is syndetically proximal. In [64] Li and Tu studied the structure of the Banach proximal relation. Particularly, they showed that for a non-trivial minimal system the Banach proximal cell of every point is of first category.\n\nA subset S of X is called Banach scrambled if for any two distinct points x, y element S, (x, y) is Banach proximal but not asymptotic.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16f77f09-cd24-4d9c-aac5-b5fa31ba1fdd": {"__data__": {"id_": "16f77f09-cd24-4d9c-aac5-b5fa31ba1fdd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65fe5291-1962-4b33-b64a-866c643c33fa", "node_type": "4", "metadata": {}, "hash": "6fca58c92e57eac4fcf8fb97aa94736f0ccf7510fd41482c8733839b31de9d2c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a7cf802-657f-4793-8802-4bf4961a98a1", "node_type": "1", "metadata": {}, "hash": "fdc0d91c026e8f59d0ea0d0ae3d8fd6ac85ad39db89ac6b4d00e4fcceec3f34a", "class_name": "RelatedNodeInfo"}}, "text": "Clearly, every set with Banach density one is syndetic, then a Banach proximal pair is syndetically proximal. In [64] Li and Tu studied the structure of the Banach proximal relation. Particularly, they showed that for a non-trivial minimal system the Banach proximal cell of every point is of first category.\n\nA subset S of X is called Banach scrambled if for any two distinct points x, y element S, (x, y) is Banach proximal but not asymptotic. Note that it was shown in [58] that for an interval map with zero topological entropy, every proximal pair is Banach proximal.", "mimetype": "text/plain", "start_char_idx": 3093, "end_char_idx": 3665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6ff1809-daf0-4e37-a83a-f99396530dff": {"__data__": {"id_": "b6ff1809-daf0-4e37-a83a-f99396530dff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cc62e09-56b1-4f5b-9090-66ab0aadf84a", "node_type": "4", "metadata": {}, "hash": "46ce6bfb76f997d85e5698f852cbc21c42ae2141fe25fe0714f5606165c95029", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# Theorem 6.15 ([64])\n\nLet f : [0, 1] &rarr; [0, 1] be a continuous map. If f is Li-Yorke chaotic, then it has a Cantor Banach scrambled set.\n\n# Theorem 6.16 ([64])\n\nLet f : [0, 1] &rarr; [0, 1] be a continuous map. Then f has positive topological entropy if and only if there is a Cantor set S \u2282 [0, 1] such that S is a Banach scrambled set and fn(S) \u2282 S for some n \u2265 1.\n\nThere exists a dynamical system with the whole space being a scrambled set (see Theorems 3.13 and 3.14), and then every pair is syndetically proximal. It is shown in [64] that the whole space also can be a Banach scrambled set.\n\n# Theorem 6.17 ([64])\n\nThere exists a transitive system with the whole space being a Banach scrambled set.\n\n# 7. LOCAL ASPECTS OF MIXING PROPERTIES\n\nAs we said before for a dynamical system with positive topological entropy, it is possible that there is no weakly mixing subsystem. The notion of weakly mixing sets captures some complicated subset of the system.\n\n# 7.1. Weakly mixing sets\n\nIn 1991, Xiong and Yang [109] showed that for in a weakly mixing system there are considerably many points in the domain whose orbits display highly erratic time dependence. More specifically, they showed that\n\n# Theorem 7.1\n\nLet (X, T) be a non-trivial dynamical system. Then\n\n1. (X, T) is weakly mixing if and only if there exists a c-dense F\u03c3 subset C of X satisfying for any subset D of C and any continuous map f : D &rarr; X, there exists an increasing sequence {qi} of positive integers such that limT qi x = f(x) for all x \u2208 D;\n2. (X, T) is strongly mixing if and only if for any increasing sequence {pi} of positive integers there exists a c-dense F\u03c3 subset C of X satisfying for any subset D of C and any continuous map f : D &rarr; X, there exists a subsequence {pni} of {pi} such that limT pni x = f(x) for all x \u2208 D.\n\nIn particular, if (X, T) is weakly mixing, then there exists some \u03b4 > 0 such that (X, T) is Li-Yorke \u03b4-chaotic.\n\nNote that a subset C is called c-dense in X if for every non-empty open subset U of X, the intersection C \u2229 U has the cardinality of the continuum c.\n\nInspired by Xiong-Yang\u2019s result (Theorem 7.1), in 2008 Blanchard and Huang [17] introduced the concept of weakly mixing sets, which can be regarded as a local version of weak mixing.\n\n# Definition 7.2\n\nLet (X, T) be a dynamical system. A closed subset A of X is said to be weakly mixing if there exists a dense Mycielski subset C of A such that for any subset D of C and any continuous map f : D &rarr; A, there exists an increasing sequence {qi} of positive integers satisfying limT qi x = f(x) for all x \u2208 D.\n\nBlanchard and Huang showed that in any positive entropy system there are many weakly mixing sets. More precisely, let W M(X, T) be the set of weakly mixing sets of (X, T) and H(X, T) be the closure of the set of entropy sets in the hyperspace.\n\n# Theorem 7.3 ([17])\n\nIf a dynamical system (X, T) has positive topological entropy, then the set H(X, T) \u2229 W M(X, T) is a dense G\u03b4 subset of H(X, T).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f1956c8-e604-491c-b2a1-ec42671f2a84": {"__data__": {"id_": "9f1956c8-e604-491c-b2a1-ec42671f2a84", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "440b5a2d-2ee8-4a6d-83d4-f5fbd56ee8b3", "node_type": "4", "metadata": {}, "hash": "7f2b429f7585724bd87c0049714a78bfb5c09b7803a3cbe05906ed54c4fcf047", "class_name": "RelatedNodeInfo"}}, "text": "# 22\n\n# J. LI AND X. YE\n\nMoreover in [37], Huang showed that in any positive entropy system there is a measure-theoretically rather big set such that the closure of the stable set of any point from the set contains a weakly mixing set. Recall that the stable set of a point x element X for T is\n\nWs(X, T) = {y element X : lim inf d(Tnx, Tny) = 0}.\n\n# Theorem 7.4 ([37]).\n\nLet (X, T) be a dynamical system and \u00b5 be an ergodic invariant measure on (X, T) with positive entropy. Then for \u00b5-a.e. x element X the closure of the stable set Ws(X, T) contains a weakly mixing set.\n\nRecently in [44], Huang, Xu and Yi showed the existence of certain chaotic sets in the stable set of positive entropy G-systems for certain countable, discrete, infinite left-orderable amenable groups G. We restate [44, Theorem 1.2] in our setting as follows.\n\n# Theorem 7.5 ([44]).\n\nLet T be a homeomorphism on a compact metric space X. If \u00b5 is an ergodic invariant measure on (X, T) with positive entropy, then there exists \u03b4 > 0 such that for \u00b5-a.e. x element X the stable set Ws(X, T) contains a Cantor \u03b4-scrambled set for T-1.\n\n# Remark 7.6.\n\nThe authors in [17] also discussed the relation between weakly mixing sets and other chaotic properties.\n\n1. Positive entropy is strictly stronger than the existence of weakly mixing sets, which in turn is strictly stronger than Li-Yorke chaos.\n2. There exists a Devaney chaotic system without any weakly mixing sets.\n\nThe following result is the well-known Furstenberg intersection lemma, which shows that weak mixing implies weak mixing of all finite orders.\n\n# Lemma 7.7 ([25]).\n\nIf a dynamical system (X, T) is weakly mixing, then for any k element N and any non-empty open subsets U1, ..., Uk, V1, ..., Vk of X,\n\n\u2229i=1k(Ui, V) \u2260 \u2205.\n\nUsing the idea in Lemma 7.7 we can give the following alternative definition of weakly mixing sets.\n\n# Proposition 7.8 ([17]).\n\nLet (X, T) be a dynamical system and let A be a closed subset of X but not a singleton. Then A is weakly mixing if and only if for any k element N and any choice of non-empty open subsets U1, ..., Uk, V1, ..., Vk of X intersecting A (that is A \u2229 Ui \u2260 \u2205, A \u2229 Vi \u2260 \u2205 for i = 1, ..., k), one has\n\n\u2229i=1k(Ui \u2229 A, V) \u2260 \u2205.\n\n# Definition 7.9.\n\nLet (X, T) be a dynamical system, A be a closed subset of X but not a singleton and k \u2265 2. The set A is said to be weakly mixing of order k if for any choice of non-empty open subsets U1, ..., Uk, V1, ..., Vk of X intersecting A, one has\n\n\u2229i=1k(Ui \u2229 A, V) \u2260 \u2205.\n\nThen A is weakly mixing if and only if it is weakly mixing of order k for all k \u2265 2.\n\nIn 2011, Oprocha and Zhang [82] studied weakly mixing sets of finite order, and constructed an example showing that the concepts of weakly mixing sets of order 2 and of order 3 are different. They generalized this result to general weakly mixing sets of finite order in [84].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d46016e8-d65c-4ab0-960e-6212617389d3": {"__data__": {"id_": "d46016e8-d65c-4ab0-960e-6212617389d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ea119fa-bfc3-48cb-a3ff-45307b729cf3", "node_type": "4", "metadata": {}, "hash": "c932421134d347a11ef6db045d996480986f809560d46afcd03fe4ca7a1cbfa5", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# Theorem 7.10 ([84])\n\nFor every n \u2265 2, there exists a minimal subshift on n symbols such that it contains a perfect weakly mixing set of order n but no non-trivial weakly mixing set of order n + 1.\n\nRecall that it was shown in [6] that if (X, T) is weakly mixing, then for every x \u2208 X, the proximal cell of x is residual in X. In [83] Oprocha and Zhang proved that for every closed weakly mixing set A and every x \u2208 A, the proximal cell of x in A is residual in A. In [63] Li, Oprocha and Zhang showed that the same is true if we consider proximal tuples instead of pairs. First recall that an n-tuple (x1, x2, ..., xn) \u2208 Xn is proximal if\n\nlim inf1 \u2264 i < j \u2264 n d(Tk(xi), Tk(x)) = 0.\n\nFor x \u2208 X, define the n-proximal cell of x as\n\nPn(x) = { (x1, ..., xn-1) \u2208 Xn-1 : (x, x1, ..., xn-1) is proximal }.\n\n# Theorem 7.11 ([63])\n\nLet (X, T) be a dynamical system and A \u2282 X be a weakly mixing set. Then for every x \u2208 A and n \u2265 2, the set Pn(x) \u2229 An-1 is residual in An-1.\n\n# Theorem 7.12 ([63])\n\nLet (X, T) be a dynamical system and A \u2282 X be a weakly mixing set. Then for every n \u2265 2, there exists some \u03b4 > 0 such that for every x \u2208 A, the set LY\u03b4(x) \u2229 An-1 is residual in An-1.\n\n# Theorem 7.13 ([63])\n\nThe following result shows that, when we look only at separation of trajectories of tuples, weak mixing of order 2 is enough to obtain rich structure of such points. Let (X, T) be a dynamical system and A \u2282 X a weakly mixing set of order 2. Then A is a sensitive set in (Orb(A, T), T), where Orb(A, T) = { Tnx : n \u2265 0, x \u2208 A }.\n\nIn particular, the system (Orb(A, T), T) is n-sensitive for every n \u2265 2.\n\n# Theorem 7.14\n\nLet (X, T) be a dynamical system. Then the following conditions are equivalent:\n\n1. (X, T) is weakly mixing;\n2. for any two non-empty open subsets U1, U2 of X, (U1, U2) has an infinite independent set;\n3. for any n \u2208 N and non-empty open subsets U1, U2, ..., Un of X, (U1, U2, ..., Un) has an IP-independent set.\n\n# Definition 7.15\n\nLet (X, T) be a dynamical system and A \u2282 X. Let U1, U2, ..., Un be open subsets of X intersecting A. We say that a non-empty subset F of Z+ is an independence set for (U1, U2, ..., Un) with respect to A, if for every non-empty finite subset J \u2282 F, and s \u2208 {1, 2, ..., n} such that\n\n\u2229j \u2208 J T-j(Us(j))\n\nis a non-empty open subset of X intersecting A.\n\nNow we can employ the introduced notion, to state a theorem analogous to Theorem 7.14.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5776b7b2-d564-4075-9c58-c9053d3cf085": {"__data__": {"id_": "5776b7b2-d564-4075-9c58-c9053d3cf085", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7fc1d3e-28a2-4af0-a9fc-eb41061ea974", "node_type": "4", "metadata": {}, "hash": "58d60c259e4119a4f2b8ce11619da25d557684810bb9cd2f008eb051e310047a", "class_name": "RelatedNodeInfo"}}, "text": "# Theorem 7.16\n\nLet (X, T) be a dynamical system and A a proper subset of X a closed set. Then the following conditions are equivalent:\n\n1. A is a weakly mixing set;\n2. for every n \u2265 2 and every open subsets U1, U2, ..., Un of X intersecting A, there exists t \u2208 N such that [0, t] is an independence set for (U1, U2, ..., Un) with respect to A;\n3. for every n \u2265 2 and every open subsets U1, U2, ..., Un in N such that [0] FS([tj] \u221e of X intersecting A, there exists a sequence [tj] such that j=1 is an independence set for (U1, U2, ..., Un) with respect to A.\n\n# 7.2. Weakly mixing sets via Furstenberg families\n\nIn 2004, Huang, Shao and Ye generalized Theorem 7.1 to F-mixing systems. Let F be a Furstenberg family. Recall that a dynamical system (X, T) is called F-mixing if it is weakly mixing and for any two non-empty open subsets U and V of X, N(U, V) \u2208 F.\n\n# Theorem 7.17\n\nLet (X, T) be a non-trivial dynamical system and F be a Furstenberg family. Then (X, T) is F-mixing if and only if for any S \u2208 \u03baF, there exists a dense Mycielski subset C of X satisfying for any subset D of C and any continuous map f: D \u2192 X, there exists an increasing sequence [qi] in S such that limi\u2192\u221e Tqi(x) = f(x) for all x \u2208 D.\n\nIt is natural that weakly mixing sets can be also generalized via Furstenberg families.\n\n# Definition 7.18\n\nLet (X, T) be a dynamical system and F be a Furstenberg family. Suppose that A is a closed subset of X with at least two points. The set A is said to be F-mixing if for any k \u2208 N, any open subsets U1, U2, ..., Uk, V1, V2, ..., Vk of X intersecting A,\n\n\u2229i=1k N(Ui \u2229 A, V) \u2208 F.\n\nInspired by the proof of Theorem 7.17, we have the following characterization of F-mixing sets.\n\n# Theorem 7.19\n\nLet (X, T) be a dynamical system and F be a Furstenberg family. Suppose that A is a closed subset of X with at least two points. Then A is an F-mixing set if and only if for every S \u2208 \u03baF (the dual family of F) there are Cantor subsets C1 \u2282 C2 \u2282 ... of A such that \u2229n=1\u221e Cn is dense in A;\n\n1. K = n\n2. for any n \u2208 N and any continuous function g: Cn \u2192 A there exists a subsequence [qi] of S such that limi\u2192\u221e Tqi(x) = g(x) uniformly on x \u2208 Cn;\n3. for any subset E of K and any continuous map g: E \u2192 A there exists a subsequence [qi] of S such that limi\u2192\u221e Tqi(x) = g(x) for every x \u2208 E.\n\nIt is shown in [61] that two classes of important dynamical systems have weakly mixing sets via proper Furstenberg families.\n\nLet F be a subset of Z+. The upper Banach density of F is defined by\n\nBD*(F) = limsup |F \u2229 I| / |I| as |I| \u2192 \u221e\n\nwhere I is taken over all non-empty finite intervals of Z+. The family of sets with positive upper Banach density is denoted by Fpubd = {F \u2282 Z+ : BD*(F) > 0}. We say that F is piecewise syndetic if it is the intersection of a thick set and a syndetic set. The family of piecewise syndetic sets is denoted by Fps.\n\n# Theorem 7.20\n\nLet (X, T) be a dynamical system.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d86021b5-204c-44cb-87c3-e67b72ae1fba": {"__data__": {"id_": "d86021b5-204c-44cb-87c3-e67b72ae1fba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "208098e8-ddc5-47ce-8eea-ce740bce78c4", "node_type": "4", "metadata": {}, "hash": "5b49e9462069692dbf96691617b56cbeb2aa15a68e8f3970aa8cbbee511ca005", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# 8. CHAOS IN THE INDUCED SPACES\n\nA dynamical system (X, T) induces two natural systems, one is (K(X), TK) on the hyperspace K(X) consisting of all closed non-empty subsets of X with the Hausdorff metric, and the other one is (M(X), TM) on the probability space M(X) consisting of all Borel probability measures with the weak*-topology. Bauer and Sigmund [13] first gave a systematic investigation on the connection of dynamical properties among (X, T), (K(X), TK) and (M(X), TM). It was proved that (X, T) is weakly mixing (resp. mildly mixing, strongly mixing) if and only if (K(X), TK) (resp. (M(X), TM)) has the same property. This leads to a natural question:\n\n# Problem 6\n\nIf one of the dynamical systems (X, T), (K(X), TK) and (M(X), TM) is chaotic in some sense, how about the other two systems?\n\nThis question attracted a lot of attention, see, e.g., [87, 12, 36] and references therein, and many partial answers were obtained. We first show that when the induced system is weakly mixing.\n\n# Theorem 8.1 ([13, 12])\n\nLet (X, T) be a dynamical system. Then (X, T) is weakly mixing if and only if (K(X), TK) is weakly mixing if and only if (K(X), TK) is transitive.\n\n# Theorem 8.2 ([13, 66])\n\nLet (X, T) be a dynamical system. Then (X, T) is weakly mixing if and only if (M(X), TM) is weakly mixing if and only if (M(X), TM) is transitive.\n\nIn [30], Glasner and Weiss studied the topological entropy of (K(X), TK) and (M(X), TM). They proved that\n\n# Theorem 8.3\n\nLet (X, T) be a dynamical system.\n\n1. The topological entropy of (X, T) is zero if and only if the one of (M(X), TM) is also zero, and the topological entropy of (X, T) is positive if and only if the one of (M(X), TM) is infinite.\n2. If the topological entropy of (X, T) is positive, then the topological entropy of (K(X), TK) is infinite, while there exists a minimal system (X, T) of zero topological entropy and (K(X), TK) with positive topological entropy.\n\nTo show that when the dynamical system on the hyperspace is Devaney chaotic, we need to introduce some concepts firstly. We say that a dynamical system (X, T) has dense small periodic sets [48] if for any non-empty open subset U of X there exists a non-empty closed subset Y of U and k element N such that TkY \u2282 Y. Clearly, if a dynamical system has a dense set of periodic points, then it also has dense small periodic sets. The dynamical system (X, T) is called an HY-system if it is totally transitive and has dense small periodic sets. Note that there exists an HY-system without periodic points (see [48, Example 3.7]). Recently, Li showed in [60] that (K(X), TK) is Devaney chaotic is equivalent to the origin system (X, T) is an HY-system.\n\n# Theorem 8.4 ([60])\n\nLet (X, T) be a dynamical system with X being infinite. Then the following conditions are equivalent:\n\n1. (K(X), TK) is Devaney chaotic;\n2. (K(X), TK) is an HY-system;\n3. (X, T) is an HY-system.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8554ee82-8217-404b-b457-4f920c8a5ca6": {"__data__": {"id_": "8554ee82-8217-404b-b457-4f920c8a5ca6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8948cff9-9944-47d4-84fb-00fa4314d57e", "node_type": "4", "metadata": {}, "hash": "e98d446f03925d9d25db3a5f8cbef5514151c39b863a5c5ccaaa6674e134347d", "class_name": "RelatedNodeInfo"}}, "text": "# 26\n\n# J. LI AND X. YE\n\nIn order to characterize Devaney chaos on the space of probability measures, we need a notion of an almost HY-system. We say that (X, T) has almost dense periodic sets if for each non-empty open subset U \u2282 X and \u03b5 > 0, there are k \u2208 N and \u00b5 \u2208 M(X) with T^k \u00b5 = \u00b5 such that \u00b5(Uc) < \u03b5, where Uc = {x \u2208 X : x \u2209 U}. We say that (X, T) is an almost HY-system if it is totally transitive and has almost dense periodic sets.\n\n# Theorem 8.5\n\n([66]). Let (X, T) be a dynamical system with X being infinite. Then the following conditions are equivalent:\n\n1. (M(X), TM) is Devaney chaotic;\n2. (M(X), TM) is an almost HY-system;\n3. (X, T) is an almost HY-system.\n\nIt is clear that every HY-system is also an almost HY-system. There is a non-trivial minimal weakly mixing almost-HY-system (see [66, Theorem 4.11]), which is not an HY-system, since every minimal HY-system is trivial.\n\nRecall that a dynamical system (X, T) is proximal if any pair (x, y) \u2208 X2 is proximal. The following proposition shows that if (K(X), TK) is proximal then (X, T) is \u201calmost\u201d trivial.\n\n# Proposition 8.6\n\n([64]). Let (X, T) be a dynamical system. Then the following conditions are equivalent:\n\n1. (K(X), TK) is proximal;\n2. \u2229n=1\u221e TnX is a singleton;\n3. X is a uniformly proximal set.\n\nIf (M(X), TM) is proximal, then (X, T) is called strongly proximal [33]. Note that if (X, T) is strongly proximal, then it is proximal, since (X, T) can be regarded as a subsystem of the proximal system (M(X), TM). We have the following characterization of strongly proximal systems.\n\n# Theorem 8.7\n\n([64]). Let (X, T) be a dynamical system. Then the following conditions are equivalent:\n\n1. (X, T) is strongly proximal;\n2. (X, T) is proximal and unique ergodic;\n3. every pair (x, y) \u2208 X2 is Banach proximal.\n\n# Acknowledgement\n\nThe authors would like to thank Wen Huang, Jie Li and Guohua Zhang for the careful reading, and thank the anonymous reviewer for his/her valuable comments and suggestions. The first author is partially supported by NNSF of China (11401362, 11471125) and NSF of Guangdong province (S2013040014084). The second author is partially supported by NNSF of China (11371339, 11431012).\n\n# REFERENCES\n\n1. R. L. Adler, A. G. Konheim, and M. H. McAndrew, Topological entropy, Trans. Amer. Math. Soc. 114 (1965), 309\u2013319. MR0175106\n2. Ethan Akin, Recurrence in topological dynamics: Furstenberg families and Ellis actions, The University Series in Mathematics, Plenum Press, New York, 1997. MR1467479\n3. Ethan Akin, Joseph Auslander, and Kenneth Berg, When is a transitive map chaotic?, Convergence in ergodic theory and probability (Columbus, OH, 1993), Ohio State Univ. Math. Res. Inst. Publ., vol. 5, de Gruyter, Berlin, 1996, pp. 25\u201340. MR1412595\n4. Ethan Akin and Eli Glasner, Residual properties and almost equicontinuity, J. Anal. Math. 84 (2001), 243\u2013286. MR1849204", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d62037c9-99f1-4bf4-8e2d-8cadf9e24324": {"__data__": {"id_": "d62037c9-99f1-4bf4-8e2d-8cadf9e24324", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "963c113d-3f6f-479f-a228-aa9db5f64747", "node_type": "4", "metadata": {}, "hash": "bb73e5e3aeba89fb9c5d599baeec232fc346109ecffc4caa979a50f4878719c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4457a6d8-e566-405c-adeb-9afb91054a3d", "node_type": "1", "metadata": {}, "hash": "708a46f01bc47f519790f31d9beff218d118dc0e2e72a1284344fb76940241ca", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n1. Ethan Akin, Eli Glasner, Wen Huang, Song Shao, and Xiangdong Ye, Sufficient conditions under which a transitive system is chaotic, Ergodic Theory Dynam. Systems 30 (2010), no. 5, 1277\u20131310. MR2718894\n2. Ethan Akin and Sergibrevedotlessi Kolyada, Li-Yorke sensitivity, Nonlinearity 16 (2003), no. 4, 1421\u20131433. MR1986303\n3. Joseph Auslander, Minimal flows and their extensions, North-Holland Mathematics Studies, vol. 153, North-Holland Publishing Co., Amsterdam, 1988. MR956049\n4. Joseph Auslander and James A. Yorke, Interval maps, factors of maps, and chaos, Tcircumflexohoku Math. J. (2) 32 (1980), no. 2, 177\u2013188. MR580273\n5. Francisco Balibrea, Juan L. G. Guirao, and Piotr Oprocha, On invariant\u03b5-scrambled sets, Internat. J. Bifur. Chaos Appl. Sci. Engrg. 20 (2010), no. 9, 2925\u20132935. MR2738744\n6. Francisco Balibrea, J. Sm\u00b4dotlessital, and M. caronStef\u00b4ankov\u00b4 The three versions of distributional chaos, Chaosa, Solitons Fractals 23 (2005), no. 5, 1581\u20131583. MR2101573\n7. J. Banks, J. Brooks, G. Cairns, G. Davis, and P. Stacey, On Devaney\u2019s definition of chaos, Amer. Math. Monthly 99 (1992), no. 4, 332\u2013334. MR1157223\n8. John Banks, Chaos for induced hyperspace maps, Chaos Solitons Fractals 25 (2005), no. 3, 681\u2013685. MR2132366\n9. Walter Bauer and Karl Sigmund, Topological dynamics of transformations induced on the space of probability measures, Monatsh. Math. 79 (1975), 81\u201392. MR0370540\n10. Franccedillaois Blanchard, Topological chaos: what may this mean?, J. Difference Equ. Appl. 15 (2009), no. 1, 23\u201346. MR2484415\n11. Franccedillaois Blanchard, Eli Glasner, Sergibrevedotlessi Kolyada, and Alejandro Maass, On Li-Yorke pairs, J. Reine Angew. Math. 547 (2002), 51\u201368. MR1900136\n12. Franccedillaois Blanchard, B. Host, and S. Ruette, Asymptotic pairs in positive-entropy systems, Ergodic Theory Dynam. Systems 22 (2002), no. 3, 671\u2013686. MR1908549\n13. Franccedillaois Blanchard and Wen Huang, Entropy sets, weakly mixing sets and entropy capacity, Discrete Contin. Dyn. Syst. 20 (2008), no. 2, 275\u2013311. MR2358261\n14. Franccedillaois Blanchard, Wen Huang, and L\u2019ubom\u00b4dotlessir Snoha, Topological size of scrambled sets, Colloq. Math. 110 (2008), no. 2, 293\u2013361. MR2353910\n15. Robert L. Devaney, An introduction to chaotic dynamical systems, second ed., Addison-Wesley Studies in Nonlinearity, Addison-Wesley Publishing Company Advanced Book Program, Redwood City, CA, 1989. MR1046376\n16. Jana Dolecaronzelov\u00b4 Scrambled and distributionally scrambled n-tuples, J. Difference Equ. Appl. 20a, (2014), no. 8, 1169\u20131177. MR3216892\n17. Tomasz Downarowicz, Positive topological entropy implies chaos DC2, Proc. Amer. Math. Soc. 142 (2014), no. 1, 137\u2013149. MR3119189\n18.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4457a6d8-e566-405c-adeb-9afb91054a3d": {"__data__": {"id_": "4457a6d8-e566-405c-adeb-9afb91054a3d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "963c113d-3f6f-479f-a228-aa9db5f64747", "node_type": "4", "metadata": {}, "hash": "bb73e5e3aeba89fb9c5d599baeec232fc346109ecffc4caa979a50f4878719c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d62037c9-99f1-4bf4-8e2d-8cadf9e24324", "node_type": "1", "metadata": {}, "hash": "506b39824c90099ef5d83001b79e2e67692efec78577b74e5e23edd833e8dc60", "class_name": "RelatedNodeInfo"}}, "text": "Math. 110 (2008), no. 2, 293\u2013361. MR2353910\n15. Robert L. Devaney, An introduction to chaotic dynamical systems, second ed., Addison-Wesley Studies in Nonlinearity, Addison-Wesley Publishing Company Advanced Book Program, Redwood City, CA, 1989. MR1046376\n16. Jana Dolecaronzelov\u00b4 Scrambled and distributionally scrambled n-tuples, J. Difference Equ. Appl. 20a, (2014), no. 8, 1169\u20131177. MR3216892\n17. Tomasz Downarowicz, Positive topological entropy implies chaos DC2, Proc. Amer. Math. Soc. 142 (2014), no. 1, 137\u2013149. MR3119189\n18. Bau-Sen Du, On the invariance of Li-Yorke chaos of interval maps, J. Difference Equ. Appl. 11 (2005), no. 9, 823\u2013828. MR2159799\n19. S. Fomin, On dynamical systems with a purely point spectrum, Doklady Akad. Nauk SSSR (N.S.) 77 (1951), 29\u201332. MR0043397\n20. Magdalena Fory\u00b4s, Wen Huang, Jian Li, and Piotr Oprocha, Invariant scrambled sets, uniform rigidity and weak mixing, to appear in Israel J. Math., arXiv:1410.7118.\n21. Harry Furstenberg, Disjointness in ergodic theory, minimal sets, and a problem in Diophantine approximation, Math. Systems Theory 1 (1967), 1\u201349. MR0213508\n22. Recurrence in ergodic theory and combinatorial number theory, Princeton University Press, Princeton, N.J., 1981, M. B. Porter Lectures. MR603625\n23. Felipe Garc\u00b4dotlessia-Ramos, Weak forms of topological and measure theoretical equicontinuity: relationships with discrete spectrum and sequence entropy, preprint, arXiv:1402.7327.\n24. Eli Glasner, Ergodic theory via joinings, Mathematical Surveys and Monographs, vol. 101, American Mathematical Society, Providence, RI, 2003. MR1958753\n25. Eli Glasner and Benjamin Weiss, Sensitive dependence on initial conditions, Nonlinearity 6 (1993), no. 6, 1067\u20131075. MR1251259\n26. Quasi-factors of zero-entropy systems, J. Amer. Math. Soc. 8 (1995), no. 3, 665\u2013686. MR1270579\n27. Eli Glasner and Xiangdong Ye, Local entropy theory, Ergodic Theory Dynam. Systems 29 (2009), no. 2, 321\u2013356. MR2486773\n28. S. Glasner and D. Maon, Rigidity in topological dynamics, Ergodic Theory Dynam. Systems 9 (1989), no. 2, 309\u2013320. MR1007412", "mimetype": "text/plain", "start_char_idx": 2207, "end_char_idx": 4292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1e43c09-f40c-4298-91b8-1ce43da10abe": {"__data__": {"id_": "a1e43c09-f40c-4298-91b8-1ce43da10abe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64d5847e-857a-4d86-bcb2-e1911a55defe", "node_type": "4", "metadata": {}, "hash": "f7176fa122c123b047261ac4299d43c744db6ef2658074e5fe51635ecd363a8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ba667f0-6b0d-401b-b146-9e8c59fbdcb7", "node_type": "1", "metadata": {}, "hash": "09222043bb7e69b3464eaf48dd6ff0fe32033112c711ca86e67d2301f9d48a27", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Shmuel Glasner, Proximal flows, Lecture Notes in Mathematics, Vol. 517, Springer-Verlag, Berlin-New York, 1976. MR0474243\n2. Walter Helbig Gottschalk and Gustav Arnold Hedlund, Topological dynamics, American Mathematical Society Colloquium Publications, Vol. 36, American Mathematical Society, Providence, R. I., 1955. MR0074810\n3. John Guckenheimer, Sensitive dependence to initial conditions for one-dimensional maps, Comm. Math. Phys. 70 (1979), no. 2, 133\u2013160. MR553966\n4. Juan Luis Garc\u00b4dotlessia Guirao, Dominik Kwietniak, Marek Lampart, Piotr Oprocha, and Alfredo Peris, Chaos on hyperspaces, Nonlinear Anal. 71 (2009), no. 1-2, 1\u20138. MR2518006\n5. Wen Huang, Stable sets and \u03b5-stable sets in positive-entropy systems, Comm. Math. Phys. 279 (2008), no. 2, 535\u2013557. MR2383598\n6. Wen Huang, Sergibrevedotlessi Kolyada, and Guohua Zhang, Multi-sensitivity, Lyapunov numbers and almost automorphic maps, preprint, 2014.\n7. Wen Huang, Hanfeng Li, and Xiangdong Ye, Family independence for topological and measurable dynamics, Trans. Amer. Math. Soc. 364 (2012), no. 10, 5209\u20135242. MR2931327\n8. Wen Huang, Jian Li, and Xiangdong Ye, Stable sets and mean Li\u2013Yorke chaos in positive entropy systems, J. Funct. Anal. 266 (2014), no. 6, 3377\u20133394. MR3165229\n9. Wen Huang, Jian Li, Xiangdong Ye, and Xiaoyao Zhou, Topological entropy and diagonal-weakly mixing sets, preprint, 2014.\n10. Wen Huang, Ping Lu, and Xiangdong Ye, Measure-theoretical sensitivity and equicontinuity, Israel J. Math. 183 (2011), 233\u2013283. MR2811160\n11. Wen Huang, Song Shao, and Xiangdong Ye, Mixing and proximal cells along sequences, Nonlinearity 17 (2004), no. 4, 1245\u20131260. MR2069703\n12. Wen Huang, Leiye Xu, and Yingfei Yi, Asymptotic pairs, stable sets and chaos in positive entropy systems, J. Funct. Anal. 268 (2015), no. 4, 824\u2013846. MR3296581\n13. Wen Huang and Xiangdong Ye, Homeomorphisms with the whole compacta being scrambled sets, Ergodic Theory Dynam. Systems 21 (2001), no. 1, 77\u201391. MR1826661\n14. Devaney\u2019s chaos or 2-scattering implies Li-Yorke\u2019s chaos, Topology Appl. 117 (2002), no. 3, 259\u2013272. MR1874089\n15. Topological complexity, return times and weak disjointness, Ergodic Theory Dynam. Systems 24 (2004), no. 3, 825\u2013846. MR2062921\n16. Dynamical systems disjoint from any minimal system, Trans. Amer. Math. Soc. 357 (2005), no. 2, 669\u2013694. MR2095626\n17. A local variational relation and applications, Israel J. Math. 151 (2006), 237\u2013279. MR2214126\n18. Anzelm Iwanik, Independence and scrambled sets for chaotic mappings, The mathematical heritage of C. F. Gauss, World Sci. Publ., River Edge, NJ, 1991, pp. 372\u2013378. MR1146241\n19. Katar\u00b4dotlessina Jankov\u00b4a and Jaroslav Sm\u00b4dotlessital, A characterization of chaos, Bull. Austral. Math. Soc. 34 (1986), no. 2, 283\u2013292. MR854575\n20.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ba667f0-6b0d-401b-b146-9e8c59fbdcb7": {"__data__": {"id_": "9ba667f0-6b0d-401b-b146-9e8c59fbdcb7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64d5847e-857a-4d86-bcb2-e1911a55defe", "node_type": "4", "metadata": {}, "hash": "f7176fa122c123b047261ac4299d43c744db6ef2658074e5fe51635ecd363a8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1e43c09-f40c-4298-91b8-1ce43da10abe", "node_type": "1", "metadata": {}, "hash": "9790bb154835ad6020a508d12ac18efb18d35aeadaa1e83dd23771bfac83b5d1", "class_name": "RelatedNodeInfo"}}, "text": "Amer. Math. Soc. 357 (2005), no. 2, 669\u2013694. MR2095626\n17. A local variational relation and applications, Israel J. Math. 151 (2006), 237\u2013279. MR2214126\n18. Anzelm Iwanik, Independence and scrambled sets for chaotic mappings, The mathematical heritage of C. F. Gauss, World Sci. Publ., River Edge, NJ, 1991, pp. 372\u2013378. MR1146241\n19. Katar\u00b4dotlessina Jankov\u00b4a and Jaroslav Sm\u00b4dotlessital, A characterization of chaos, Bull. Austral. Math. Soc. 34 (1986), no. 2, 283\u2013292. MR854575\n20. Yitzhak Katznelson and Benjamin Weiss, When all points are recurrent/generic, Ergodic theory and dynamical systems, I (College Park, Md., 1979\u201380), Progr. Math., vol. 10, Birkhdieresisauser Boston, Mass., 1981, pp. 195\u2013210. MR633765\n21. David Kerr and Hanfeng Li, Independence in topological and Casteriskmath-dynamics, Math. Ann. 338 (2007), no. 4, 869\u2013926. MR2317754\n22. Combinatorial independence and sofic entropy, Commun. Math. Stat. 1 (2013), no. 2, 213\u2013257. MR3197860\n23. Harvey B. Keynes and James B. Robertson, Eigenvalue theorems in topological transformation groups, Trans. Amer. Math. Soc. 139 (1969), 359\u2013369. MR0237748\n24. Sergibrevedotlessi Kolyada, Li-Yorke sensitivity and other concepts of chaos, Ukradieresisdotlessin. Mat. Zh. 56 (2004), no. 8, 1043\u20131061. MR2136308\n25. Sergibrevedotlessi Kolyada and Lubom\u00b4dotlessir Snoha, Some aspects of topological transitivity\u2014a survey, Iteration theory (ECIT 94) (Opava), Grazer Math. Ber., vol. 334, Karl-Franzens-Univ. Graz, Graz, 1997, pp. 3\u201335. MR1644768\n26. Jian Li, Chaos and entropy for interval maps, J. Dynam. Differential Equations 23 (2011), no. 2, 333\u2013352. MR2802890\n27. Transitive points via Furstenberg family, Topology Appl. 158 (2011), no. 16, 2221\u20132231. MR2831911", "mimetype": "text/plain", "start_char_idx": 2305, "end_char_idx": 4029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79927d35-332d-4a20-80dd-a7e950bcceca": {"__data__": {"id_": "79927d35-332d-4a20-80dd-a7e950bcceca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2990194-8cef-40f0-9992-c5c6069dad44", "node_type": "4", "metadata": {}, "hash": "836214770fd5870f64ca197a00473fe772b34813342abddfd4fae206ec7432d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e601421-6d25-4f5f-8e26-ef68f1769e04", "node_type": "1", "metadata": {}, "hash": "4f3077ab78d097b36ba333c66f91018acf1e416faee9eb4d9205a09995708ab8", "class_name": "RelatedNodeInfo"}}, "text": "# RECENT DEVELOPMENT OF CHAOS THEORY IN TOPOLOGICAL DYNAMICS\n\n# References\n\n1. Jian Li, Equivalent conditions of Devaney chaos on the hyperspace, J. Univ. Sci. Technol. China 44 (2014), no. 2, 93\u201395, 111. MR3236061\n2. Jian Li, Localization of mixing property via Furstenberg families, Discrete Contin. Dyn. Syst. 35 (2015), no. 2, 725\u2013740. MR3267420\n3. Jian Li and Piotr Oprocha, On n-scrambled tuples and distributional chaos in a sequence, J. Difference Equ. Appl. 19 (2013), no. 6, 927\u2013941. MR3173461\n4. Jian Li, Piotr Oprocha, and Guohua Zhang, On recurrence over subsets and weak mixing, to appear in Pac. J. Math.\n5. Jian Li and Siming Tu, On proximality with Banach density one, J. Math. Anal. Appl. 416 (2014), no. 1, 36\u201351. MR3182747\n6. Jian Li, Siming Tu, and Xiangdong Ye, Mean equicontinuity and mean sensitivity, to appear in Ergodic Theory and Dynamical Systems, arXiv:1312.7663.\n7. Jie Li, Kesong Yan, and Xiangdong Ye, Recurrence properties and disjointness on the induced spaces, Discrete Contin. Dyn. Syst. 35 (2015), no. 3, 1059\u20131073. MR3277185\n8. Risong Li and Yuming Shi, Stronger forms of sensitivity for measure-preserving maps and semiflows on probability spaces, Abstr. Appl. Anal. (2014), Art. ID 769523, 10 pages. MR3208565\n9. ShiHai Li, \u03c9-chaos and topological entropy, Trans. Amer. Math. Soc. 339 (1993), no. 1, 243\u2013249. MR1108612\n10. TienYien Li and James A. Yorke, Period three implies chaos, Amer. Math. Monthly 82 (1975), no. 10, 985\u2013992. MR0385028\n11. Gongfu Liao and Qinjie Fan, Minimal subshifts which display Schweizer-Smacutedotlessital chaos and have zero topological entropy, Sci. China Ser. A 41 (1998), no. 1, 33\u201338. MR1612875\n12. Heng Liu, Li Liao, and Lidong Wang, Thickly syndetical sensitivity of topological dynamical system, Discrete Dyn. Nat. Soc. (2014), Art. ID 583431, 4 pages. MR3200824\n13. Edward N. Lorenz, Deterministic nonperiodic flow, Journal of Atmospheric Sciences 20 (1963), no. 2, 130\u2013148.\n14. Jiehua Mai, Continuous maps with the whole space being a scrambled set, Chinese Sci. Bull. 42 (1997), no. 19, 1603\u20131606. MR1641013\n15. Jiehua Mai, The structure of equicontinuous maps, Trans. Amer. Math. Soc. 355 (2003), no. 10, 4125\u20134136. MR1990578\n16. Jiehua Mai, Devaney\u2019s chaos implies existence of s-scrambled sets, Proc. Amer. Math. Soc. 132 (2004), no. 9, 2761\u20132767. MR2054803\n17. T. K. Subrahmonian Moothathu, Stronger forms of sensitivity for dynamical systems, Nonlinearity 20 (2007), no. 9, 2115\u20132126. MR2351026\n18. T. K. Subrahmonian Moothathu, Syndetically proximal pairs, J. Math. Anal. Appl. 379 (2011), no. 2, 656\u2013663. MR2784349\n19.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e601421-6d25-4f5f-8e26-ef68f1769e04": {"__data__": {"id_": "3e601421-6d25-4f5f-8e26-ef68f1769e04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2990194-8cef-40f0-9992-c5c6069dad44", "node_type": "4", "metadata": {}, "hash": "836214770fd5870f64ca197a00473fe772b34813342abddfd4fae206ec7432d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79927d35-332d-4a20-80dd-a7e950bcceca", "node_type": "1", "metadata": {}, "hash": "f4f8da5b6638ebff63da77b324999e11c068dbc787d245e6b3a2a76244bd1652", "class_name": "RelatedNodeInfo"}}, "text": "Math. Soc. 355 (2003), no. 10, 4125\u20134136. MR1990578\n16. Jiehua Mai, Devaney\u2019s chaos implies existence of s-scrambled sets, Proc. Amer. Math. Soc. 132 (2004), no. 9, 2761\u20132767. MR2054803\n17. T. K. Subrahmonian Moothathu, Stronger forms of sensitivity for dynamical systems, Nonlinearity 20 (2007), no. 9, 2115\u20132126. MR2351026\n18. T. K. Subrahmonian Moothathu, Syndetically proximal pairs, J. Math. Anal. Appl. 379 (2011), no. 2, 656\u2013663. MR2784349\n19. T. K. Subrahmonian Moothathu and Piotr Oprocha, Syndetic proximality and scrambled sets, Topol. Methods Nonlinear Anal. 41 (2013), no. 2, 421\u2013461. MR3114317\n20. Jan Mycielski, Independent sets in topological algebras, Fund. Math. 55 (1964), 139\u2013147. MR0173645\n21. Piotr Oprocha, Relations between distributional and Devaney chaos, Chaos 16 (2006), no. 3, 033112, 5. MR2265261\n22. Piotr Oprocha, Minimal systems and distributionally scrambled sets, Bull. Soc. Math. France 140 (2012), no. 3, 401\u2013439. MR3059120\n23. Piotr Oprocha and Guohua Zhang, On local aspects of topological weak mixing in dimension one and beyond, Studia Math. 202 (2011), no. 3, 261\u2013288. MR2771654\n24. Piotr Oprocha, On weak product recurrence and synchronization of return times, Adv. Math. 244 (2013), 395\u2013412. MR3077877\n25. Piotr Oprocha, On local aspects of topological weak mixing, sequence entropy and chaos, Ergodic Theory Dynam. Systems 34 (2014), no. 5, 1615\u20131639. MR3255435\n26. Piotr Oprocha, Topological aspects of dynamics of pairs, tuples and sets, Recent progress in general topology. III, Atlantis Press, Paris, 2014, pp. 665\u2013709. MR3205496\n27. Rafalslash Pikulslasha, On some notions of chaos in dimension zero, Colloq. Math. 107 (2007), no. 2, 167\u2013177. MR2284159\n28. Heriberto Rom\u00e1n-Flores, A note on transitivity in set-valued discrete systems, Chaos Solitons Fractals 17 (2003), no. 1, 99\u2013104. MR1960765\n29. David Ruelle, Dynamical systems with turbulent behavior, Mathematical problems in theoretical physics (Proc. Internat. Conf., Univ. Rome, Rome, 1977), Lecture Notes in Phys., vol. 80, Springer, Berlin, 1978, pp. 341\u2013360. MR518445", "mimetype": "text/plain", "start_char_idx": 2155, "end_char_idx": 4234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "975de8f5-d5f4-469d-acd2-50c34de9b1fa": {"__data__": {"id_": "975de8f5-d5f4-469d-acd2-50c34de9b1fa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ec05c59-a930-4469-b083-26ba92bfdd93", "node_type": "4", "metadata": {}, "hash": "234e43a7b5db82241d929b4d99cf95a3af269fe821452ed30032a604dc4db640", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c75a001f-0c52-477d-a08d-88042578ac06", "node_type": "1", "metadata": {}, "hash": "8614b4ba37f7ef7ac8de77b05e5998374a02797bd1f282f9fc20ee470fab79fe", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. David Ruelle and Floris Takens, On the nature of turbulence, Comm. Math. Phys. 20 (1971), 167\u2013192. MR0284067\n2. Sylvie Ruette, Transitive sensitive subsystems for interval maps, Studia Math. 169 (2005), no. 1, 81\u2013104. MR2139644\n3. Bruno Scarpellini, Stability properties of flows with pure point spectrum, J. London Math. Soc. (2) 26 (1982), no. 3, 451\u2013464. MR684559\n4. Berthold Schweizer and Jaroslav Sm\u00b4dotlessital, Measures of chaos and a spectral decomposition of dynamical systems on the interval, Trans. Amer. Math. Soc. 344 (1994), no. 2, 737\u2013754. MR1227094\n5. Song Shao, Proximity and distality via Furstenberg families, Topology Appl. 153 (2006), no. 12, 2055\u20132072. MR2237597\n6. Song Shao, Xiangdong Ye, and Ruifeng Zhang, Sensitivity and regionally proximal relation in minimal systems, Sci. China Ser. A 51 (2008), no. 6, 987\u2013994. MR2410978\n7. Jaroslav Sm\u00b4dotlessital, A chaotic function with some extremal properties, Proc. Amer. Math. Soc. 87 (1983), no. 1, 54\u201356. MR677230\n8. Jaroslav Sm\u00b4dotlessital, Chaotic functions with zero topological entropy, Trans. Amer. Math. Soc. 297 (1986), no. 1, 269\u2013282. MR849479\n9. Jaroslav Sm\u00b4dotlessital, Topological entropy and distributional chaos, Real Anal. Exchange (2006), no. 30th Summer Symposium Conference, 61\u201365. MR2323823\n10. Jaroslav Sm\u00b4dotlessital and Marta caronStef\u00b4ankov, Distributional chaos for triangular maps, Chaos Solitons Fractals 21 (2004), no. 5, 1125\u20131128. MR2047330\n11. L\u2019ubom\u00b4dotlessir Snoha, Generic chaos, Comment. Math. Univ. Carolin. 31 (1990), no. 4, 793\u2013810. MR1091377\n12. Feng Tan and Heman Fu, On distributional n-chaos, Acta Math. Sci. Ser. B Engl. Ed. 34 (2014), no. 5, 1473\u20131480. MR3244574\n13. Feng Tan and Jincheng Xiong, Chaos via Furstenberg family couple, Topology Appl. 156 (2009), no. 3, 525\u2013532. MR2492300\n14. Feng Tan and Ruifeng Zhang, On F-sensitive pairs, Acta Math. Sci. Ser. B Engl. Ed. 31 (2011), no. 4, 1425\u20131435. MR2856453\n15. Peter Walters, An introduction to ergodic theory, Graduate Texts in Mathematics, vol. 79, Springer-Verlag, New York, 1982. MR648108\n16. Huoyun Wang, Jincheng Xiong, and Feng Tan, Furstenberg families and sensitivity, Discrete Dyn. Nat. Soc. (2010), Art. ID 649348, 12 pages. MR2600324\n17. Stephen Wiggins, Introduction to applied nonlinear dynamical systems and chaos, Texts in Applied Mathematics, vol. 2, Springer-Verlag, New York, 1990. MR1056699\n18. Jincheng Xiong, A chaotic map with topological entropy, Acta Math. Sci. (English Ed.) 6 (1986), no. 4, 439\u2013443. MR924033\n19. Jincheng Xiong, Chaos in a topologically transitive system, Sci. China Ser. A 48 (2005), no. 7, 929\u2013939. MR2179632\n20.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2645, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c75a001f-0c52-477d-a08d-88042578ac06": {"__data__": {"id_": "c75a001f-0c52-477d-a08d-88042578ac06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ec05c59-a930-4469-b083-26ba92bfdd93", "node_type": "4", "metadata": {}, "hash": "234e43a7b5db82241d929b4d99cf95a3af269fe821452ed30032a604dc4db640", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "975de8f5-d5f4-469d-acd2-50c34de9b1fa", "node_type": "1", "metadata": {}, "hash": "94575e81e1d9c56c376550ae0747f97b66c6286ae9ab743116df6ea0441ab9b0", "class_name": "RelatedNodeInfo"}}, "text": "Nat. Soc. (2010), Art. ID 649348, 12 pages. MR2600324\n17. Stephen Wiggins, Introduction to applied nonlinear dynamical systems and chaos, Texts in Applied Mathematics, vol. 2, Springer-Verlag, New York, 1990. MR1056699\n18. Jincheng Xiong, A chaotic map with topological entropy, Acta Math. Sci. (English Ed.) 6 (1986), no. 4, 439\u2013443. MR924033\n19. Jincheng Xiong, Chaos in a topologically transitive system, Sci. China Ser. A 48 (2005), no. 7, 929\u2013939. MR2179632\n20. Jincheng Xiong, Jie Ldieresisu, and Feng Tan, Furstenberg family and chaos, Sci. China Ser. A 50 (2007), no. 9, 1325\u20131333. MR2370619\n21. Jincheng Xiong and Zhongguo Yang, Chaos caused by a topologically mixing map, Dynamical systems and related topics (Nagoya, 1990), Adv. Ser. Dynam. Systems, vol. 9, World Sci. Publ., River Edge, NJ, 1991, pp. 550\u2013572. MR1164918\n22. Xiangdong Ye and Tao Yu, Sensitivity, proximal extension and higher order almost automorphy, preprint, 2014.\n23. Xiangdong Ye and Ruifeng Zhang, On sensitive sets in topological dynamics, Nonlinearity 21 (2008), no. 7, 1601\u20131620. MR2425336\n24. Dalian Yuan and Jie Ldieresis, Invariant scrambled sets in transitive systems, Adv. Math. (China) 38 (2009), no. 3, 302\u2013308. MR2561797\n\n# Authors\n\n(J. Li) DEPARTMENT OF MATHEMATICS, SHANTOU UNIVERSITY, SHANTOU, GUANGDONG, 515063, P.R. CHINA\n\nE-mail address: lijian09@mail.ustc.edu.cn\n\n(X. Ye) WU WEN-TSUN KEY LABORATORY OF MATHEMATICS, USTC, CHINESE ACADEMY OF SCIENCES AND SCHOOL OF MATHEMATICS, UNIVERSITY OF SCIENCE AND TECHNOLOGY OF CHINA, HEFEI, ANHUI, 230026, P.R. CHINA\n\nE-mail address: yexd@ustc.edu.cn", "mimetype": "text/plain", "start_char_idx": 2179, "end_char_idx": 3769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18f16d88-133c-442b-8c75-1af9d0ac897c": {"__data__": {"id_": "18f16d88-133c-442b-8c75-1af9d0ac897c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "468a45b4-6bf6-4eff-9d4a-907649c18f1b", "node_type": "4", "metadata": {}, "hash": "af195d51e838e72cd789b24fb798754902d00b0f8a79ccdc067e4ff3cacfcb63", "class_name": "RelatedNodeInfo"}}, "text": "# Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models\n\n# Optimizing RAG Techniques Based on Locally Deployed Ollama Models\n\n# Fei Liu *\n\nChina Automotive Technology & Research Center, liufei@catarc.ac.cn\n\n# Zejun Kang\n\nChina Automotive Technology & Research Center, kangzejun@catarc.ac.cn\n\n# Xing Han\n\nChina Automotive Technology & Research Center, hanxing@catarc.ac.cn\n\nWith the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using locally deployed Ollama models.\n\nBased on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding pipelines and an agent supporting self-RAG based on LangGraph best practices.\n\nTo evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive industry dataset.\n\nOur optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing information processing and intelligent production in the automotive industry.\n\n* Place the footnote text for the author (if applicable) here.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3cb3843-2523-4991-b7d1-9b56e326719e": {"__data__": {"id_": "a3cb3843-2523-4991-b7d1-9b56e326719e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "188a4754-1d4a-4a12-b66c-99fc4078f207", "node_type": "4", "metadata": {}, "hash": "47871a203f97532683f2e21fe7b1ed521dd12bf90b0cd6d2092e52d6c5771dd7", "class_name": "RelatedNodeInfo"}}, "text": "# CCS CONCEPTS\n\n\u2022 Computing methodologies \u2022 Artificial intelligence \u2022 Natural language processing \u2022 Natural language generation\n\n# Additional Keywords and Phrases\n\nAutomotive Industry, Langchain, self0rag, 123 1rocessing, RA4, 5llama\n\n# 1 INTRODUCTION\n\n# 1.1 Research Background\n\nThe automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex technical documentation for various processes 71]. This shift encompasses design, manufacturing, and quality control, all of which now heavily depend on efficient information management systems 72]. The growing volume of technical documents, often in 123 format, has created a pressing need for advanced information retrieval and question0answering capabilities in industrial settings 73].\n\nLarge Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating remarkable abilities in tasks such as document understanding and question answering 74]. These models have shown potential in handling the complex, domain0specific language often found in automotive documentation. However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of computational resources and data privacy 75].\n\nAmong the various techniques developed to enhance LLM performance, Retrieval0Augmented 4eneration (RA4) has gained significant attention 76]. RA4 combines the generative capabilities of LLMs with external knowledge retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by Lewis et al., has shown superior performance in generating specific, diverse, and factual language compared to traditional models 77].\n\nThe implementation of RA4 techniques in the automotive industry, however, faces several industry0specific challenges:\n\n1. 2ocument Complexity: Automotive technical documents often feature intricate layouts, including multi0column formats and complex tables. These structural elements pose significant challenges for standard document processing methods 78].\n2. 2ata 1rivacy: The automotive industry deals with highly confidential information related to proprietary designs and manufacturing processes. This necessitates solutions that can operate securely within the company's infrastructure, without relying on external cloud services 79].\n3. Resource Constraints: Many industrial environments operate with limited computational resources. This constraint requires the development of optimized, lightweight models capable of running efficiently on standard hardware 710].\n4. 2omain Specificity: The automotive sector employs a vast array of specialized terminology and concepts. 4eneric language models often lack the specific knowledge required to accurately interpret and respond to queries about automotive processes and specifications 711].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "467ed3b5-e90c-4240-b3f0-80d406d78bda": {"__data__": {"id_": "467ed3b5-e90c-4240-b3f0-80d406d78bda", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a16032be-2c9b-4c61-84c0-8217293d2517", "node_type": "4", "metadata": {}, "hash": "a747839e8576b3928291fa21e410098478bd4bc7ad7abbea5b5ea96b5932915f", "class_name": "RelatedNodeInfo"}}, "text": "# 5. Real-time Performance\n\nIn fast-paced manufacturing environments, the ability to quickly retrieve and process relevant information is crucial. This necessitates high-performance information retrieval systems capable of operating under time constraints 712.\n\nThe open-source large language model service framework 5llama has gained attention for its ability to rapidly deploy LLMs in low-performance environments 713. This framework offers potential solutions to some of the resource constraints faced in industrial settings. However, its application in the context of RA4 for automotive documentation processing remains an area ripe for exploration and optimization.\n\nAs the automotive industry continues to evolve, particularly with the advent of electric and autonomous vehicles, the complexity and volume of technical documentation are expected to increase further 714. This evolution underscores the importance of developing robust, efficient, and secure information retrieval systems tailored to the specific needs of the automotive sector 715.\n\nThe intersection of these technological advancements and industry-specific challenges presents a unique opportunity for research 716. By addressing the particular needs of the automotive industry in the context of RA4 and local LLM deployment, there is potential to significantly enhance information access and utilization in automotive engineering and manufacturing processes 717.\n\n# 1.2 Research Status and Gaps\n\nRecent advancements in RA4 techniques have shown promise in various domains. Jiang et al. proposed 3LARE, which uses predicted next-sentence content to proactively retrieve relevant information 718. Wang et al. introduced 3ILC5, a method for identifying and filtering useful contexts to improve generation quality 719. These approaches demonstrate the potential for more context-aware retrieval in complex document environments, such as those found in automotive engineering.\n\nThe concept of self-reflective RA4, as explored by Asai et al 720, introduces a novel framework designed to enhance the quality and factual accuracy of LLMs through on-demand retrieval and a self-reflection mechanism. This approach could be especially valuable in the automotive context, where precision and accuracy in technical information are paramount.\n\nIn the domain of optimizing RA4 for specific industries, Rajpathak et al 721, proposed a domain-adaptive retrieval method that is particularly relevant for the automotive sector's unique terminology and document structures. Similarly, the work of Siriwardhana et al 722, on improving retrieval efficiency in large-scale industrial datasets offers insights that could be applied to the vast repositories of technical documentation in automotive manufacturing.\n\nThe open-source large language model service framework 5llama has gained attention for its ability to rapidly deploy LLMs in low-performance environments 723. Burgan et al. developed RamChat, an AI chatbot aimed at improving accessibility 724. These developments in local LLM deployment are particularly relevant to the automotive industry's need for on-premises, resource-efficient solutions.\n\nRecent work by Wang et al 725, on on-device language models for function calling of software A1Is presents potential applications in integrating RA4 systems with existing software infrastructure in automotive production environments. This could lead to more seamless integration of AI-powered information retrieval within established industrial processes.\n\nThe challenge of processing complex 123 documents, a common format for technical specifications in the automotive industry, has been addressed by several researchers. Lin et al 726, proposed an advanced 123 parsing.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2709435-5fdf-4050-9bd7-ad5ba6df4a48": {"__data__": {"id_": "a2709435-5fdf-4050-9bd7-ad5ba6df4a48", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9ef321e-f8fe-44c3-bcf5-704efe6d04cf", "node_type": "4", "metadata": {}, "hash": "32da43775612fcce404ef1214ed5fbee77b5f49cd42d77c97c9333b90675942e", "class_name": "RelatedNodeInfo"}}, "text": "# Research on RA4 Systems in Automotive Industry\n\nTechnique that could be adapted to handle the multi-column layouts and intricate tables often found in automotive documentation. Furthermore, the work of Bensch et al [727], on information extraction from semi-structured documents offers promising approaches for dealing with the varied formats of automotive technical literature.\n\nIn the realm of domain-specific language understanding, the research of Baysse et al [728], on fine-tuning language models for specialized industries provides valuable insights that could be applied to tailoring RA4 systems for automotive terminology and concepts. This is complemented by the work of Kumar et al [729], on entity recognition in technical documents, which could enhance the precision of information retrieval in automotive contexts.\n\nThe integration of RA4 with other AI methodologies has also shown promise. For instance, the combination of RA4 with reinforcement learning, as explored by Belhadj et al [730], could lead to more adaptive and context-aware retrieval systems capable of handling the diverse query types encountered in automotive engineering and production.\n\nPrivacy and security concerns, which are paramount in the automotive industry, have been addressed in the context of RA4 by researchers such as Zeng et al [731], who proposed privacy-preserving retrieval methods that could be crucial for protecting proprietary automotive designs and processes.\n\nThe challenge of maintaining coherence in long-form text generation, often necessary when addressing complex automotive queries, has been tackled by researchers like Borgeaud et al [732], whose work on improving long-range dependencies in language models could enhance the quality of responses in automotive RA4 applications.\n\nRecent advancements in few-shot learning, as demonstrated by Izacard et al [733], with 41T03, offer potential for rapidly adapting RA4 systems to new automotive subdomains or emerging technologies without extensive retraining. This could be particularly valuable in the fast-evolving landscape of automotive technology.\n\nThe application of RA4 in multilingual settings, as explored by Ahmad et al [734], is especially relevant for global automotive companies dealing with documentation in multiple languages. Their work on cross-lingual retrieval and generation could facilitate more efficient knowledge sharing across international teams.\n\nIn the domain of optimizing retrieval mechanisms, the research of Su et al [735], on dense retrieval methods offers potential improvements in the speed and accuracy of information lookup, crucial for real-time query resolution in fast-paced automotive production environments.\n\nThe challenge of handling numerical data and calculations, often present in automotive specifications and performance metrics, has been addressed by researchers like Noorbakhsh et al. [736], whose work on integrating symbolic mathematics with neural language models could enhance the precision of RA4 systems when dealing with quantitative automotive data.\n\nHowever, despite these advancements, there remains a significant gap in research specifically addressing the unique challenges of implementing RA4 systems in the automotive industry, particularly in resource-constrained, offline environments.\n\n# Significance of Research\n\n1. Providing an effective optimization scheme for local RA4 deployment of 5llama in automotive industrial environments, addressing key challenges in document processing and information retrieval.\n2. Exploring the application of self-RA4 in offline, industry-specific scenarios, offering new insights into function calling implementations for domain-specific tasks.\n3. Contributing to the advancement of intelligent information processing in automotive manufacturing, potentially improving efficiency and accuracy in technical document analysis and query resolution.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60267760-356a-42a0-a0f7-b62a7ea0dde4": {"__data__": {"id_": "60267760-356a-42a0-a0f7-b62a7ea0dde4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dfaab604-af68-41c7-b1ef-8904e2d6a91a", "node_type": "4", "metadata": {}, "hash": "54817c7a305bf0ea1dfe66b54554edb07f1e2969ef83d8ccb267f666b382072c", "class_name": "RelatedNodeInfo"}}, "text": "# 1.3 Research Objectives and Significance\n\nGiven the identified challenges and research gaps, this study aims to develop a multi-dimensional optimization scheme for applying RA4 technology with Llama in local, low-performance automotive industry environments. Our specific research objectives include:\n\n- Proposing a 123 file processing method optimized for automotive industry documents, capable of handling multi-column layouts and complex tables.\n- Developing an advanced RA4 system based on the Langchain framework, introducing reranking models and BM25 retrievers to build an efficient context compression pipeline.\n- Designing an intelligent agent that supports self-RA4 and exploring a function calling mechanism to enhance Llama's response generation in automotive-specific scenarios.\n- Evaluating the proposed system using a proprietary dataset of automotive industry documents, alongside public datasets, to demonstrate its effectiveness in real-world industrial applications.\n\nThe significance of this research lies in:\n\n- Providing an effective optimization scheme for local RA4 deployment of Llama in automotive industrial environments, addressing key challenges in document processing and information retrieval.\n- Exploring the application of self-RA4 in offline, industry-specific scenarios, offering new insights into function calling implementations for domain-specific tasks.\n- Contributing to the advancement of intelligent information processing in automotive manufacturing, potentially improving efficiency and accuracy in technical document analysis and query resolution.\n\nThis research builds upon and extends existing work in several key areas:\n\n- The potential of RA4 systems to support decision-making processes, a critical application in automotive design and manufacturing, has been explored by researchers such as Damage et al [737]. Their work on using RA4 for few evidences-based reasoning could be adapted to support complex decision-making scenarios in automotive engineering.\n- Recent developments in efficient transformer architectures, such as the work of Zhuang et al [738], on Reformer, offer potential for deploying more powerful RA4 models within the computational constraints of industrial environments.\n- The integration of visual information with text-based retrieval, as explored by Chen et al [739], presents opportunities for enhancing RA4 systems to handle technical diagrams and schematics common in automotive documentation.\n\nBy addressing these research objectives and building upon recent advancements in the field, this study aims to significantly enhance the applicability and effectiveness of RA4 technologies in the automotive industry, potentially transforming how technical information is accessed, processed, and utilized in automotive engineering and manufacturing processes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5b7c28a-0ca1-4675-8a64-d527fb828337": {"__data__": {"id_": "c5b7c28a-0ca1-4675-8a64-d527fb828337", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e17aecf3-a572-4667-8a18-87d01123adc6", "node_type": "4", "metadata": {}, "hash": "86cfc773954be6f39aa552e8b3012e5a901ebb75af0f52f59297f449855d7d4e", "class_name": "RelatedNodeInfo"}}, "text": "# MATERIALS AND METHODS\n\n# 2.1 Foundation\n\nOur research builds upon the Langchain framework and Llama model, adapting them to meet the specific needs of the automotive industry. We began by constructing a preliminary retrieval-based chatbot framework using Langchain's components, which we then optimized for processing automotive technical documents.\n\nFigure 1 illustrates the key components of this system. This architecture integrates advanced document loading capabilities for various file formats, efficient text splitting, and a robust retrieval mechanism using the Chroma vector store. The basic framework includes:\n\n|Vector Store|Documents|Document Loader|Text Splitter|Embeddings|Query|\n|---|---|---|---|---|---|\n|(Chroma)|(PDF)|(RecursiveCharacterTextSplitter)|(OllamaEmbeddings)| | |\n| | |Retriever|LLM (Ollama)|ConversationalRetrievalChain|Response|\n\nFigure 1: Basic RAG Architecture for Automotive Document Processing.\n\n- Document Loading: Utilizing Langchain's Loader class to recursively load 123 documents from specified directories, simulating the document structure in automotive manufacturing environments.\n- Text Chunking: Implementing an embedded tokenization model to split documents into fixed-length text chunks, optimized for technical specifications and multi-column layouts common in automotive documentation.\n- Vector Storage: Encoding text chunks into semantic vectors and storing them in the Chroma vector database, creating an indexed text retriever tailored for automotive terminology and concepts.\n- Dialogue Generation: Employing Langchain's ConversationalRetrievalChain to process user queries and retrieved context, generating responses using the locally deployed Llama model.\n\nThis foundational setup serves as the baseline for our subsequent optimizations, each designed to address specific challenges in automotive document processing and information retrieval.\n\n# 2.2 PDF File Processing Optimization\n\nTo address the unique challenges posed by automotive industry documents, we developed an enhanced 123 processing method combining PDFMiner and Tabula libraries.\n\n# 2.2.1 Overview of PDFMiner and Tabula\n\nPDFMiner is a Python library used for extracting information from PDF documents. It is capable of extracting text, images, metadata, and structural information from PDF documents. PDFMiner provides both high-level and low-level APIs to satisfy different levels of requirements. Automotive technical documents often feature multi-column layouts.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec377e55-ce64-4821-8992-24c16f743aa4": {"__data__": {"id_": "ec377e55-ce64-4821-8992-24c16f743aa4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bee98ea9-8b7e-403b-b2cb-e7e526fac9f3", "node_type": "4", "metadata": {}, "hash": "93023d6f9b1a24e27f8ccdc010e6434c6446d7d082fbcf9d8719ff253e09dd25", "class_name": "RelatedNodeInfo"}}, "text": "# Document Title\n\n# 1. Introduction\n\nWe implemented a custom algorithm using 123Miner's extract_pages function to accurately extract content while preserving the logical flow of information:\n\n1. Page Segmentation: We analyze each page to identify distinct content regions, including text columns, diagrams, and tables.\n2. Content Ordering: Implementing a left-to-right, top-to-bottom reading order algorithm to ensure proper sequencing of extracted information.\n3. Diagram Extraction: Utilizing 123Miner's image extraction capabilities to preserve technical diagrams crucial for understanding automotive specifications.\n\n# 2. Tabula and Data Extraction\n\nTabula is a Java library used for extracting tabular data from PDF files. It provides a Python wrapper, making it more convenient to use Tabula in Python. Tabula can automatically detect table boundaries and convert tabular data into DataFrame objects, facilitating subsequent data analysis and processing.\n\n# 2.2.2 Multi-column Layout and Table Information Recognition Optimization\n\nTables in automotive documents often contain critical data such as part specifications, test results, or compliance information. Our approach uses Tabula's read_pdf function with custom parameters like Algorithm 1:\n\n# ALGORITHM 1: Extract Text and Tables from PDF to Markdown\n\ninput1 output2\npages = extract_pages(input2)\nopen output >o write as out\nfor page_num in enumerate(pages) start=121 do\nelements = [e for e in page if isinstance(e, LTTextContainer)]\ntables = tabula.read_pdf(input1 pages=page_num multiple_tables=True)\nmidline = \"max(e.bbox[1]) for e in elements\"\nleft = [e for e in elements if e.bbox[0] < midline]\nright = [e for e in elements if e.bbox[0] >= midline]\nsort left by -e.bbox[1]\nsort right by -e.bbox[1]\nfor col in [left, right] do\nfor e in col do\ncleaned_text = clean_text(e.get_text())\nwrite cleaned_text + \"\\0\" to out\nend\nend\nfor table in tables do\nwrite pd.DataFrame(table).to_markdown(index=False) + \"\\0\" to out\nend\nwrite \"\\0\\0\" to out\nend", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2006, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98793adc-6e5d-4174-a5b3-e3379e22851e": {"__data__": {"id_": "98793adc-6e5d-4174-a5b3-e3379e22851e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5dcd931d-7360-4d7d-a208-b0cf6b88d3f2", "node_type": "4", "metadata": {}, "hash": "0ce438a05fb6672823d5267568597c2b18562db0023240bf88a0fed4b5855e0e", "class_name": "RelatedNodeInfo"}}, "text": "# 1. Table Detection\n\nImplementing heuristics to identify table structures within automotive documents, considering common formats used in the industry.\n\n# 2. Data Extraction\n\nConverting recognized tables into structured DataFrame objects, preserving relationships between data points.\n\n# 3. Contextual Integration\n\nSeamlessly integrating extracted table data with surrounding text to maintain document coherence.\n\nWhen integrating text and table information, we first write the text information extracted by the multi-column layout recognition algorithm into the Markdown file in the order of appearance. Then, after the text information of each page, we convert the table information recognized on that page into Markdown format and write it into the file. By doing so, we can ensure that the content order in the generated Markdown file is consistent with the original file, and the table information can be correctly embedded in the corresponding positions.\n\nTo illustrate the effectiveness of this approach, let's examine two figures that demonstrate the conversion process for a complex academic paper layout. Figure 2 shows a sample page from the original document, which features a challenging two-column layout with embedded tables and various formatting elements. Figure presents the corresponding Markdown output after processing with our optimized method.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59db6a0c-2382-47e4-958f-13a833bfb60d": {"__data__": {"id_": "59db6a0c-2382-47e4-958f-13a833bfb60d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35e1c939-3db2-43c9-88d3-2afcaff459f7", "node_type": "4", "metadata": {}, "hash": "092ef4b0c55e2507dd8c8a816a428fd1e4ac2ad83d5e4ebcea64ec48f0df3540", "class_name": "RelatedNodeInfo"}}, "text": "# 4. Results\n\n# 4.1. UCF Crowd Counting\n\nThe UCF dataset is particularly challenging due to the large number of people in the images, the variety of the scenes as well as the low number of training images. We see in Figure that because the UCF dataset has over 1000 people on average in each image, the shapes output by the network in the density map are not as well defined or separated as in the UCSD dataset:\n\nWe report state of the art result on this dataset in Table following the standard protocol of 5-fold cross validation. Our MAE on the dataset is 290.82, which is approximately 5 lower than the previous state of the art, HydraCNN. This is particularly indicative of the power of an aggregated making - use of multicolumn dilation network: Despite not perspective information, the AMDCN is still able to produce highly accurate density maps for UCF.\n\n# 4.2. TRANCOS Traffic Counting\n\nOur network performs very well on the TRANCOS dataset: Indeed confirmed the GAME score, AMDCN produces the most accurate count and shape combined as compared to other methods. Table shows that we achieve state of the art results as measured by the GAME metric across all levels.\n\n# 4.3. UCSD Crowd Counting\n\nThe Results are shown in Table and Figure. We see that original split as defined by the creators of the dataset gives us somewhat worse results for counting on this dataset. Results were consistent over multiple trainings. Again, including the perspective map does not seem to increase performance on this dataset: Despite the state of the art: In fact; for two of the splits proposed network beats the state of the art; For the upscale split the AMDCN is the state of the art by large relative margin. This is compelling because it shows that counting can be achieved without accurate perspective-free.\n\n# 4.4. WorldExpo '10 Crowd Counting\n\nOur network performs reasonably well on the more challenging WorldExpo dataset. While it does not beat the state of the art, our results are comparable. What is more we do not need to use the perspective maps to obtain these results. As seen in Table the AMDCN is capable of incorporating the perspective effects without scaling the Gaussians with perspective information. This shows that it is possible to achieve counting results that approach the state of the art with much simpler labels for the counting training data.\n\n# 4.5. Ablation Studies\n\nWe report the results of the ablation studies in Figure. We note from these plots that while there is variation in performance, few trends stand out. Most importantly, the lowest errors are consistently with the combination of larger number of columns and including the aggregator module. Notably for the TRANCOS dataset, including the aggregator consistently improves performance. Generally, the aggregator tends to decrease the variance in performance of the network: Some of the variance that we see in the plots can be explained by: (1) for lower numbers of columns, including an aggregator is not as likely to help as there is not much separation of multiscale information across columns and (2) for the UCSD dataset; there is less of a positive effect than TRANCOS and WorldExpo so perform comparably to simpler network. These results verify the notion that using more columns increases accuracy; and also support our justification for the use of the aggregator module.\n\n# Table: Mean Absolute Error of Various Methods on UCF Crowds\n\n|Method|MAE|\n|---|---|\n|AMDCN|290.82|\n|HydraCNN|333.73|\n|MCNN|377.60|\n| |467.00|\n| |295.80|\n| |318.10|\n\n# Table: Mean Absolute Error of Various Methods on TRANCOS\n\n|Method|GAME (L=0)|GAME (L=1)|GAME (L=2)|GAME (L=3)|\n|---|---|---|---|---|\n|AMDCN|9.77|13.16|15.00|15.87|\n|[18]|10.99|13.75|16.69|19.32|\n|[5] SIFT|13.76|16.72|20.72|24.36|\n|RGB|17.68|19.97|23.54|25.84|\n|HOG-2|13.29|18.05|23.65|28.41|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51be5534-ae6e-4fc3-be0e-404fdad4a874": {"__data__": {"id_": "51be5534-ae6e-4fc3-be0e-404fdad4a874", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b96ec1ca-e8b1-45e6-8d0b-8548856adef6", "node_type": "4", "metadata": {}, "hash": "46273e7fd7dd4a5e8613dacedfdac36fcbf143a5773737880efc39e68b12bd2e", "class_name": "RelatedNodeInfo"}}, "text": "# Table\n\n|Method|MAE|\n|---|---|\n|AMdON|290.82|\n|Hydrazs [18]|333.73|\n|KON [281]|377.60|\n|[27]|467.00|\n|[23]|295.80|\n| |318.10|\n\n# Results\n\nThe UcF dataset is particularly challenging due to the large number of people in various shapes and the number of training images. Figure shows that because the UCF dataset has 1030 people, the density map is well defined and separated.\n\nDespite not making the previous state of the art, the AMDON network is still able to produce highly accurate density maps due to aggregated multicolumn dilation.\n\n# Traffic Counting\n\nThe network performs across the art. Indeed, all levels by the GAME score the #GANET contained measures. AKDCN produces accurate counts compared to other methods. Table shows that.\n\n# Ucsd Crowd Counting Results\n\nTable and Figure show that this dataset gives somewhat worse results. Counting results were consistent over multiple trainings. Again, including perspective maps seem to increase the dataset's performance.\n\nDespite this, Table and Figure show that recuts are comparable to the state of the art.\n\n# Optimization of Advanced RAG Based on Langchain\n\nTo enhance the RA4 system's performance for automotive industry applications, we introduced several optimizations to the Langchain-based implementation. We introduce the B4E reranker model and BM25 algorithm.\n\nBuilding upon the groundwork laid in previous sections, we first combine the BM25Retriever with the default retriever using EnsembleRetriever in Langchain, assigning different weights to achieve more comprehensive and accurate retrieval. Then, a custom class is designed to integrate the reranking model into the context compression pipeline, further enhancing the retrieval and generation quality of the model.\n\n# Overview of BGE Reranker Model and BM25 Retriever\n\nA reranker model is a general semantic vector model used to optimize the ranking of retrieval results. It can adapt to prioritize automotive-relevant information in retrieved contexts. In this study, we employ BAAI/bge-reranker-large as the reranker model. B4E (BAAI General Embedding) is a reranker model proposed by the Beijing Academy of Artificial Intelligence (BAAI), specifically optimized for Chinese queries.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d4a89d0-1ff4-4b4c-b010-34a64c4eb844": {"__data__": {"id_": "3d4a89d0-1ff4-4b4c-b010-34a64c4eb844", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b2bb181-9a61-4184-8b42-1a9e96e2a2ca", "node_type": "4", "metadata": {}, "hash": "81e19ef9c3484103d6d7df42405602e4b0964c4b00ff73d2f8e5dbede4bc9c84", "class_name": "RelatedNodeInfo"}}, "text": "# BM25 (Best Matching 25)\n\nBM25 (Best Matching 25) is a classic bag-of-words retrieval algorithm that evaluates relevance by calculating the term frequency-inverse document frequency (TF-IDF) score between the query and documents. The BM25 retriever can quickly and efficiently filter out the most query-relevant documents from a large-scale text corpus. We introduce this traditional relevance assessment method in combination with the reranker model to further strengthen the effectiveness of the RA4 retriever.\n\n# 2.3.2 Building the Context Compression Pipeline and Custom Class Design\n\nTo optimize the context information processing of the RA4 model, we construct a context compression pipeline DocumentCompressorPipeline that is specifically tailored for automotive technical content. In addition to the collection retrievers mentioned earlier, we also introduce:\n\n- EmbeddingsRedundantFilter: An embedding-based redundancy filter to remove redundant information from the retrieval results.\n- LongContextReorder: Optimizes the order of context information, prioritizing key automotive specifications and procedures.\n- BgeRerank: A custom class that inherits from BaseDocumentCompressor. Since Langchain's official support for the B4E model is relatively limited, the purpose of designing this class is to seamlessly integrate the B4E reranker model into the pipeline. This custom class can improve relevance scoring for automotive queries.\n\nThe following Algorithm 2 is the pseudocode for the BgeRerank class:\n\n# ALGORITHM 2: BgeRerank\n\nBgeRerank(documents, query)\ninitialize model = CrossEncoder(model_name)\ndoc_list = list(documents)\ndocs = [d.page_content for d in doc_list]\nmodel.inputs = [[query, doc] for doc in docs]\nscores = model.predict(model.inputs)\nresults = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)[:top_n]\nfinal_results = []\nfor doc, score in results:\ndoc.metadata[\"relevance_score\"] = score\nappend doc to final_results\nend\nreturn final_results\n\nBy overriding the compress_documents method, the B4E model is used to calculate the relevance between the query and documents. The documents are then sorted based on their scores, and the top N documents are selected as the final compression results. This custom class design compensates for the insufficient support for B4E in Langchain, allowing us to flexibly incorporate it into the pipeline structure.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a0130cd-477e-48ef-ad32-affcf781fabe": {"__data__": {"id_": "3a0130cd-477e-48ef-ad32-affcf781fabe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec7c7831-457e-46ba-a816-4a9770139dbc", "node_type": "4", "metadata": {}, "hash": "62c6dabdcd084c9dcfa892da9150279df82ddae09cadcb5ab3735ec0c4d00c4f", "class_name": "RelatedNodeInfo"}}, "text": "# In summary\n\nThe innovative aspects of this section include: introducing the B4E reranker model and BM25 retriever, building the context compression pipeline, and seamlessly integrating B4E into the Langchain framework through the custom BgeRerank class. This pipeline significantly enhances the quality and relevance of retrieved information, ensuring that the most pertinent automotive technical details are presented to the language model.\n\n# 2.4 Optimization of Advanced RAG Based on Langchain\n\nIn the previous section, we completed the basic design of the RA4 system. However, integrating LLMs into practical applications and constructing end-to-end intelligent systems still present numerous challenges. To address the complex, multi-step problem of querying 123 profiles, which is common in automotive engineering and manufacturing processes, we have developed an advanced Self-RA4 agent based on the Langraph framework.\n\n# 2.4.1 Overview of SELF-RAG\n\nSelf-Reflective Retrieval-Augmented Generation (SELF-RA4) is a novel framework designed to enhance the quality and factual accuracy of LLMs through on-demand retrieval and a self-reflection mechanism. Unlike traditional RA4 methods, SELF-RA4 endows LLMs with the following capabilities:\n\n1. On-demand Retrieval: The LLMs autonomously determines whether to retrieve relevant information from an external knowledge base based on the input it receives.\n2. Self-Reflection: The LLMs evaluates and reflects upon both the retrieved information and its own generated content, thereby improving the quality and reliability of its output.\n\nThe training process of SELF-RA4 consists of two stages:\n\n1. Offline Critic Model Training: An independent critic model is trained to generate \"reflection tokens\". These tokens are inserted into the LLMs' output to guide its self-reflection process.\n2. Generative Model Training: The LLMs is fine-tuned using a corpus that includes reflection tokens and retrieved documents. This enables the LLMs to understand and utilize these tokens, incorporating self-reflection into its generation process.\n\nDuring inference, the LLMs dynamically decides whether to retrieve information based on the requirements of the task at hand. It also leverages the retrieved information and the self-reflection mechanism to generate high-quality output. For instance, in tasks demanding factual accuracy, the LLMs is more inclined to retrieve and utilize relevant information, while in more open-ended tasks, it may prioritize creativity and rely less on retrieval.\n\n# 2.4.2 Agent Design Supporting Self-RAG\n\nConsidering this, based on Langchain, we designed an intelligent Agent class called AgenticRA4, which aims to utilize Self-RA4 technology to answer user questions. AgenticRA4 combines various components from the Langraph and LangChain ecosystems to achieve a modular and scalable question-answering system.\n\nThe core of the AgenticRA4 class is the create_graph method, which defines a workflow based on a directed acyclic graph (DAG). The workflow consists of multiple nodes, each responsible for a specific task in the question-answering process. The nodes are connected by directed edges, forming a complete question-answering flow. The main components in the AgenticRA4 class are as follows:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d078578a-31e1-4344-a6ad-ef3a528cbdb1": {"__data__": {"id_": "d078578a-31e1-4344-a6ad-ef3a528cbdb1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "338268c9-79bf-497a-bcf1-49dacb39b48e", "node_type": "4", "metadata": {}, "hash": "b6b57703deb4a6fa6301469fa2d575e03b211359f3ff577ea1dc47f29b54dc8e", "class_name": "RelatedNodeInfo"}}, "text": "# 4raphState\n\nRepresents the current state of the graph, including the user's question, retrieved documents, generated answer, and chat history.\n\n# Node\n\nRepresents a step or task in the question-answering process, accepts the current state as input, performs specific operations, and returns the updated state.\n\nWe designed the AgenticRA4 class to handle sophisticated automotive-related questions. Key components as\n\n# Figure 4:\n\nFlowchart of Self-RAG Agent Implementation via LangGraph.\n\n# 1. Retrieve Node\n\nOptimized to fetch relevant information from automotive technical documents, considering industry-specific terminology and concepts.\n\n# 2. Grade Documents Node\n\nEvaluates retrieved documents based on their relevance to automotive queries, considering factors like technical accuracy and applicability to specific manufacturing processes.\n\n# 3. Generate Node\n\nProduces responses tailored to automotive industry needs, incorporating technical specifications and industry standards searched from documents.\n\n# 4. Transform Query Node\n\nRefines queries to better capture the intent behind automotive-specific questions, improving retrieval accuracy in subsequent iterations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86b84949-95ad-491f-916c-86a8c1b3d0e8": {"__data__": {"id_": "86b84949-95ad-491f-916c-86a8c1b3d0e8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ed954d9-ca30-4e47-b076-84b95d32108b", "node_type": "4", "metadata": {}, "hash": "9a1ba7535154d11016d91237a08baeedfff6b423274fe2038f4a498f01ffaae9", "class_name": "RelatedNodeInfo"}}, "text": "# 2.4.3 Function Calling Design for Optimizing Ollama Output in RAG Scenarios\n\nFunction calling is a powerful technique that significantly enhances the output quality of LLMs in RAG scenarios. By integrating external functions with LLMs, specialized logic and algorithms can be employed to guide the model, resulting in more accurate, coherent, and informative responses. This approach leverages the few-shot learning capabilities of LLMs, enabling them to adapt to new tasks while mitigating their inherent limitations in reasoning and computation.\n\nHowever, most of the relevant work has focused on standard transformer-based LLMs, with a lack of implementation for optimizing llama callbacks, especially in RAG scenarios. Although Langchain officially provides ollamafunction.bind to call functions, further research on this method is currently scarce. Therefore, to enhance the agent's capability in handling automotive-specific tasks, we designed a custom class adapted to ollamafunction.bind, called ChatFunction. It inherits from the BaseFunction class and overrides the __init__ and __call__ methods. The main features of ChatFunction are as follows:\n\n# ALGORITHM 3: ChatFunction\n\n|inputProp|Property \"type=INPUT\" desc=\"InputDesc\"|\n|---|---|\n|outputProp|Property \"type=OUTPUT\" desc=\"OutputDesc\"|\n|props|[inputProp, outputProp]|\n|params|Parameters \"props=props\" required=\"[input, output]\"|\n|initialize|BaseFunction \"name=ChatFuncName\" desc=\"ChatFuncDesc\" params|\n|call|output = args[output]|\n|input|input = args[input]|\n|if|IsEmpty(output) or IsEmpty(input) then|\n|raise|ValueError \"Missing Input/Output Message\"|\n|end| |\n|detail|GetPromptByRetry()|\n|persona|GetPersonaString()|\n|history|GetHistoryString(input)|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f93b48fe-d89e-4d0c-a7d2-19341627af1c": {"__data__": {"id_": "f93b48fe-d89e-4d0c-a7d2-19341627af1c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0a7f33f-47fb-46b3-8332-fcc3a868a4ca", "node_type": "4", "metadata": {}, "hash": "685904a8aac92e5be991ad8bbf259afa56be37afd60dc0da10a32b9cb5759df7", "class_name": "RelatedNodeInfo"}}, "text": "# Current Page\n\n# Procedure\n\ncu]]e0t = GetCu]]e0tSt]i0+\"i0put2\n\np]ompt = Fo]mat\"detail1 pe]so0a1 histo]y1 cu]]e0t2\n\n]espTool = LookupTool\"ChatRespo0seE0ha0ce]\"22\n\n]etu]0 {\n\ntool: ]espTool1\n\ntool{i0put: {\n\n]espo0se: output1\n\nque]y: p]ompt\n\n}\n\n}\n\ne0d p]ocedu]e\n\n# GetP]omptByRet]y\n\ni> ]et]y = 01 do\n\n]etu]0 GetB]ie>P]ompt\"2\n\nelse i> ]et]y = 11 do\n\n]etu]0 GetMediumP]ompt\"2\n\nelse\n\n]etu]0 GetDetailedP]ompt\"2\n\ne0d\n\ne0d >u0ctio0\n\n# Initialization\n\nThe initialization accepts three parameters: the AI assistant's personality description personality, language preference language, and retry count retry_count.\n\nTwo properties are defined: query_input and output, representing the complete input (including personality description, language instructions, and conversation history) and the AI assistant's response, respectively. This ensures that the model is provided with the necessary background knowledge for each retry, enabling it to generate responses based on the specified role characteristics and language style.\n\nThe constructor of the parent class BaseFunction is called, passing the function name, description, and parameter list.\n\nWhen invoked, it accepts an arguments dictionary containing two required parameters: output and query_input.\n\nAccording to the retry count retry_count in the self0rag Agent in 3.3.2, the level of detail in the answer detail_prompt is dynamically adjusted to modify prompts based on the complexity of automotive queries and the depth of technical detail required.\n\nThe query_input for the next round is constructed by encoding the personality description, language preference, detail level requirement, and current conversation history. To maintain awareness of the ongoing conversation context, crucial for addressing multi0step automotive processes or complex diagnostic queries.\n\nA dictionary is returned, containing the tool field (specifying the callback function name) and the tool_input field (containing the optimized response and updated query_input), allowing the next round of callback requests to continue optimizing based on the current results and state, forming a closed0loop self0improvement process.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "647e389b-2499-4745-b1d6-34309a8d1d05": {"__data__": {"id_": "647e389b-2499-4745-b1d6-34309a8d1d05", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "afb64066-9829-4bf6-84f7-37cfb5327063", "node_type": "4", "metadata": {}, "hash": "dea4eeff56172ddc1bdd77f1308bfd8d4c9e28504c6ce40252d9372e4b6ac162", "class_name": "RelatedNodeInfo"}}, "text": "# The innovation of ChatFunction\n\nThe innovation of ChatFunction lies in its full utilization of 5llama's JS5N mode callback function potential, optimizing the response effectiveness in the RA4 scenario. This customized function call mechanism greatly improves the agent's ability to handle complex automotive queries, provide technically accurate information, and can guide users through the efficient acquisition of knowledge common to automotive design and manufacturing.\n\n# 3 RESULTS\n\n# 3.1 Overview of the RAGAS Performance Evaluation Framework\n\nRA4AS (Retrieval Augmented 4eneration Assessment Suite) is a comprehensive evaluation framework for assessing the performance of RA4 models. It provides a series of metrics to quantify the quality of generated results, with a particular focus on the impact of information retrieval on the generation process. The evaluation metrics in RA4AS include:\n\n- Context Precision: Measures the precision of relevant contextual information contained in the generated results.\n- Faithfulness: Evaluates the faithfulness of the generated results to the original contextual information, i.e., the consistency between the generated content and the original information.\n- Answer Relevancy: Assesses the relevance of the generated answers to the questions, i.e., whether the answers are on-topic and meet the requirements of the questions.\n- Context Recall: Measures the coverage of relevant contextual information in the generated results, i.e., how much key information is captured.\n\nWe adopt RA4AS as the evaluation framework to compare and analyze the performance of different RA4 models. The quantitative metrics provided by RA4AS enable us to objectively evaluate the strengths and weaknesses of the models in terms of contextual information utilization and generated content quality, providing important references for model selection and optimization.\n\n# 3.2 Experimental Results and Analysis\n\n# 3.2.1 Experimental Scenario Design\n\nIn this paper, we select two public datasets, QReCC and CoQA, and introduce a self-constructed dataset as the experimental data sources. The QReCC dataset contains 10,000 questions, each corresponding to a background knowledge text, with questions presented in the form of multi-turn dialogues. The CoQA dataset contains over 8,000 multi-domain dialogues, each based on a given text and consisting of multiple question-answer turns.\n\nThe self-constructed dataset is an automotive industry proprietary dataset compiled from internal documents of a leading automotive manufacturer. This dataset includes:\n\n- Technical specifications and design documents\n- Manufacturing process guidelines\n- Quality control procedures\n- Corporate regulations and standards", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e9bcaa8-b900-46f4-b20f-f977907fed8b": {"__data__": {"id_": "4e9bcaa8-b900-46f4-b20f-f977907fed8b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc34d6a5-4e38-49e6-9a32-c1d3b327d95f", "node_type": "4", "metadata": {}, "hash": "20e4844026231903ec93ec439febcec2b46e8dd1e5bc373fc13c734b31110d84", "class_name": "RelatedNodeInfo"}}, "text": "# Document Overview\n\nDue to the confidential nature of these documents, we cannot provide detailed statistics or examples. However, this dataset represents the core focus of our study, reflecting real-world challenges in automotive document processing.\n\n# Dataset Inclusion\n\nWhile the self-constructed dataset serves as our primary testbed, we included QReCC and CoQA for several crucial reasons:\n\n1. Structural Similarity: The question-answer pairs in QReCC and CoQA share a similar format with our custom-designed test cases for the self-constructed dataset. This structural consistency allows for a fair comparison of our model's performance across different domains.\n2. Conversational Nature: Both QReCC and CoQA feature multi-turn dialogues, mirroring the complex, context-dependent queries often encountered in automotive engineering and manufacturing processes.\n3. Diverse Domain Coverage: These datasets cover a wide range of topics, helping us evaluate our model's generalization capabilities beyond the automotive domain.\n4. Benchmark Comparability: As widely used public datasets, QReCC and CoQA enable us to benchmark our system against other state-of-the-art models in a reproducible manner.\n5. Confidentiality Compliance: By using these public datasets alongside our proprietary data, we can openly discuss and compare results without compromising sensitive corporate information.\n6. Robustness Testing: The inclusion of these datasets helps demonstrate that our optimizations, while tailored for automotive applications, do not compromise performance on general conversational tasks.\n\n# Research Focus\n\nConsidering that our research aims at the requirements of 123 dialogue chatbots and uses the B4E reordering model, we first translate the knowledge or link information in the QReCC and CoQA datasets into Chinese and convert it to 123 format. This step ensures the consistency of experimental data with complex 123 documents in real application scenarios and facilitates the evaluation of the performance of different RA4 models and their optimization schemes in practice. The self-constructed dataset does not require additional processing as it is already in 123 format.\n\n# Experimental Design\n\nAfter data preparation, we design or directly utilize a number of question-answer pairs with contextual memory content from these datasets as test cases. Here, we take a set of question-answer pairs from the QReCC dataset as an example, given a background text:\n\n\u201cJohn is a boy who likes to play outside. After school, he always goes to the park to meet his friends.\u201d\n\nThe following is a set of multi-turn question-answering based on this background text:\n\n\u201cQ: What did John do after school?\n\nA: John went to the park after school.\n\nQ: Who did he meet at the park?\n\nA: He met his friends at the park.\u201d\n\n# System Evaluation\n\nWe designed our experiments to evaluate the performance of four systems:\n\n- Naive RA4 (Baseline)\n- Advanced RA4 (5ur 5ptimized Model)\n- Self-RA4 Agent (Baseline)\n- Self-RA4 Agent (5ur 1roposed Agent with Custom 3unction Calling)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2632011e-478b-4382-94dd-9ba40515bbaf": {"__data__": {"id_": "2632011e-478b-4382-94dd-9ba40515bbaf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "475e5dbf-5221-4388-b92b-48af66d45d70", "node_type": "4", "metadata": {}, "hash": "267a033d4923521bce5ef036505f10dc3d4628c1f82bc264181f4a2e0760748c", "class_name": "RelatedNodeInfo"}}, "text": "# 3.2.2 Optimization Effects of Langchain-based RAG\n\nFor each dataset, we created test sets comprising:\n\n- 500 question-answer pairs from the self-constructed dataset\n- 500 pairs from QReCC\n- 500 pairs from CoQA\n\nThese test sets were carefully curated to ensure a balance of simple queries, multi-turn conversations, and complex technical questions, mirroring real-world usage scenarios in the automotive industry and beyond.\n\n# Figure 5: Performance Comparison of Naive RAG vs. Advanced RAG with Custom Context Compression Pipeline across Datasets.\n\n# Table 1: Comparative Analysis of Naive RAG vs. Advanced RAG with Custom Context Compression Pipeline across Datasets\n\n|Datasets|Metrics|QReCC|CoQA|Self-constructed dataset|\n|---|---|---|---|---|\n|Naive RAG|Answer Relevancy|0.782|0.772|0.759|\n| |Faithfulness|0.81|0.803|0.803|\n| |Context Precision|0.845|0.838|0.847|\n| |Context Recall|0.831|0.824|0.822|\n|Advanced RAG|Answer Relevancy|0.811|0.829|0.817|\n| |Faithfulness|0.847|0.85|0.832|\n| |Context Precision|0.839|0.841|0.836|\n| |Context Recall|0.842|0.811|0.835|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78968f00-3f45-4d24-8d6b-da87b31a300b": {"__data__": {"id_": "78968f00-3f45-4d24-8d6b-da87b31a300b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e2b690b-069d-473f-92b5-6f42188744fe", "node_type": "4", "metadata": {}, "hash": "3215e452ccf8f0c42b68f367d5435541141ecb727113fb49fbd0840aed81785d", "class_name": "RelatedNodeInfo"}}, "text": "# According to the Figure 5 and Table 1\n\nExperimental results show that the proposed Langchain-based RA4 optimization approach achieves certain improvements on some metrics compared to the naive RA4 model.\n\nBy introducing the B4E reordering model and BM25 retriever, and constructing a context compression pipeline, the optimized RA4 model improves context precision by 0.7%, 0.4%, and 1.3% on the QReCC, CoQA, and self-constructed dataset, respectively. The context recall also increases by 1.3% and 1.6% on the QReCC and self-constructed dataset, but decreases by 1.6% on the CoQA dataset. These results indicate that while the optimized model shows some improvements in capturing question-relevant background knowledge, it is not always the case.\n\nHowever, the optimized model exhibits 3.7%, 7.4%, and 7.6% relative improvements in answer relevancy, as well as 4.6%, 5.9%, and 3.6% boosts in faithfulness on QReCC, CoQA, and self-constructed dataset, respectively. These results underscore the effectiveness of our optimizations in handling complex automotive documentation and queries. The Advanced RA4 model also showed improvements, but the Self-RA4 Agent's performance was notably superior, particularly in dealing with multi-step technical queries common in automotive applications.\n\n# 3.2.3 Optimization Effects of Langgraph-based Self-RAG Agent\n\n# Figure 6: Performance Comparison of Self-RAG Agent vs. Self-RAG Agent with Custom Function Calling across Datasets.\n\n# Table 2: Comparative Analysis of Self-RAG Agent vs. Self-RAG Agent with Custom Function Calling across Datasets\n\n|Datasets|Metrics|QReCC|CoQA|Self-constructed dataset|\n|---|---|---|---|---|\n| |Answer Relevancy|0.839|0.821|0.83|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "113105fa-f455-4f67-b4b3-63893a3d9885": {"__data__": {"id_": "113105fa-f455-4f67-b4b3-63893a3d9885", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fa42d2d-b2bf-458c-a7a9-f21998214119", "node_type": "4", "metadata": {}, "hash": "87d26208ab2cf6752cc20e6aa578a45f5418d04d0d1c30d2daa832e3c5646fff", "class_name": "RelatedNodeInfo"}}, "text": "# Datasets\n\n# Metrics\n\n|Datasets|Metrics|QReCC|CoQA|Self-constructed dataset| |\n|---|---|---|---|---|---|\n|Self-RAG|Faithfulness|0.847|0.858|0.847| |\n| |Context Precision|0.864|0.866|0.867| |\n| |Context Recall|0.851|0.841|0.849| |\n|Self-RAG Agent with custom function calling|Answer Relevancy|0.852|0.851|0.86| |\n| |Faithfulness|0.859|0.86|0.857| |\n| |Context Precision|0.861|0.882|0.872| |\n| |Context Recall|0.871|0.87|0.86| |\n\nAccording to Figure 6 and Table 2, evaluation results show that the self-RAG agent obtains more significantly improvements over the naive RAG model on most metrics across the three datasets. Specifically, it surpasses the naive RAG by 7.3%, 6.3%, and 9.4% in answer relevancy, 4.6%, 6.8%, and 5.5% in faithfulness, 2.2%, 3.3%, and 2.4% in context precision, and 2.4%, 2.1%, and 3.3% in context recall on QReCC, CoQA, and self-constructed dataset, respectively. The superior performance on most metrics demonstrates the effectiveness of the proposed self-asking and self-verification mechanism in enabling more targeted and reliable context retrieval and answer inference.\n\nFurthermore, by introducing the custom function calling mechanism that optimizes the llama output, the self-RAG agent obtains additional 9.0%, 10.2%, and 13.3% gains in answer relevancy, 6.0%, 7.1%, and 6.7% increases in faithfulness, 1.9%, 5.3%, and 3.0% improvements in context precision, as well as 4.8%, 5.6%, and 4.6% boosts in context recall on the three datasets compared to the naive RAG model. These results validate that dynamically adjusting question and context inputs to the llama model based on conversation states can effectively guide it to generate more accurate, informative, and coherent responses that better satisfy user needs.\n\nThis performance demonstrates that our optimizations enhance general conversational AI capabilities while excelling in domain-specific tasks. The consistent improvement across datasets suggests that our approach successfully balances domain-specific optimization with general language understanding.\n\n# 4 DISCUSSION\n\nOur innovative approach to handling complex automotive queries is exemplified in Figure 7, which depicts the Self-RAG process flow with custom function calling, applied to an Anti-lock Braking System (ABS) query. This diagram showcases the integration of our custom ChatFunction within the Self-RAG framework, demonstrating how it dynamically adjusts the detail and focus of responses based on the system's self-assessment and retry count. The process illuminates the system's capability to iteratively refine its answers, ensuring high relevance and accuracy in the context of specialized automotive knowledge.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "333738bf-9a26-49ea-aa02-d285910f3ede": {"__data__": {"id_": "333738bf-9a26-49ea-aa02-d285910f3ede", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bec8982-5bda-43c7-8e25-7b2528c20693", "node_type": "4", "metadata": {}, "hash": "0bafb7a2715bc64634f2244e002182463e490954461a9b6f9078f7fc5548af10", "class_name": "RelatedNodeInfo"}}, "text": "# Example\n\n# 1. User Query\n\n# 2. Document Retrieval\n\nExplain ABS working principle and safety advantages\n\nABS prevents wheel lock... ABS components...\n\n# 3. Retrieved Fragments\n\n# Example\n\n# 6. Initial Answer\n\nSelf-RAG Loop\n\nABS prevents wheel lock, improves safety\n\n# 4. Relevance Scoring\n\n# 5. Generate Initial Answer\n\n# Example\n\n# 9. Optimized Answer\n\n# 8. Self-reflection and Optimization\n\n# 7. Quality Assessment\n\nABS prevents wheel lock, uses sensors and ECU, reduces braking distance by 10-20%\n\n# Custom Function Calling (ChatFunction):\n\n- Adjusts detail based on retry_count\n- Integrates personality and language\n- Optimizes for user needs\n\n# 10. Final Output\n\nFigure 7: Optimized Self-RAG Process Flow with Custom Function Calling - examples.\n\nWhen comparing performance across datasets, we observed the fact that while our models were optimized for automotive applications, they also showed improvements on the QReCC and CoQA datasets:\n\n- The most significant improvements were on the AI2, aligning with our focus on automotive applications.\n- Performance gains on QReCC and CoQA, while smaller, were still substantial, indicating the robustness of our approach.\n- The Self0RA4 Agent showed the most consistent performance across all datasets, highlighting its adaptability to various query types and domains.\n\nThese results validate our approach of using diverse datasets for evaluation. While our primary focus remains on optimizing for automotive applications, the improvements seen across datasets suggest that our methods enhance fundamental aspects of information retrieval and generation, benefiting both specialized and general applications.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2174000f-bef5-4a9a-a95f-abcb0f2e34d6": {"__data__": {"id_": "2174000f-bef5-4a9a-a95f-abcb0f2e34d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "acdc039c-fc47-4a43-b500-8fabdedbba78", "node_type": "4", "metadata": {}, "hash": "4bee1d4dfb3f0fa3e7881d1e27c31b494599506aacb0cc2cc6c7f247c12a84e6", "class_name": "RelatedNodeInfo"}}, "text": "# 5 DISCUSSION\n\nThis study presents a comprehensive approach to optimizing RA4 techniques for automotive industry applications, specifically focusing on 123 chatbots deployed in local, low-performance environments. Our research addresses critical challenges in processing complex automotive documentation and responding to industry-specific queries.\n\n# 5.1 Key Contributions\n\n- Enhanced 123 Processing: We developed a novel method combining 123Miner and Tabula to effectively handle multi-column layouts and complex tables prevalent in automotive technical documents. This significantly improves information extraction accuracy from industry-specific 123s.\n- Advanced RA4 Optimization: Our Langchain-based RA4 system, featuring a custom retriever ensemble and context compression pipeline, demonstrates substantial improvements in retrieving and utilizing automotive-specific information.\n- Self-RA4 Agent Design: The proposed AgenticRA4, enhanced with a custom function calling mechanism, shows superior performance in handling complex, multi-step queries typical in automotive engineering and manufacturing processes.\n- Cross-Domain Effectiveness: While optimized for automotive applications, our approach also shows improvements in general conversational AI tasks, as evidenced by performance gains on QReCC and CoQA datasets.\n\n# 5.2 Implications for the Automotive Industry\n\nOur research has significant implications for the automotive sector:\n\n1. Improved Information Access: The optimized 123 chatbot can greatly enhance access to technical information for engineers, technicians, and other stakeholders in the automotive industry.\n2. Enhanced Decision Making: By providing more accurate and contextually relevant information, our system can support better decision-making in design, manufacturing, and quality control processes.\n3. Resource Efficiency: The ability to deploy these advanced capabilities in low-performance, local environments addresses the industry's needs for data privacy and resource constraints.\n\n# 5.3 Limitations and Future Work\n\nWhile our study demonstrates significant advancements, there are areas for further research:\n\n1. Expanding Domain Coverage: Future work could focus on adapting the system to cover a broader range of automotive sub-domains, such as electric vehicle technology or autonomous driving systems.\n2. Real-Time Performance Optimization: Further research is needed to enhance the system's real-time performance in resource-constrained industrial environments.\n3. Multi-Modal Integration: Incorporating the ability to process and respond to queries about visual elements in technical diagrams and schematics could greatly enhance the system's utility.\n4. Longitudinal Study: A long-term study in real automotive manufacturing settings could provide insights into the system's impact on operational efficiency and decision-making processes.\n5. Ethical and Privacy Considerations: As the system deals with proprietary information, future work should explore advanced methods for ensuring data privacy and ethical use of AI in industrial settings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b905824-6d28-43e7-b92a-d1ac49a6de75": {"__data__": {"id_": "8b905824-6d28-43e7-b92a-d1ac49a6de75", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5d4b175-eb20-463f-8b6b-10d6d07c66cd", "node_type": "4", "metadata": {}, "hash": "4893c448c78333a1b05ad3507cfd1763f86951166bc48caa1b8cdebad767bec6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc35a834-8749-411f-a101-a41b4c276450", "node_type": "1", "metadata": {}, "hash": "35881124d2a20a3dc54aa36c505a5bf948c91f2198248f11a04deeb80f0d9a0f", "class_name": "RelatedNodeInfo"}}, "text": "# Conclusion\n\nIn conclusion, this research represents a significant step forward in applying advanced natural language processing techniques to the specific needs of the automotive industry. By bridging the gap between cutting-edge AI capabilities and the practical constraints of industrial environments, our work contributes to the ongoing digital transformation of the automotive sector. The demonstrated improvements in handling complex, domain-specific information retrieval and query resolution pave the way for more intelligent, efficient, and responsive information systems in automotive manufacturing and engineering.\n\n# References\n\n1. C. Llopis-Albert, ,. :ubio, an. ,. -alero. 2021. Impact Of Digital Transformation On The Automoti0e In.ustr1. Technological ,orecasting an. 2ocial Change 132 (2021), 120454. https://doi.org/10.10137j.techfore.2020.120454\n2. D. G. 2chnie.erjans, C. Cura.o, an. M. Khalajhe.a1ati. 2020. 2uppl1 Chain Digitisation Tren.s: An Integration of Knowle.ge Management. International Journal of Pro.uction Economics 220 (2020), 107549. https://doi.org/10.10137j.ijpe.2019.07.001\n3. A. Zahra an. M. 2aee.eh. 2021. Text-Base. Question Answering ,rom Information :etrie0al An. Deep Neural Network Perspecti0es: A 2ur0e1. Wile1 Inter.isciplinar1 :e0iews: Data Mining an. Knowle.ge Disco0er1 11, 3 (2021), e1512. https://doi.org/10.10027wi.m.1512\n4. M. 2hanahan. 2025. Talking about Large Language Mo.els. Communications of the ACM 37, 2 (2025), 38-79. https://doi.org/10.115574341448\n5. M. Mozes, X. He, B. Kleinberg, an. L. D. Griffin. 2024. Use of LLMs for Illicit Purposes: Threats, Pre0ention Measures, an. -ulnerabilities. arXi0 preprint arXi0:2408.12844 (2024).\n6. Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. 2un, M. Wang, an. H. Wang. 2024. :etrie0al-Augmente. Generation for Large Language Mo.els: A 2ur0e1. arXi0 preprint arXi0:2412.10997 (2024).\n7. P. Lewis, E. Perez, A. Piktus, ,. Petroni, -. Karpukhin, N. Go1al, H. K\u00fcttler, M. Lewis, W. Yih, T. :ockt\u00e4schel, 2. :ie.el, an. D. Kiela. 2020. :etrie0al-Augmente. Generation for Knowle.ge-Intensi0e NLP Tasks. In A.0ances in Neural Information Processing 21stems, -ol. 44. 9559-9575.\n8. 2. :aja, A. Mon.al, an. C. -. Jawahar. 2022. -isual Un.erstan.ing of Complex Table 2tructures from Document Images. In Procee.ings of the IEEE7C-, Winter Conference on Applications of Computer -vision. 2554-2552.\n9. M. Krichen. 2024. ,ormal Metho.s an. -ali.ation Techniques for Ensuring Automoti0e 21stems 2ecurit1. Electronics 15, 4 (2024), 333. https://doi.org/10.44907electronics15040333\n10. H. Liu, M. Galin.o, H. Xie, L. Wong, H. 2huai, Y. Li, an. W. Cheng. 2025. Lightweight Deep Learning for :esource-Constraine. En0ironments: A 2ur0e1. arXi0 preprint arXi0:2505.07243 (2025).\n11. -. D. -iellieber an. M. A\u00dfenmacher.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc35a834-8749-411f-a101-a41b4c276450": {"__data__": {"id_": "fc35a834-8749-411f-a101-a41b4c276450", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5d4b175-eb20-463f-8b6b-10d6d07c66cd", "node_type": "4", "metadata": {}, "hash": "4893c448c78333a1b05ad3507cfd1763f86951166bc48caa1b8cdebad767bec6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b905824-6d28-43e7-b92a-d1ac49a6de75", "node_type": "1", "metadata": {}, "hash": "704930ef9336bed89cc4df4e7496564e5bfb6c034efda620bdc6b7d390934b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2b288ee-3eb4-4b24-83b8-4fcf7ca88042", "node_type": "1", "metadata": {}, "hash": "6dc46171aca6f95be98051b88b9151801353d8019d68e39685284b717acfe77f", "class_name": "RelatedNodeInfo"}}, "text": "2554-2552.\n9. M. Krichen. 2024. ,ormal Metho.s an. -ali.ation Techniques for Ensuring Automoti0e 21stems 2ecurit1. Electronics 15, 4 (2024), 333. https://doi.org/10.44907electronics15040333\n10. H. Liu, M. Galin.o, H. Xie, L. Wong, H. 2huai, Y. Li, an. W. Cheng. 2025. Lightweight Deep Learning for :esource-Constraine. En0ironments: A 2ur0e1. arXi0 preprint arXi0:2505.07243 (2025).\n11. -. D. -iellieber an. M. A\u00dfenmacher. 2020. Pre-traine. language mo.els as knowle.ge bases for Automoti0e Complaint Anal1sis. arXi0 preprint arXi0:2012.02558 (2020).\n12. M. Ghaleb, H. Zolfagharinia, an. 2. Taghipour. 2020. :eal-time pro.uction sche.uling in the In.ustr1-5.0 context: A..ressing uncertainties in job arri0als an. machine break.owns. Computers & In.ustrial Engineering 124 (2020), 105041. https://doi.org/10.10137j.cie.2020.105041\n13. J. B. Gruber an. M. Weber. 2025. rollama: An : package for using generati0e large language mo.els through Ollama. arXi0 preprint arXi0:2505.07355 (2025).\n14. :. Hussain an. 2. Zea.all1. 2019. Autonomous Cars: :esearch :esults, Issues an. ,uture Challenges. IEEE Communications 2ur0e1s & Tutorials 21, 2 (2019), 1275-1414. https://doi.org/10.11097COM2T.2018.2839430\n15. Q. Ai, T. Bai, Z. Cao, Y. Chang, J. Chen, Z. Chen, Z. Cheng, 2. Dong, Z. Dou, ,. ,eng, 2. Gao, J. Guo, X. He, Y. Lan, C. Li, Y. Liu, Z. L1u, W. Ma, J. Ma, an. Z. :en. 2024. Information :etrie0al meets Large Language Mo.els: A strategic report from Chinese I: communit1. ,un.amental :esearch 5 (2024), 80-90. https://doi.org/10.10137j.fmre.2024.03.009\n16. M. Krafft, L. 2ajtos, an. M. Haenlein. 2020. Challenges an. Opportunities for Marketing 2cholars in Times of the ,ourth In.ustrial :e0olution. Journal of Interacti0e Marketing 51 (2020), 1-8. https://doi.org/10.10137j.intmar.2020.03.001\n17. D. Amalfitano, -. De 2imone, :. :. Maietta, 2. 2cala, an. A. :. ,asolino. 2019. Using tool integration for impro0ing traceabilit1 management testing processes: An automoti0e in.ustrial experience. Journal of 21stems an. 2oftware 151 (2019), e2171. https://doi.org/10.10027smr.2171\n18. Z. Jiang, ,. ,. Xu, L. Gao, Z. 2un, Q. Liu, J. Dwi0e.i-Yu, Y. Yang, J. Callan, an. G. Neubig. 2024. Acti0e :etrie0al Augmente. Generation. In Procee.ings of the 2024 Conference on Empirical Metho.s in Natural Language Processing. 7939-7992.\n19. Z. Wang, J. Araki, Z. Jiang, M. :. Par0ez, an. G. Neubig.", "mimetype": "text/plain", "start_char_idx": 2394, "end_char_idx": 4781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2b288ee-3eb4-4b24-83b8-4fcf7ca88042": {"__data__": {"id_": "d2b288ee-3eb4-4b24-83b8-4fcf7ca88042", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5d4b175-eb20-463f-8b6b-10d6d07c66cd", "node_type": "4", "metadata": {}, "hash": "4893c448c78333a1b05ad3507cfd1763f86951166bc48caa1b8cdebad767bec6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc35a834-8749-411f-a101-a41b4c276450", "node_type": "1", "metadata": {}, "hash": "35881124d2a20a3dc54aa36c505a5bf948c91f2198248f11a04deeb80f0d9a0f", "class_name": "RelatedNodeInfo"}}, "text": "Journal of 21stems an. 2oftware 151 (2019), e2171. https://doi.org/10.10027smr.2171\n18. Z. Jiang, ,. ,. Xu, L. Gao, Z. 2un, Q. Liu, J. Dwi0e.i-Yu, Y. Yang, J. Callan, an. G. Neubig. 2024. Acti0e :etrie0al Augmente. Generation. In Procee.ings of the 2024 Conference on Empirical Metho.s in Natural Language Processing. 7939-7992.\n19. Z. Wang, J. Araki, Z. Jiang, M. :. Par0ez, an. G. Neubig. 2024. Learning to ,ilter Context for :etrie0al-Augmente. Generation. arXi0 preprint arXi0:2411.08477 (2024).\n20. A. Asai, Z. Wu, Y. Wang, A. 2il, an. H. Hajishirzi. 2024. 2elf-:AG: Learning to :etrie0e, Generate, an. Critique through 2elf-:eflection. arXi0 preprint arXi0:2410.11511 (2024).\n21. D. :ajpathak, Y. Xu, an. I. Gibbs. 2020. An Integrate. ,ramework ,or Automatic Ontolog1 Learning ,rom Unstructure. :epair Text Data ,or Effecti0e ,ault Detection An. Isolation In Automoti0e Domain. Expert 21stems with Applications 124 (2020), 104448. https://doi.org/10.10137j.eswa.2020.104448\n22. 2. 2iriwar.hana, :. Weerasekera, E. Wen, T. Kaluarachchi, :. :ana, an. 2. Nana1akkara. 2022. Impro0ing the Domain A.aptation of :etrie0al", "mimetype": "text/plain", "start_char_idx": 4391, "end_char_idx": 5512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ead50d3-b982-476f-bf7c-a336ca49ffaf": {"__data__": {"id_": "9ead50d3-b982-476f-bf7c-a336ca49ffaf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a594c83a-1f35-4197-b839-36dc23daf492", "node_type": "4", "metadata": {}, "hash": "baa4c9aaea48529a6cebab44b31f306345bb953a4a366483d2a895f37ce31871", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4f4bc73-6784-4fcd-9991-b05455299885", "node_type": "1", "metadata": {}, "hash": "ddd29d568a7dcc982b8bc91df5cc936c0d7f3c652ba6e584ea34b70c2ed4db11", "class_name": "RelatedNodeInfo"}}, "text": "# Augmente. Generation (:AG) Mo.els for Open Domain Question Answering\n\nElectronics 11, 20 (2022), 4488.\n\nhttps://doi.org/10.44907/electronics11204488\n\n# References\n\n1. Yin, C., Zhao, K., Li, X., Xu, T., Chen, E. 2024. A survey on Multimodal Large Language Models. arXiv preprint arXiv:2403.14559 (2024).\n2. Burgan, C., Kowalski, J., Liao, W. 2025. Developing a Retrieval-Augmented Generation (:AG) Chatbot App Using Adaptive Large Language Models (LLM) and LangChain framework. IEEE Access 93 (2025). https://doi.org/10.1109/ACCESS.2025.4459173\n3. Wang, B., Li, G., Li, Y. 2024. Enabling Conversational Interaction with Mobile UI using Large Language Models. In Proceedings of the 43rd Annual ACM Symposium on User Interface Software and Technology. Article 542, 17 pages. https://doi.org/10.11574/583184.4303713\n4. Lin, D. 2025. Revolutionizing Retrieval-Augmented Generation with Enhanced PD, Structure Recognition. arXiv preprint arXiv:2501.12599 (2025).\n5. Bensch, O., Popa, M., Spille, C. 2021. Key Information Extraction from Documents - Evaluation and Generator. In Proceedings of the 4th International Conference on Deep Learning Theory and Applications. 57-54. https://doi.org/10.522070010517700570054\n6. Mass\u00e9, M., Huet, G., Colombo, P. 2024. Revisiting Instruction Fine-tuning Model Evaluation to Guide Industrial Applications. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 9044-9058.\n7. Kumar, A., Starl, B. 2021. labNE:: information extraction from manufacturing process science domain literature using named entity recognition. Journal of Intelligent Manufacturing 44 (2021), 2494-2507. https://doi.org/10.1007/s10855-021-01837-z\n8. Belhaj, D., Bela\u00ef, A., Bela\u00ef, Y. 2024. Improving Information Extraction from Semi-structured Documents Using Attention Based Semi-Variational Graph Auto-encoder. In Document Analysis Systems. 114-129. https://doi.org/10.1007/978-4-041-51382-8_8\n9. Zeng, J., Zhang, P., He, Y., Xing, Y., Liu, H., Xu, J., Zeng, Y., Wang, D., Yin, Y., Chang, J., Tang, J. 2025. The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (:AG). arXiv preprint arXiv:2502.13894 (2025).\n10. Borgeaud, S., Mensch, J., Hoffmann, T., Cai, E., Rutherford, K., Millican, G., Van den Driessche, J., Lespiau, B., Damoc, A., Clark, D., de Las Casas, A., Gu, J., Menick, T., Hennigan, J., Huang, L., Maggiore, C., Jones, A., Cassirer, A., Brock, M., Paganini, G., Irving, O., Lins, Z., Osinero, K., Simonian, J., W. Rae, E., Elsen, L. 2022. Improving Language Models by Retrieving from Trillions of Tokens. arXiv preprint arXiv:2112.05523 (2022).\n11. Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwiwedi-Yu, A., Joulin, A., Tieleman, O., Grace, E. 2024. ATLA2: Few-shot Learning with Retrieval-Augmented Language Models. Journal of Machine Learning Research 25 (2024), 251:1-251:54.\n12. Ahmad, A. 2025. Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A :AG Model Implementation for Multicultural Enterprise.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4f4bc73-6784-4fcd-9991-b05455299885": {"__data__": {"id_": "e4f4bc73-6784-4fcd-9991-b05455299885", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a594c83a-1f35-4197-b839-36dc23daf492", "node_type": "4", "metadata": {}, "hash": "baa4c9aaea48529a6cebab44b31f306345bb953a4a366483d2a895f37ce31871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ead50d3-b982-476f-bf7c-a336ca49ffaf", "node_type": "1", "metadata": {}, "hash": "b718896182da26d2150a5a5cf97476616bb099bc957f31592e6b7b9274920c0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aaf63c24-568b-4042-9470-9c26374b8d6c", "node_type": "1", "metadata": {}, "hash": "aaede1cc0cd0ded2d3ee9197244b4fa1fe0ad5392de682464a2d22d4e8e6d18b", "class_name": "RelatedNodeInfo"}}, "text": "2022. Improving Language Models by Retrieving from Trillions of Tokens. arXiv preprint arXiv:2112.05523 (2022).\n11. Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwiwedi-Yu, A., Joulin, A., Tieleman, O., Grace, E. 2024. ATLA2: Few-shot Learning with Retrieval-Augmented Language Models. Journal of Machine Learning Research 25 (2024), 251:1-251:54.\n12. Ahmad, A. 2025. Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A :AG Model Implementation for Multicultural Enterprise. arXiv preprint arXiv:2501.01511 (2025).\n13. Wu, Y., Tang, Q., Ai, Z., Wu, Y., Liu, Y. 2025. D:AGIN: Dynamic Retrieval-Augmented Generation based on the Information Needs of Large Language Models. arXiv preprint arXiv:2504.10081 (2025).\n14. Noorbakhsh, K., Zulaiman, M., Sharifi, M., Koi, P., Jamshidi, P. 2024. Pretrained Language Models are Symbolic Mathematics Solvers too!. arXiv preprint arXiv:2110.04501 (2024).\n15. Gamage, G., Mills, N., De Silva, D., Manic, M., Moraliage, H., Jennings, A., Alahakoon, D. 2025. Multi-Agent :AG Chatbot Architecture for Decision Support in Net-Zero Emission Energy Systems. In 2025 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT). 1-3. https://doi.org/10.1109/ISGT58487.2025.10571952\n16. Zhuang, B., Liu, J., Pan, Z., He, H., Weng, Y., Chen, C. 2024. A survey on efficient training of transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 3824-3841.\n17. Chen, W., Hu, H., Chen, X., Perga, P., Cohen, W. W. 2022. Mu:AG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 5558-5570.\n18. Xiao, Z., Liu, P., Zhang, N., Muennighoff, N. 2024. C-Pack: Packages Resources To Advance General Chinese Embedding. arXiv preprint arXiv:2409.07597 (2024). https://doi.org/10.585507/arXiv.2409.07597\n19. Karpukhin, V., O\u011fuz, A., Min, Z., Wu, L., E. Unno, D., Chen, W., Yih, W. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 3739-3781. https://doi.org/10.18354/70172020.emnlp-main.550\n20. Asai, A., Wu, Z., Wang, Y., Zil, A., Hajishirzi, H. 2024. Self-:AG: Learning to Retrieve, Generate, and Critique through Self-Reflection. arXiv preprint arXiv:2410.11511 (2024). https://doi.org/10.585507/arXiv.2410.11511\n21. Chen, W., Li, Z., Ma, M. 2025. Octopus: One-piece language model for function calling of software APIs. arXiv preprint arXiv:2505.01559 (2025). https://doi.org/10.585507/arXiv.2505.01559\n22. Anantha, S., Sakulenko, Z., Tu, Z., Longpre, D., Pulman, G. 2021. Open-Domain Question Answering Goes Conversational via Question Rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "mimetype": "text/plain", "start_char_idx": 2519, "end_char_idx": 5491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aaf63c24-568b-4042-9470-9c26374b8d6c": {"__data__": {"id_": "aaf63c24-568b-4042-9470-9c26374b8d6c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a594c83a-1f35-4197-b839-36dc23daf492", "node_type": "4", "metadata": {}, "hash": "baa4c9aaea48529a6cebab44b31f306345bb953a4a366483d2a895f37ce31871", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4f4bc73-6784-4fcd-9991-b05455299885", "node_type": "1", "metadata": {}, "hash": "ddd29d568a7dcc982b8bc91df5cc936c0d7f3c652ba6e584ea34b70c2ed4db11", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2410.11511 (2024). https://doi.org/10.585507/arXiv.2410.11511\n21. Chen, W., Li, Z., Ma, M. 2025. Octopus: One-piece language model for function calling of software APIs. arXiv preprint arXiv:2505.01559 (2025). https://doi.org/10.585507/arXiv.2505.01559\n22. Anantha, S., Sakulenko, Z., Tu, Z., Longpre, D., Pulman, G. 2021. Open-Domain Question Answering Goes Conversational via Question Rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. https://doi.org/10.18354/70172021.naacl-main.55\n23. Reynolds, D., Chen, D., Manning, C.D. 2019. CoQA: A Conversational Question Answering Challenge. Transactions of the Association of Computational Linguistics 7 (2019), 259-233. https://doi.org/10.11327/tacl_a_00233", "mimetype": "text/plain", "start_char_idx": 4926, "end_char_idx": 5758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fde08a8-70a6-413b-b7f4-848176d97431": {"__data__": {"id_": "4fde08a8-70a6-413b-b7f4-848176d97431", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a2f4afc-1c83-4217-9fd9-20cdb7ed568e", "node_type": "4", "metadata": {}, "hash": "c834be96c81910a2f499bbdbc2c7a3c1cdc9915e9cbd6c3b1b1b2b6f08c8d762", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b972ba3f-c2ca-4331-b913-a5d6c79776f0", "node_type": "1", "metadata": {}, "hash": "b95c8fcba4748fe2d5cad3f1bafd73a66d2ad24054334004c41a03ec471e0113", "class_name": "RelatedNodeInfo"}}, "text": "# Dynamical conductivity of disordered quantum chains\n\n# Shintaro Takayoshi1 and Thierry Giamarchi2\n\n# 1Department of Physics, Konan University, Kobe 658-8501, Japan\n\n# 2Department of Quantum Matter Physics, University of Geneva, Geneva 1211, Switzerland\n\n(Dated: November 15, 2022)\n\nWe study the transport properties of a one dimensional quantum system with disorder. We numerically compute the frequency dependence of the conductivity of a fermionic chain with nearest neighbor interaction and a random chemical potential by using the Chebyshev matrix product state (CheMPS) method. As a benchmark, we investigate the noninteracting case first. Comparison with exact diagonalization and analytical solutions demonstrates that the results of CheMPS are reliable over a wide range of frequencies. We then calculate the dynamical conductivity spectra of the interacting system for various values of the interaction and disorder strengths. In the high frequency regime, the conductivity decays as a power law, with an interaction dependent exponent. This behavior is qualitatively consistent with the bosonized field theory predictions, although the numerical evaluation of the exponent shows deviations from the analytically expected values. We also compute the characteristic pinning frequency at which a peak in the conductivity appears. We confirm that it is directly related to the inverse of the localization length, even in the interacting case. We demonstrate that the localization length follows a power law of the disorder strength with an exponent dependent on the interaction, and find good quantitative agreement with the field theory predictions. In the low frequency regime, we find a behavior consistent with the one of the noninteracting system \u03c92(ln \u03c9)2 independently of the interaction. We discuss the consequences of our finding for experiments in cold atomic gases.\n\n# I. INTRODUCTION\n\nDisorder has profound effects on quantum systems, as directly evidenced in the celebrated Anderson localization[1]. As shown by Anderson, disorder can change the plane waves of free particles to exponentially localized states with spectacular consequences for transport. Anderson localization is relevant for a host of systems ranging from condensed matter to classical waves. In particular, cold atomic gases, due to their remarkable controllability, have been instrumental in evidencing the localization of the wavefunction through seminal experiments of the groups of A. Aspect with laser speckles[2, 3] and M. Inguscio with quasi-periodic potentials[4, 5]. After sixty years since its proposal, Anderson localization still continues to present new challenges and mathematical developments[6].\n\nThe combination of disorder and interactions poses an additional layer of challenge especially in the context of condensed matter physics. This complicated problem in thermalized systems was tackled by perturbative[7], or renormalization group (RG) techniques in one dimension[8] and two dimensions[9\u201311]. While the disorder basically decelerates particles, which leads to a reinforcement of interactions, it can also weaken them due to the exponentially small overlap between two localized states. This competition is highly non-trivial and constitutes an intensively studied topic, which we call the problems of localization of interacting particles. Another direction related with this problem is to study the thermalization and ergodicity in isolated quantum systems with disorder and interactions, which is known under the name of many-body localization[12\u201315].\n\nOur target in this paper is one-dimensional systems, where strong quantum fluctuations lead to special states such as the Tomonaga-Luttinger liquid (TLL) characterized by correlations decaying in power law[16], and the effects of interactions are particularly strong. Furthermore, the disorder effects are also at their maximum, and even an infinitesimal disorder localizes all states for noninteracting particles. Thus one expects a severe competition between disorder and interactions. Renormalization group (RG) analysis shows the existence of a localized-delocalized transition both for fermions and bosons[8]. The localized phase for bosons, the Bose glass, persists in higher dimensions[17], and cold atomic systems again provides a controlled experimental access, confirming the Bose glass phase in biperiodic systems[18, 19].\n\nTo characterize disordered systems, transport is an important property. RG can access DC conductivity down to the temperature related to the inverse localization scale[8]. Below this scale, more phenomenological calculations predict the Mott variable range hopping behavior in the thermalized case of localization of interacting particles[20, 21] and zero DC conductivity in the isolated case of many-body localization[22]. AC transport also reflects the competition between disorder and interactions. Dynamical conductivity is exactly known for Anderson localization[23\u201325]. At high frequency, the behavior for the interacting particles can be extracted from the RG[8], and the low frequency behavior has been investigated by approximate methods such as a variational approach[26].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b972ba3f-c2ca-4331-b913-a5d6c79776f0": {"__data__": {"id_": "b972ba3f-c2ca-4331-b913-a5d6c79776f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a2f4afc-1c83-4217-9fd9-20cdb7ed568e", "node_type": "4", "metadata": {}, "hash": "c834be96c81910a2f499bbdbc2c7a3c1cdc9915e9cbd6c3b1b1b2b6f08c8d762", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fde08a8-70a6-413b-b7f4-848176d97431", "node_type": "1", "metadata": {}, "hash": "1e92068acaa0cb06d8bbf64871aee3bd02f8e93a10e6d605289758ff8117a579", "class_name": "RelatedNodeInfo"}}, "text": "The localized phase for bosons, the Bose glass, persists in higher dimensions[17], and cold atomic systems again provides a controlled experimental access, confirming the Bose glass phase in biperiodic systems[18, 19].\n\nTo characterize disordered systems, transport is an important property. RG can access DC conductivity down to the temperature related to the inverse localization scale[8]. Below this scale, more phenomenological calculations predict the Mott variable range hopping behavior in the thermalized case of localization of interacting particles[20, 21] and zero DC conductivity in the isolated case of many-body localization[22]. AC transport also reflects the competition between disorder and interactions. Dynamical conductivity is exactly known for Anderson localization[23\u201325]. At high frequency, the behavior for the interacting particles can be extracted from the RG[8], and the low frequency behavior has been investigated by approximate methods such as a variational approach[26]. Despite these efforts, no general methods are applicable to the full frequency range for dynamical conductivity. This situation is regrettable since cold atoms would be perfect systems to investigate such AC behavior of the conductivity with methods such as phase shaking of the optical lattice[27\u201329]. Indeed in biperiodic lattices, signatures of the localization such as a localization peak in the amplitude shaking of the optical lattice have been predicted[30] and observed[19].\n\nIn the present paper, we study the dynamical conductivity, i.e., the AC transport property as a function of frequency, in simple spinless fermion chains with nearest neighbor interactions in a random chemical potential. We perform numerical calculations using a variant of a Density Matrix Renormalization Group (DMRG) method.", "mimetype": "text/plain", "start_char_idx": 4186, "end_char_idx": 5999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "174eeaa9-7522-442d-bf5f-58881c2e1ef7": {"__data__": {"id_": "174eeaa9-7522-442d-bf5f-58881c2e1ef7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc05fd68-c0a4-49e3-8713-9dd61675966f", "node_type": "4", "metadata": {}, "hash": "3b15868a94b58c99faf3684425e73dce8f5874e4ee6c67c5c22e3f1882c357a3", "class_name": "RelatedNodeInfo"}}, "text": "# 2\n\ntion Group (DMRG) method to compute the dynamical quan-\ntity of disordered quantum systems with good precision. We\ncompare the obtained result for dynamical conductivity with\nthe field theory, and discuss its AC behavior over the full fre-\nquency regime. Such fermionic systems can be mapped either\nto Ising anisotropic spin chains in a random magnetic field or\nto hard core bosonic chains in a random chemical potential.\nThus the system on which we focus is quite generic to demon-\nstrate the applicability of our method and to study the physics\nof the dynamical conductivity in one-dimensional disordered\nquantum systems.\n\nThis paper is organized as follows. In Sec. II, we introduce\nthe model of spinless fermions with a random chemical poten-\ntial and nearest neighbor interactions, which is a target of this\npaper. This model is connected to the XXZ spin chain with\na random magnetic field. We also give the expression of dy-\nnamical conductivity using the Kubo formula. Section III ex-\nplains the numerical technique called Chebyshev matrix prod-\nuct state (CheMPS), which we mainly utilize to investigate\nthe interacting system. In Sec. IV, we describe the calculated\nresults of the dynamical conductivity obtained by the numer-\nics and its behavior in the high and low frequency regimes.\nThe numerical results are compared with analytical prediction\nfrom field theory. We summarize our results and discuss fu-\nture problems in Sec. V.\n\n# II. MODEL AND PHYSICAL QUANTITIES\n\nLet us consider a spinless fermion system with a nearest-\nneighbor interaction\n\nH\u02c6 = J N\u22121 [ 1(\u02c6l a\u2211 2 a\u2020\u02c6l+1 + H.c.) + \u2206(\u02c6l \u2212 2)(\u02c6l+1 \u2212 2n1)]\n\nwhere N is the number of sites, \u02c6l aa\u2020(\u02c6l) is the fermion creation\n(annihilation) operator at the site l, and \u02c6l \u2261 \u02c6l an a\u2020\u02c6l is the num-\nber operator. The random chemical potential hl on each site\ndistributes uniformly in the finite interval hl \u2208 [\u2212W, W]. Thus\nW represents the strength of disorder. In view of the numerical\nsolution of this model we assume open boundary conditions,\nwhile the analytic solutions are usually performed with pe-\nriodic boundary conditions. Without disorder, this system is\nparticle-hole symmetric and half-filled. In this case, the sys-\ntem is known to be described at low energy by a TLL Hamilto-\nnian (see Appendix A). Since this model is solvable by Bethe\nansatz, the TLL parameters can be exactly computed. For ex-\nample, the parameter K controlling the decay of correlation\nfunctions is given by\n\nK = [2(1 \u2212 arccos(\u2206)/\u03c0)]\u22121.\n\nFor \u2206 = 0, the Hamiltonian represents free fermions with a random\nchemical potential. It is known that in one dimension, such a system\nis always exponentially localized for W > 0. The situation becomes\ncomplicated in the interacting case \u2206 , 0, but an analysis using field\ntheory and bosonization is possible. Such an analysis, using a RG\nprocedure shows the existence of a quantum phase transition between\na localized and delocalized phases. The system is localized when K\nis smaller than 3/2 even for an infinitesimal W. The dependency of K\non \u2206 is shown in Fig. 1 and it can be seen that K < 3/2 corresponds\nto \u2206 > \u22121/2.\n\nBesides the phase diagram itself the main physical quantity we will\nbe computing in this paper is the frequency dependence of the\nconductivity. All calculations will be done at zero temperature. We\nuse the Kubo formula relating the conductivity to current-current\ncorrelations. The current operator is\n\nj\u02c6l = 2 iJ(\u02c6l aa\u2020\u02c6l+1 \u2212 \u02c6l+1 ala\u2020 \u02c6).\n\nand the current-current retarded correlation function is written", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73be25cd-c1c9-4077-8464-bb335acbd5ea": {"__data__": {"id_": "73be25cd-c1c9-4077-8464-bb335acbd5ea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99554233-b8dd-4d76-a9a5-2f1933e60f8c", "node_type": "4", "metadata": {}, "hash": "91854c49a1eba0705cc5d96dbe0159257c912184f8f269bca4166d4a06bf4387", "class_name": "RelatedNodeInfo"}}, "text": "# III. METHODS\n\nWe explain the numerical method that we use to treat the interacting systems with disorder. In Sec. IV, we discuss the dynamical conductivity of such systems by comparing the results from this numerical method with those from a field theory for the low energy limit of this model. The details of the field theory are described in Appendix A.\n\nTo tackle the problem of low-dimensional interacting quantum systems, numerical methods utilizing matrix product states, such as DMRG, are very effective. The dynamical quantities such as conductivity and Green\u2019s function can be computed by performing a real-time evolution with e.g., time-evolving block decimation, after obtaining the ground state by DMRG, and such techniques have been widely used.\n\nThe spectral functions are calculable through the Fourier transformation of the temporal correlation functions. However, in the real-time evolution of matrix product states, the entanglement of the systems grows exponentially and the achievable time interval is limited. The acquired frequency resolution in this way was not sufficient for our purpose. Therefore, we used a numerical method which calculates the spectral functions directly in the frequency space.\n\nIn particular, to perform our calculation of the conductivity, we focus on the method CheMPS. This is a combination of DMRG and the kernel polynomial method, a method to evaluate the spectral function.\n\nThe dynamical conductivity is calculated from the current-current correlation function as given by Eq. (7). However, in the spectral representation, the real part of the conductivity is rewritten from Eq. (7) as:\n\n\u03c3(\u03c9) = \u03c9N\nwhere |\u03a80\u3009 (|\u03a8\u03bd\u3009) is the ground (\u03bd-th excited) state and E0 (E\u03bd) is its energy eigenvalue. We will use the expression for the numerical evaluation of the conductivity.\n\nUsing the Chebyshev polynomials:\n\nTn(\u03c9\u2032) = cos(n arccos \u03c9)\nwe can expand the spectral function as:\n\nA \u02c61 O\u02c62 (\u03c9) = 2(1 \u2212 s)\u03c0 \u221a1 \u2212 \u03c9\u20322[\u03bc0 + 2\u2211\u03bcnTn(\u03c9\u2032)]\nThe Chebyshev moments are represented as \u03bcn = \u3008\u03a80|\u02c61|tn\u3009, where |tn\u3009 = Tn(H\u2032) \u02c62 |\u03a80\u3009 are Chebyshev vectors. The recurrence equations:\n\n|tn\u3009 = 2 \u02c6H\u2032 |tn\u22121\u3009 \u2212 |tn\u22122\u3009|t0\u3009 = \u02c62 |\u03a80\u3009|t1\u3009 = \u02c6H\u2032 |t0\u3009\nare useful to evaluate the coefficients \u03bcn numerically. In the numerical calculations, the expansion of Eq. (14) is performed up to some finite order M, and we multiply the weight \u03bcn by the Jackson damping factor:\n\ngn = (M \u2212 n + 1) cos +M + 1\nTherefore, the spectral function is numerically obtained as:\n\nA \u02c61 2 (\u03c9) ' 2(1 \u2212 s)\u03c0 \u221a1 \u2212 \u03c9\u203221\nIn this study, we calculate the ground state |\u03a80\u3009 using DMRG, then obtain the matrix product state representation of |tn\u3009 by the recurrence equations. The system size is N = 250, the energy width is \u03a9 = 6, the bond dimension of matrix product representation is MB = 64, and the order of expansion is M = 200.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d975e5d-030c-4343-a9b6-aea2f6450c42": {"__data__": {"id_": "7d975e5d-030c-4343-a9b6-aea2f6450c42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1502670-14ae-4b25-a506-eef06caab003", "node_type": "4", "metadata": {}, "hash": "78100ac5a296feb388e0e0b2050a2d2fe78fa5cf6334f305d59f52e5743bbbdd", "class_name": "RelatedNodeInfo"}}, "text": "# IV. RESULTS\n\nWe now examine the results obtained by the numerical method described in Sec. III. Before dealing with the interacting case, which is the focus of our study, let us first discuss the dynamical conductivity in the noninteracting case, which corresponds to Anderson localization, where both simpler numerical solutions and analytical approaches are available. Through the comparison of the results from CheMPS with from simpler methods, we can provide a benchmark for the reliability and applicability of CheMPS.\n\n# A. Noninteracting case\n\nLet us first revisit the noninteracting case \u2206 = 0. Without interactions, Eq. (1) is a tight binding system of free spinless fermions at half-filling.\n\nThe low energy region \u03c9/J 1, the 1/\u03c9 factor in the right hand side of Eq. (7) enhances the numerical error. Hence, to avoid this problem we employ the polarization-current correlation function instead of the current-current one. The polarization operator is defined as\n\nP = \u2211l=1N \u02c6nl\n\nand it is related to the current operator through the time derivative\n\n\u2202P/\u2202t = \u2212i\u2211l=1N[ \u02c6l, jtot ]\n\nThe polarization-current correlation function becomes\n\nCpc(t) = \u2212i\u03d1(t) \u3008[ \u02c6P(t), jtot(0)]\u3009\n\nand its time derivative is the current-current correlation function\n\n\u2202Cpc(t)/\u2202t = \u2212i\u03d1(t) \u3008[ \u2202P(t), jtot(0)]\u3009 = Ccc(t).\n\nPerforming the integral by part in Eq. (7), we obtain\n\n\u03c3(\u03c9) + i\u03c3\u2032(\u03c9) = \u03c9 \u222b\u2212\u221e\u221e dtei\u03c9t \u2202Cpc(t) = N \u222b\u2212\u221e\u221e dtei\u03c9tCpc(t).\n\nThus, the dynamical conductivity is represented as\n\n\u03c3(\u03c9) = N \u3008\u03a80| \u02c6\u03b4(\u03c9 \u2212 H + E0) \u02c6jtot|\u03a80</sub\u3009\n\nwhich can be evaluated by CheMPS with \u02c61 = \u02c6P and \u02c62 = \u02c6jtot in Eq. (9).\n\nThe expression for the dynamical conductivity is given as\n\n\u03c3(\u03c9) = \u03c30\u2211n=0\u221e Qn(Rn \u2212 Rn+1)\n\nwhere Qn and Rn are the solution of the recurrence equations\n\n2i\u03c9\u03c4Rn + n(Rn+1 + Rn\u22121 \u2212 2Rn) = 0\n\n2i\u03c9\u03c4(n + 1/2)Qn + (n + 1)2(Qn+1 \u2212 Qn) \u2212 n2(Qn \u2212 Qn\u22121) + Rn \u2212 Rn+1 = 0\n\nwith the boundary condition\n\nR0 = 1, i\u03c9\u03c4Q0 + Q1 \u2212 Q0 + R0 \u2212 R1 = 0\n\nIn practice, starting from the initial condition Qn = 0 and Rn = 0 for large enough n, we can obtain numerically Qn\u22121, . . . , Q0 and Rn\u22121, . . . , R0 from Eqs. (19) and (20), and finally normalize the sequence {Q0, . . . , Qn} and {R0, . . . , Rn} so as to satisfy the condition Eq. (21). Here, \u03c4i and \u03c30 are fitting parameters. Fig. 2 shows the dynamical conductivity \u03c3(\u03c9) calculated by the above procedure. In the high frequency region, the dynamical conductivity decays as a power law \u03c3(\u03c9) \u221d \u03c9\u22122. In the low frequency region, the predicted analytical behavior is \u03c3(\u03c9) \u221d \u03c92(ln \u03c9)2 [20, 23, 38]. As can be seen from Fig. 2, such a behavior fits much better the data than \u03c3(\u03c9) \u221d \u03c92.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b29baf98-2f39-4209-9fcb-cb911f38da8a": {"__data__": {"id_": "b29baf98-2f39-4209-9fcb-cb911f38da8a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d43540c1-b004-4f4c-b6a0-539000df6522", "node_type": "4", "metadata": {}, "hash": "3164f45305e21bf9198bb7eb4ff18e001fcf4d9bf646e44f074adc98a426fdf3", "class_name": "RelatedNodeInfo"}}, "text": "# 5\n\n# B. Interacting case\n\nLet us now turn to the interacting disordered systems. In this case, since the dimension of the Hilbert space grows exponentially by increasing the system size, ED is not a viable method any more and we employ the CheMPS method described in the Sec. III as a numerical approach as well as the bosonization and variational replica approach as analytical methods to calculate the dynamical conductivity.\n\nTo benchmark the method, let us first compare the results of CheMPS for system size N = 250 with \u2206 = 0 and disorders W/J = 0.1, 0.2, 0.4, and 0.8 with the ones obtained in the previous section with ED. The comparison is shown in Fig. 4. As can be seen from the comparison, the agreement is good over two decades of frequency, particularly in the regime \u03c9/J > 0.05. To check the finite size effect, we also compare the same CheMPS data with the ED result for N = 3200 (the same data as Fig. 3) in Fig. 4(b). While the agreement is good for large W, there is a deviation for small W. However, in the high energy region, the deviation is just a multiplication of a constant factor, and the power of the decay does not change. This confirms that our numerical approach properly captures the behavior of the dynamical conductivity in both low and high frequency regions at zero temperature. Hence CheMPS is a very useful technique for dealing with interacting systems, for which no other quantitative method is available.\n\nIn Fig. 5(a), we show the numerically calculated dynamical conductivity for the cases of \u2206 = 0, 0.2, and 0.4. We can see that the decay power of the high frequency region changes as we increase the interaction \u2206. To discuss the high frequency region and to compare the results with the bosonized field theory, a relatively small disorder strength is desirable, and we adopt W/J = 0.1 here. We fit the data in the high frequency region by a power law \u221d \u03c9\u207b\u03bc and plot \u03bc as a function of \u2206 in Fig. 5(c). We confirm that the variance of the data for dynamical conductivity shown in Fig. 5(a) is negligibly small by comparing the results for three bins of the various disorder configurations over which the average is taken. The variations of the data in Fig. 5(c) mainly arise from the power law fitting, and we estimate the error bars from the fittings in several frequency intervals on the high frequency regime.\n\nLet us compare the numerical results with the prediction from the field theory on the continuum model [8] (see Appendix).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "122cafec-d41e-4b4a-853e-e5f2f2c42f34": {"__data__": {"id_": "122cafec-d41e-4b4a-853e-e5f2f2c42f34", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06e0aba7-993a-432b-a014-8befc79c2344", "node_type": "4", "metadata": {}, "hash": "b5569f65c55830158381cf896a3e32a656f04dd89c50f3e904cc1bac0314dc66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dffcc6b-65d4-45b1-bcf8-788d3f5a318b", "node_type": "1", "metadata": {}, "hash": "ba0275052baf0a389c85bf6f86536393cf62512f293a0de7ccae5b7ed45fb630", "class_name": "RelatedNodeInfo"}}, "text": "# 6\n\npendix A). The behavior in the high frequency regime remains a power law while the exponent is indeed modified by introducing the interaction. Note that the precise analytical form is more complicated than a simple power law since the TLL parameter K is renormalized and depends on the scale. However, far from the transition point K = 3/2, the simple power law decay which neglects this renormalization becomes a good approximation [8, 16]. The modification of the exponent by the interaction reflects the renormalization of the scattering on the disorder and the power law behavior of the susceptibility of the charge-charge correlations in TLL. The analytical prediction for the exponent is \u03bc = 4 \u2212 2K [8, 16], which naturally reproduces the exponent \u03bc = 2 for the noninteracting case K = 1. The parameter K takes the value of K < 1 for repulsive interactions and K > 1 for attractive ones. As seen in Fig. 5(c), the numerically evaluated \u03bc has a similar global trend as the expectations from the field theory (dashed line), which is obtained by substituting the Bethe ansatz evaluation of K for TLL in the XXZ chain Eq. (2) into \u03bc = 4 \u2212 2K. In particular, in the region of \u22120.2 . \u2206 . 0.5, the numerically calculated \u03bc agrees quantitatively well with the analytical prediction. Figure 5(b) shows the plotting of (\u03c9/J)\u00b2\u03c3(\u03c9) for the same data as Fig. 5(a). The plotting of the data for \u2206 = 0 is almost horizontal, which indicates the decay follows \u03c3(\u03c9) \u221d \u03c9\u207b\u00b2. We can see that the decay power \u03bc increases as \u2206 becomes larger.\n\nHowever, there exist surprising quantitative differences between the numerical results and the field theoretical predictions in the attractive regime (\u2206 . \u22120.2) and the strong repulsive regime (\u2206 & 0.5). The origin of these discrepancies is not clear at the present stage. On the repulsive side, a plateau-like structure appears in the regime \u2206 & 0.5 for the numerical results, which is incompatible with the exponent predicted by the continuum model. Several reasons are conceivable for this behavior. One possibility is the effect of irrelevant operators neglected in the field theoretical treatment. In particular, the scaling dimension of the irrelevant operator cos(4\u03c6) representing the umklapp processes on the lattice lowers as \u2206 is increased, and it finally becomes marginal in the \u2206 \u2192 1 limit. Another possibility is an error of the numerical extraction of \u03bc. As seen from Fig. 5(a), the localization frequency scale, i.e., the pinning frequency characterized by the peak of the dynamical conductivity, shifts to higher frequency as \u2206 is increased. Hence the region where the curve is fitted by the power law becomes narrower, and the estimation becomes less precise. On the attractive side, if we approach the localization-delocalization transition point K = 3/2 (\u2206 = \u22120.5), the renormalization flow is closer to the separatrix and the direction of the flow is not parallel to the disorder axis. Thus the effect of renormalization of the parameter K should become more important. To elucidate the reasons for this discrepancy between the numerics and the field theory is a very challenging problem and we leave it for a future study.\n\nLet us now turn to the disorder dependence of the pinning frequency and the conductivity at the peak for the interacting system. In Fig. 7(a), we show the dynamical conductivity calculated for the interaction \u2206 = 0.5 and various disorder strengths W/J = 0.1, 0.2, 0.4, 0.8. Similarly than for the noninteracting case [see Fig. 3(a)], the pinning frequency \u03c9p increases and the peak height \u03c3(\u03c9p) decreases as the disorder strength W is increased. We plot \u03c9p and \u03c3(\u03c9p) as a function of W in Fig. 3(b). The pinning frequency and the peak height are well fitted as:\n\n\u03c9p \u221d W4/3\n\u03c3(\u03c9p) \u221d W\u22124/3\n# FIG. 5.\n\n(a) The dynamical conductivity \u03c3(\u03c9) for interactions \u2206 = 0, 0.2, and 0.4, and the disorder strength W/J = 0.1. The solid lines represent the power-law fitting in the high energy region. The frequency dependence of the conductivity is well represented by an interaction dependent exponent at high frequencies. (b) The plotting of (\u03c9/J)\u00b2\u03c3(\u03c9) for the same data as Fig.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dffcc6b-65d4-45b1-bcf8-788d3f5a318b": {"__data__": {"id_": "0dffcc6b-65d4-45b1-bcf8-788d3f5a318b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06e0aba7-993a-432b-a014-8befc79c2344", "node_type": "4", "metadata": {}, "hash": "b5569f65c55830158381cf896a3e32a656f04dd89c50f3e904cc1bac0314dc66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "122cafec-d41e-4b4a-853e-e5f2f2c42f34", "node_type": "1", "metadata": {}, "hash": "a967d6d5eaef2c00c6558dc4caf5592e0a9826024c144f4848b1b7aafcf18bc1", "class_name": "RelatedNodeInfo"}}, "text": "3(a)], the pinning frequency \u03c9p increases and the peak height \u03c3(\u03c9p) decreases as the disorder strength W is increased. We plot \u03c9p and \u03c3(\u03c9p) as a function of W in Fig. 3(b). The pinning frequency and the peak height are well fitted as:\n\n\u03c9p \u221d W4/3\n\u03c3(\u03c9p) \u221d W\u22124/3\n# FIG. 5.\n\n(a) The dynamical conductivity \u03c3(\u03c9) for interactions \u2206 = 0, 0.2, and 0.4, and the disorder strength W/J = 0.1. The solid lines represent the power-law fitting in the high energy region. The frequency dependence of the conductivity is well represented by an interaction dependent exponent at high frequencies. (b) The plotting of (\u03c9/J)\u00b2\u03c3(\u03c9) for the same data as Fig. 5(a). (c) Exponent, as a function of the interaction \u2206, of the decay of the conductivity with frequency in the high frequency region. This exponent results from fits \u03c3(\u03c9) \u221d \u03c9\u207b\u03bc similar to the ones of (a). The disorder strength is W/J = 0.1. The dashed line is the theoretical value of the exponent from the field theory \u03bc = 4 \u2212 2K and the Bethe-ansatz value of the TLL parameter K (see text). For repulsive interactions \u2206 > 0 there is a reasonable agreement up to \u2206 \u223c 0.5 beyond which there is a plateau like behavior not expected by the field theory. On the attractive side \u2206 < 0 strong deviations are observed even for relatively small attraction.\n\n# FIG. 6.\n\n(a) Dynamical conductivity calculated for the interaction \u2206 = 0.5 with disorder strengths W/J = 0.1, 0.2, 0.4, and 0.8. as for the noninteracting case one observes a peak of the conductivity at a pinning frequency \u03c9p. (b) The pinning frequency \u03c9p and the corresponding value of the conductivity at this frequency \u03c3(\u03c9) obtained from the data in the panel (a). The solid lines represent the fitting curves \u03c9p/J = 0.34(W/J)4/3 and \u03c3(\u03c9p) = 0.48(W/J)\u22124/3.", "mimetype": "text/plain", "start_char_idx": 3504, "end_char_idx": 5253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cffdc72-4148-4604-abcf-769e44753198": {"__data__": {"id_": "8cffdc72-4148-4604-abcf-769e44753198", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "436ab925-3e02-4549-882d-b89e3aa38c86", "node_type": "4", "metadata": {}, "hash": "9eeac75aa0b6dc25cffc9f03de3c3fa5ab1a2d61ffffb0c3d4c84379b8e5340f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6df22e93-5190-40ab-b469-f9544af75f37", "node_type": "1", "metadata": {}, "hash": "a22622fe0f783774cba9fef4d05c1388a3992ff9941e82510aa55ae4c6545bb4", "class_name": "RelatedNodeInfo"}}, "text": "# V. DISCUSSIONS AND SUMMARY\n\nNote that the data points of \u03c9p and \u03c3(\u03c9p) for W/J = 0.1 deviate from the fitting curves. We attribute it to the large finite size effect in the case of small W, as mentioned in the benchmark result [Fig. 4(b)].\n\nThis scaling is in good agreement with the analytical predictions that \u03c9p \u221d \u03be\u22121 and \u03c3(\u03c9p) \u221d \u03be [See Eq. (A16) in Appendix A]. Using the dependence of K on interactions Eq. (2), we obtain K = 3/4 for \u2206 = 0.5. This leads to \u03be \u221d W\u22124/3 by using the formula Eq. (A14) in excellent agreement with the numerical results. As can be seen both from the numerics and the above formula, repulsive interactions lead to a shorter localization length than for the noninteracting case.\n\nFinally let us discuss the behavior in the low frequency regime. In order to get a sizable range of the frequency interval below the pinning peak, and to prevent finite size effects from playing a major role, we take a relatively large value of disorder W/J = 0.8. The dynamical conductivity calculated for \u2206 = 0, 0.3, 0.5, and 0.8 is shown in Fig. 7(a). While there is a clear dependence of the exponent of the power law decay on the interaction \u2206 in the high frequency regime, the behavior remains similar on the low frequency side. One can see it more clearly in Fig. 7(b), where the curves have been rescaled by the value of the pinning frequency and conductivity at the peak.\n\nThe fitting curves by \u03c3(\u03c9) \u221d \u03c92(ln \u03c9)2 and \u03c3(\u03c9) \u221d \u03c92 are also shown, and the former fitting looks better than the latter. However, the present interval of data fitting below the peak is just one decade of frequency, and the acquisition of the data over a wider range is desirable for a more precise analysis.\n\nOn a qualitative side one would indeed expect to recover at low enough frequencies essentially the noninteracting behavior. Indeed frequencies lower than the pinning frequency correspond to probing scales large compared to the localization length. At that scales since the particles are exponentially localized, the effect of interactions should drastically decrease, leading back to the noninteracting behavior. On a more quantitative level, it is difficult to make an unambiguous comparison with existing analytical formulas, since the various calculations of the low frequency behavior suffer from their own limitations. The variational calculation [26] is unable to capture the logarithmic correction to the \u03c92 behavior. Calculations containing the logarithmic correction rely either.\n\nIn this paper, we have numerically computed the frequency dependence of the conductivity in one-dimensional spinless fermions with a nearest neighbor interaction and a random chemical potential. This problem is equivalent to XXZ spin chains in a random magnetic field along the z axis and to hard core bosons with nearest neighbor interactions and a random chemical potential. Using the CheMPS method, a variant of DMRG, adapted to the case we study, we have numerically calculated the dynamical conductivity over a broad range of frequencies, interactions, and disorder strength. We have benchmarked our method by comparisons with the non-interacting case. Since analytical approaches and numerical exact diagonalizations are applicable for the noninteracting systems, these results have been compared with those from CheMPS and we have confirmed that our method does capture the frequency dependence of the conductivity in a broad range of frequencies.\n\nWe have then investigated the dynamical conductivity of the interacting systems with CheMPS. In the high frequency regime, the conductivity follows a power law \u03c3(\u03c9) \u221d \u03c9\u2212\u03bc. We have calculated \u03bc for various interaction \u2206, and found it agrees quantitatively with the expectation from the field theory \u03bc = 4 \u2212 2K and the K value from BA in \u22120.2 . \u2206 . 0.5. However there exist a deviation in the attractive and strongly repulsive regions. On the attractive side \u2206 . \u22120.2 (K &gt; 1.15), the estimated exponent \u03bc seems larger than expected from the TLL determination. On the repulsive side, a plateau-like structure appears in the region \u2206 &gt; 0.5 (0.5 \u2264 K &lt; 0.75). We leave the clarification of the reasons for this deviation for future studies.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6df22e93-5190-40ab-b469-f9544af75f37": {"__data__": {"id_": "6df22e93-5190-40ab-b469-f9544af75f37", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "436ab925-3e02-4549-882d-b89e3aa38c86", "node_type": "4", "metadata": {}, "hash": "9eeac75aa0b6dc25cffc9f03de3c3fa5ab1a2d61ffffb0c3d4c84379b8e5340f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cffdc72-4148-4604-abcf-769e44753198", "node_type": "1", "metadata": {}, "hash": "5acee882696475fb785d52905e23653f3adf026aae5d7a3fbb63e5025fd7bdb4", "class_name": "RelatedNodeInfo"}}, "text": "In the high frequency regime, the conductivity follows a power law \u03c3(\u03c9) \u221d \u03c9\u2212\u03bc. We have calculated \u03bc for various interaction \u2206, and found it agrees quantitatively with the expectation from the field theory \u03bc = 4 \u2212 2K and the K value from BA in \u22120.2 . \u2206 . 0.5. However there exist a deviation in the attractive and strongly repulsive regions. On the attractive side \u2206 . \u22120.2 (K &gt; 1.15), the estimated exponent \u03bc seems larger than expected from the TLL determination. On the repulsive side, a plateau-like structure appears in the region \u2206 &gt; 0.5 (0.5 \u2264 K &lt; 0.75). We leave the clarification of the reasons for this deviation for future studies. We have also evaluated the localization length \u03be from the pinning frequency \u03c9p \u221d \u03be\u22121 as a function of the disorder strength, and found a reasonable agreement with the expected behavior of the localization length as determined by RG: \u03be \u221d (1/W2)1/(3\u22122K) and which is now dependent on both interactions and disorder. In the low frequency regime, we have performed numerical calculations for a large disorder W/J = 0.8. The scaling of dynamical conductivity is compatible with an \u03c92(ln \u03c9)2 behavior similar to the one of the noninteracting case but with a prefactor varying as (\u03be(\u2206)/\u03be(\u2206 = 0))3.\n\nThe low frequency behavior is difficult to access and although the numerics is indeed compatible with the \u03c92(ln \u03c9)2 behavior to ascertain the existence and power of the log correction.", "mimetype": "text/plain", "start_char_idx": 3543, "end_char_idx": 4970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dc69084-6ea9-4abe-82e9-d858612e5277": {"__data__": {"id_": "0dc69084-6ea9-4abe-82e9-d858612e5277", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b214e52d-3193-41a0-a7d9-5ae726bfbcc9", "node_type": "4", "metadata": {}, "hash": "2ecd9e13c2cb0bbbaf92e53b8974b645fc007e073094d00cf763e3091ce44499", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceb623d5-af3f-4a33-86a6-d6c64c9c5bdd", "node_type": "1", "metadata": {}, "hash": "46e26e2d23722ef2e8540aaf85bc72c71d685dcd72e49f088fa94a1e3b8c3817", "class_name": "RelatedNodeInfo"}}, "text": "# 8\n\nrection data over a wider range of frequency is needed, an-\n\nanother challenge for future studies.\n\nIn addition to pushing the numerical investigations of the conductivity, it would of course be extremely interesting to test the results obtained in our study in cold atom experi-\n\nments. The existence of a peak as a response to shaking, sim-\n\nilar to the pinning peak in the conductivity discussed here, was indeed observed for bosons in a quasiperiodic potential and has been used as a key signature of the existence of the Bose glass in these systems. However, due to the large in-\n\nhomogeneities arising from the quadratic trapping potential, testing the power law behavior was not practically feasible. The existence of fermionic systems with disorder in quantum microscopes makes it possible to observe the features com-\n\nputed here more easily. In preforming a comparison with ex-\n\nperiments, we need to note the following points: i) the ex-\n\nperimental system size realizable for the moment are still rel-\n\natively small, typically of the order of 20 to 50 atoms per chain; ii) the temperature is still relatively high in fermionic systems. These conditions should not drastically affect the properties in the high frequency regime, roughly for \u03c9 > T, but of course will essentially change the low frequency behav-\n\nior of the dynamical conductivity. The low frequency regime is ultimately connected to the question of variable range hop-\n\nping and many-body localization. Addressing these issues via experiments and by the extension of numerical techniques to\n\nfinite temperature is clearly a considerable challenge.\n\n# ACKNOWLEDGMENTS\n\nThe authors appreciate fruitful discussions with Michele Filippone. S. T. is supported by JSPS KAKENHI Grant No. JP21K03412 and JST CREST Grant No. JPMJCR19T3, Japan. This work is partly supported by the Swiss National Science Foundation under Division II.\n\nThis paper is dedicated to Alain Aspect, who in addition to his numerous contributions in atomic physics and quan-\n\ntum information is responsible, via key experiments, to having made, the subject of disorder a very hot topic in cold atomic systems. Thank you Alain for being a constant source of in-\n\nspiration and stimulation, also in this field of \u201cdirty\u201d quantum systems.\n\n# Appendix A: Field theory solution\n\nAs a framework to discuss the numerical solution, let us give here a short summary of the field theory approach to this problem [8]. The most convenient way to deal with the inter-\n\nactions is to use the bosonization representation [16].\n\nThrough the Fourier transformation of the fermionic opera-\n\ntors (the lattice constant is set to unity):\n\na\u02c6k = 1/\u221aN \u2211 e^(-ikl) \u02c6l = 1/\u221aN \u2211 e^(ikl) \u02c6l a\u2020,\n\nthe disorder term in the Hamiltonian becomes\n\nH\u02c6dis = -\u2211hl(\u02c6l - 2) = -NN \u2211h(q)\u02c6k+q a\u2020 \u02c6k,\n\nFermionic systems Eq. (1) with a Fermi momentum kF = \u03c0/2 (half-filled band) corresponds to the nonmagnetized case in spin chains. For such systems special care should be ex-\n\nerted to treat the disorder and the situation is slightly more involved than starting from an incommensurate filling. As is well-known in one dimension, the system has low energy\n\nexcitations at momenta q ' 0 and q ' 2kF [16]. It is thus useful to separate the disorder into two slowly varying fields centered around these momenta, which are called forward and backward scattering respectively,\n\nh(x) = \u2211 eiqx h(q) for q'0\nhb(x) = \u2211 eiqx h(q + \u03c0) for q'0.\n\nNote that in the case of 2kF = \u03c0, the backward scattering leads to a real field hb(x). Since h(f) and hb(x) involve components of h(q) which are different in q, their cross averages are zero and they can be considered as two independent random fields with essentially delta function correlations upon average.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ceb623d5-af3f-4a33-86a6-d6c64c9c5bdd": {"__data__": {"id_": "ceb623d5-af3f-4a33-86a6-d6c64c9c5bdd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b214e52d-3193-41a0-a7d9-5ae726bfbcc9", "node_type": "4", "metadata": {}, "hash": "2ecd9e13c2cb0bbbaf92e53b8974b645fc007e073094d00cf763e3091ce44499", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dc69084-6ea9-4abe-82e9-d858612e5277", "node_type": "1", "metadata": {}, "hash": "6c0fb0d697b0e095f9fb712fd45e27389e2bb229d3674663318b3643e65a27e6", "class_name": "RelatedNodeInfo"}}, "text": "For such systems special care should be ex-\n\nerted to treat the disorder and the situation is slightly more involved than starting from an incommensurate filling. As is well-known in one dimension, the system has low energy\n\nexcitations at momenta q ' 0 and q ' 2kF [16]. It is thus useful to separate the disorder into two slowly varying fields centered around these momenta, which are called forward and backward scattering respectively,\n\nh(x) = \u2211 eiqx h(q) for q'0\nhb(x) = \u2211 eiqx h(q + \u03c0) for q'0.\n\nNote that in the case of 2kF = \u03c0, the backward scattering leads to a real field hb(x). Since h(f) and hb(x) involve components of h(q) which are different in q, their cross averages are zero and they can be considered as two independent random fields with essentially delta function correlations upon average. The fermionic field \u03c8(x) = \u2211k e^(ikx) \u02c6k can be represented in terms of right- and left-movers [16]\n\n\u03c8(x) = e^(ikF x) \u03c8R(x) + e^(-ikx) \u03c8L(x),\n\nand the disorder term is recast into\n\nH\u02c6dis = -\u222b dx h(x) [\u03c8R\u2020(x) \u03c8R(x) + \u03c8L\u2020(x) \u03c8L(x)]\n- \u222b dx hb(x) [\u03c8R\u2020(x) \u03c8L(x) + \u03c8L\u2020(x) \u03c8R(x)],\n\nby noting hb*(x) = hb(x). The current operator becomes\n\nj\u02c6 = vF [\u03c8R\u2020(x) - \u03c8L\u2020(x) \u03c8L(x)],\n\nwhere vF is the Fermi velocity. The forward scattering part of the disorder can be eliminated by the gauge transforma-\n\ntion [8]:\n\n\u03c8R \u2192 e^(viF \u222b x dy h f (y)) \u03c8R(x)\n\u03c8L \u2192 e^(-vFi \u222b x dy h f (y)) \u03c8L(x).\n\nThis transformation does not affect the current operator, but changes the backscattering term to\n\n\u222b dx hb(x) [e^(v2iF \u222b x dy h(y)) \u03c8R\u2020(x) \u03c8L(x) + H.c.].\n\nOne thus see that the backscattering part of disorder is re-\n\n\u222b x dy h(y) with correlations\n\u03be(x) = hb(x)e^(2 vFi \u222b x dy h f (y)),\n\nwhere\n\n\u03be(x) \u03be(y) = 0,\n\u03be(x) \u03be*(y) = Db \u03b4(x \u2212 y).", "mimetype": "text/plain", "start_char_idx": 2939, "end_char_idx": 4650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29286be9-d9d2-4f9d-b294-736eeeb0aec3": {"__data__": {"id_": "29286be9-d9d2-4f9d-b294-736eeeb0aec3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ef9dd3c-45de-41c4-8960-4619101e42dd", "node_type": "4", "metadata": {}, "hash": "ea0838ca8e283347ef1a1c59cf02f16a19da1fedd153dbd9ce4e84ea63d44158", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5817d98-5744-4dd2-acf1-3b393086b5f5", "node_type": "1", "metadata": {}, "hash": "6735d42e39492a4b704ec6a3b44035d9c70ab57cce24931a8acc67f6239a5331", "class_name": "RelatedNodeInfo"}}, "text": "# 9\n\nwhere Db \u221d W2. This reflects the breaking of the particle-hole symmetry which is caused by one realization of the random chemical potential. Note that a system with perfect particle hole symmetry, such as a random bond system, would lead to a different fixed point, namely the random singlet phase [41].\n\nWe employ the usual representation of the fermion operators in terms of collective fields related to density and current, respectively [16], and connect them in the spin language [Eq. (4)] to the two angles needed to dictate the direction of the spin vector:\n\nSl+ ' (\u22121)l ei\u03b8(x) + ei\u03b8(x) cos(2\u03c6(x)),\n\nSl ' \u2212\u2207\u03c6(x)z + (\u22121)l cos(2\u03c6(x)),\n\n(A10)\n\nwhere x is the position of l-th spin, and the prefactors are omitted. Then the Hamiltonian is written as [8, 16]:\n\nH\u02c6 = 1 \u222b dx [uK(\u2207\u03b8(x))2 + K u(\u2207\u03c6(x))2]\n\n2\u03c0 \u222b\u2212 dx [\u03be(x)ei2\u03c6(x) + H.c]\n\n(A11)\n\nHere u is the velocity of excitations (u = vF for the noninteracting case) and K is the dimensionless parameter controlling the decay of correlations. We have neglected the irrelevant operator cos(4\u03c6(x)) which appears for the special case of nonmagnetized spin chains, or equivalently half-filled fermion chains. This operator is irrelevant for K > 1/2.\n\nIn this representation the current operator is the time derivative of the field \u03c6(x) [16] and thus the conductivity Eq. (7) is simply related to the Green\u2019s function of the field \u03c6(x) by:\n\n\u03c3(\u03c9) = \u2212 i2\u03c9 \u222b0\u221e dx dt \u3008[\u03c6(x, t), \u03c6(0, 0)]\u3009 ei\u03c9t\u2212t\n\n(A12)\n\nwhere \u03c3 is in units of e/h and \u03b5 = 0+ is a convergence factor.\n\nThe disorder term in Eq. (A11) can be eliminated for a Gaussian disorder by using the replica trick [8, 16]. This leads to an action of the form:\n\nS = 1 \u2211 \u222b dxd\u03c4 [1(\u2202\u03c4\u03c6\u03b1(x))2 + u(\u2202x\u03c6\u03b1(x))2]\n\n\u03c0K2 \u2212 Db \u2211 \u222b dxd\u03c41d\u03c42 cos[2\u03c6\u03b1(x, \u03c41) \u2212 2\u03c6\u03bd(x, \u03c42)]\n\n(A13)\n\nwhere \u03b1, \u03bd = 1, 2, . . . , n are the replica indices and the limit n \u2192 0 must be taken.\n\nThe disorder term is relevant for K < 3/2 even for an infinitesimal strength of the disorder Db. For K > 3/2 (\u2206 < \u22120.5), there is a separatrix, with a transition of the Berezinskii-Kosterlitz-Thouless universality class between the localized and a delocalized TLL phase [8].\n\nThe localization length can be captured by an RG evaluation of this problem. Far from the transition point K = 3/2 the localization length behaves as [8]:\n\n\u03be \u221d ( 1 / Db )\u22123/2K\n\n(A14)\n\nIn addition to the RG method which can only give access to the physical quantities, either in the regime where the disorder is irrelevant or for scales smaller than the localization length, physical observables can be computed in the localized phase using a replica Gaussian variational approach [26]. This approximate approach leads to a low frequency behavior of the real part of the conductivity as:\n\n\u03c3(\u03c9) \u221d \u03c92\u03be3\n\n(A15)\n\nwhere \u03be is the localization length. It does not include the logarithmic correction, which is known to occur for noninteracting particles \u03c3(\u03c9) \u221d \u03c92(ln \u03c9)2 [23, 25]. This is clearly an artifact of the variational approach and a logarithmic correction is also a priori expected for interacting systems as suggested by semiclassical calculations K \u2192 0 [39] and instanton expansions [21, 40].\n\nEquation (A15) implies that the pinning frequency \u03c9p and the corresponding value of the dynamical conductivity \u03c3(\u03c9p) scale as:\n\n\u03c9p \u221d \u03be\u22121, \u03c3(\u03c9p) \u221d \u03be.\n\n(A16)\n\nin agreement with the predictions from the RG calculations as well. We use this scaling in comparison with the numerical results.\n\n[1] P. W. Anderson, Phys. Rev. 109, 1492 (1958).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5817d98-5744-4dd2-acf1-3b393086b5f5": {"__data__": {"id_": "d5817d98-5744-4dd2-acf1-3b393086b5f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ef9dd3c-45de-41c4-8960-4619101e42dd", "node_type": "4", "metadata": {}, "hash": "ea0838ca8e283347ef1a1c59cf02f16a19da1fedd153dbd9ce4e84ea63d44158", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29286be9-d9d2-4f9d-b294-736eeeb0aec3", "node_type": "1", "metadata": {}, "hash": "ac7c6b90af6d696d229df873e33ffe5c5325e6f937e86612aa26318d14a6abed", "class_name": "RelatedNodeInfo"}}, "text": "It does not include the logarithmic correction, which is known to occur for noninteracting particles \u03c3(\u03c9) \u221d \u03c92(ln \u03c9)2 [23, 25]. This is clearly an artifact of the variational approach and a logarithmic correction is also a priori expected for interacting systems as suggested by semiclassical calculations K \u2192 0 [39] and instanton expansions [21, 40].\n\nEquation (A15) implies that the pinning frequency \u03c9p and the corresponding value of the dynamical conductivity \u03c3(\u03c9p) scale as:\n\n\u03c9p \u221d \u03be\u22121, \u03c3(\u03c9p) \u221d \u03be.\n\n(A16)\n\nin agreement with the predictions from the RG calculations as well. We use this scaling in comparison with the numerical results.\n\n[1] P. W. Anderson, Phys. Rev. 109, 1492 (1958).\n\n[2] L. Sanchez-Palencia, D. Cl\u00e9ment, P. Lugan, P. Bouyer, G. V. Shlyapnikov, and A. Aspect, Phys. Rev. Lett. 98, 210401 (2007).\n\n[3] J. Billy, V. Josse, Z. Zuo, A. Bernard, B. Hambrecht, P. Lugan, D. Clement, L. Sanchez-Palencia, P. Bouyer, and A. Aspect, Nature (London) 453, 891 (2008).\n\n[4] L. Fallani, J. E. Lye, V. Guarrera, C. Fort, and M. Inguscio, Phys. Rev. Lett. 98, 130404 (2007).\n\n[5] G. Roati, C. D\u2019Errico, L. Fallani, M. Fattori, C. Fort, M. Zaccanti, G. Modugno, M. Modugno, and M. Inguscio, Nature (London) 453, 895 (2008).\n\n[6] M. Filoche and S. Mayboroda, Proc. Natl. Acad. Sci. 109, 14761 (2012).\n\n[7] B. L. Altshuler, A. G. Aronov, and P. A. Lee, Phys. Rev. Lett. 44, 1288 (1980).\n\n[8] T. Giamarchi and H. J. Schulz, Phys. Rev. B 37, 325 (1988).\n\n[9] C. Castellani, C. Di Castro, P. A. Lee, and M. Ma, Phys. Rev.", "mimetype": "text/plain", "start_char_idx": 2770, "end_char_idx": 4293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3437e0f7-6bc0-4cdc-906c-114bbab9d8f0": {"__data__": {"id_": "3437e0f7-6bc0-4cdc-906c-114bbab9d8f0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16b69735-06cb-4db9-941c-5790c02b4ebf", "node_type": "4", "metadata": {}, "hash": "25c7a9450df9a28894e16205766d9eb48ea1b99965af1c033291056ada44cfbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "705e5f2f-bd44-4f94-b431-581755811e7a", "node_type": "1", "metadata": {}, "hash": "cb4f23527b979181466111e2785a09980a3043b0934e46729d8a4b7f2b19244a", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. A. M. Finkel\u2019stein, Z. Phys. B Condens. Matter 56, 189 (1984).\n2. D. Belitz and T. R. Kirkpatrick, Rev. Mod. Phys. 66, 261 (1994).\n3. I. V. Gornyi, A. D. Mirlin, and D. G. Polyakov, Phys. Rev. Lett. 95, 206603 (2005).\n4. D. M. Basko, I. L. Aleiner, and B. L. Altshuler, Phys. Rev. B 76, 052203 (2007).\n5. V. Oganesyan and D. A. Huse, Phys. Rev. B 75, 155111 (2007).\n6. D. A. Abanin, E. Altman, I. Bloch, and M. Serbyn, Rev. Mod. Phys. 91, 021001 (2019).\n7. T. Giamarchi, Quantum physics in one dimension (Oxford university press, Oxford, 2004).\n8. M. P. A. Fisher, P. B. Weichman, G. Grinstein, and D. S. Fisher, Phys. Rev. B 40, 546 (1989).\n9. L. Tanzi, E. Lucioni, S. Chaudhuri, L. Gori, A. Kumar, C. D\u2019Errico, M. Inguscio, and G. Modugno, Phys. Rev. Lett. 111, 115301 (2013).\n10. C. D\u2019Errico, E. Lucioni, L. Tanzi, L. Gori, G. Roux, I. P. McCulloch, T. Giamarchi, M. Inguscio, and G. Modugno, Phys. Rev. Lett. 113, 095301 (2014).\n11. N. F. Mott, Phil. Mag. 22, 7 (1970).\n12. T. Nattermann, T. Giamarchi, and P. Le Doussal, Phys. Rev. Lett. 91, 056603 (2003).\n13. I. L. Aleiner, B. L. Altshuler, and G. V. Shlyapnikov, Nat. Phys. 6, 900 (2010).\n14. V. L. Berezinskii, JETP 38, 620 (1974).\n15. A. A. Gogolin and V. I. Melnikov, phys. stat. sol. (b) 88, 377 (1978).\n16. T. Giamarchi and P. Le Doussal, Phys. Rev. B 53, 15206 (1996).\n17. A. Tokuno and T. Giamarchi, Phys. Rev. Lett. 106, 205301 (2011).\n18. Z. Wu, E. Taylor, and E. Zaremba, Europhys. Lett. 110, 26002 (2015).\n19. R. Anderson, F. Wang, P. Xu, V. Venu, S. Trotzky, F. Chevy, and J. H. Thywissen, Phys. Rev. Lett. 122, 153602 (2019).\n20. G. Orso, A. Iucci, M. A. Cazalilla, and T. Giamarchi, Phys. Rev. A 80, 033625 (2009).\n21. S. R. White, Phys. Rev. Lett. 69, 2863 (1992).\n22. U. Schollw\u00f6ck, Ann. Phys. 326, 96 (2011).\n23. M. A. Cazalilla and J. B. Marston, Phys. Rev. Lett. 88, 256403 (2002).\n24. G. Vidal, Phys. Rev. Lett. 93, 040502 (2004).\n25. K. A. Hallberg, Adv. Phys. 55, 477 (2006).\n26. A. Holzner, A. Weichselbaum, I. P. McCulloch, U. Schollw\u00f6ck, and J. von Delft, Phys.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "705e5f2f-bd44-4f94-b431-581755811e7a": {"__data__": {"id_": "705e5f2f-bd44-4f94-b431-581755811e7a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16b69735-06cb-4db9-941c-5790c02b4ebf", "node_type": "4", "metadata": {}, "hash": "25c7a9450df9a28894e16205766d9eb48ea1b99965af1c033291056ada44cfbc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3437e0f7-6bc0-4cdc-906c-114bbab9d8f0", "node_type": "1", "metadata": {}, "hash": "0672fb8c470ab22dc0da0aefbf98dce83ee52020cc7cae46beb68f5fdc50dd37", "class_name": "RelatedNodeInfo"}}, "text": "Rev. A 80, 033625 (2009).\n21. S. R. White, Phys. Rev. Lett. 69, 2863 (1992).\n22. U. Schollw\u00f6ck, Ann. Phys. 326, 96 (2011).\n23. M. A. Cazalilla and J. B. Marston, Phys. Rev. Lett. 88, 256403 (2002).\n24. G. Vidal, Phys. Rev. Lett. 93, 040502 (2004).\n25. K. A. Hallberg, Adv. Phys. 55, 477 (2006).\n26. A. Holzner, A. Weichselbaum, I. P. McCulloch, U. Schollw\u00f6ck, and J. von Delft, Phys. Rev. B 83, 195115 (2011).\n27. A. Wei\u00dfe, G. Wellein, A. Alvermann, and H. Fehske, Rev. Mod. Phys. 78, 275 (2006).\n28. A. A. Abrikosov and I. A. Ryzhkin, Adv. Phys. 27, 147 (1978).\n29. M. M. Fogler, Phys. Rev. Lett. 88, 186402 (2002).\n30. B. Rosenow and T. Nattermann, Phys. Rev. B 73, 085103 (2006).\n31. D. S. Fisher, Phys. Rev. B 50, 3799 (1994).", "mimetype": "text/plain", "start_char_idx": 1678, "end_char_idx": 2408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3a4e0c6-e964-45b1-a9d8-818f863a7422": {"__data__": {"id_": "f3a4e0c6-e964-45b1-a9d8-818f863a7422", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be8755f4-f435-49a8-b027-c6d8575843f1", "node_type": "4", "metadata": {}, "hash": "6c1d7d00e0cfa9a36483f012a6930b8c6219dc2f0e7797a1701d160ec12a1fd1", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# Authors\n\nBOCI PENG\u2217, School of Intelligence Science and Technology, Peking University, China\n\nYUN ZHU\u2217, College of Computer Science and Technology, Zhejiang University, China\n\nYONGCHAO LIU, Ant Group, China\n\nXIAOHE BO, Gaoling School of Artificial Intelligence, Renmin University of China, China\n\nHAIZHOU SHI, Rutgers University, US\n\nCHUNTAO HONG, Ant Group, China\n\nYAN ZHANG\u2020, School of Intelligence Science and Technology, Peking University, China\n\nSILIANG TANG, College of Computer Science and Technology, Zhejiang University, China\n\n# Abstract\n\nRecently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as \u201challucination\u201d, lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at https://github.com/pengboci/GraphRAG-Survey.\n\n# CCS Concepts\n\n- Computing methodologies \u2192 Knowledge representation and reasoning;\n- Information systems \u2192 Information retrieval; Data mining.\n\n# Additional Key Words and Phrases\n\nLarge Language Models, Graph Retrieval-Augmented Generation, Knowledge Graphs, Graph Neural Networks\n\n# Footnotes\n\n\u2217Both authors contributed equally to this research.\n\n\u2020Corresponding Author.\n\n# Authors\u2019 Contact Information\n\nBoci Peng, School of Intelligence Science and Technology, Peking University, Beijing, China, bcpeng@stu.pku.edu.cn;\n\nYun Zhu, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, zhuyun_dcd@zju.edu.cn;\n\nYongchao Liu, Ant Group, Hangzhou, China, yongchao.ly@antgroup.com;\n\nXiaohe Bo, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China, bellebxh@gmail.com;\n\nHaizhou Shi, Rutgers University, New Brunswick, New Jersey, US, haizhou.shi@rutgers.edu;\n\nChuntao Hong, Ant Group, Hangzhou, China, chuntao.hct@antgroup.com;\n\nYan Zhang, School of Intelligence Science and Technology, Peking University, Beijing, China, zhyzhy001@pku.edu.cn;\n\nSiliang Tang, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, siliang@zju.edu.cn.\n\n# Copyright Notice\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\n\u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM 1557-735X/2024/9-ART111 https://doi.org/XXXXXXX.XXXXXXX\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d08392c-a989-427a-8810-d067508bca12": {"__data__": {"id_": "4d08392c-a989-427a-8810-d067508bca12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2d032fe-ef47-4568-b2f3-916ace081419", "node_type": "4", "metadata": {}, "hash": "58cf6bda5294783e3c81ef9cb51c07188b7d04e518e05b844e16e410d0bccf9a", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\nACM Reference Format: Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. 2024. Graph Retrieval-Augmented Generation: A Survey. J. ACM 37, 4, Article 111 (September 2024), 41 pages. https://doi.org/XXXXXXX.XXXXXXX\n\n# 1 Introduction\n\nThe development of Large Language Models like GPT-4 [127], Qwen2 [184], and LLaMA [31] has sparked a revolution in the field of artificial intelligence, fundamentally altering the landscape of natural language processing. These models, built on Transformer [161] architectures and trained on diverse and extensive datasets, have demonstrated unprecedented capabilities in understanding, interpreting, and generating human language. The impact of these advancements is profound, stretching across various sectors including healthcare [103, 166, 203], finance [93, 125], and education [46, 169], where they facilitate more nuanced and efficient interactions between humans and machines.\n\nDespite their remarkable language comprehension and text generation capabilities, LLMs may exhibit limitations due to a lack of domain-specific knowledge, real-time updated information, and proprietary knowledge, which are outside LLMs\u2019 pre-training corpus. These gaps can lead to a phenomenon known as \u201challucination\u201d [61] where the model generates inaccurate or even fabricated information. Consequently, it is imperative to supplement LLMs with external knowledge to mitigate this problem. Retrieval-Augmented Generation (RAG) [34, 45, 59, 62, 178, 195, 202] emerged as a significant evolution, which aims to enhance the quality and relevance of generated content by integrating a retrieval component within the generation process. The essence of RAG lies in its ability to dynamically query a large text corpus to incorporate relevant factual knowledge into the responses generated by the underlying language models. This integration not only enriches the contextual depth of the responses but also ensures a higher degree of factual accuracy and specificity. RAG has gained widespread attention due to its exceptional performance and broad applications, becoming a key focus within the field.\n\nAlthough RAG has achieved impressive results and has been widely applied across various domains, it faces limitations in real-world scenarios: (1) Neglecting Relationships: In practice, textual content is not isolated but interconnected. Traditional RAG fails to capture significant structured relational knowledge that cannot be represented through semantic similarity alone. For instance, in a citation network where papers are linked by citation relationships, traditional RAG methods focus on finding the relevant papers based on the query but overlook important citation relationships between papers. (2) Redundant Information: RAG often recounts content in the form of textual snippets when concatenated as prompts. This makes context become excessively lengthy, leading to the \u201clost in the middle\u201d dilemma [104]. (3) Lacking Global Information: RAG can only retrieve a subset of documents and fails to grasp global information comprehensively, and hence struggles with tasks such as Query-Focused Summarization (QFS).\n\nGraph Retrieval-Augmented Generation (GraphRAG) [32, 58, 119] emerges as an innovative solution to address these challenges. Unlike traditional RAG, GraphRAG retrieves graph elements containing relational knowledge pertinent to a given query from a pre-constructed graph database, as depicted in Figure 1. These elements may include nodes, triples, paths, or subgraphs, which are utilized to generate responses. GraphRAG considers the interconnections between texts, enabling a more accurate and comprehensive retrieval of relational information. Additionally, graph data, such as knowledge graphs, offer abstraction and summarization of textual data, thereby significantly shortening the length of the input text and mitigating concerns of verbosity. By retrieving subgraphs or graph communities, we can access comprehensive information to effectively address the QFS challenge by capturing the broader context and interconnections within the graph structure.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4af695cc-042d-461e-a635-e0bce52dd39b": {"__data__": {"id_": "4af695cc-042d-461e-a635-e0bce52dd39b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d321f45-6ab4-4a48-b6cf-e2220f2097f6", "node_type": "4", "metadata": {}, "hash": "3c5f782bbce8b05e707fdb9a8e5c44e647a8414d643a8320faf366553139dc4c", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# Query\n\nHow did the artistic movements of the 19th century impact the development of modern art in the 20th century?\n\n# Retriever\n\nHow did the artistic movements of the 19th century impact the development of modern art in the 20th century?\n\n# Response\n\n1. Impressionist artists like Claude Monet introduced new techniques that revolutionized the depiction of light and color.\n2. The Impressionist techniques influenced later art movements.\n3. Pablo Picasso pioneered Cubism, which radically transformed the approach to visual representation.\n4. Cubism emerged in the early 20th century and challenged traditional perspectives on art.\n\nThe artistic movements of the 19th century influenced modern art in the 20th century by encouraging experimentation with color, form, and subject matter. These movements paved the way for abstraction, expressionism, and other innovative approaches.\n\n# Retrieved Text\n\nImpressionist artists like Claude Monet in the 19th century introduced new techniques that influenced later art movements. Pablo Picasso's Cubism, which emerged in the early 20th century, helped shape innovative approaches to fragmented perspectives.\n\n# Fig. 1. Comparison between Direct LLM, RAG, and GraphRAG\n\nGiven a user query, direct answering by LLMs may suffer from shallow responses or lack of specificity. RAG addresses this by retrieving relevant textual information, somewhat alleviating the issue. However, due to the text\u2019s length and flexible natural language expressions of entity relationships, RAG struggles to emphasize \u201cinfluence\u201d relations, which is the core of the question. While GraphRAG methods leverage explicit entity and relationship representations in graph data, enabling precise answers by retrieving relevant structured information.\n\n# Introduction\n\nIn this paper, we are the first to provide a systematic survey of GraphRAG. Specifically, we begin by introducing the GraphRAG workflow, along with the foundational background knowledge that underpins the field. Then, we categorize the literature according to the primary stages of the GraphRAG process: Graph-Based Indexing (G-Indexing), Graph-Guided Retrieval (G-Retrieval), and Graph-Enhanced Generation (G-Generation) in Section 5, Section 6 and Section 7 respectively, detailing the core technologies and training methods within each phase. Furthermore, we investigate downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. This exploration elucidates how GraphRAG is being utilized in practical settings and reflects its versatility and adaptability across various sectors. Finally, acknowledging that research in GraphRAG is still in its early stages, we delve into potential future research directions. This prognostic discussion aims to pave the way for forthcoming studies, inspire new lines of inquiry, and catalyze progress within the field, ultimately propelling GraphRAG toward more mature and innovative horizons.\n\n# Contributions\n\n- We provide a comprehensive and systematic review of existing state-of-the-art GraphRAG methodologies. We offer a formal definition of GraphRAG, outlining its universal workflow which includes G-Indexing, G-Retrieval, and G-Generation.\n- We discuss the core technologies underpinning existing GraphRAG systems, including G-Indexing, G-Retrieval, and G-Generation. For each component, we analyze the spectrum of model selection, methodological design, and enhancement strategies currently being explored. Additionally, we contrast the diverse training methodologies employed across these modules.\n- We delineate the downstream tasks, benchmarks, application domains, evaluation metrics, current challenges, and future research directions pertinent to GraphRAG, discussing both.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "887d1bed-2420-49c3-9e7c-4d54f4624b58": {"__data__": {"id_": "887d1bed-2420-49c3-9e7c-4d54f4624b58", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c86bc1d5-a8d3-40e0-8cd6-34ea589877ce", "node_type": "4", "metadata": {}, "hash": "66d230b25d91b8f01e864811f419c1560f084c055ef6d170d440ebde9d6a3725", "class_name": "RelatedNodeInfo"}}, "text": "# Peng et al.\n\n# Input Query\n\nHow did the artistic movements of the 19th century impact the development of modern art in the 20th century?\n\n# G-Retrieval\n\n# Retrieval Graph Format\n\n# Results\n\n\u2026\n\n# Query Expansion\n\n# Merging Nodes\n\n# Adjacency/Edge Table\n\n# Pre-Generation Enhancements\n\n# Query Decomposition\n\n# Retriever Pruning\n\n# Natural Language Triplets\n\n# Knowledge Enhancements\n\n# Code-Like Forms\n\n# Mid-Generation Enhancements\n\n# Graph Database - Indexing & Subgraphs\n\n# Node Sequence\n\n# Post-Generation Enhancements\n\n# Open Knowledge Graphs\n\n# Self-Constructed Graph Data\n\n# Hybrid Graph Embedding\n\n# Output\n\n# Response\n\n\u2026\n\nMonet introduced new techniques that revolutionized the depiction of light and color. His Impressionist techniques\n\n# Fig. 2. The overview of the GraphRAG framework for question answering task.\n\nIn this survey, we divide GraphRAG into three stages: G-Indexing, G-Retrieval, and G-Generation. We categorize the retrieval sources into open-source knowledge graphs and self-constructed graph data. Various enhancing techniques like query enhancement and knowledge enhancement may be adopted to boost the relevance of the results. Unlike RAG, which uses retrieved text directly for generation, GraphRAG requires converting the retrieved graph information into patterns acceptable to generators to enhance the task performance.\n\nthe progress and prospects of this field. Furthermore, we compile an inventory of existing industry GraphRAG systems, providing insights into the translation of academic research into real-world industry solutions.\n\n# Organization\n\nThe rest of the survey is organized as follows: Section 2 compares related techniques, while Section 3 outlines the general process of GraphRAG. Sections 5 to 7 categorize the techniques associated with GraphRAG\u2019s three stages: G-Indexing, G-Retrieval, and G-Generation. Section 8 introduces the training strategies of retrievers and generators. Section 9 summarizes GraphRAG\u2019s downstream tasks, corresponding benchmarks, application domains, evaluation metrics, and industrial GraphRAG systems. Section 10 provides an outlook on future directions. Finally, Section 11 concludes the content of this survey.\n\n# 2 Comparison with Related Techniques and Surveys\n\nIn this section, we compare Graph Retrieval-Augmented Generation (GraphRAG) with related techniques and corresponding surveys, including RAG, LLMs on graphs, and Knowledge Base Question Answering (KBQA).\n\n# 2.1 RAG\n\nRAG combines external knowledge with LLMs for improved task performance, integrating domain-specific information to ensure factuality and credibility. In the past two years, researchers have written many comprehensive surveys about RAG [34, 45, 59, 62, 178, 195, 202]. For example, Fan et al. [34] and Gao et al. [45] categorize RAG methods from the perspectives of retrieval, generation, and augmentation. Zhao et al. [202] review RAG methods for databases with different modalities. Yu et al. [195] systematically summarize the evaluation of RAG methods. These works\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79ff01b0-efe1-4525-996c-31de767043d3": {"__data__": {"id_": "79ff01b0-efe1-4525-996c-31de767043d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ba4857a-d1e3-4958-9a7f-efbfab50e460", "node_type": "4", "metadata": {}, "hash": "19c79c0d2211c963517f064264cbcb307809fa68219255bee6c60317698c8838", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 2.2 LLMs on Graphs\n\nLLMs are revolutionizing natural language processing due to their excellent text understanding, reasoning, and generation capabilities, along with their generalization and zero-shot transfer abilities. Although LLMs are primarily designed to process pure text and struggle with non-Euclidean data containing complex structural information, such as graphs [49, 165], numerous studies [17, 35, 74, 92, 102, 116, 130, 131, 173, 204] have been conducted in these fields. These papers primarily integrate LLMs with GNNs to enhance modeling capabilities for graph data, thereby improving performance on downstream tasks such as node classification, edge prediction, graph classification, and others. For example, Zhu et al. [204] propose an efficient fine-tuning method named ENGINE, which combines LLMs and GNNs through a side structure for enhancing graph representation.\n\nDifferent from these methods, GraphRAG focuses on retrieving relevant graph elements using queries from an external graph-structured database. In this paper, we provide a detailed introduction to the relevant technologies and applications of GraphRAG, which are not included in previous surveys of LLMs on Graphs.\n\n# 2.3 KBQA\n\nKBQA is a significant task in natural language processing, aiming to respond to user queries based on external knowledge bases [41, 85, 86, 188], thereby achieving goals such as fact verification, passage retrieval enhancement, and text understanding. Previous surveys typically categorize existing KBQA approaches into two main types: Information Retrieval (IR)-based methods and Semantic Parsing (SP)-based methods. Specifically, IR-based methods [69, 70, 112, 113, 154, 167, 182, 196] retrieve information related to the query from the knowledge graph (KG) and use it to enhance the generation process. While SP-based methods [16, 19, 36, 48, 153, 191] generate a logical form (LF) for each query and execute it against knowledge bases to obtain the answer.\n\nGraphRAG and KBQA are closely related, with IR-based KBQA methods representing a subset of GraphRAG approaches focused on downstream applications. In this work, we extend the discussion beyond KBQA to include GraphRAG\u2019s applications across various downstream tasks. Our survey provides a thorough and detailed exploration of GraphRAG technology, offering a comprehensive understanding of existing methods and potential improvements.\n\n# 3 Preliminaries\n\nIn this section, we introduce background knowledge of GraphRAG for easier comprehension of our survey. First, we introduce Text-Attributed Graphs which is a universal and general format of\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "661da3a0-7bc8-47bd-ae73-02f2b86aab46": {"__data__": {"id_": "661da3a0-7bc8-47bd-ae73-02f2b86aab46", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a121382b-a36e-41e7-b53b-2a3028f2144c", "node_type": "4", "metadata": {}, "hash": "816cce2ea2f630b2110576ff9b24dbb7c3937d183dc99963a681117b98d2ab94", "class_name": "RelatedNodeInfo"}}, "text": "# Peng et al.\n\n# 3.1 Text-Attributed Graphs\n\nThe graph data used in Graph RAG can be represented uniformly as Text-Attributed Graphs (TAGs), where nodes and edges possess textual attributes. Formally, a text-attributed graph can be denoted as G = (V, E, A, {xv}v \u2208 V, {ei,j}i,j \u2208 E), where V is the set of nodes, E \u2286 V \u00d7 V is the set of edges, A \u2208 {0, 1}|V| \u00d7 |V|  is the adjacent matrix. Additionally, {xv}v \u2208 V and {ei,j}i,j \u2208 E are textual attributes of nodes and edges, respectively. One typical kind of TAGs is Knowledge Graphs (KGs), where nodes are entities, edges are relations among entities, and text attributes are the names of entities and relations.\n\n# 3.2 Graph Neural Networks\n\nGraph Neural Networks (GNNs) are a kind of deep learning framework to model the graph data. Classical GNNs, e.g., GCN [83], GAT [162], GraphSAGE [52], adopt a message-passing manner to obtain node representations. Formally, each node representation hi(l \u2212 1) by aggregating the information from neighboring nodes and edges:\n\nhi(l) = UPD(hi(l \u2212 1), AGGj \u2208 N(i) MSG(hi(l \u2212 1), hj(l \u2212 1), ei,j(l \u2212 1)))\n\nin the l-th layer is updated.\n\nwhere N(i) represents the neighbors of node i. MSG denotes the message function, which computes the message based on the node, its neighbor, and the edge between them. AGG refers to the aggregation function that combines the received messages using a permutation-invariant method, such as mean, sum, or max. UPD represents the update function, which updates each node\u2019s attributes with the aggregated messages.\n\nSubsequently, a readout function, e.g., mean, sum, or max pooling, can be applied to obtain the global-level representation:\n\nhG = READOUTi \u2208 V G (hi(L)).\n\nIn GraphRAG, GNNs can be utilized to obtain representations of graph data for the retrieval phase, as well as to model the retrieved graph structures.\n\n# 3.3 Language Models\n\nLanguage models (LMs) excel in language understanding and are mainly classified into two types: discriminative and generative. Discriminative models, like BERT [28], RoBERTa [107] and Sentence-BERT [140], focus on estimating the conditional probability P(y|x) and are effective in tasks such as text classification and sentiment analysis. In contrast, generative models, including GPT-3 [14] and GPT-4 [127], aim to model the joint probability P(x, y) for tasks like machine translation and text generation. These generative pre-trained models have significantly advanced the field of natural language processing (NLP) by leveraging massive datasets and billions of parameters, contributing to the rise of Large Language Models (LLMs) with outstanding performance across various tasks.\n\nIn the early stages, RAG and GraphRAG focused on improving pre-training techniques for discriminative language models [28, 107, 140]. Recently, LLMs such as ChatGPT [128], LLaMA [31], and Qwen2 [184] have shown great potential in language understanding, demonstrating powerful in-context learning capabilities. Subsequently, research on RAG and GraphRAG shifted towards enhancing information retrieval for language models, addressing increasingly complex tasks and mitigating hallucinations, thereby driving rapid advancements in the field.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "951501b3-014a-420f-9d16-32ff3ac4b082": {"__data__": {"id_": "951501b3-014a-420f-9d16-32ff3ac4b082", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a583551-0779-463a-96fa-acf6ceec8b90", "node_type": "4", "metadata": {}, "hash": "9a2e84f3d68d6c0982c0b71fab6feade97d72829f854362b7ba64056d4abb656", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 4 Overview of GraphRAG\n\nGraphRAG is a framework that leverages external structured knowledge graphs to improve contextual understanding of LMs and generate more informed responses, as depicted in Figure 2. The goal of GraphRAG is to retrieve the most relevant knowledge from databases, thereby enhancing the answers of downstream tasks. The process can be defined as\n\n\ud835\udc4e\u2217 = arg max\ud835\udc5d (\ud835\udc4e|\ud835\udc5e, G), \ud835\udc4e\u2208\ud835\udc34\n\nwhere \ud835\udc4e\u2217 is the optimal answer of the query \ud835\udc5e given the TAG G, and \ud835\udc34 is the set of possible responses. After that, we jointly model the target distribution \ud835\udc5d (\ud835\udc4e|\ud835\udc5e, G) with a graph retriever \ud835\udc5d\ud835\udf03 (\ud835\udc3a |\ud835\udc5e, G) and an answer generator \ud835\udc5d\ud835\udf19 (\ud835\udc4e|\ud835\udc5e, \ud835\udc3a) where \ud835\udf03, \ud835\udf19 are learnable parameters, and utilize the total probability formula to decompose \ud835\udc5d (\ud835\udc4e|\ud835\udc5e, G), which can be formulated as\n\n\u2211\ufe01 \ud835\udc5d (\ud835\udc4e|\ud835\udc5e, G) = \ud835\udc5d\ud835\udf19 (\ud835\udc4e|\ud835\udc5e, \ud835\udc3a)\ud835\udc5d\ud835\udf03 (\ud835\udc3a |\ud835\udc5e, G) \ud835\udc3a \u2286 G\n\n\u2248 \ud835\udc5d\ud835\udf19 (\ud835\udc4e|\ud835\udc5e, \ud835\udc3a\u2217)\ud835\udc5d\ud835\udf03 (\ud835\udc3a\u2217 |\ud835\udc5e, G),\n\nwhere \ud835\udc3a\u2217 is the optimal subgraph. Because the number of candidate subgraphs can grow exponentially with the size of the graph, efficient approximation methods are necessary. The first line of Equation 4 is thus approximated by the second line. Specifically, a graph retriever is employed to extract the optimal subgraph \ud835\udc3a\u2217, after which the generator produces the answer based on the retrieved subgraph.\n\nTherefore, in this survey, we decompose the entire process of GraphRAG into three main stages: Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. The overall workflow of GraphRAG is illustrated in Figure 2 and detailed introductions of each stage are as follows.\n\n# Graph-Based Indexing (G-Indexing)\n\nGraph-Based Indexing constitutes the initial phase of GraphRAG, aimed at identifying or constructing a graph database G that aligns with downstream tasks and establishing indices on it. The graph database can originate from public knowledge graphs [4, 10, 100, 142, 150, 163], graph data [123], or be constructed based on proprietary data sources such as textual [32, 51, 89, 172] or other forms of data [183]. The indexing process typically includes mapping node and edge properties, establishing pointers between connected nodes, and organizing data to support fast traversal and retrieval operations. Indexing determines the granularity of the subsequent retrieval stage, playing a crucial role in enhancing query efficiency.\n\n# Graph-Guided Retrieval (G-Retrieval)\n\nFollowing graph-based indexing, the graph-guided retrieval phase focuses on extracting pertinent information from the graph database in response to user queries or input. Specifically, given a user query \ud835\udc5e which is expressed in natural language, the retrieval stage aims to extract the most relevant elements (e.g., entities, triplets, paths, subgraphs) from knowledge graphs, which can be formulated as\n\n\ud835\udc3a\u2217 = G-Retriever(\ud835\udc5e, G) = arg max\ud835\udc5d\ud835\udf03 (\ud835\udc3a |\ud835\udc5e, G) \ud835\udc3a \u2286 R (G)\n\n= arg max Sim(\ud835\udc5e, \ud835\udc3a), \ud835\udc3a \u2286 R (G)\n\nwhere \ud835\udc3a\u2217 is the optimal retrieved graph elements and Sim(\u00b7, \u00b7) is a function that measures the semantic similarity between user queries and the graph data. R (\u00b7) represents a function to narrow down the search range of subgraphs, considering the efficiency.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf92f5aa-a7b0-45d9-8e32-bc5f94630e17": {"__data__": {"id_": "cf92f5aa-a7b0-45d9-8e32-bc5f94630e17", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8192aa70-9635-4ee9-9ea2-12c392e44cf9", "node_type": "4", "metadata": {}, "hash": "f8dbc88e09cb64e5523bb64902c6b83b62aa0ef9572388c8e02abb662108fa3f", "class_name": "RelatedNodeInfo"}}, "text": "# Graph-Enhanced Generation (G-Generation)\n\nThe graph-enhanced generation phase involves synthesizing meaningful outputs or responses based on the retrieved graph data. This could encompass answering user queries, generating reports, etc. In this stage, a generator takes the query, retrieved graph elements, and an optional prompt as input to generate a response, which can be denoted as\n\n\ud835\udc4e\u2217 = G-Generator(\ud835\udc5e, \ud835\udc3a\u2217) = arg max\ud835\udc5d\ud835\udf19 (\ud835\udc4e|\ud835\udc5e, \ud835\udc3a\u2217) (6) \ud835\udc4e\u2208\ud835\udc34 = arg max\ud835\udc5d\ud835\udf19 (\ud835\udc4e|F (\ud835\udc5e, \ud835\udc3a\u2217)),\ud835\udc4e\u2208\ud835\udc34\n\nwhere F (\u00b7, \u00b7) is a function that converts graph data into a form the generator can process.\n\n# 5 Graph-Based Indexing\n\nThe construction and indexing of graph databases form the foundation of GraphRAG, where the quality of the graph database directly impacts GraphRAG\u2019s performance. In this section, we categorize and summarize the selection or construction of graph data and various indexing methods that have been employed.\n\n# 5.1 Graph Data\n\nVarious types of graph data are utilized in GraphRAG for retrieval and generation. Here, we categorize these data into two categories based on their sources, including Open Knowledge Graphs and Self-Constructed Graph Data.\n\n# 5.1.1 Open Knowledge Graphs\n\nOpen knowledge graphs refer to graph data sourced from publicly available repositories or databases [4, 10, 150, 163]. Using these knowledge graphs could dramatically reduce the time and resources required to develop and maintain. In this survey, we further classify them into two categories according to their scopes, i.e., General Knowledge Graphs and Domain Knowledge Graphs.\n\n1. # General Knowledge Graphs\n\nGeneral knowledge graphs primarily store general, structured knowledge, and typically rely on collective input and updates from a global community, ensuring a comprehensive and continually refreshed repository of information.\n\nEncyclopedic knowledge graphs are a typical type of general knowledge graph, which contains large-scale real-world knowledge collected from human experts and encyclopedias. For example, Wikidata1 [163] is a free and open knowledge base that stores structured data of its Wikimedia sister projects like Wikipedia, Wikivoyage, Wiktionary, and others. Freebase2 [10] is an extensive, collaboratively edited knowledge base that compiles data from various sources, including individual contributions and structured data from databases like Wikipedia. DBpedia3 [4] represents information about millions of entities, including people, places, and things, by leveraging the infoboxes and categories present in Wikipedia articles. YAGO4 [150] collects knowledge from Wikipedia, WordNet, and GeoNames.\n\nCommonsense knowledge graphs are another type of general knowledge graph. They include abstract commonsense knowledge, such as semantic associations between concepts and causal relationships between events. Typical Commonsense Knowledge Graphs include: ConceptNet5 [100]\n\n1 https://www.wikidata.org/\n\n2 http://www.freebase.be/\n\n3 https://www.dbpedia.org/\n\n4 https://yago-knowledge.org/\n\n5 https://conceptnet.io/\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "267d3945-b3c0-4a2b-ab84-e20c30245adc": {"__data__": {"id_": "267d3945-b3c0-4a2b-ab84-e20c30245adc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f34c844-643c-445b-91bd-d00f90101431", "node_type": "4", "metadata": {}, "hash": "65de157938ae22b9008751b6b23ab9d7e8295a3995229f55ea9216a15dd61df5", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 5.1 Knowledge Graphs\n\n# 5.1.1 Domain Knowledge Graphs\n\nis a semantic network built from nodes representing words or phrases connected by edges denoting semantic relationships. ATOMIC [64, 142] models the causal relationships between events.\n\nDomain-specific knowledge graphs are crucial for enhancing LLMs in addressing domain-specific questions. These KGs offer specialized knowledge in particular fields, aiding models in gaining deeper insights and a more comprehensive understanding of complex professional relationships. In the biomedical field, CMeKG6 encompasses a wide range of data, including diseases, symptoms, treatments, medications, and relationships between medical concepts. CPubMed-KG7 is a medical knowledge database in Chinese, building on the extensive repository of biomedical literature in PubMed. In the movie domain, Wiki-Movies [121] extracts structured information from Wikipedia articles related to films, compiling data about movies, actors, directors, genres, and other relevant details into a structured format. Additionally, Jin et al. [75] construct a dataset named GR-Bench, which includes five domain knowledge graphs spanning academic, E-commerce, literature, healthcare, and legal fields. Furthermore, He et al. [55] convert triplet-format and JSON files from ExplaGraphs and SceneGraphs into a standard graph format and selects questions requiring 2-hop reasoning from WebQSP to create the universal graph-format dataset GraphQA for evaluating GraphRAG systems.\n\n# 5.1.2 Self-Constructed Graph Data\n\nSelf-Constructed Graph Data facilitates the customization and integration of proprietary or domain-specific knowledge into the retrieval process. For downstream tasks that do not inherently involve graph data, researchers often propose constructing a graph from multiple sources (e.g., documents, tables, and other databases) and leveraging GraphRAG to enhance task performance. Generally, these self-constructed graphs are closely tied to the specific design of the method, distinguishing them from the open-domain graph data previously mentioned.\n\nTo model the structural relationships between the documents, Munikoti et al. [124] propose to construct a heterogeneous document graph capturing multiple document-level relations, including co-citation, co-topic, co-venue, etc. Li et al. [96] and Wang et al. [172] establish relationship between passages according to shared keywords. To capture the relations between entities in documents, Delile et al. [26], Edge et al. [32], Guti\u00e9rrez et al. [51] and Li et al. [89] utilize the named entity recognition tools to extract entities from documents and language models to further extract relations between entities, where the retrieved entities and relations then form a knowledge graph. There are also some mapping methods for downstream tasks that need to be designed based on the characteristics of the task itself. For example, to solve the patent phrase similarity inference task, Peng and Yang [133] convert the patent database into a patent-phrase graph. Connections between patent nodes and phrase nodes are established if the phrases appear in the patents, while connections between patent nodes are based on citation relations. Targeting customer service technical support scenarios, Xu et al. [183] propose to model historical issues into a KG, which transforms the issues into tree representations to maintain the intra-issue relations, and utilize semantic similarities and a threshold to preserve inter-issue relations.\n\n# 5.2 Indexing\n\n# 5.2.1 Graph Indexing\n\nGraph-Based Indexing plays a crucial role in enhancing the efficiency and speed of query operations on graph databases, directly influencing subsequent retrieval methods and granularity. Common graph-based indexing methods include graph indexing, text indexing, and vector indexing.\n\nGraph indexing represents the most commonly used approach, preserving the entire structure of the graph. This method ensures that for any given node, all its edges and\n\n6 https://cmekg.pcl.ac.cn/\n\n7 https://cpubmed.openi.org.cn/graph/wiki\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a7d4bed-170f-4c96-bc1f-d280fa9abcc2": {"__data__": {"id_": "5a7d4bed-170f-4c96-bc1f-d280fa9abcc2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a32db838-bb6d-4dea-ae93-9cdc8eec0ff3", "node_type": "4", "metadata": {}, "hash": "7649078b1086b64e11cd3fcd2b1fb0b2673b1be01ab008644435430c8e013f78", "class_name": "RelatedNodeInfo"}}, "text": "# Peng et al.\n\n# \u00a7 5.1 Graph Data\n\n- Self-constructed Knowledge Graphs\n- Open Knowledge Graphs\n- General Knowledge Graphs\n- Wikipedia Text Corpus\n- Domain Knowledge Graphs\n\n# \u00a7 5.2 Indexing\n\n- Graph Indexing\n- Text Indexing\n- Vector Indexing\n- Hybrid Indexing\n\n# Graph Database\n\nFig. 3. The overview of graph-based indexing.\n\nNeighboring nodes are easily accessible. During subsequent retrieval stages, classic graph search algorithms such as BFS and Shortest Path Algorithms can be employed to facilitate retrieval tasks [73, 75, 112, 113, 154, 158, 189].\n\n# 5.2.2 Text Indexing\n\nText indexing involves converting graph data into textual descriptions to optimize retrieval processes. These descriptions are stored in a text corpus, where various text-based retrieval techniques, such as sparse retrieval and dense retrieval, can be applied. Some approaches transform knowledge graphs into human-readable text using predefined rules or templates. For instance, Li et al. [90], Huang et al. [63] and Li et al. [95] use predefined templates to convert each triple in knowledge graphs into natural language, while Yu et al. [193] merge triplets with the same head entity into passages. Additionally, some methods convert subgraph-level information into textual descriptions. For example, Edge et al. [32] perform community detection on the graph and generate summaries for each community using LLMs.\n\n# 5.2.3 Vector Indexing\n\nVector indexing transforms graph data into vector representations to enhance retrieval efficiency, facilitating rapid retrieval and effective query processing. For example, entity linking can be seamlessly applied through query embeddings, and efficient vector search algorithms such as Locality Sensitive Hashing (LSH) [66] can be utilized. G-Retriever [55] employs language models to encode textual information associated with each node and edge within the graph, while GRAG [58] uses language models to convert \ud835\udc58-hop ego networks into graph embeddings, thereby better preserving structural information.\n\n# 5.2.4 Hybrid Indexing\n\nEach of the above three indexing methods offers distinct advantages: graph indexing facilitates easy access to structural information, text indexing simplifies retrieval of textual content, and vector indexing enables quick and efficient searches. Therefore, in practical applications, a hybrid approach combining these indexing methods is often preferred over relying solely on one. For instance, HybridRAG [144] retrieves both vector and graph data simultaneously to enhance the content retrieved. While EWEK-QA [24] uses both text and knowledge graphs.\n\n# 6 Graph-Guided Retrieval\n\nIn GraphRAG, the retrieval process is crucial for ensuring the quality and relevance of generated outputs by extracting pertinent and high-quality graph data from external graph databases. However, retrieving graph data presents two significant challenges: (1) Explosive Candidate Subgraphs: As the graph size increases, the number of candidate subgraphs grows exponentially, requiring heuristic search algorithms to efficiently explore and retrieve relevant subgraphs. (2) Insufficient Similarity Measurement: Accurately measuring similarity between textual queries and graph data necessitates the development of algorithms capable of understanding both textual and structural information.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f70ff4be-fe0f-4ca4-9a94-cc51ab1b414b": {"__data__": {"id_": "f70ff4be-fe0f-4ca4-9a94-cc51ab1b414b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af6fe410-42af-4244-8cc5-ff8e24656a57", "node_type": "4", "metadata": {}, "hash": "9d286599b7bc64a1db083132c8609c92172da89f39f2f58950ae2dcfac4d90d1", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# Input Query\n\n# \u00a7 6.4.1 Query Enhancement\n\n- Query Expansion\n- Query Decomposition\n\n# \u00a7 6.2 Retrieval Paradigm\n\n- Multi-Stage Retrieval\n\n# \u00a7 6.3 Retrieval\n\n- Iterative Retrieval\n- Granularity\n- Once Retrieval\n\n# \u00a7 6.4.2 Knowledge Enhancement\n\n- Nodes\n- Triplets\n- Paths\n- Subgraphs\n- Hybrid\n\n# \u00a7 6.1 Retriever\n\nIn GraphRAG, various retrievers possess unique strengths for addressing different aspects of retrieval tasks. We categorize retrievers into three types based on their underlying models: Non-parametric Retriever, LM-based Retriever, and GNN-based Retriever. It is important to note that models used in pre-processing steps, such as query encoding and entity linking, are not considered here, as these models vary across different methods and are not the primary focus of this paper.\n\n# 6.1.1 Non-parametric Retriever\n\nNon-parametric retrievers, based on heuristic rules or traditional graph search algorithms, do not rely on deep-learning models, thereby achieving high retrieval efficiency. For instance, G-Retriever [55] enhances the conventional Prize-Collecting Steiner Tree (PCST) algorithm by incorporating edge prices and optimizing relevant subgraph extraction. These methods often involve an entity linking pre-processing step to identify nodes in the graph before retrieval.\n\nYasunaga et al. [189] and Taunk et al. [158] retrieve \ud835\udc58-hop paths. Delile et al. [26] and Mavromatis and Karypis [119] first extract.\n\n# 6.1.2 LM-based Retriever\n\nLMs serve as effective retrievers in GraphRAG due to their strong natural language understanding capabilities. These models excel in processing and interpreting diverse natural language queries, making them versatile for a wide range of retrieval tasks within graph-based frameworks. We primarily categorized LMs into two types: discriminative and generative language models. Subgraph Retriever [196] trains RoBERTa [107] as the retriever, which expands from the topic entity and retrieves the relevant paths in a sequential decision process. KG-GPT [80] adopts LLMs to generate the set of top-\ud835\udc3e relevant relations of the specific entity. Wold et al. [176] utilize fine-tuned GPT-2 to generate reasoning paths. StructGPT [67] utilizes LLMs to automatically invoke several pre-defined functions, by which relevant information can be retrieved and combined to assist further reasoning.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c2b6b53-991e-40c2-9908-b5605ec2482b": {"__data__": {"id_": "9c2b6b53-991e-40c2-9908-b5605ec2482b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9680fa5d-2f1e-4f33-bbd8-dd53b819475a", "node_type": "4", "metadata": {}, "hash": "0968e9ac761f356ad4ee6e750dd1eef4e7e02b2ea2ce62a462e02411e05ffd99", "class_name": "RelatedNodeInfo"}}, "text": "# 6.1.3 GNN-based Retriever\n\nGNNs are adept at understanding and leveraging complex graph structures. GNN-based retrievers typically encode graph data and subsequently score different retrieval granularities based on their similarity to the query. For example, GNN-RAG [119] first encodes the graph, assigns a score to each entity, and retrieves entities relevant to the query based on a threshold. EtD [99] iterates multiple times to retrieve relevant paths. During each iteration, it first uses LLaMA2 [160] to select edges connecting the current node, then employs GNNs to obtain embeddings of the new layer of nodes for the next round of LLM selection.\n\n# 6.1.4 Discussion\n\nDuring the retrieval process, non-parametric retrievers exhibit good retrieval efficiency, but they may suffer from inaccurate retrieval due to a lack of training on downstream tasks. Meanwhile, although LM-based retrievers and GNN-based retrievers offer higher retrieval accuracy, they require significant computational overhead. Considering this complementarity, many methods propose hybrid retrieval approaches to improve both retrieval efficiency and accuracy. Many approaches adopt a multi-stage retrieval strategy, employing different models at each stage. For example, RoG [112] first utilizes LLMs to generate planning paths and then extracts paths satisfying the planning paths from knowledge graphs. GenTKGQA [44] infers crucial relations and constraints from the query using LLMs and extracts triplets according to these constraints.\n\n# 6.2 Retrieval Paradigm\n\nWithin GraphRAG, different retrieval paradigms, including once retrieval, iterative retrieval, and multi-stage retrieval, play crucial roles in improving the relevance and depth of the retrieved information. Once retrieval aims to gather all pertinent information in a single operation. Iterative retrieval conducts further searches based on previously retrieved information, progressively narrowing down to the most relevant results. Here we further divide iterative retrieval into adaptive retrieval and non-adaptive retrieval, with the only difference lying in whether the stopping of the retrieval is determined by the model. Another retrieval paradigm is multi-stage retrieval, where retrieval is divided into multiple stages. Different types of retrievers may be employed at each stage for more precise and diversified search results. Below, we will provide a detailed introduction to these types of retrieval paradigms.\n\n# 6.2.1 Once Retrieval\n\nOnce retrieval aims to retrieve all the relevant information in a single query. One category of approaches [51, 58, 90] utilize embedding similarities to retrieve the most relevant pieces of information. Another category of methods design pre-defined rules or patterns to directly extract specific structured information such as triplets, paths or subgraphs from graph databases. For example, G-Retriever [55] utilizes an extended PCST algorithm to retrieve the most relevant subgraph. KagNet [97] extracts paths between all pairs of topic entities with lengths not exceeding \ud835\udc58. Yasunaga et al. [189] and Taunk et al. [158] extract the subgraph that contains all topic entities along with their 2-hop neighbors.\n\nFurthermore, in this subsection, we also include some multiple retrieval methods that involve decoupled and independent retrievals, allowing them to be computed in parallel and executed only once. For example, Luo et al. [112] and Cheng et al. [20] first instruct LLMs to generate multiple reasoning paths and then use a BFS retriever to sequentially search for subgraphs in the knowledge graphs that match each path. KG-GPT [80] decomposes the original query into several sub-queries, retrieving relevant information for each sub-query in a single retrieval process.\n\n# 6.2.2 Iterative Retrieval\n\nIn iterative retrieval, multiple retrieval steps are employed, with subsequent searches depending on the results of prior retrievals. These methods aim to deepen the understanding or completeness of the retrieved information over successive iterations.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26e62c74-9e54-402f-90b9-9893dbf2b0c3": {"__data__": {"id_": "26e62c74-9e54-402f-90b9-9893dbf2b0c3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66af1053-2d81-4eab-84ba-119e7abc6f22", "node_type": "4", "metadata": {}, "hash": "9aac624bafbfbcbac94dc95d4994d9c51c373bb44f0019204da861af7b5f0dbd", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 6.2.2 Iterative Retrieval\n\nIn this survey, we further classify iterative retrieval into two categories: (1) non-adaptive and (2) adaptive retrieval. We provide a detailed summary of these two categories of methods below.\n\n# (1) Non-Adaptive Retrieval\n\nNon-adaptive methods typically follow a fixed sequence of retrieval, and the termination of retrieval is determined by setting a maximum time or a threshold. For example, PullNet [151] retrieves problem-relevant subgraphs through \ud835\udc47 iterations. In each iteration, the paper designs a retrieval rule to select a subset of retrieved entities, and then expands these entities by searching relevant edges in the knowledge graph. In each iteration, KGP [172] first selects seed nodes based on the similarity between the context and the nodes in the graph. It then uses LLMs to summarize and update the context of the neighboring nodes of the seed nodes, which is utilized in the subsequent iteration.\n\n# (2) Adaptive Retrieval\n\nOne distinctive characteristic of adaptive retrieval is to let models autonomously determine the optimal moments to finish the retrieval activities. For instance, [50, 182] leverage an LM for hop prediction, which serves as an indicator to end the retrieval. There is also a group of researchers who utilize model-generated special tokens or texts as termination signals for the retrieval process. For example, ToG [113, 154] prompts the LLM agent to explore the multiple possible reasoning paths until the LLM determines the question can be answered based on the current reasoning path. [196] trains a RoBERTa to expand a path from each topic entity. In the process, a virtual relation named as \u201c[END]\u201d is introduced to terminate the retrieval process.\n\nAnother common approach involves treating the large model as an agent, enabling it to directly generate answers to questions to signal the end of iteration. For instance, [67, 69, 75, 155, 170] propose LLM-based agents to reason on graphs. These agents could autonomously determine the information for retrieval, invoke the pre-defined retrieval tools, and cease the retrieval process based on the retrieved information.\n\n# 6.2.3 Multi-Stage Retrieval\n\nMulti-stage retrieval divides the retrieval process linearly into multiple stages, with additional steps such as retrieval enhancement, and even generation processes occurring between these stages. In multi-stage retrieval, different stages may employ various types of retrievers, which enables the system to incorporate various retrieval techniques tailored to different aspects of the query. For example, Wang et al. [171] first utilize a non-parametric retriever to extract \ud835\udc5b-hop paths of entities in the query\u2019s reasoning chain, then after a pruning stage, it further retrieves the one-hop neighbors of the entities in the pruned subgraph. OpenCSR [53] divides the retrieval process into two stages. In the first stage, it retrieves all 1-hop neighbors of the topic entity. In the second stage, it compares the similarity between these neighbor nodes and other nodes, selecting the top-\ud835\udc58 nodes with the highest similarity for retrieval. GNN-RAG [119] first employs GNNs to retrieve the top-\ud835\udc58 nodes most likely to be the answer. Subsequently, it retrieves all shortest paths between query entities and answer entities pairwise.\n\n# 6.2.4 Discussion\n\nIn GraphRAG, once retrieval typically exhibits lower complexity and shorter response times, making it suitable for scenarios requiring real-time responsiveness. In contrast, iterative retrieval often involves higher time complexity, especially when employing LLMs as retrievers, potentially leading to longer processing times. However, this approach can yield higher retrieval accuracy by iteratively refining retrieved information and generating responses. Therefore, the choice of retrieval paradigm should balance accuracy and time complexity based on specific use cases and requirements.\n\n# 6.3 Retrieval Granularity\n\nAccording to different task scenarios and indexing types, researchers design distinct retrieval granularities (i.e., the form of related knowledge retrieved from graph data), which can be divided.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d40da6f0-f4bd-4377-a281-48b604d1fbe5": {"__data__": {"id_": "d40da6f0-f4bd-4377-a281-48b604d1fbe5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9843422b-c27b-4756-8766-023566c8c7f3", "node_type": "4", "metadata": {}, "hash": "615966631500f77e96ee0401d5de188eb8b62a5091223ab4d1264d953b37f695", "class_name": "RelatedNodeInfo"}}, "text": "# 6.3 Retrieval Granularity\n\n# 6.3.1 Nodes\n\nNodes allow for precise retrieval focused on individual elements within the graph, which is ideal for targeted queries and specific information extraction. In general, for knowledge graphs, nodes refer to entities. For other types of text attribute graphs, nodes may include textual information that describes the node\u2019s attributes. By retrieving nodes within the graph, GraphRAG systems could provide detailed insights into their attributes, relationships, and contextual information. For example, Munikoti et al. [124], Li et al. [96] and Wang et al. [172] construct document graphs and retrieves relevant passage nodes. Liu et al. [99], Sun et al. [151] and Guti\u00e9rrez et al. [51] retrieve entities from constructed knowledge graphs.\n\n# 6.3.2 Triplets\n\nGenerally, triplets consist of entities and their relationships in the form of subject-predicate-object tuples, providing a structured representation of relational data within a graph. The structured format of triplets allows for clear and organized data retrieval, making it advantageous in scenarios where understanding relationships and contextual relevance between entities is critical. Yang et al. [185] retrieve triplets containing topic entities as relevant information. Huang et al. [63], Li et al. [90] and Li et al. [95] first convert each triplet of graph data into textual sentences using predefined templates and subsequently adopt a text retriever to extract relevant triplets. However, directly retrieving triplets from graph data may still lack contextual breadth and depth, thus being unable to capture indirect relationships or reasoning chains. To address this challenge, Wang et al. [164] propose to generate the logical chains based on the original question, and retrieve the relevant triplets of each logical chain.\n\n# 6.3.3 Paths\n\nThe retrieval of path-granularity data can be seen as capturing sequences of relationships between entities, enhancing contextual understanding and reasoning capabilities. In GraphRAG, retrieving paths offers distinct advantages due to their ability to capture complex relationships and contextual dependencies within a graph. However, path retrieval can be challenging due to the exponential growth in possible paths as graph size increases, which escalates computational complexity. To address this, some methods retrieve relevant paths based on pre-defined rules. For example, Wang et al. [171] and Lo and Lim [108] first select entity pairs in the query and then traverse to find all the paths between them within \ud835\udc5b-hop. HyKGE [73] first defines three types of paths: path, co-ancestor chain, and co-occurrence chain, and then utilizes corresponding rules to retrieve each of these three types of paths. In addition, some methods utilize models to perform path searching on graphs. ToG [113, 154] proposes to prompt the LLM agent to perform the beam search on KGs and find multiple possible reasoning paths that help answer the question. Luo et al. [112], Wu et al. [182] and Guo et al. [50] first utilize the model to generate faithful reasoning plans and then retrieve relevant paths based on these plans. GNN-RAG [119] first identifies the entities in the question. Subsequently, all paths between entities that satisfy a certain length relationship are extracted.\n\n# 6.3.4 Subgraphs\n\nRetrieving subgraphs offers significant advantages due to its ability to capture comprehensive relational contexts within a graph. This granularity enables GraphRAG to extract and analyze complex patterns, sequences, and dependencies embedded within larger structures, facilitating deeper insights and a more nuanced understanding of semantic connections. To ensure both information completeness and retrieval efficiency, some methods propose an initial rule-based approach to retrieve candidate subgraphs, which are subsequently refined or processed further. Peng and Yang [133] retrieve the ego graph of the patent phrase from the self-constructed patent-phrase graph. Yasunaga et al. [189], Feng et al. [40] and Taunk et al.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cde4160-9ee2-4451-ab6e-ba93a62f2c11": {"__data__": {"id_": "6cde4160-9ee2-4451-ab6e-ba93a62f2c11", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2df8c5bc-7c37-4fce-9ebe-5c0a69d49b66", "node_type": "4", "metadata": {}, "hash": "d24a84959e7caa44f69b1dc04a6ed6a1854b540fbd18deb8aeb1120a7067335f", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 6.3.5 Hybrid Granularities\n\nConsidering the advantages and disadvantages of various retrieval granularities mentioned above, some researchers propose using hybrid granularities, that is, retrieving relevant information of multiple granularities from graph data. This type of granularity enhances the system\u2019s ability to capture both detailed relationships and broader contextual understanding, thus reducing noise while improving the relevance of the retrieved data. Various previous works propose to utilize LLM agents to retrieve complex hybrid information. Jin et al. [75], Jiang et al. [67], Jiang et al. [69], Wang et al. [170] and Sun et al. [155] propose to adopt LLM-based agents for adaptively selecting nodes, triplets, paths, and subgraphs.\n\n# 6.3.6 Discussion\n\n(1) In real applications, there are no clear boundaries between these retrieval granularities, as subgraphs can be composed of multiple paths, and paths can be formed by several triplets. (2) Various granularities such as nodes, triplets, paths, and subgraphs offer distinct advantages in the GraphRAG process. Balancing between retrieval content and efficiency is crucial when selecting the granularity, depending on the specific context of the task. For straightforward queries or when efficiency is paramount, finer granularities such as entities or triplets may be preferred to optimize retrieval speed and relevance. In contrast, complex scenarios often benefit from a hybrid approach that combines multiple granularities. This approach ensures a more comprehensive understanding of the graph structure and relationships, enhancing the depth and accuracy of the generated responses. Thus, GraphRAG\u2019s flexibility in granularity selection allows it to adapt effectively to diverse information retrieval needs across various domains.\n\n# 6.4 Retrieval Enhancement\n\nTo ensure high retrieval quality, researchers propose techniques to enhance both user queries and the knowledge retrieved. In this paper, we categorize query enhancement into query expansion and query decomposition, and knowledge enhancement into merging and pruning. These strategies collectively optimize the retrieval process. Although other techniques such as query rewriting [114, 117, 132, 137] are commonly used in RAG, they are less frequently applied in GraphRAG. We do not delve into these methods, despite their potential adaptation for GraphRAG.\n\n# 6.4.1 Query Enhancement\n\nStrategies applied to queries typically involve pre-processing techniques that enrich the information for better retrieval. This may include query expansion and query decomposition.\n\n(1) Query Expansion. Due to the generally short length of queries and their limited information content, query expansion aims to improve search results by supplementing or refining the original query with additional relevant terms or concepts. Luo et al. [112] generate relation paths grounded.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90cee7f3-bfa3-43e5-a4b6-1c14e6a3e43d": {"__data__": {"id_": "90cee7f3-bfa3-43e5-a4b6-1c14e6a3e43d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d767e90-1eac-4a11-9a73-83995809e8c6", "node_type": "4", "metadata": {}, "hash": "7853d0a97c2dcce2a5ce548012b23bb01e2e1d78d13418ebd3a6c5d697986f70", "class_name": "RelatedNodeInfo"}}, "text": "# 111:16\n\n# Peng et al.\n\nby KGs with LLMs to enhance the retrieval query. Cheng et al. [20] adopt SPARQL to get all the aliases of the query entities from Wikidata to augment the retrieval queries, which capture lexical variations of the same entity. Huang et al. [63] propose a consensus-view knowledge retrieval method to improve retrieval accuracy, which first discover semantically relevant queries, and then re-weight the original query terms to enhance the retrieval performance. HyKGE [73] utilizes a large model to generate the hypothesis output of the question, concatenating the hypothesis output with the query as input to the retriever. Golden-Retriever [2] first recognizes the jargon in the query and then retrieves explanations of the jargon as a supplement to the query.\n\n# 6.4.2 Knowledge Enhancement\n\nAfter retrieving initial results, knowledge enhancement strategies are employed to refine and improve the retriever\u2019s results. This phase often involves knowledge merging and knowledge pruning processes to present the most pertinent information prominently. These techniques aim to ensure that the final set of retrieved results is not only comprehensive but also highly relevant to the user\u2019s information needs.\n\n# (1) Knowledge Merging\n\nKnowledge merging retrieved information enables compression and aggregation of information, which assists in obtaining a more comprehensive view by consolidating relevant details from multiple sources. This approach not only enhances the completeness and coherence of the information but also mitigates issues related to input length constraints in models. KnowledgeNavigator [50] merges nodes and condenses the retrieved sub-graph through triple aggregation to enhance the reasoning efficiency. In Subgraph Retrieval [196], after retrieving top-k paths from each topic entity to form a single subgraph, researchers propose to merge the same entities from different subgraphs to form the final subgraph. Wen et al. [175] and Li et al. [89] merge retrieved subgraphs based on relations, combining head entities and tail entities that satisfy the same relation into two distinct entity sets, ultimately forming a relation paths.\n\n# (2) Knowledge Pruning\n\nKnowledge pruning involves filtering out less relevant or redundant retrieved information to refine the results. Previous approaches for pruning encompass two main categories: (re)-ranking-based approaches and LLM-based approaches. (Re)-ranking methods involve the reordering or prioritization of retrieved information using tailored metrics or criteria.\n\nOne line of methods introduces stronger models for reranking. For example, Li et al. [90] concatenate each retrieved triplet with the question-choice pair, and adopt a pre-trained cross-encoder [140] to re-rank the retrieved triplets. Jiang et al. [73] utilize the FlagEmbedding to encode the text to re-rank top-k documents returned by embedding model \u201cbge_reranker_large\u201d. Liu et al. [101] train a PLM to.\n\nAnother category utilizes the similarity between queries and retrieved information for ranking. For instance, Cheng et al. [20] re-rank the candidate subgraphs based on the similarity for both relation and fine-grained concept between subgraphs and the query. Taunk et al. [158] first cluster the 2-hop neighbors and then delete the cluster with the lowest similarity score with the input query. Yasunaga et al. [189] prune the retrieved subgraph according to the relevance score between the question context and the KG entity nodes calculated by a pre-trained language model. Wang et al. [171], Jiang et al. [70], Guti\u00e9rrez et al. [51] and Luo et al. [110] adopt Personalized PageRank algorithm to rank the retrieved candidate information for further filtering. Liu et al. [101] trains.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3519d667-1b12-4f9d-be19-724b600f6ee2": {"__data__": {"id_": "3519d667-1b12-4f9d-be19-724b600f6ee2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d982e4f4-50c1-4ae1-9a0b-318cc7fd9764", "node_type": "4", "metadata": {}, "hash": "507cb34a267758bdb5fe0da647d4dd4eb0c26f7ef76213237818aa0ab0be1cff", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 7 Graph-Enhanced Generation\n\nThe generation stage is another crucial step in GraphRAG, aimed at integrating the retrieved graph data with the query to enhance response quality. In this stage, suitable generation models must be selected based on the downstream tasks. The retrieved graph data is then transformed into formats compatible with the generators. The generator takes both the query and the transformed graph data as inputs to produce the final response. Beyond these fundamental processes, generative enhancement techniques can further improve the output by intensifying the interaction between the query and the graph data and enriching the content generation itself. The organization of this section and the overview of graph-enhanced generation are depicted in Figure 5.\n\n# 7.1 Generators\n\nThe selection of generators often depends on the type of downstream task at hand. For discriminative tasks (e.g., multi-choice question answering) or generative tasks that can be formulated as discriminative tasks (e.g., KBQA), one can utilize GNNs or discriminative language models to learn representations of the data. These representations can then be mapped to the logits associated with different answer options to provide responses. Alternatively, generative language models can be employed to directly generate answers. For generative tasks, however, the use of GNNs and discriminative language models alone is insufficient. These tasks require the generation of text, which necessitates the deployment of decoders.\n\n# 7.1.1 GNNs\n\nDue to the powerful representational capabilities of GNNs for graph data, they are particularly effective for discriminative tasks. GNNs can directly encode graph data, capturing complex relationships and node features inherent in the graph structure. This encoding is then processed through a Multi-Layer Perceptron (MLP) to generate predictive outcomes. These approaches\n\n# 7.2 Graph Formats\n\n- Graph Languages\n- Graph Embeddings\n\n# 7.3 Generation Enhancement\n\n- Pre-Generation Enhancement\n- Mid-Generation Enhancement\n- Post-Generation Enhancement\n\n# Fig. 5. The overview of graph-enhanced generation.\n\na PLM to score the similarity between the retrieved information and the query, and rerank the retrieved paths based on the similarity score. G-G-E [43] first divides the retrieved subgraph into several smaller subgraphs, then compares the similarity between each smaller subgraph and the query. Subgraphs with low similarity are removed, and the remaining smaller subgraphs are merged into a larger subgraph.\n\nAdditionally, a third category of methods proposes new metrics for reranking. For example, Munikoti et al. [124] propose a metric that measures both the impact and recency of the retrieved text chunks. KagNet [97] decomposes the retrieved paths into triplets and reranks the paths based on the confidence score measured by the knowledge graph embedding (KGE) techniques. LLM-based methods excel in capturing complex linguistic patterns and semantic nuances, which enhances their ability to rank search results or generate responses more accurately. To avoid introducing noisy information, Wang et al. [171] and Kim et al. [80] propose to prune the irrelevant graph data by calling LLMs to check.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8ffcf57-6c08-4645-b6ad-ad92fa9414cc": {"__data__": {"id_": "e8ffcf57-6c08-4645-b6ad-ad92fa9414cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b87e77a8-c678-4578-82de-2e705185ed55", "node_type": "4", "metadata": {}, "hash": "3733aa14c47cebca85bc18906b304757f2ec49a536452a91248bc9aa49c77a01", "class_name": "RelatedNodeInfo"}}, "text": "# 111:18\n\n# Peng et al.\n\nprimarily utilize classical GNN models (e.g., GCN [83], GAT [162], GraphSAGE [52], and Graph Transformers [147]), either in their original form or modified to better align with downstream tasks. For example, HamQA [30] designs a hyperbolic GNN to learn the representations of retrieved graph data, which learns from the mutual hierarchical information between query and graphs. Sun et al. [152] compute PageRank scores for neighboring nodes and aggregates them weighted by these scores, during message-passing. This approach enhances the central node\u2019s ability to assimilate information from its most relevant neighboring nodes. Mavromatis and Karypis [118] decode the query into several vectors (instructions), and enhances instruction decoding and execution for effective reasoning by emulating breadth-first search (BFS) with GNNs to improve instruction execution and using adaptive reasoning to update the instructions with KG-aware information.\n\n# 7.1.2 LMs.\n\nLMs possess strong capabilities in text understanding, which also allows them to function as generators. In the context of integrating LMs with graph data, it is necessary to first convert the retrieved graph data into specific graph formats. This conversion process ensures that the structured information is effectively understood and utilized by the LMs. These formats, which will be elaborated on in Section 7.2, are crucial for preserving the relational and hierarchical structure of the graph data, thereby enhancing the model\u2019s ability to interpret complex data types. Once the graph data is formatted, it is then combined with a query and fed into an LM.\n\nFor encoder-only models, such as BERT [28] and RoBERTa [107], their primary use is in discriminative tasks. Similar to GNNs, these models first encode the input text and then utilize MLPs to map it to the answer space [63, 70, 90]. On the other hand, encoder-decoder and decoder-only models, such as T5 [138], GPT-4 [127], and LLaMA [31], are adept at both discriminative and generative tasks. These models excel in text understanding, generation, and reasoning, allowing them to process textual inputs directly and generate textual responses [32, 73, 75, 112, 119, 154, 164, 171].\n\n# 7.1.3 Hybrid Models.\n\nConsidering the strengths of GNNs at representing the structure of graph data, and the robust understanding of text demonstrated by LMs, many studies are exploring the integration of these two technologies to generate coherent responses. This paper categorizes the hybrid generative approaches into two distinct types: cascaded paradigm and parallel paradigm.\n\n# (1) Cascaded Paradigm.\n\nIn the cascaded approaches, the process involves a sequential interaction where the output from one model serves as the input for the next. Specifically, the GNN processes the graph data first, encapsulating its structural and relational information into a form that the LM can understand. Subsequently, this transformed data is fed into the LM, which then generates the final text-based response. These methods leverage the strengths of each model in a step-wise fashion, ensuring detailed attention to both structural and textual data.\n\nIn these methods, prompt tuning [88, 91, 105, 106] is a typical approach, where GNNs are commonly employed to encode the retrieved graph data. The encoded graph data is subsequently pre-pended as a prefix to the input text embeddings of an LM. The GNN is then optimized through downstream tasks to produce enhanced encodings of the graph data [44, 55, 58, 197].\n\n# (2) Parallel Paradigm.\n\nOn the other hand, the parallel approach operates by concurrently utilizing the capabilities of both the GNN and the LLM. In this setup, both models receive the initial inputs simultaneously and work in tandem to process different facets of the same data. The outputs are then merged, often through another model or a set of rules, to produce a unified response that integrates insights from both the graphical structure and the textual content.\n\nIn the parallel paradigm, a typical approach involves separately encoding inputs using both GNNs and LMs, followed by integrating these two representations, or directly integrating their output responses. For instance, Jiang et al. [68] aggregate predictions from GNNs and LMs by weighted summation to obtain the final answer. Lin et al. [97] and Pahuja et al. [129] integrate.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d5d7654-dc8b-4710-8207-92b93a57af43": {"__data__": {"id_": "1d5d7654-dc8b-4710-8207-92b93a57af43", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae1afc5b-3dff-4620-a507-2dffa8c7b274", "node_type": "4", "metadata": {}, "hash": "6e5bc63f1348a691ef2ddbcc00ef3e8a2ee9f72d08b08a2ed44d1a180c2bd414", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# Retrieved Graph Data\n\n|Adjacency/Edge Table|Natural Language|Node Sequence|\n|---|---|---|\n|Claude|(Claude Monet, introduced, new techniques)|Claude Monet \u2192 new techniques|\n|introduced Monet|new techniques, emerged in 19th century|Claude Monet \u2192 new techniques|\n| |(new techniques, revolutionized, later art movements)|new techniques revolutionized later \u2192 19th century|\n\n# Code-like Forms\n\n{uaph Id}\n\n{ndeld}\n\n# Syntax Tree\n\nNode feature:\n\n- 0: Claude Monet\n- 1: new techniques\n- 2: 19th century\n- 3: later art movements\n\nEdge feature:\n\n- (0,1): introduced\n- (0,2): emerged in\n- (0,3): revolutionized\n\nStructure:\n\n- Tree Construction\n- center node: 0\n- 1st-hop: 1\n- 2nd-hop: 2, 3\n\nFig. 6. Illustration of the graph languages. Given the retrieved subgraph on the left part, we show how to transform it into adjacency/edge table, natural language, node sequence, code-like forms and syntax trees to adapt the input form requirements of different generators.\n\nThe graph representations derived from GNNs and the text representations generated by LMs using attention mechanisms. Yasunaga et al. [189], Munikoti et al. [124] and Taunk et al. [158] directly concatenate graph representations with text representations.\n\nAnother approach involves designing dedicated modules that integrate GNNs with LMs, enabling the resulting representations to encapsulate both structural and textual information. For instance, Zhang et al. [199] introduce a module called the GreaseLM Layer, which incorporates both GNN and LM layers. At each layer, this module integrates textual and graph representations using a two-layer MLP before passing them to the next layer. Similarly, ENGINE [204] proposes G-Ladders, which combine LMs and GNNs through a side structure, enhancing node representations for downstream tasks.\n\n# Discussion\n\nHybrid models that harness both the representation capabilities of GNNs for graph data and LMs for text data hold promising applications. However, effectively integrating information from these two modalities remains a significant challenge.\n\n# 7.2 Graph Formats\n\nWhen using GNNs as generators, the graph data can be directly encoded. However, when utilizing LMs as generators, the non-Euclidean nature of graph data poses a challenge, as it cannot be directly combined with textual data for input into the LMs. To address this, graph translators are employed to convert the graph data into a format compatible with LMs. This conversion enhances the generative capabilities of LMs by enabling them to effectively process and utilize structured graph information.\n\nIn this survey, we summarize two distinct graph formats: graph languages and graph embeddings. We illustrate this process with an example in Figure 6, with detailed introductions provided below.\n\n# 7.2.1 Graph Languages\n\nA graph description language is a formalized system of notation that is specifically crafted to characterize and represent graph data. It prescribes a uniform syntax and semantic framework that describes the components and interconnections within a graph. Through these languages, users can consistently generate, manipulate, and interpret graph data in a comprehensible format to machines. They enable the definition of graph architectures, the specification of attributes for nodes and edges, and the implementation of operations and queries.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a64b7a3-1ad7-4f79-b740-68e026c0d9cc": {"__data__": {"id_": "6a64b7a3-1ad7-4f79-b740-68e026c0d9cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9772db31-bc51-4b36-9d9d-0b29ff654126", "node_type": "4", "metadata": {}, "hash": "0fd8a4028f7a71bf76a948bad6409872953be95da7a6ff9445e2d8b403221aeb", "class_name": "RelatedNodeInfo"}}, "text": "# Peng et al.\n\n# Graph Languages\n\nNext, we will introduce five types of graph languages separately: Adjacency / Edge Table, Natural Language, Codes, Syntax Tree, and Node Sequence.\n\n# (1) Adjacency / Edge Table\n\nThe adjacency table and the edge table are widely used methods for describing graph structures [38, 49, 94, 165]. The adjacency table enumerates the immediate neighbors of each vertex, offering a compact way to represent connections in sparse graphs. For example, KG-GPT [80] linearizes the triples in the retrieved subgraph, which are then concatenated and fed into the LLMs. Conversely, the edge table details all the edges within the graph, providing a straightforward representation that is particularly useful for processing and analyzing graphs in a linear format. Both two methods are brief, easy to understand, and intuitive.\n\n# (2) Natural Language\n\nGiven that user queries are typically presented in natural language, and considering the outstanding natural language comprehension capabilities of LMs, it becomes a compelling approach to describe the retrieved graph data using natural language. By translating graph data into descriptive, easily comprehensible language, LMs can bridge the gap between raw data representation and user-friendly information, facilitating more effective interactions with data-driven applications. For example, some researchers [63, 90] propose defining a natural language template for each type of edge in advance and subsequently filling in the endpoints of each edge into the corresponding template based on its type. Ye et al. [190] employ natural language to describe the information of 1-hop and 2-hop neighboring nodes of the central node. Edge et al. [32] utilize LLMs to generate report-like summaries for each detected graph community. Wu et al. [182] and Guo et al. [50] adopt LMs to rewrite the edge table of retrieved subgraphs, generating a natural language description. Fatemi et al. [38] explore different representations of nodes (e.g., Integer encoding, alphabet letters, names, etc.) and edges (e.g., parenthesis, arrows, incident, etc.). Jin et al. [75], Jiang et al. [67], Jiang et al. [69], Wang et al. [170] and Sun et al. [155] integrate information from different granularities within the graph into prompts through natural language in the form of dialogue.\n\n# (3) Code-Like Forms\n\nConsidering that natural language descriptions and other 1-D sequences are inherently inadequate for directly representing the 2-D structure of graph data, and given the robust code comprehension capabilities of LMs, many researchers [49] explore using code-like formats to represent graph structures. For example, Guo et al. [49] examine the use of Graph Modeling Language (GML) [56] and Graph Markup Language (GraphML) [141] for representing graphs. These standardized languages are specifically designed for graph data, providing comprehensive descriptions that encompass nodes, edges, and their interrelationships.\n\n# (4) Syntax Tree\n\nCompared to direct flattening of graphs, some research [201] propose transforming graphs into structures akin to syntax trees. Syntax trees possess a hierarchical structure and, being topological graphs, also maintain a topological order. This method retains more structural information, enhancing the understanding and analysis of the graph\u2019s intrinsic properties. Such a transformation not only preserves the relational dynamics between different graph elements but also facilitates more sophisticated algorithms for graph analysis and processing. GRAPHTEXT [201] proposes transforming the ego network of a central node into a graph-syntax tree format. This format not only encapsulates structural information but also integrates the features of the nodes. By traversing this syntax tree, it is possible to obtain a node sequence that maintains both topological order and hierarchical structure.\n\n# (5) Node Sequence\n\nSome studies [18, 119] propose representing graphs through sequences of nodes, which are often generated using predefined rules. Compared to natural language descriptions, these node sequences are more concise and incorporate prior knowledge, specifically the structural.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcb2102a-d100-46c3-ae23-c5e90ae5c168": {"__data__": {"id_": "bcb2102a-d100-46c3-ae23-c5e90ae5c168", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d52e1993-8a76-4d9d-8209-c568521d16d1", "node_type": "4", "metadata": {}, "hash": "7e35dbdd9ad7491b97ff8b5330151e3b7e77bdb2646adc9cc710b6285bedea28", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 7.2.2 Graph Embeddings\n\nThe above graph language methods transform graph data into text sequences, which may result in overly lengthy contexts, incurring high computational costs and potentially exceeding the processing limits of LLMs. Additionally, LLMs currently struggle to fully comprehend graph structures even with graph languages [49]. Thus, using GNNs to represent graphs as embeddings presents a promising alternative. The core challenge lies in integrating graph embeddings with textual representations into a unified semantic space. Current research focuses on utilizing prompt tuning methodologies, as discussed earlier. There are also some methods that adopt FiD (Fusion-in-Decoder) [65, 194], which first convert the graph data into text, then encode it using an LM-based encoder and input it into the decoders [29, 37, 193]. Notably, feeding graph representations into LMs is feasible primarily with open-source LMs, not closed-source models like GPT-4 [127]. While graph embedding methods avoid handling long text inputs, they face other challenges, such as difficulty in preserving precise information like specific entity names and poor generalization.\n\n# 7.3 Generation Enhancement\n\nIn the generation phase, besides converting the retrieved graph data into formats acceptable by the generator and inputting it together with the query to generate the final response, many researchers explore various methods of generation enhancement techniques to improve the quality of output responses. These methods can be classified into three categories based on their application stages: pre-generation enhancement, mid-generation enhancement, and post-generation enhancement.\n\n# 7.3.1 Pre-Generation Enhancement\n\nPre-generation enhancement techniques focus on improving the quality of input data or representations before feeding them into the generator. In fact, there is no clear boundary between Pre-Generation Enhancement and Retrieval. In this survey, we categorize the retrieval stage as the process of retrieving knowledge from the original graph, and merging and pruning retrieved knowledge. Subsequent operations are considered Pre-Generation Enhancements.\n\nCommonly used pre-generation enhancement approaches primarily involve semantically enriching the retrieved graph data to achieve tighter integration between the graph data and textual query. Wu et al. [182] employ LLMs to rewrite retrieved graph data, enhancing the naturalness and semantic richness of the transformed natural language output. This method not only ensures that\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4710bec7-dc6d-4780-8a37-05f1149f825a": {"__data__": {"id_": "4710bec7-dc6d-4780-8a37-05f1149f825a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8df9836c-9846-4216-a16d-c853d8126b87", "node_type": "4", "metadata": {}, "hash": "53e44091801bbd289c7ae79dc75fabec184bed3227ae614605ab9645ca42ca4d", "class_name": "RelatedNodeInfo"}}, "text": "# 7.3.2 Mid-Generation Enhancement\n\nMid-generation enhancement involves techniques applied during the generation process. These methods typically adjust the generation strategies based on intermediate results or contextual cues. TIARA [148] introduces constrained decoding to control the output space and reduce generation errors. When generating logical forms, if the constrained decoder detects that it is currently generating a pattern item, it restricts the next generated token to options that exist in tries containing KB classes and relations. Compared with the Beam Search, this approach ensures that pattern items generated are guaranteed to exist in the knowledge graph, thereby reducing generation errors. There are other methods adjusting the prompts of LLMs to achieve multi-step reasoning. For example, MindMap [175] not only produces answers but also generates the reasoning process.\n\n# 7.3.3 Post-Generation Enhancement\n\nPost-generation enhancement occurs after the initial response is generated. Post-generation enhancement methods primarily involve integrating multiple generated responses to obtain the final response. Some methods focus on integrating outputs from the same generator under different conditions or inputs. For example, Edge et al. [32] generate a summary for each graph community, followed by generating responses to queries based on the summary, and then scoring these responses using an LLM. Ultimately, the responses are sorted in descending order according to their scores and sequentially incorporated into the prompt until the token limit is reached. Subsequently, the LLM generates the final response. Wang et al. [164] and Kim et al. [80] first decompose the query into several sub-questions, then generate answers for each sub-question, and finally merge the answers of all sub-questions to obtain the final answer. Alternatively, other methods combine or select responses generated by different models. Lin et al. [97] and Jiang et al. [68] combine the outputs generated by both GNNs and LLMs to reach a synergistic effect. UniOQA [95] explores two methods for generating answers: one involves generating queries in Cypher Query Language (CQL) to execute and obtain results, while the other method directly generates answers based on retrieved triplets. The final answer is determined through a dynamic selection mechanism. In EmbedKGQA [145], besides the learned scoring function, researchers additionally design a rule-based score based on the graph structures. These two scores are combined to find the answer entity. Li et al. [94] combine answers based on retrieved graph data with responses generated according to the LLM\u2019s own knowledge. In addition to integrating multiple responses, KALMV [7] trains a verifier to judge whether the generated answer is correct, and if it is not, to further determine whether the error is due to generation or retrieval.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f23a4765-136e-4f2b-bbf8-376dbaad96cb": {"__data__": {"id_": "f23a4765-136e-4f2b-bbf8-376dbaad96cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4d7e487-56e2-439b-8a0b-1dc0dc69003a", "node_type": "4", "metadata": {}, "hash": "fefa5e77817ef64fb5f8c73758a790589e59a075c46dfa38a900c5046421b86d", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 8 Training\n\nIn this section, we summarize the individual training of retrievers, generators, and their joint training. We categorize previous works into Training-Free and Training-Based approaches based on whether explicit training is required. Training-Free methods are commonly employed when using closed-source LLMs such as GPT-4 [127] as retrievers or generators. These methods primarily rely on carefully crafted prompts to control the retrieval and generation capabilities of LLMs. Despite LLMs\u2019 strong abilities in text comprehension and reasoning, a challenge of Training-Free methods lies in the potential sub-optimality of results due to the lack of specific optimization for downstream tasks. Conversely, Training-Based methods involve training or fine-tuning models using supervised signals. These approaches enhance the model performance by adapting them to specific task objectives, thereby potentially improving the quality and relevance of retrieved or generated content. Joint training of retrievers and generators aims to enhance their synergy, thereby boosting performance on downstream tasks. This collaborative approach leverages the complementary strengths of both components to achieve more robust and effective results in information retrieval and content generation applications.\n\n# 8.1 Training Strategies of Retriever\n\n# 8.1.1 Training-Free\n\nThere are two primary types of Training-Free Retrievers currently in use. The first type consists of non-parametric retrievers. These retrievers rely on pre-defined rules or traditional graph search algorithms rather than specific models [158, 189]. The second type utilizes pre-trained LMs as retrievers. Specifically, one group of works utilizes pre-trained embedding models to encode the queries and perform retrieval directly based on the similarity between the query and graph elements [90]. Another group of works adopts generative language models for training-free retrieval. Candidate graph elements such as entities, triples, paths, or subgraphs are included as part of the prompt input to the LLMs. The LLMs then leverage semantic associations to select appropriate graph elements based on the provided prompt [32, 75, 80, 119, 154, 164, 171]. These methods harness the powerful semantic understanding capabilities of LMs to retrieve relevant graph elements without the need for explicit training.\n\n# 8.1.2 Training-Based\n\nWhen the retrieval granularity is nodes or triplets, many methods train retrievers to maximize the similarity between the retrieval ground truth and the query. For instance, MemNNs [12] leverages metric learning to closely align the ground truth with the query in semantic space while differentiating unrelated facts from the query. On the contrary, when the retrieval granularity is paths, training retrievers often adopts an autoregressive approach, where the previous relationship path is concatenated to the end of the query. The model then predicts the next relation based on the concatenated input [50, 182].\n\nHowever, the lack of ground truth for retrieval content in the majority of datasets poses a significant challenge. To address this issue, many methods attempt to construct reasoning paths based on distant supervision to guide retriever training. For example, Zhang et al. [196], Feng et al. [39] and Luo et al. [112] extract all paths (or shortest paths) between entities in the queries and entities in the answers, using them as training data for the retriever. In addition, Zhang et al. [196] also employ a relationship extraction dataset for distant supervision in unsupervised settings. There is another category of methods that utilize implicit intermediate supervision signals to train Retrievers. For instance, NSM [54] employs a bidirectional search strategy, where two retrievers start searching from the head entity and tail entity, respectively. The supervised objective is to ensure that the paths searched by the two retrievers converge as closely as possible. KnowGPT [198] and MINERVA [23] treat the selection of adjacent nodes to build paths or subgraphs as a Markov process.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5a857e7-7649-4d20-ad31-691e2aa41f37": {"__data__": {"id_": "c5a857e7-7649-4d20-ad31-691e2aa41f37", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36161822-dcb1-4104-a0b7-fc0fedd5c14c", "node_type": "4", "metadata": {}, "hash": "6822d8749373c366c8f8af20b775d86d5a678780bc8a13f1eed78363003a50c6", "class_name": "RelatedNodeInfo"}}, "text": "# Peng et al.\n\nThey design the reward function around the inclusion of the answer in the retrieved information and adopt reinforcement learning methods e.g. policy gradient to optimize the retriever. Some methods argue that distant supervision signals or implicit intermediate supervision signals may contain considerable noise, making it challenging to train effective retrievers. Therefore, they consider employing self-supervised methods for pre-training retrievers. SKP [29] pre-trains the DPR (Dense Passage Retrieval) model [78]. Initially, it conducts random sampling on subgraphs and transforms the sampled subgraphs into passages. Subsequently, it randomly masks passages, trains the model using a Masked Language Model (MLM), and employs contrastive learning by treating the masked passages and original passages as positive pairs for comparison.\n\n# 8.2 Training of Generator\n\n# 8.2.1 Training-Free\n\nTraining-Free Generators primarily cater to closed-source LLMs or scenarios where avoiding high training costs is essential. In these methods, the retrieved graph data is fed into the LLM alongside the query. The LLMs then generate responses based on the task description provided in the prompt, heavily relying on their inherent ability to understand both the query and the graph data.\n\n# 8.2.2 Training-Based\n\nTraining the generator can directly receive supervised signals from downstream tasks. For generative LLMs, fine-tuning can be achieved using supervised fine-tuning (SFT), where task descriptions, queries, and graph data are inputted, and the output is compared against the ground truth for the downstream task [55, 58, 112]. On the other hand, for GNNs or discriminative models functioning as generators, specialized loss functions tailored to the downstream tasks are employed to train the models effectively [68, 90, 158, 189, 199].\n\n# 8.3 Joint Training\n\nJointly training retrievers and generators simultaneously enhances performance on downstream tasks by leveraging their complementary strengths. Some approaches unify retrievers and generators into a single model, typically LLMs, and train them with both retrieval and generation objectives simultaneously [112]. This method capitalizes on the cohesive capabilities of a unified architecture, enabling the model to seamlessly retrieve relevant information and generate coherent responses within a single framework.\n\nOther methodologies involve initially training retrievers and generators separately, followed by joint training techniques to fine-tune both components. For instance, Subgraph Retriever [196] adopts an alternating training paradigm, where the retriever\u2019s parameters are fixed to use the graph data for training the generator. Subsequently, the generator\u2019s parameters are fixed, and feedback from the generator is used to guide the retriever\u2019s training. This iterative process helps both components refine their performance in a coordinated manner.\n\n# 9 Applications and Evaluation\n\nIn this section, we will summarize the downstream tasks, application domains, benchmarks and metrics, and industrial applications related to GraphRAG. Table 1 collects existing GraphRAG techniques, categorizing them by downstream tasks, benchmarks, methods, and evaluation metrics. This table serves as a comprehensive overview, highlighting the various aspects and applications of GraphRAG technologies across different domains.\n\n# 9.1 Downstream Tasks\n\nGraphRAG is applied in various downstream tasks (especially NLP tasks), including Question Answering, Information Extraction, and others.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ea4eff0-573c-4dec-b1d6-ca4dde98ebf3": {"__data__": {"id_": "1ea4eff0-573c-4dec-b1d6-ca4dde98ebf3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed524dc3-03d0-4245-be4d-2e27060b96ba", "node_type": "4", "metadata": {}, "hash": "c1c60504f3208c7052c099a4281116de93d011f3b6a2b94f9a42db5ce6dadff2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c41a67a6-bd2b-46c4-90a7-ecfd7f0eca39", "node_type": "1", "metadata": {}, "hash": "13e978e0877dc33ccc0e2a70e4eb789efd40f2c5a3505e13016892e2dfffce74", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# Table 1. The tasks, benchmarks, methods, and metrics of GraphRAG.\n\n|Tasks|Benchmarks|Methods|Metrics|\n|---|---|---|---|\n|KBQA|WebQSP [192]|[112], [154], [113], [196], [182], [50], [167], [67], [69], [111], [164], [5], [148], [99], [119], [151], [193], [29], [43], [110], [6], [145], [54], [70], [21], [152], [71], [7], [24]|Accuracy, EM, Recall, F1, BERTScore, GPT-4 Average Ranking|\n| |WebQ [8]|[171], [154], [60], [182], [118], [71], [12], [24]| |\n| |CWQ [156]|[112], [154], [60], [196], [167], [69], [111], [118], [87], [99], [119], [151], [193], [43], [110], [70], [94], [54], [21], [24]| |\n|QA|GrailQA [47]|[154], [69], [148], [24]|Accuracy, EM, Recall, F1, BERTScore, GPT-4 Average Ranking|\n| |QALD10-en [134]|[154], [113], [94], [155]| |\n| |SimpleQuestions [13]|[154], [5], [12], [24]| |\n|CMCQA8|[171]| |Accuracy, EM, Recall, F1, BERTScore, GPT-4 Average Ranking|\n|MetaQA [200]|[112], [182], [50], [80], [164], [118], [151], [145], [67], [99], [54], [70], [21], [101]| | |\n|Natural Question [84]|[60], [7], [24]| | |\n|TriviaQA [77]|[60], [70]| | |\n|HotpotQA [187]|[60], [51], [113], [7], [24]| | |\n|Mintaka [146]|[5], [94], [6], [7]| | |\n|FreebaseQA [72]|[193], [110]| | |\n|CSQA [157]|[158], [189], [63], [90], [97], [39], [30]| | |\n|OBQA [120]|[158], [189], [63], [90], [39], [53], [30]| | |\n|MedQA [76]|[158], [39], [89]| | |\n|SocialIQA [143]|[63]| | |\n|PIQA [9]|[63]| | |\n|RiddleSenseQA [98]|[63]| | |\n|Entity Linking|ZESHEL [109]|[180]|Recall@K|\n| |CoNLL [57]|[180]| |\n|IE|T-Rex [33]|[155], [154]| |\n| |ZsRE [135]|[94], [155], [154], [113]|Hits@1|\n|Fact Verification|Creak [126]|[94], [155], [154], [113]|Accuracy, F1|\n| |FACTKG [82]|[80], [87], [101]| |\n|Link Prediction|FB15K-237 [159]|[22], [129]|MRR, Hits@K|\n| |FB15k [11]|[22]| |\n|WN18RR [27]|[129]| | |\n|Others|NELL995 [15]|[22]| |\n| |OpenDialKG [122]|[5]|MRR, Hits@K|\n|Recommendation|Yelp9|[168]|NDCG@K, Recall@K|\n\n# 9.1.1 Question Answering.\n\nThe QA tasks specifically include Knowledge Base Question Answering (KBQA) and CommonSense Question Answering (CSQA).\n\n# (1) KBQA.\n\nKBQA serves as a cornerstone downstream task for GraphRAG. In KBQA, questions typically pertain to specific knowledge graphs, and answers often involve entities, relationships, or operations between sets of entities within the knowledge graph. The task tests the systems\u2019 ability to retrieve and reason over structured knowledge bases, which is crucial in facilitating complex query responses.\n\n# (2) CSQA.\n\nDistinguished from KBQA, CSQA primarily takes the form of multiple-choice questions. Commonsense reasoning typically presents a commonsense question along with several answer options, each potentially representing either the name of an entity or a statement.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c41a67a6-bd2b-46c4-90a7-ecfd7f0eca39": {"__data__": {"id_": "c41a67a6-bd2b-46c4-90a7-ecfd7f0eca39", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed524dc3-03d0-4245-be4d-2e27060b96ba", "node_type": "4", "metadata": {}, "hash": "c1c60504f3208c7052c099a4281116de93d011f3b6a2b94f9a42db5ce6dadff2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ea4eff0-573c-4dec-b1d6-ca4dde98ebf3", "node_type": "1", "metadata": {}, "hash": "bd0746c1f3573f17f9f7578052617106658105c8cf9a37d10508982b8b21606f", "class_name": "RelatedNodeInfo"}}, "text": "The QA tasks specifically include Knowledge Base Question Answering (KBQA) and CommonSense Question Answering (CSQA).\n\n# (1) KBQA.\n\nKBQA serves as a cornerstone downstream task for GraphRAG. In KBQA, questions typically pertain to specific knowledge graphs, and answers often involve entities, relationships, or operations between sets of entities within the knowledge graph. The task tests the systems\u2019 ability to retrieve and reason over structured knowledge bases, which is crucial in facilitating complex query responses.\n\n# (2) CSQA.\n\nDistinguished from KBQA, CSQA primarily takes the form of multiple-choice questions. Commonsense reasoning typically presents a commonsense question along with several answer options, each potentially representing either the name of an entity or a statement. The objective is for machines to utilize external commonsense knowledge graphs, such as ConceptNet, to find relevant knowledge pertaining to the question and options, and to engage in appropriate reasoning and derive the correct answer.\n\n# 9.1.2 Information Retrieval.\n\nInformation Retrieval tasks consist of two categories: Entity Linking (EL) and Relation Extraction (RE).\n\n# (1) Entity Linking.\n\nEntity Linking (EL) is a critical task in the field of natural language processing that involves identifying entities mentioned in text segments and linking them to their corresponding entities in a knowledge graph. By leveraging a system such as Graph RAG, it is possible to retrieve relevant information from the knowledge graph, which facilitates the accurate inference of the specific entities that match the mentions in the text [180].\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 1964, "end_char_idx": 3674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92ef9da6-dce9-4880-aa29-b0c4c6fb2b42": {"__data__": {"id_": "92ef9da6-dce9-4880-aa29-b0c4c6fb2b42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8bd1842-3d30-45b7-b425-c7fa9bc97027", "node_type": "4", "metadata": {}, "hash": "4bd841d884e989aea106b56dd455f6ff6f9da80f27083ae3f61291df45b68d6c", "class_name": "RelatedNodeInfo"}}, "text": "# 111:26\n\n# Peng et al.\n\n# 9.1.3 Others\n\nIn addition to the aforementioned downstream tasks, GraphRAG can be applied to various other tasks in the realm of natural language processing such as fact verification, link prediction, dialogue systems, and recommendation.\n\n# (1) Fact Verification\n\nThe fact verification task typically involves assessing the truthfulness of a factual statement using knowledge graphs. Models are tasked with determining the validity of a given factual assertion by leveraging structured knowledge repositories. GraphRAG techniques can be utilized to extract evidential connections between entities to enhance the system\u2019s efficiency and accuracy [94, 136, 154, 155].\n\n# (2) Link Prediction\n\nLink prediction involves predicting missing relationships or potential connections between entities in a graph. GraphRAG is applied to this task [22, 129] by leveraging its ability to retrieve and analyze structured information from graphs, enhancing prediction accuracy by uncovering latent relationships and patterns within the graph data.\n\n# (3) Dialogue Systems\n\nDialogue Systems is designed to converse with humans using natural language, handling various tasks such as answering questions, providing information, or facilitating user interactions. By structuring conversation histories and contextual relationships in a graph-based framework, GraphRAG systems [5] can improve the model\u2019s ability to generate coherent and contextually relevant responses.\n\n# (4) Recommendation\n\nIn the context of E-commerce platforms, the purchase relationships between users and products naturally form a network graph. The primary objective of recommendation within these platforms is to predict the future purchasing intentions of users, effectively forecasting the potential connections within this graph [168].\n\n# 9.2 Application Domains\n\nGraphRAG is widely applied in E-commerce and biomedical, academic, literature, legal, and other application scenarios for its outstanding ability to integrate structured knowledge graphs with natural language processing, which will be introduced below.\n\n# 9.2.1 E-Commerce\n\nThe primary goal in the E-commerce area involves improving customer shopping experiences and increasing sales through personalized recommendations and intelligent customer services. In this area, historical interactions between users and products can naturally form a graph, which implicitly encapsulates users\u2019 behavioral patterns and preference information. However, due to the increasing number of E-commerce platforms and the growing volume of user interaction data, using GraphRAG technology to extract key subgraphs is crucial. Wang et al. [168] ensemble multiple retrievers under different types or with different parameters to extract relevant subgraphs, which are then encoded for temporal user action prediction. To improve the model performance of customer service question answering systems, Xu et al. [183] construct a past-issue graph with intra-issue and inter-issue relations. For each given query, subgraphs of similar past issues are retrieved to enhance the system\u2019s response quality.\n\n# 9.2.2 Biomedical\n\nRecently, GraphRAG techniques are increasingly applied in biomedical question answering systems, achieving advanced medical decision-making performance. In this area,\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3387, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0ce007b-2a53-4663-8539-1c004097a251": {"__data__": {"id_": "d0ce007b-2a53-4663-8539-1c004097a251", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f0e0e5a-4b83-4278-810c-c147423aa1c6", "node_type": "4", "metadata": {}, "hash": "666ff94a1df8b0e8d310d23176a9815b81469bc5452b2c035b11a249e9ce1b5f", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 9.2 Applications of GraphRAG\n\n# 9.2.3 Academic\n\nIn the academic research domain, each paper is authored by one or more researchers and is associated with a field of study. Authors are affiliated with institutions, and there exist relationships among authors, such as collaboration or shared institutional affiliations. These elements can be structured into a graph format. Utilizing GraphRAG on this graph can facilitate academic exploration, including predicting potential collaborators for an author, identifying trends within a specific field, etc.\n\n# 9.2.4 Literature\n\nSimilar to academic research, a knowledge graph can be constructed in the realm of literature, with nodes representing books, authors, publishers, and series, and edges labeled \u201cwritten-by\u201d, \u201cpublished-in\u201d, and \u201cbook-series\u201d. GraphRAG can be utilized to enhance realistic applications like smart libraries.\n\n# 9.2.5 Legal\n\nIn legal contexts, extensive citation connections exist between cases and judicial opinions, as judges frequently reference previous opinions when making new decisions. This naturally creates a structured graph where nodes represent opinions, opinion clusters, dockets, and courts, and edges encompass relationships such as \u201copinion-citation\u201d, \u201copinion-cluster\u201d, \u201ccluster-docket\u201d, and \u201cdocket-court\u201d. The application of GraphRAG in legal scenarios could aid lawyers and legal researchers in various tasks such as case analysis and legal consultation.\n\n# 9.2.6 Others\n\nIn addition to the above applications, GraphRAG is also applied to other real-world scenarios such as intelligence report generation [139], patent phrase similarity detection [133] and software understanding [1]. Ranade and Joshi [139] first construct an Event Plot Graph (EPG) and retrieve the critical aspects of the events to aid the generation of intelligence reports. Peng and Yang [133] create a patent-phrase graph and retrieve the ego network of the given patent phrase to assist the judgment of phrase similarity. Alhanahnah et al. [1] propose a Chatbot to understand properties about dependencies in a given software package, which first automatically constructs the dependency graph and then the user can ask questions about the dependencies in the dependency graph.\n\n# 9.3 Benchmarks and Metrics\n\n# 9.3.1 Benchmarks\n\nThe benchmarks used to evaluate the performance of the GraphRAG system can be divided into two categories. The first category is the corresponding datasets of downstream tasks. We summarize the benchmarks and papers tested with them according to the classification in Section 9.1, details of which are shown in Table 1. The second category consists of benchmarks specifically designed for the GraphRAG systems. These benchmarks usually cover multiple task domains to provide a comprehensive test result. For example, STARK [179] benchmarks LLM Retrieval on semi-structured knowledge bases covering three domains, including product search, academic paper search, and queries in precision medicine to access the capacity of current GraphRAG systems. He et al. [55] propose a flexible question-answering benchmark targeting real-world textual graphs, named GraphQA, which is applicable to multiple applications including scene graph understanding, commonsense reasoning, and knowledge graph reasoning. Graph Reasoning Benchmark (GRBENCH) [75] is constructed to facilitate the research of augmenting.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7bb57b4-6157-4d93-a1ac-12e9068f0479": {"__data__": {"id_": "a7bb57b4-6157-4d93-a1ac-12e9068f0479", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08b13d92-834f-4a87-99ba-afa1f83b90b1", "node_type": "4", "metadata": {}, "hash": "adc962d0910d4ecfe131532cfe33aeb59ca30ec142021696f404b7f37314da21", "class_name": "RelatedNodeInfo"}}, "text": "# 9.3.2 Metrics\n\nThe evaluation metrics for GraphRAG can be broadly categorized into two main types: downstream task evaluation (generation quality) and retrieval quality.\n\n# (1) Downstream Task Evaluation (Generation Quality)\n\nIn the majority of research studies, downstream task evaluation metrics serve as the primary method for assessing GraphRAG\u2019s performance. For example, in KBQA, Exact Match (EM) and F1 score are commonly used to measure the accuracy of answering entities. In addition, many researchers utilize BERT4Score and GPT4Score to mitigate instances where LLMs generate entities that are synonymous with the ground truth but not exact matches. In CSQA, Accuracy is the most commonly used evaluation metric. For generative tasks such as QA systems, metrics like BLEU, ROUGE-L, METEOR, and others are commonly employed to assess the quality of the text generated by the model.\n\n# (2) Retrieval Quality Evaluation\n\nWhile evaluating GraphRAG based on downstream task performance is feasible, directly measuring the accuracy of retrieved content poses challenges. Therefore, many studies employ specific metrics to gauge the precision of retrieved content. For instance, when ground truth entities are available, retrieval systems face a balance between the quantity of retrieved information and the coverage of answers. Hence, some studies utilize the ratio between answer coverage and the size of the retrieval subgraph to evaluate the performance of the retrieval system. In addition, several studies have explored metrics such as query relevance, diversity, and faithfulness score to respectively assess the similarity between retrieved content and queries, the diversity of retrieved content, and the faithfulness of the information retrieved.\n\n# 9.4 GraphRAG in Industry\n\nIn this section, we mainly focus on industrial GraphRAG systems. These systems are characterized by their reliance on industrial graph database systems or their focus on large-scale graph data, details of which are as follows:\n\n- GraphRAG (by Microsoft): The system uses LLMs to construct entity-based knowledge graphs and pre-generate community summaries of related entity groups, which enables the capture of both local and global relationships within a document collection, thereby enhancing Query-Focused Summarization (QFS) task. The project can also utilize open-source RAG toolkits for rapid implementation, such as LlamaIndex, LangChain, etc.\n- GraphRAG (by NebulaGraph): The project is the first industrial GraphRAG system, which is developed by NebulaGraph Corporation. The project integrates LLMs into the NebulaGraph database, which aims to deliver more intelligent and precise search results.\n- GraphRAG (by Antgroup): The framework is developed on the foundation of several AI engineering frameworks such as DB-GPT, knowledge graph engine OpenSPG, and graph database TuGraph. Specifically, the system begins by extracting triples from documents using LLMs, which are then stored in the graph database. During the retrieval phase, it identifies keywords from the query, locates corresponding nodes in the graph database, and traverses the subgraph using BFS.\n\n10 https://github.com/microsoft/graphrag\n\n11 https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html\n\n12 https://python.langchain.com/docs/use_cases/graph\n\n13 https://www.nebula-graph.io/posts/graph-RAG\n\n14 https://github.com/eosphoros-ai/DB-GPT\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19b1cb52-b163-415a-82da-818d8839c468": {"__data__": {"id_": "19b1cb52-b163-415a-82da-818d8839c468", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5086d743-c697-4cd5-b5ae-c4ba86a9087a", "node_type": "4", "metadata": {}, "hash": "0bc9e7ac1010959abd8cd7af1def9546ac82c2a52996256eb264dc3c6e8c79e4", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# 10 Future Prospects\n\nWhile GraphRAG technology has made substantial strides, it continues to face enduring challenges that demand comprehensive exploration. This section will delve into the prevalent obstacles and outline prospective avenues for future research in the field of GraphRAG.\n\n# 10.1 Dynamic and Adaptive Graphs\n\nMost GraphRAG methods [32, 41, 85, 86, 111, 188] are built upon static databases; however, as time progresses, new entities and relationships inevitably emerge [20, 44, 181]. Rapidly updating these changes is both promising and challenging. Incorporating updated information is crucial for achieving better results and addressing emerging trends that require current data. Developing efficient methods for dynamic updates and real-time integration of new data will significantly enhance the effectiveness and relevance of GraphRAG systems.\n\n# 10.2 Multi-Modality Information Integration\n\nMost knowledge graphs primarily encompass textual information, thereby lacking the inclusion of other modalities such as images, audio, and videos, which hold the potential to significantly enhance the overall quality and richness of the database [174]. The incorporation of these diverse modalities could provide a more comprehensive and nuanced understanding of the stored knowledge. However, the integration of such multi-modal data presents considerable challenges. As the volume of information increases, the graph\u2019s complexity and size grow exponentially, rendering it increasingly difficult to manage and maintain. This escalation in scale necessitates the development of advanced methodologies and sophisticated tools to efficiently handle and seamlessly integrate the diverse data types into the existing graph structure, ensuring both the accuracy and accessibility of the enriched knowledge graph.\n\n# 10.3 Scalable and Efficient Retrieval Mechanisms\n\nKnowledge graphs in the industrial setting may encompass millions or even billions of entities, representing a vast and intricate scale. However, most contemporary methods are tailored for small-scale knowledge graphs [32], which may only comprise thousands of entities. Efficiently and effectively retrieving pertinent entities within large-scale knowledge graphs remains a practical and significant challenge. Developing advanced retrieval algorithms and scalable infrastructure is essential to address this issue, ensuring that the system can manage the extensive data volume while maintaining high performance and accuracy in entity retrieval.\n\n15 https://github.com/neo4j/NaLLM\n\n16 https://github.com/neo4j-labs/llm-graph-builder", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81133736-a125-4eef-a80d-6b8701d87e11": {"__data__": {"id_": "81133736-a125-4eef-a80d-6b8701d87e11", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "227bf9fe-e7fb-4e34-a61c-43c4e48cbe74", "node_type": "4", "metadata": {}, "hash": "91447c3fd795955e967ed4b5d00ae902b979b2ab24f0fc2cea99b491e3c23449", "class_name": "RelatedNodeInfo"}}, "text": "# 10.4 Combination with Graph Foundation Model\n\nRecently, graph foundation models [42, 115], which can effectively address a wide range of graph tasks, have achieved significant success. Deploying these models to enhance the current GraphRAG pipeline is an essential problem. The input data for graph foundation models is inherently graph-structured, enabling them to handle such data more efficiently than LLM models. Integrating these advanced models into the GraphRAG framework could greatly improve the system\u2019s ability to process and utilize graph-structured information, thereby enhancing overall performance and capability.\n\n# 10.5 Lossless Compression of Retrieved Context\n\nIn GraphRAG, the retrieved information is organized into a graph structure containing entities and their interrelations. This information is then transformed into a sequence that can be understood by LLMs, resulting in a very long context. There are two issues with inputting such long contexts: LLMs cannot handle very long sequences, and extensive computation during inference can be a hindrance for individuals. To address these problems, lossless compression of long contexts is crucial. This approach removes redundant information and compresses lengthy sentences into shorter, yet meaningful ones. It helps LLMs capture the essential parts of the context and accelerates inference. However, designing a lossless compression technique is challenging. Current works [41, 86] make a trade-off between compression and preserving information. Developing an effective lossless compression technique is crucial but challenging for GraphRAG.\n\n# 10.6 Standard Benchmarks\n\nGraphRAG is a relatively new field that lacks unified and standard benchmarks for evaluating different methods. Establishing a standard benchmark is crucial for this area as it can provide a consistent framework for comparison, facilitate objective assessments of various approaches, and drive progress by identifying strengths and weaknesses. This benchmark should encompass diverse and representative datasets, well-defined evaluation metrics, and comprehensive test scenarios to ensure robust and meaningful evaluations of GraphRAG methods.\n\n# 10.7 Broader Applications\n\nCurrent GraphRAG applications primarily focus on common tasks such as customer service systems [183], recommendation systems [25], and KBQA [41]. Extending GraphRAG to broader applications such as healthcare [79], financial services [3], legal and compliance [81], smart cities and IoT [149], and more, involves incorporating more complex techniques. For instance, in healthcare, GraphRAG can support medical diagnosis, patient record analysis, and personalized treatment plans by integrating medical literature, patient histories, and real-time health data. In financial services, GraphRAG can be utilized for fraud detection, risk assessment, and personalized financial advice by analyzing transactional data, market trends, and customer profiles. Legal and compliance applications can benefit from GraphRAG by enabling comprehensive legal research, contract analysis, and regulatory compliance monitoring through the integration of legal documents, case law, and regulatory updates. Expanding GraphRAG to these diverse and complex domains will enhance its utility and impact, providing more sophisticated and targeted solutions across various fields.\n\n# 11 Conclusion\n\nIn summary, this survey offers a comprehensive retrospective of GraphRAG technology, systematically categorizing and organizing its fundamental techniques, training methodologies, and\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54eebd33-5aef-4bd8-8ac8-988ad0ae3d76": {"__data__": {"id_": "54eebd33-5aef-4bd8-8ac8-988ad0ae3d76", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5da12076-0526-4c4a-9ab1-ea170fb0ddbc", "node_type": "4", "metadata": {}, "hash": "6d226dca457c06b5684e4c856fc0e4d2923f348d95924571320a8f840d3ca35b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a267d57e-aec1-4575-908c-57be13d506f9", "node_type": "1", "metadata": {}, "hash": "dfff0ed000d4d4571335649d0bc1c27f72f45e4ef3bdd3790e3e6db45b3ba3bc", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\napplication scenarios. GraphRAG significantly enhances the relevance, accuracy, and comprehensiveness of information retrieval by leveraging pivotal relational knowledge derived from graph datasets, thereby addressing critical limitations associated with traditional Retrieval-Augmented Generation approaches. Furthermore, as GraphRAG represents a relatively nascent field of study, we delineate the benchmarks, analyze prevailing challenges, and illuminate prospective future research directions within this domain.\n\n# Acknowledgments\n\nThis work is supported by Ant Group through Ant Research Intern Program.\n\n# References\n\n1. Mohannad Alhanahnah, Yazan Boshmaf, and Benoit Baudry. 2024. DepsRAG: Towards Managing Software Dependencies using Large Language Models. arXiv:2405.20455 [cs.SE] https://arxiv.org/abs/2405.20455\n2. Zhiyu An, Xianzhong Ding, Yen-Chun Fu, Cheng-Chung Chu, Yan Li, and Wan Du. 2024. Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base. arXiv:2408.00798 [cs.IR] https://arxiv.org/abs/2408.00798\n3. Muhammad Arslan and Christophe Cruz. 2024. Business-RAG: Information Extraction for Business Insights. ICSBT 2024 (2024), 88.\n4. S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007. DBpedia: A Nucleus for a Web of Open Data. In The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007 (Lecture Notes in Computer Science, Vol. 4825). 722\u2013735.\n5. Jinheon Baek, Alham Fikri Aji, Jens Lehmann, and Sung Ju Hwang. 2023. Direct Fact Retrieval from Knowledge Graphs without Entity Linking. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. 10038\u201310055.\n6. Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. arXiv:2306.04136 [cs.CL] https://arxiv.org/abs/2306.04136\n7. Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, and Sung Ju Hwang. 2023. Knowledge-Augmented Language Model Verification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023. 1720\u20131736.\n8. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL. 1533\u20131544.\n9. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: Reasoning about Physical Commonsense in Natural Language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. 7432\u20137439.\n10. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. 1247\u20131250.\n11.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a267d57e-aec1-4575-908c-57be13d506f9": {"__data__": {"id_": "a267d57e-aec1-4575-908c-57be13d506f9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5da12076-0526-4c4a-9ab1-ea170fb0ddbc", "node_type": "4", "metadata": {}, "hash": "6d226dca457c06b5684e4c856fc0e4d2923f348d95924571320a8f840d3ca35b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54eebd33-5aef-4bd8-8ac8-988ad0ae3d76", "node_type": "1", "metadata": {}, "hash": "c5069176207b02cd5ee3e4d7cdbfb8353904ecdf83d15b321d02987b26f08eb8", "class_name": "RelatedNodeInfo"}}, "text": "2020. PIQA: Reasoning about Physical Commonsense in Natural Language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. 7432\u20137439.\n10. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. 1247\u20131250.\n11. Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008. 1247\u20131250.\n12. Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale Simple Question Answering with Memory Networks. arXiv:1506.02075 [cs.LG] https://arxiv.org/abs/1506.02075\n13. Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale Simple Question Answering with Memory Networks. arXiv:1506.02075 [cs.LG] https://arxiv.org/abs/1506.02075\n14. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901.\n15. Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an Architecture for Never-Ending Language Learning. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010. 1306\u20131313.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2859, "end_char_idx": 4887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65a8c4d7-a3c2-4286-9491-29fbbe1f6203": {"__data__": {"id_": "65a8c4d7-a3c2-4286-9491-29fbbe1f6203", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d916a01-f698-49bf-a6e0-71eb48ca45b8", "node_type": "4", "metadata": {}, "hash": "03306e49af8ca28d773ed276cf39dc56633a1b22b0cb0d8f5753013b879708db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a1a4304-3c29-40e6-880a-73aa2b340154", "node_type": "1", "metadata": {}, "hash": "b5e1f22c3829b2c4f0781c04895b8ac795cf027f632d0c6074410b00483c3963", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Abir Chakraborty. 2024. Multi-hop Question Answering over Knowledge Graphs using Large Language Models. arXiv:2404.19234 [cs.AI] https://arxiv.org/abs/2404.19234\n2. Huajun Chen. 2024. Large Knowledge Model: Perspectives and Challenges. arXiv:2312.02706 [cs.AI] https://arxiv.org/abs/2312.02706\n3. Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. 2024. LLaGA: Large Language and Graph Assistant. arXiv:2402.08170 [cs.LG] https://arxiv.org/abs/2402.08170\n4. Shuang Chen, Qian Liu, Zhiwei Yu, Chin-Yew Lin, Jian-Guang Lou, and Feng Jiang. 2021. ReTraCk: A flexible and efficient framework for knowledge base question answering. In Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing: system demonstrations. 325\u2013336.\n5. Keyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan Zhai, Lu Yu, Muhammad Asif Ali, Lijie Hu, and Di Wang. 2024. Multi-hop Question Answering under Temporal Knowledge Editing. arXiv:2404.00492 [cs.CL] https://arxiv.org/abs/2404.00492\n6. Hyeong Kyu Choi, Seunghun Lee, Jaewon Chu, and Hyunwoo J. Kim. 2023. NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\n7. Nurendra Choudhary and Chandan K. Reddy. 2024. Complex Logical Reasoning over Knowledge Graphs using Large Language Models. arXiv:2305.01157 [cs.LO] https://arxiv.org/abs/2305.01157\n8. Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, and Andrew McCallum. 2018. Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.\n9. Mohammad Dehghan, Mohammad Ali Alomrani, Sunyam Bagga, David Alfonso-Hermelo, Khalil Bibi, Abbas Ghaddar, Yingxue Zhang, Xiaoguang Li, Jianye Hao, Qun Liu, Jimmy Lin, Boxing Chen, Prasanna Parthasarathi, Mahdi Biparva, and Mehdi Rezagholizadeh. 2024. EWEK-QA: Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024. 14169\u201314187.\n10. Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, Ren\u00e9 Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, and Silvia Milano. 2024. A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys). arXiv:2404.00579 [cs.IR] https://arxiv.org/abs/2404.00579\n11. Julien Delile, Srayanta Mukherjee, Anton Van Pamel, and Leonid Zhukov. 2024. Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge. arXiv:2402.12352 [cs.CL] https://arxiv.org/abs/2402.12352\n12. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a1a4304-3c29-40e6-880a-73aa2b340154": {"__data__": {"id_": "3a1a4304-3c29-40e6-880a-73aa2b340154", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d916a01-f698-49bf-a6e0-71eb48ca45b8", "node_type": "4", "metadata": {}, "hash": "03306e49af8ca28d773ed276cf39dc56633a1b22b0cb0d8f5753013b879708db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65a8c4d7-a3c2-4286-9491-29fbbe1f6203", "node_type": "1", "metadata": {}, "hash": "9a38431536cf6927d524eadddfd9b4b8404840a9802d41d1836523a0fa4c03aa", "class_name": "RelatedNodeInfo"}}, "text": "2024. A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys). arXiv:2404.00579 [cs.IR] https://arxiv.org/abs/2404.00579\n11. Julien Delile, Srayanta Mukherjee, Anton Van Pamel, and Leonid Zhukov. 2024. Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge. arXiv:2402.12352 [cs.CL] https://arxiv.org/abs/2402.12352\n12. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2D Knowledge Graph Embeddings. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018. 1811\u20131818.\n13. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 4171\u20134186.\n14. Guanting Dong, Rumei Li, Sirui Wang, Yupeng Zhang, Yunsen Xian, and Weiran Xu. 2023. Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023. 3854\u20133859.\n15. Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, and Zhimeng Jiang. 2023. Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023. ACM, 2519\u20132527.\n16. Abhimanyu Dubey, Abhinav Jauhri, and et al. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783\n17. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From Local to Global: A Graph RAG Approach to Query-Focused Summarization. arXiv:2404.16130 [cs.CL] https://arxiv.org/abs/2404.16130\n18. Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Fr\u00e9d\u00e9rique Laforest, and Elena Simperl. 2018. T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2701, "end_char_idx": 5286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c1c924b-2582-40b6-86b9-3301725d8a2c": {"__data__": {"id_": "7c1c924b-2582-40b6-86b9-3301725d8a2c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58d5eaa9-1676-4862-b0ad-d2b8a38fc48c", "node_type": "4", "metadata": {}, "hash": "c19d1ec6f8ac2c85237ea46a4e311752f0c3dfbc4a7dae7bb88cfcbd57307fe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1515b072-5689-48f8-9cfc-a4f341a0b9ac", "node_type": "1", "metadata": {}, "hash": "dc2dd3a225a87e6e47257050effdf105e731ce5b98557a9719c297b14d4bc43f", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# References\n\n1. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models. arXiv:2405.06211 [cs.CL]\n2. Wenqi Fan, Shijie Wang, Jiani Huang, Zhikai Chen, Yu Song, Wenzhuo Tang, Haitao Mao, Hui Liu, Xiaorui Liu, Dawei Yin, and Qing Li. 2024. Graph Machine Learning in the Era of Large Language Models (LLMs). arXiv:2404.14928 [cs.LG]\n3. Haishuo Fang, Xiaodan Zhu, and Iryna Gurevych. 2024. DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs. arXiv:2406.07080 [cs.CL]\n4. Jinyuan Fang, Zaiqiao Meng, and Craig MacDonald. 2024. REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024. 2094\u20132112.\n5. Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2023. Talk like a Graph: Encoding Graphs for Large Language Models. arXiv:2310.04560 [cs.LG]\n6. Chao Feng, Xinyu Zhang, and Zichu Fei. 2023. Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs. arXiv:2309.03118 [cs.CL]\n7. Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. 2020. Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020. 1295\u20131309.\n8. Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, and Jian Sun. 2020. A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges. arXiv:2007.13069 [cs.CL]\n9. Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. 2023. Towards Foundation Models for Knowledge Graph Reasoning. In The Twelfth International Conference on Learning Representations.\n10. Hanning Gao, Lingfei Wu, Po Hu, Zhihua Wei, Fangli Xu, and Bo Long. 2022. Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022 - Volume 1: Long Papers, Online Only, November 20-23, 2022. 82\u201392.\n11. Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, and Dongsheng Li. 2024. Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models. arXiv:2402.16568 [cs.CL]\n12. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL]\n13. Aashish Ghimire, James Prather, and John Edwards. 2024. Generative AI in Education: A Study of Educators\u2019 Awareness, Sentiments, and Influencing Factors. arXiv:2403.15586 [cs.AI]\n14. Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1515b072-5689-48f8-9cfc-a4f341a0b9ac": {"__data__": {"id_": "1515b072-5689-48f8-9cfc-a4f341a0b9ac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58d5eaa9-1676-4862-b0ad-d2b8a38fc48c", "node_type": "4", "metadata": {}, "hash": "c19d1ec6f8ac2c85237ea46a4e311752f0c3dfbc4a7dae7bb88cfcbd57307fe6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c1c924b-2582-40b6-86b9-3301725d8a2c", "node_type": "1", "metadata": {}, "hash": "e266cbfcdf5057b3f1e5ccfb39e066a71986abc5812132bcd7b39e7ef93c99ce", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:2402.16568 [cs.CL]\n12. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL]\n13. Aashish Ghimire, James Prather, and John Edwards. 2024. Generative AI in Education: A Study of Educators\u2019 Awareness, Sentiments, and Influencing Factors. arXiv:2403.15586 [cs.AI]\n14. Yu Gu, Sue Kase, Michelle Vanni, Brian M. Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021. Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases. In WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. 3477\u20133488.\n15. Yu Gu and Yu Su. 2022. ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering. In Proceedings of the 29th International Conference on Computational Linguistics. 1718\u20131731.\n16. Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, and Shi Han. 2023. GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking. arXiv:2305.15066 [cs.AI]\n17. Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li, Jiawei Tang, Dapeng Li, and Yingyou Wen. 2024. KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph. arXiv:2312.15880 [cs.CL]\n18. Bernal Jim\u00e9nez Guti\u00e9rrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. arXiv:2405.14831 [cs.CL]\n19. William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. 1024\u20131034.\n20. Zhen Han, Yue Feng, and Mingming Sun. 2023. A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering. arXiv:2303.10395 [cs.CL]\n21. Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals. In WSDM \u201921, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021. 553\u2013561.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2667, "end_char_idx": 5076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bc1cbfd-4621-49d8-8faf-31c43a6702c0": {"__data__": {"id_": "7bc1cbfd-4621-49d8-8faf-31c43a6702c0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01c35645-577b-472a-88c7-f1b65921fa4f", "node_type": "4", "metadata": {}, "hash": "85334645fbff86aa086fc98606503957fa21ba31638f3ac4005941e0e1fba0f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43a7fa61-a7a4-44b7-9e37-54038a7c7776", "node_type": "1", "metadata": {}, "hash": "2a19dfed6b756193b03c785489f6062e327341ff9fd05b0f94daf1d35791ab1c", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi.\n2024. G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering.\narXiv:2402.07630 [cs.LG] https://arxiv.org/abs/2402.07630\n2. Michael Himsolt. 1996. GML: Graph Modelling Language. University of Passau (1996).\n3. Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F\u00fcrstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum.\n2011. Robust Disambiguation of Named Entities in Text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL. 782\u2013792.\n4. Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao.\n2024. GRAG: Graph Retrieval-Augmented Generation. arXiv:2405.16506 [cs.LG] https://arxiv.org/abs/2405.16506\n5. Yucheng Hu and Yuxing Lu.\n2024. RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing.\narXiv:2404.19543 [cs.CL] https://arxiv.org/abs/2404.19543\n6. Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Kai-Wei Chang, and Yizhou Sun.\n2022. Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. 9562\u20139581.\n7. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu.\n2023. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions.\narXiv:2311.05232 [cs.CL] https://arxiv.org/abs/2311.05232\n8. Yizheng Huang and Jimmy Huang.\n2024. A Survey on Retrieval-Augmented Text Generation for Large Language Models.\narXiv:2404.10981 [cs.IR] https://arxiv.org/abs/2404.10981\n9. Yongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang, Ruyi Gan, Jiaxing Zhang, and Liwei Wang.\n2023. MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. 13417\u201313432.\n10. Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi.\n2021. (Comet-) Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs.\nIn Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. 6384\u20136392.\n11. Gautier Izacard and Edouard Grave.\n2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.\nIn Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021. 874\u2013880.\n12.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43a7fa61-a7a4-44b7-9e37-54038a7c7776": {"__data__": {"id_": "43a7fa61-a7a4-44b7-9e37-54038a7c7776", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01c35645-577b-472a-88c7-f1b65921fa4f", "node_type": "4", "metadata": {}, "hash": "85334645fbff86aa086fc98606503957fa21ba31638f3ac4005941e0e1fba0f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bc1cbfd-4621-49d8-8faf-31c43a6702c0", "node_type": "1", "metadata": {}, "hash": "de9d06839d80c8abb9773d69f7a5cb69146db8da5be7a45af60c7cefbd38b034", "class_name": "RelatedNodeInfo"}}, "text": "2021. (Comet-) Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs.\nIn Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. 6384\u20136392.\n11. Gautier Izacard and Edouard Grave.\n2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.\nIn Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021. 874\u2013880.\n12. Omid Jafari, Preeti Maurya, Parth Nagarkar, Khandker Mushfiqul Islam, and Chidambaram Crushev.\n2021. A Survey on Locality Sensitive Hashing Algorithms and their Applications.\narXiv:2102.08942 [cs.DB] https://arxiv.org/abs/2102.08942\n13. Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen.\n2023. StructGPT: A General Framework for Large Language Model to Reason over Structured Data.\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023. 9237\u20139251.\n14. Jinhao Jiang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao.\n2022. $Great Truths are Always Simple: $ A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models.\nIn Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022. 1730\u20131741.\n15. Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and Ji-Rong Wen.\n2024. KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph.\narXiv:2402.11163 [cs.CL] https://arxiv.org/abs/2402.11163\n16. Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen.\n2023. UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph.\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\n17. Jinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen.\n2023. UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph.\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\n18. Kelvin Jiang, Dekun Wu, and Hui Jiang.\n2019. FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 318\u2013323.\n19. Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding, Xu Chu, Junfeng Zhao, and Yasha Wang.\n2024. HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2543, "end_char_idx": 5604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2ce38a1-a059-4439-b376-0d944b526178": {"__data__": {"id_": "d2ce38a1-a059-4439-b376-0d944b526178", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3910924b-4439-4dda-993e-1ce4e29bada4", "node_type": "4", "metadata": {}, "hash": "f45c3f1788e5664f05ab076cb7d68ea767b4721360600250aca0231b6b356660", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a5b1c88-f6a3-4938-b26d-83a7660cfbe9", "node_type": "1", "metadata": {}, "hash": "0fd56fc09b39a028cfab7986b55fe1fc830eac06a855c44bae5f9f48995a2f8d", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# References\n\n1. Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2024. Large Language Models on Graphs: A Comprehensive Survey. arXiv:2312.02783 [cs.CL]\n2. Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, and Jiawei Han. 2024. Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs. arXiv:2404.07103 [cs.CL]\n3. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2020. What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams. arXiv:2009.13081 [cs.CL]\n4. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers. 1601\u20131611.\n5. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020. 6769\u20136781.\n6. Sohum Kashyap et al. 2024. Knowledge Graph Assisted Large Language Models. (2024).\n7. Jiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi. 2023. KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023. 9410\u20139421.\n8. Jaewoong Kim and Moohong Min. 2024. From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process. arXiv:2402.01717 [cs.CL]\n9. Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, and Edward Choi. 2023. FactKG: Fact Verification via Reasoning on Knowledge Graphs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. 16190\u201316206.\n10. Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\n11. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a Benchmark for Question Answering Research. Trans. Assoc. Comput. Linguistics 7 (2019), 452\u2013466.\n12. Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021. 4483\u20134491.\n13.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a5b1c88-f6a3-4938-b26d-83a7660cfbe9": {"__data__": {"id_": "0a5b1c88-f6a3-4938-b26d-83a7660cfbe9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3910924b-4439-4dda-993e-1ce4e29bada4", "node_type": "4", "metadata": {}, "hash": "f45c3f1788e5664f05ab076cb7d68ea767b4721360600250aca0231b6b356660", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2ce38a1-a059-4439-b376-0d944b526178", "node_type": "1", "metadata": {}, "hash": "aa2ebe2f5eb2e2558228410d9d0b9b5580e0549f6092ceda31d019dd3467f194", "class_name": "RelatedNodeInfo"}}, "text": "2019. Natural Questions: a Benchmark for Question Answering Research. Trans. Assoc. Comput. Linguistics 7 (2019), 452\u2013466.\n12. Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021. 4483\u20134491.\n13. Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Complex Knowledge Base Question Answering: A Survey. IEEE Trans. Knowl. Data Eng. 35, 11 (2023), 11196\u201311215.\n14. Yunshi Lan and Jing Jiang. 2020. Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. 969\u2013974.\n15. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021. 3045\u20133059.\n16. Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, and Tianlong Chen. 2024. DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer\u2019s Disease Questions with Scientific Literature. arXiv:2405.04819 [cs.CL]\n17. Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, and Bing Yin. 2023. Graph Reasoning for Question Answering with Triplet Retrieval. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023. 3366\u20133375.\n18. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021. 4582\u20134597.\n19. Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. 2024. A Survey of Graph Meets Large Language Model: Progress and Future Directions. arXiv:2311.12399 [cs.LG]\n20. Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2024. Large Language Models in Finance: A Survey. arXiv:2311.10723 [q-fin.GN]\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2750, "end_char_idx": 5337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33f1e18d-93eb-4ec2-b124-f8bb210de54b": {"__data__": {"id_": "33f1e18d-93eb-4ec2-b124-f8bb210de54b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d060d58d-e467-46a0-923f-0b499427aab0", "node_type": "4", "metadata": {}, "hash": "3e563a289dd61de2b44f297d0e63addd6dff51ab871d349d8f4759edc86d85eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "055c34f5-6bbc-43cf-b0e8-b633f0ab0e4d", "node_type": "1", "metadata": {}, "hash": "159a773a8124a5b593f639c2f985a5e0ae7a8818a15a513959822a805fb56051", "class_name": "RelatedNodeInfo"}}, "text": "# Peng et al.\n\n# References\n\n1. Yihao Li, Ru Zhang, and Jianyi Liu. 2024. An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration. arXiv:2402.04978 [cs.CL]\n2. Zhuoyang Li, Liran Deng, Hui Liu, Qiaoqiao Liu, and Junzhao Du. 2024. UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models. arXiv:2406.02110 [cs.CL]\n3. Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, and Rui Wang. 2024. Graph Neural Network Enhanced Retrieval for Question Answering of LLMs. arXiv:2406.06572 [cs.CL]\n4. Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. 2829\u20132839.\n5. Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. 2021. RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021). 1504\u20131515.\n6. Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. 2024. Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over Knowledge Graph. arXiv:2406.01145 [cs.CL]\n7. H Liu and P Singh. 2004. ConceptNet\u2014a practical commonsense reasoning tool-kit. BT technology journal 22, 4 (2004), 211\u2013226.\n8. Haochen Liu, Song Wang, Yaochen Zhu, Yushun Dong, and Jundong Li. 2024. Knowledge Graph-Enhanced Large Language Models via Path Selection. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024. 6311\u20136321.\n9. Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi. 2024. Towards Graph Foundation Models: A Survey and Beyond. arXiv:2310.11829 [cs.LG]\n10. Lei Liu, Xiaoyan Yang, Junchi Lei, Xiaoyang Liu, Yue Shen, Zhiqiang Zhang, Peng Wei, Jinjie Gu, Zhixuan Chu, Zhan Qin, and Kui Ren. 2024. A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions. arXiv:2406.03712 [cs.CL]\n11. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contexts. Trans. Assoc. Comput. Linguistics 12 (2024), 157\u2013173.\n12. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 61\u201368.\n13. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. GPT Understands, Too. arXiv:2103.10385 [cs.CL]\n14.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "055c34f5-6bbc-43cf-b0e8-b633f0ab0e4d": {"__data__": {"id_": "055c34f5-6bbc-43cf-b0e8-b633f0ab0e4d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d060d58d-e467-46a0-923f-0b499427aab0", "node_type": "4", "metadata": {}, "hash": "3e563a289dd61de2b44f297d0e63addd6dff51ab871d349d8f4759edc86d85eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33f1e18d-93eb-4ec2-b124-f8bb210de54b", "node_type": "1", "metadata": {}, "hash": "18cec91ca59afbe957046b81a94bb7e6fbeb3d5ecaf8e66089ba3c74f8674be4", "class_name": "RelatedNodeInfo"}}, "text": "2024. Lost in the Middle: How Language Models Use Long Contexts. Trans. Assoc. Comput. Linguistics 12 (2024), 157\u2013173.\n12. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 61\u201368.\n13. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. GPT Understands, Too. arXiv:2103.10385 [cs.CL]\n14. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n15. Pei-Chi Lo and Ee-Peng Lim. 2023. Contextual Path Retrieval: A Contextual Entity Relation Embedding-based Approach. ACM Trans. Inf. Syst. 41, 1 (2023), 1:1\u20131:38.\n16. Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-Shot Entity Linking by Reading Entity Descriptions. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers. 3449\u20133460.\n17. Dan Luo, Jiawei Sheng, Hongbo Xu, Lihong Wang, and Bin Wang. 2023. Improving Complex Knowledge Base Question Answering with Relation-Aware Subgraph Retrieval and Reasoning Network. In International Joint Conference on Neural Networks, IJCNN 2023, Gold Coast, Australia, June 18-23, 2023. 1\u20138.\n18. Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, and Luu Anh Tuan. 2024. ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. arXiv:2310.08975 [cs.CL]\n19. Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2024. Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. arXiv:2310.01061 [cs.CL]\n20. Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, and Jian Guo. 2024. Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval. arXiv:2407.10805 [cs.CL]\n21. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting for Retrieval-Augmented Large Language Models. arXiv:2305.14283 [cs.CL]\n22. Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Mikhail Galkin, and Jiliang Tang. 2024. Position: Graph Foundation Models Are Already Here. In Forty-first International Conference on Machine Learning.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2479, "end_char_idx": 5253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "741fc647-3b0e-43db-bc96-9c82f210cdca": {"__data__": {"id_": "741fc647-3b0e-43db-bc96-9c82f210cdca", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33068781-96be-4704-87a7-3d869e815ea6", "node_type": "4", "metadata": {}, "hash": "ed9cc09fa3808ccdd7160645ebd75aaa016c158d235c925a4021aba78fe38118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d250dfc-909c-45e8-ba68-3ec2e4065a00", "node_type": "1", "metadata": {}, "hash": "4b9a37ea523f6590102ee6aea93ec6ffce44bb88b0ed1a11b3ba12753ff1f772", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# References\n\n1. [116] Qiheng Mao, Zemin Liu, Chenghao Liu, Zhuo Li, and Jianling Sun. 2024. Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques. arXiv:2402.05952 [cs.LG] https://arxiv.org/abs/2402.05952\n2. [117] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024. RaFe: Ranking Feedback Improves Query Rewriting for RAG. arXiv:2405.14431 [cs.CL] https://arxiv.org/abs/2405.14431\n3. [118] Costas Mavromatis and George Karypis. 2022. ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. 2447\u20132458.\n4. [119] Costas Mavromatis and George Karypis. 2024. GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning. arXiv:2405.20139 [cs.CL] https://arxiv.org/abs/2405.20139\n5. [120] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018. 2381\u20132391.\n6. [121] Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-Value Memory Networks for Directly Reading Documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016. 1400\u20131409.\n7. [122] Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019. OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers. 845\u2013854.\n8. [123] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. 2020. TU-Dataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020).\n9. [124] Sai Munikoti, Anurag Acharya, Sridevi Wagle, and Sameera Horawalavithana. 2023. ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science. arXiv:2311.12289 [cs.CL] https://arxiv.org/abs/2311.12289\n10. [125] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Vincent Poor, Qingsong Wen, and Stefan Zohren. 2024. A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. arXiv:2406.11903 [q-fin.GN] https://arxiv.org/abs/2406.11903\n11. [126] Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\n12. [127] OpenAI. 2024. GPT-4 Technical Report.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d250dfc-909c-45e8-ba68-3ec2e4065a00": {"__data__": {"id_": "6d250dfc-909c-45e8-ba68-3ec2e4065a00", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33068781-96be-4704-87a7-3d869e815ea6", "node_type": "4", "metadata": {}, "hash": "ed9cc09fa3808ccdd7160645ebd75aaa016c158d235c925a4021aba78fe38118", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "741fc647-3b0e-43db-bc96-9c82f210cdca", "node_type": "1", "metadata": {}, "hash": "040126f552aa07b544dfdc0b4c7a9d09ee1b5ecce5f36dbede8bab0d2ef60ca5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d0758b1-6d69-4c17-a35b-1d9d4d7115d2", "node_type": "1", "metadata": {}, "hash": "a55ea42931caf115120f09c76fb27c0ded0cf549768c160da2769ed63a193151", "class_name": "RelatedNodeInfo"}}, "text": "[125] Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M. Mulvey, H. Vincent Poor, Qingsong Wen, and Stefan Zohren. 2024. A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. arXiv:2406.11903 [q-fin.GN] https://arxiv.org/abs/2406.11903\n11. [126] Yasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\n12. [127] OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774\n13. [128] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730\u201327744.\n14. [129] Vardaan Pahuja, Boshi Wang, Hugo Latapie, Jayanth Srinivasa, and Yu Su. 2023. A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023. 1992\u20132002.\n15. [130] Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, Russa Biswas, Gerard de Melo, Angela Bonifati, Edlira Vakaj, Mauro Dragoni, and Damien Graux. 2023. Large Language Models and Knowledge Graphs: Opportunities and Challenges. TGDK 1, 1 (2023), 2:1\u20132:38.\n16. [131] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024. Unifying Large Language Models and Knowledge Graphs: A Roadmap. IEEE Trans. Knowl. Data Eng. 36, 7 (2024), 3580\u20133599.\n17. [132] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. 2024. Large Language Model based Long-tail Query Rewriting in Taobao Search. In Companion Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, Singapore, May 13-17, 2024. 20\u201328.\n18. [133] Zhuoyi Peng and Yi Yang. 2024. Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs. arXiv:2403.16265 [cs.CL] https://arxiv.org/abs/2403.16265\n19. [134] Aleksandr Perevalov, Dennis Diefenbach, Ricardo Usbeck, and Andreas Both. 2022. QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers. In 16th IEEE International Conference on Semantic Computing, ICSC 2022, Laguna Hills, CA, USA, January 26-28, 2022. 229\u2013234.\n20. [135] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North\n\nJ. ACM, Vol. 37, No. 4, Article 111.", "mimetype": "text/plain", "start_char_idx": 2510, "end_char_idx": 5623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d0758b1-6d69-4c17-a35b-1d9d4d7115d2": {"__data__": {"id_": "3d0758b1-6d69-4c17-a35b-1d9d4d7115d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33068781-96be-4704-87a7-3d869e815ea6", "node_type": "4", "metadata": {}, "hash": "ed9cc09fa3808ccdd7160645ebd75aaa016c158d235c925a4021aba78fe38118", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d250dfc-909c-45e8-ba68-3ec2e4065a00", "node_type": "1", "metadata": {}, "hash": "4b9a37ea523f6590102ee6aea93ec6ffce44bb88b0ed1a11b3ba12753ff1f772", "class_name": "RelatedNodeInfo"}}, "text": "QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers. In 16th IEEE International Conference on Semantic Computing, ICSC 2022, Laguna Hills, CA, USA, January 26-28, 2022. 229\u2013234.\n20. [135] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 4998, "end_char_idx": 5657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e47ba44a-ae8c-4883-bff9-fc5d71ceb44e": {"__data__": {"id_": "e47ba44a-ae8c-4883-bff9-fc5d71ceb44e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca7dbfd1-126d-400b-a8fb-91cfb991a071", "node_type": "4", "metadata": {}, "hash": "696289d607027c6e90a94eca1ea76db0300131c0ebff3f54106c27605a2ef85a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bf77513-49e7-4389-918e-15b8f27ba614", "node_type": "1", "metadata": {}, "hash": "66bb18a4e97649c4eb06e48d6d1aed0d3e563f5468c25a2ed80c2e9e709d832e", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Peng et al. American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. 2523\u20132544.\n2. Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt. arXiv:2308.10173 [cs.CL] https://arxiv.org/abs/2308.10173\n3. Zile Qiao, Wei Ye, Yong Jiang, Tong Mo, Pengjun Xie, Weiping Li, Fei Huang, and Shikun Zhang. 2024. Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling. arXiv:2406.08116 [cs.CL] https://arxiv.org/abs/2406.08116\n4. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21 (2020), 140:1\u2013140:67.\n5. Priyanka Ranade and Anupam Joshi. 2023. FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction. In Proceedings of the International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2023, Kusadasi, Turkey, November 6-9, 2023. 603\u2013610.\n6. Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. 3980\u20133990.\n7. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n8. Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019. 3027\u20133035.\n9. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. SocialIQA: Commonsense Reasoning about Social Interactions. arXiv:1904.09728 [cs.CL] https://arxiv.org/abs/1904.09728\n10. Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. 2024. HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction. arXiv:2408.04948 [cs.CL] https://arxiv.org/abs/2408.04948\n11. Apoorv Saxena, Aditay Tripathi, and Partha P. Talukdar. 2020. Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. 4498\u20134507.\n12. Priyanka Sen, Alham Fikri Aji, and Amir Saffari. 2022. Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bf77513-49e7-4389-918e-15b8f27ba614": {"__data__": {"id_": "7bf77513-49e7-4389-918e-15b8f27ba614", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca7dbfd1-126d-400b-a8fb-91cfb991a071", "node_type": "4", "metadata": {}, "hash": "696289d607027c6e90a94eca1ea76db0300131c0ebff3f54106c27605a2ef85a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e47ba44a-ae8c-4883-bff9-fc5d71ceb44e", "node_type": "1", "metadata": {}, "hash": "069b8c1cf6368ac46d7a87c49f40d27245e9882dc1a07397566a136d794f4b2a", "class_name": "RelatedNodeInfo"}}, "text": "2024. HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction. arXiv:2408.04948 [cs.CL] https://arxiv.org/abs/2408.04948\n11. Apoorv Saxena, Aditay Tripathi, and Partha P. Talukdar. 2020. Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. 4498\u20134507.\n12. Priyanka Sen, Alham Fikri Aji, and Amir Saffari. 2022. Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering. In Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022. 1604\u20131619.\n13. Ahsan Shehzad, Feng Xia, Shagufta Abid, Ciyuan Peng, Shuo Yu, Dongyu Zhang, and Karin Verspoor. 2024. Graph Transformers: A Survey. arXiv:2407.09777 [cs.LG] https://arxiv.org/abs/2407.09777\n14. Yiheng Shu, Zhiwei Yu, Yuhan Li, B\u00f6rje F. Karlsson, Tingting Ma, Yuzhong Qu, and Chin-Yew Lin. 2022. TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Bases. arXiv:2210.12925 [cs.CL] https://arxiv.org/abs/2210.12925\n15. Saurabh Srivastava, Milind D Jain, Harshita Jain, Kritik Jaroli, VJ Mayank Patel, and L Khan. 2020. IOT monitoring bin for smart cities. In 3rd Smart Cities Symposium (SCS 2020), Vol. 2020. IET, 533\u2013536.\n16. Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web. 697\u2013706.\n17. Haitian Sun, Tania Bedrax-Weiss, and William W. Cohen. 2019. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. 2380\u20132390.\n18. Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W. Cohen. 2018. Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018. 4231\u20134242.\n19. Hao Sun, Yang Li, Liwei Deng, Bowen Li, Binyuan Hui, Binhua Li, Yunshi Lan, Yan Zhang, and Yongbin Li. 2023. History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. 3521\u20133533.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2644, "end_char_idx": 5448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60388c28-7e4e-4aef-beaf-73fe5972f207": {"__data__": {"id_": "60388c28-7e4e-4aef-beaf-73fe5972f207", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8c51a62-1749-4fbb-b7f7-aadfbf9de368", "node_type": "4", "metadata": {}, "hash": "37b5757256a7e9862cf2bed0a0d0711f765247787231ea141dfe41c54eac4bde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8369aa2-87b4-4d80-8281-4f949f203b3a", "node_type": "1", "metadata": {}, "hash": "c881090d48b1e71f8566a940efc875d1b2e53740c548e5807a17ca52521c1d29", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# References\n\n1. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2024. Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. arXiv:2307.07697 [cs.CL] https://arxiv.org/abs/2307.07697\n2. Lei Sun, Zhengwei Tao, Youdi Li, and Hiroshi Arakawa. 2024. ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs. arXiv:2404.07677 [cs.CL] https://arxiv.org/abs/2404.07677\n3. Alon Talmor and Jonathan Berant. 2018. The Web as a Knowledge-Base for Answering Complex Questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers). 641\u2013651.\n4. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4149\u20134158.\n5. Dhaval Taunk, Lakshya Khanna, Siri Venkata Pavan Kumar Kandru, Vasudeva Varma, Charu Sharma, and Makarand Tapaswi. 2023. GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering. In Companion Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023. 1138\u20131144.\n6. Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. 2015. Representing Text for Joint Embedding of Text and Knowledge Bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. 1499\u20131509.\n7. Hugo Touvron, Louis Martin, and et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL] https://arxiv.org/abs/2307.09288\n8. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. 5998\u20136008.\n9. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2018. Graph Attention Networks. arXiv:1710.10903 [stat.ML] https://arxiv.org/abs/1710.10903\n10. Denny Vrande\u010di\u0107 and Markus Kr\u00f6tzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM 57, 10 (2014), 78\u201385.\n11. Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen, Xinrun Wang, Lei Feng, and Bo An. 2023. keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM. arXiv:2401.00426 [cs.CL] https://arxiv.org/abs/2401.00426\n12. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8369aa2-87b4-4d80-8281-4f949f203b3a": {"__data__": {"id_": "f8369aa2-87b4-4d80-8281-4f949f203b3a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8c51a62-1749-4fbb-b7f7-aadfbf9de368", "node_type": "4", "metadata": {}, "hash": "37b5757256a7e9862cf2bed0a0d0711f765247787231ea141dfe41c54eac4bde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60388c28-7e4e-4aef-beaf-73fe5972f207", "node_type": "1", "metadata": {}, "hash": "ccf1e003d1376e220d89d730b162bd23dfcd8aa39464dcfa66ea9d0eb8859c3f", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:1710.10903 [stat.ML] https://arxiv.org/abs/1710.10903\n10. Denny Vrande\u010di\u0107 and Markus Kr\u00f6tzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM 57, 10 (2014), 78\u201385.\n11. Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen, Xinrun Wang, Lei Feng, and Bo An. 2023. keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM. arXiv:2401.00426 [cs.CL] https://arxiv.org/abs/2401.00426\n12. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. 2023. Can Language Models Solve Graph Problems in Natural Language?. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\n13. Jinqiang Wang, Huansheng Ning, Yi Peng, Qikai Wei, Daniel Tesfai, Wenwei Mao, Tao Zhu, and Runhe Huang. 2024. A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations. arXiv:2406.10303 [cs.CL] https://arxiv.org/abs/2406.10303\n14. Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong, and Zhang Xiong. 2023. Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. arXiv:2308.13259 [cs.CL] https://arxiv.org/abs/2308.13259\n15. Ruijie Wang, Zheng Li, Danqing Zhang, Qingyu Yin, Tong Zhao, Bing Yin, and Tarek F. Abdelzaher. 2022. RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph. In WWW \u201922: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. 462\u2013472.\n16. Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, and Qingsong Wen. 2024. Large Language Models for Education: A Survey and Outlook. arXiv:2403.18105 [cs.CL] https://arxiv.org/abs/2403.18105\n17. Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. 2023. KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. arXiv:2308.11761 [cs.CL] https://arxiv.org/abs/2308.11761\n18. Yuqi Wang, Boran Jiang, Yi Luo, Dawei He, Peng Cheng, and Liangcai Gao. 2024. Reasoning on Efficient Knowledge Paths: Knowledge Graph Guides Large Language Model for Domain Question Answering. arXiv:2404.10384 [cs.CL] https://arxiv.org/abs/2404.10384\n19. Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa F. Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge Graph Prompting for Multi-Document Question Answering. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada. 19206\u201319214.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2567, "end_char_idx": 5509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcb41f78-4de8-4abd-971e-515670f27517": {"__data__": {"id_": "bcb41f78-4de8-4abd-971e-515670f27517", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f328036-899e-4b64-8151-1a54855d4edc", "node_type": "4", "metadata": {}, "hash": "20fd129abb4b02269b42b34a9c6cb53b83af4d312d4b612ec329c9eb4c1dda92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6db06705-b622-43dd-967c-4f9a181122ed", "node_type": "1", "metadata": {}, "hash": "2e60dac77fa4fe56fd5cc765e36c37791d1741529f4959590eac379a15f18827", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Yaoke Wang, Yun Zhu, Wenqiao Zhang, Yueting Zhuang, Yunfei Li, and Siliang Tang. 2024. Bridging Local Details and Global Context in Text-Attributed Graphs. arXiv:2406.12608 [cs.CL] https://arxiv.org/abs/2406.12608\n2. Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM international conference on multimedia. 1437\u20131445.\n3. Yilin Wen, Zifeng Wang, and Jimeng Sun. 2024. MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. arXiv:2308.09729 [cs.AI] https://arxiv.org/abs/2308.09729\n4. Sondre Wold, Lilja \u00d8vrelid, and Erik Velldal. 2023. Text-To-KG Alignment: Comparing Current Methods on Classification Tasks. arXiv:2306.02871 [cs.CL] https://arxiv.org/abs/2306.02871\n5. Junde Wu, Jiayuan Zhu, and Yunli Qi. 2024. Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation. arXiv:2408.04187 [cs.CV] https://arxiv.org/abs/2408.04187\n6. Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun Jason Xue. 2024. Retrieval-Augmented Generation for Natural Language Processing: A Survey. arXiv:2407.13193 [cs.CL] https://arxiv.org/abs/2407.13193\n7. Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis N. Ioannidis, Karthik Subbian, James Zou, and Jure Leskovec. 2024. STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases. arXiv:2404.13207 [cs.IR] https://arxiv.org/abs/2404.13207\n8. Taiqiang Wu, Xingyu Bai, Weigang Guo, Weijie Liu, Siheng Li, and Yujiu Yang. 2023. Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrieval. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM 2023, Singapore, 27 February 2023 - 3 March 2023. 1021\u20131029.\n9. Yuxia Wu, Yuan Fang, and Lizi Liao. 2024. Retrieval Augmented Generation for Dynamic Graph Modeling. arXiv:2408.14523 [cs.LG] https://arxiv.org/abs/2408.14523\n10. Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, and Wei Song. 2023. Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. arXiv:2309.11206 [cs.CL] https://arxiv.org/abs/2309.11206\n11. Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024. 2905\u20132909.\n12. An Yang, Baosong Yang, and et al. 2024. Qwen2 Technical Report. arXiv:2407.10671 [cs.CL] https://arxiv.org/abs/2407.10671\n13.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6db06705-b622-43dd-967c-4f9a181122ed": {"__data__": {"id_": "6db06705-b622-43dd-967c-4f9a181122ed", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f328036-899e-4b64-8151-1a54855d4edc", "node_type": "4", "metadata": {}, "hash": "20fd129abb4b02269b42b34a9c6cb53b83af4d312d4b612ec329c9eb4c1dda92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcb41f78-4de8-4abd-971e-515670f27517", "node_type": "1", "metadata": {}, "hash": "13ccf5921018c62dd09b1fc9a97b1baefdedc5b6ffa84f30ec3633e74f3cfa62", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:2309.11206 [cs.CL] https://arxiv.org/abs/2309.11206\n11. Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024. 2905\u20132909.\n12. An Yang, Baosong Yang, and et al. 2024. Qwen2 Technical Report. arXiv:2407.10671 [cs.CL] https://arxiv.org/abs/2407.10671\n13. Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, and Irene Li. 2024. KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques. arXiv:2403.05881 [cs.CL] https://arxiv.org/abs/2403.05881\n14. Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024. CRAG \u2013 Comprehensive RAG Benchmark. arXiv:2406.04744 [cs.CL] https://arxiv.org/abs/2406.04744\n15. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018. 2369\u20132380.\n16. Mohammad Yani and Adila Alfa Krisnadhi. 2021. Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering: A Survey. Inf. 12, 7 (2021), 271.\n17. Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. 535\u2013546.\n18. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. 2024. Language is All a Graph Needs. arXiv:2308.07134 [cs.CL] https://arxiv.org/abs/2308.07134\n19. Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021. Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering. arXiv preprint arXiv:2109.08678 (2021).\n20. Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. 2016. The Value of Semantic Parse Labeling for Knowledge Base Question Answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers.\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2348, "end_char_idx": 5371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2ca69d1-d01d-455c-93c9-93c433c113e2": {"__data__": {"id_": "c2ca69d1-d01d-455c-93c9-93c433c113e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a666ccd-6cf8-407f-adf0-9feb3d1651fe", "node_type": "4", "metadata": {}, "hash": "7e521fb5c5e599a26ac34776b5b5346d1f0b788ef56f1650f62a1f2b30dacb3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23b24d1c-41f5-4a92-815a-7277d5cbbd96", "node_type": "1", "metadata": {}, "hash": "7ce61766bc9794f49564b84581cd2c7cbe1ff5627a478013436f2392a298f4b8", "class_name": "RelatedNodeInfo"}}, "text": "# Graph Retrieval-Augmented Generation: A Survey\n\n# References\n\n1. Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Yiqun Hu, William Yang Wang, Zhiguo Wang, and Bing Xiang. 2023. DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\n2. Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. 2022. KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. 4961\u20134974.\n3. Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval-Augmented Generation: A Survey. arXiv:2405.07437 [cs.CL] https://arxiv.org/abs/2405.07437\n4. Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. 2022. Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. 5773\u20135784.\n5. Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, and Chuan Shi. 2024. GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks. In Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, May 13-17, 2024. 1003\u20131014.\n6. Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, and Xiao Huang. 2024. KnowGPT: Knowledge Graph based Prompting for Large Language Models. arXiv:2312.06185 [cs.CL] https://arxiv.org/abs/2312.06185\n7. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, and Jure Leskovec. 2022. GreaseLM: Graph REASoning Enhanced Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.\n8. Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. 2018. Variational Reasoning for Question Answering With Knowledge Graph. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018. 6069\u20136076.\n9. Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. 2023. GraphText: Graph Reasoning in Text Space. arXiv:2310.01089 [cs.CL] https://arxiv.org/abs/2310.01089\n10. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-Augmented Generation for AI-Generated Content: A Survey. arXiv:2402.19473 [cs.CV] https://arxiv.org/abs/2402.19473\n11. Yanxin Zheng, Wensheng Gan, Zefeng Chen, Zhenlian Qi, Qian Liang, and Philip S. Yu. 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23b24d1c-41f5-4a92-815a-7277d5cbbd96": {"__data__": {"id_": "23b24d1c-41f5-4a92-815a-7277d5cbbd96", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a666ccd-6cf8-407f-adf0-9feb3d1651fe", "node_type": "4", "metadata": {}, "hash": "7e521fb5c5e599a26ac34776b5b5346d1f0b788ef56f1650f62a1f2b30dacb3f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2ca69d1-d01d-455c-93c9-93c433c113e2", "node_type": "1", "metadata": {}, "hash": "827075a7c98daab954d59991ea0828992151fa52748abfb998bf3d0214993b96", "class_name": "RelatedNodeInfo"}}, "text": "2023. GraphText: Graph Reasoning in Text Space. arXiv:2310.01089 [cs.CL] https://arxiv.org/abs/2310.01089\n10. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-Augmented Generation for AI-Generated Content: A Survey. arXiv:2402.19473 [cs.CV] https://arxiv.org/abs/2402.19473\n11. Yanxin Zheng, Wensheng Gan, Zefeng Chen, Zhenlian Qi, Qian Liang, and Philip S. Yu. 2024. Large Language Models for Medicine: A Survey. arXiv:2405.13055 [cs.CL] https://arxiv.org/abs/2405.13055\n12. Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. 2024. Efficient Tuning and Inference for Large Language Models on Textual Graphs. arXiv:2401.15569 [cs.CL] https://arxiv.org/abs/2401.15569\n\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024.", "mimetype": "text/plain", "start_char_idx": 2726, "end_char_idx": 3563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47ba346a-f06a-40ca-b728-2f0edb57e776": {"__data__": {"id_": "47ba346a-f06a-40ca-b728-2f0edb57e776", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "977d42fe-75e3-4446-b8e4-da8163132ddf", "node_type": "4", "metadata": {}, "hash": "eb0df6fc2d51b1445d2f86fa55af3c19ed14fe064c73e7517eddf8b1f55c0390", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f2c5204-542e-401a-b56f-aa2240d51da1", "node_type": "1", "metadata": {}, "hash": "80a695024ea1e4008e64643945ffc4c4e7cf4b5803e03946fbe74bb0f6443e8e", "class_name": "RelatedNodeInfo"}}, "text": "# Automated Fuzzing of Automotive Control Units\n\nTimothy Werquin, Mathijs Hubrechtsen, Ashok Thangarajan, Frank Piessens and Jan Tobias M\u00fchlberg\n\nKU Leuven, Dept. of Computer Science, imec-DistriNet\n\nB-3001 Leuven, Belgium\n\n\u3008firstname.lastname\u3009@cs.kuleuven.be\n\n# Abstract\n\nModern vehicles are governed by a network of Electronic Control Units (ECUs), which are programmed to sense inputs from the driver and the environment, to process these inputs, and to control actuators that, e.g., regulate the engine or even control the steering system. ECUs within a vehicle communicate via automotive bus systems such as the Controller Area Network (CAN), and beyond the vehicles boundaries through upcoming vehicle-to-vehicle and vehicle-to-infrastructure channels. Approaches to manipulate the communication between ECUs for the purpose of security testing and reverse-engineering of vehicular functions have been presented in the past, all of which struggle with automating the detection of system change in response to message injection. In this paper we present our findings with fuzzing CAN networks, in particular while observing individual ECUs with a sensor harness. The harness detects physical responses, which we then use in a oracle functions to inform the fuzzing process. We systematically define fuzzers, fuzzing configurations and oracle functions for testing ECUs. We evaluate our approach based on case studies of commercial instrument clusters and with an experimental framework for CAN authentication. Our results show that the approach is capable of identifying interesting ECU states with a high level of automation. Our approach is applicable in distributed cyber-physical systems beyond automotive computing.\n\nIndex Terms\u2014automotive control networks, CAN, security, testing, fuzzing\n\n# I. INTRODUCTION\n\nModern cars are largely controlled by software. This software forms a distributed mixed-criticality system that executes on a number of interconnected Electronic Control Units (ECUs). Jointly, these ECUs govern the vehicle\u2019s behaviour \u2013 from convenience and infotainment functions to safety-critical functionality. ECUs are connected via automotive bus systems that facilitate the exchange of messages, most of which communicate sensor readings and control instructions. The control software then interprets these messages and reacts to events by triggering the relevant actuators, e.g., brakes, airbags, or steering gear. In 2016, the Ford Motor Company reported that their latest models are running on 150 million lines of code. Given the enormous complexity of these systems, they are notoriously hard to test, for safety as well as for security properties.\n\nSince around 2004, researchers have been expressing their concerns with respect to the security limitations of communication standards, including the widely used Controller Area Network (CAN) [1]\u2013[3]. Since 2010, a series of high-profile attacks [4]\u2013[7] illustrate that with increased vehicular connectivity even remote adversaries can take control of critical functions of a vehicle. These risks have been acknowledged and are partly addressed in emerging industry standards [8], [9] that encompass authentication and software security for control systems, and prototypes that showcase secure system designs for automotive computing based on software attestation and Trusted Computing primitives [10] have been proposed. Meanwhile, more and more low-level vulnerabilities in these communication systems are being revealed (e.g., [11] and [12]), guidance for the reverse-engineering and penetration testing of vehicular communications and control systems becomes readily available [13], and the need for advanced testing methodology for these systems is generally acknowledged. A testing approach that promises a particularly high level of automation is fuzzing.\n\nFuzz testing [14], [15] is a well established methodology to expose software and systems to unexpected conditions, for example by providing random input streams that may crash the target. Yet, the approach does not easily apply to embedded software [16] and few approaches have been made to fuzz embedded control systems or automotive ECUs in particular [5], [17], [18]. A key difficulty to overcome here is the definition of oracle functions that define when a fuzzer has potentially triggered a bug or at least an \u201cinteresting\u201d system state, and to automatically evaluate these functions.\n\n# 1) Our Contributions:\n\nIn this paper we discuss fuzz testing in the context of automotive control networks. Specifically, our research investigates the use and automation of fuzzing so as to find vulnerabilities and to reverse engineer ECU functionality in CAN networks. We make the following contributions:\n\n1. We systematically define fuzzers, fuzzing configurations and oracle functions for testing automotive ECUs through their CAN interface.\n2. We develop a sensor harness to automatically evaluate fuzzing oracles for ECUs with physical outputs.\n3. We evaluate our approach, taking commercial automotive instrument clusters and an experimental setup for testing AUTOSAR-compliant message authentication as case studies.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f2c5204-542e-401a-b56f-aa2240d51da1": {"__data__": {"id_": "7f2c5204-542e-401a-b56f-aa2240d51da1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "977d42fe-75e3-4446-b8e4-da8163132ddf", "node_type": "4", "metadata": {}, "hash": "eb0df6fc2d51b1445d2f86fa55af3c19ed14fe064c73e7517eddf8b1f55c0390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47ba346a-f06a-40ca-b728-2f0edb57e776", "node_type": "1", "metadata": {}, "hash": "36b8f4353177e999760c3804fd524e598f8159efbbf3fc0b5ff0cc3ca3e507fd", "class_name": "RelatedNodeInfo"}}, "text": "A key difficulty to overcome here is the definition of oracle functions that define when a fuzzer has potentially triggered a bug or at least an \u201cinteresting\u201d system state, and to automatically evaluate these functions.\n\n# 1) Our Contributions:\n\nIn this paper we discuss fuzz testing in the context of automotive control networks. Specifically, our research investigates the use and automation of fuzzing so as to find vulnerabilities and to reverse engineer ECU functionality in CAN networks. We make the following contributions:\n\n1. We systematically define fuzzers, fuzzing configurations and oracle functions for testing automotive ECUs through their CAN interface.\n2. We develop a sensor harness to automatically evaluate fuzzing oracles for ECUs with physical outputs.\n3. We evaluate our approach, taking commercial automotive instrument clusters and an experimental setup for testing AUTOSAR-compliant message authentication as case studies.\n\nTo the best of our knowledge, this paper is the first to largely automate a methodical fuzzing approach (e.g. following [15]) for automotive ECUs. Although our implementation is targeting CAN components, our approach can be generalised to cyber-physical systems with any underlying communication technology. Our fuzzer implementation, instructions to build the sensor harness and to repeat our experiments are available under an open-source license at https://github.com/timower/caringcaribou/tree/autoFuzz.", "mimetype": "text/plain", "start_char_idx": 4217, "end_char_idx": 5674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "781fcfb3-87c9-4673-9532-f8071b317876": {"__data__": {"id_": "781fcfb3-87c9-4673-9532-f8071b317876", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44171e08-ebbb-4ad2-a72b-caad0fbbd2a6", "node_type": "4", "metadata": {}, "hash": "b7527a2ec575e9124c03acc580fd1ff83122ee708fd701d3b307fd2383bd7916", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99e0e83b-9110-443a-9968-0bdc465380c9", "node_type": "1", "metadata": {}, "hash": "0a76c0f757b437dc42fdbc76bab8ce3d8ea48ae087f092e806a0e4f8ef1ce0d2", "class_name": "RelatedNodeInfo"}}, "text": "# II. BACKGROUND\n\nIn this section we briefly introduce the CAN bus, which is commonly used to facilitate communication between automotive ECUs but also in industrial control systems. We further introduce the CaringCaribou penetration testing framework, which our fuzzing toolchain is integrated with.\n\n# A. Controller Area Network (CAN) & Security\n\nThe CAN bus is the most commonly used broadcast network in modern cars. A CAN message consists of an 11-bit arbitration ID, followed by an optional 18-bit extended ID, and up to 8 bytes of data payload (cf. Fig. 1). Dedicated transceiver hardware implements a protocol for message acknowledgement and bus arbitration for sending/receiving data frames. CAN requires a fixed data transmission rate, and allows recessive bits (one) to be overwritten by dominant bits (zero) during transmission. Message acknowledgement can thus simply be implemented by overwriting the ACK bit at the end of the data frame in real-time. Likewise, to implement bus arbitration, CAN transceivers are required to continuously listen on the bus while sending the message ID at the beginning of the data frame, and to back off when one of their ID bits has been overwritten. This scheme ensures that messages with lower IDs effectively have higher priorities. Finally, each CAN frame features a 16-bit CRC field to detect transmission errors.\n\nFigure 1. Extended data frame standardised by CAN 2.0B.\n\nCAN was originally developed in 1983, when cyber attacks were of no concern. Thus, the protocol does not provide any form of message authentication. Any ECU connected to the network can spoof messages with arbitrary sender ID and payload, which forms the basis of many attacks [4]\u2013[7]. As a response, the AUTOSAR [9] standardisation body published industry guidelines for backwards-compatible message authentication in vehicular networks. The VulCAN framework [10], which we study in Sect. V-A is one implementation of these authentication extensions.\n\n# B. CaringCaribou and Automotive Penetration Testing\n\nWe built our implementations of CAN fuzzers as modules for the open-source penetration-testing framework CaringCaribou. Our fuzzing extensions are freely available for further experimentation and follow-up research. Since fuzzers are widely used as a means to perform black-box testing in the regular penetration testing industry, it is our belief that a similar tool could prove useful in the automotive penetration testing community.\n\nCaringCaribou1 is a tool developed for the purpose of being the \u201cnmap\u201d of automotive security. Cyber security research in the automotive industry is a new field that is rapidly expanding. Yet, it still lacks the mature tooling available to the mainstream security community. Tools like CaringCaribou aim to fill that gap. CaringCaribou has a modular architecture that allows developers of penetration testing techniques for automotive systems to easily write new modules for their specific purpose, and deploy these modules using a unified tooling infrastructure. Thus, CaringCaribou provides the developer with a layer of abstraction which protects them from the specifics of CAN and other automotive communication protocols, allowing them to focus on writing the actual penetration testing tool instead of dealing with the lower-layer interactions. CaringCaribou is supposed to be a zero-knowledge tool that can be deployed on any CAN network regardless of its specific configuration.\n\n# III. FUZZING CAN NETWORKS\n\nIn this section we lay out our approach to define a fuzzing tool for CAN-based automotive control systems. We follow the approach of Manes et al. [15] and dissect the tool into an oracle component, the actual fuzzer and the run-time configuration for the fuzzer for a particular run.\n\n# A. Bug Oracles for ECUs\n\nAccording to [15], a (bug) oracle is a program that determines whether a given execution of the target system violates a specific (security) policy. In Sect. V we outline two very different case studies for our system: In one of these we have a partial specification of security properties of the network available, and where we are looking for violations of this specification. In the second case study we have no reliable specification but we are interested in reverse-engineering such a specification. Both case studies are characterised by not being able to observe software interactions directly (as opposed to software fuzzing with code instrumentation in a simulator [15]). Instead, we are looking at black-box systems to which our fuzzer can provide an input stream, while any observable communication output, physical output (actuation of a LED or a relay) or even the timing or absence of such outputs (e.g., due to a software crash) may indicate that an \u201cinteresting\u201d system state has been reached.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99e0e83b-9110-443a-9968-0bdc465380c9": {"__data__": {"id_": "99e0e83b-9110-443a-9968-0bdc465380c9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44171e08-ebbb-4ad2-a72b-caad0fbbd2a6", "node_type": "4", "metadata": {}, "hash": "b7527a2ec575e9124c03acc580fd1ff83122ee708fd701d3b307fd2383bd7916", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "781fcfb3-87c9-4673-9532-f8071b317876", "node_type": "1", "metadata": {}, "hash": "b4a2f0a5d9f9c2f124365665f3564a68bfa4e440943926509b7573a83a3f4a4b", "class_name": "RelatedNodeInfo"}}, "text": "In Sect. V we outline two very different case studies for our system: In one of these we have a partial specification of security properties of the network available, and where we are looking for violations of this specification. In the second case study we have no reliable specification but we are interested in reverse-engineering such a specification. Both case studies are characterised by not being able to observe software interactions directly (as opposed to software fuzzing with code instrumentation in a simulator [15]). Instead, we are looking at black-box systems to which our fuzzer can provide an input stream, while any observable communication output, physical output (actuation of a LED or a relay) or even the timing or absence of such outputs (e.g., due to a software crash) may indicate that an \u201cinteresting\u201d system state has been reached.\n\nRecent related work in the field of intrusion detection for industrial control system (e.g., [19] and [20]) suggests that machine-learning approaches can be used to train detectors that then report anomalies in the communication behaviour of vehicular networks. We have not implemented such oracle functions.\n\nPhysical outputs of control units can certainly be observed by human operators. They can also be sensed and electronically reported through sensor networks, or in our case a sensor harness that is attached to the target system. In the following sections we emphasise on this form fuzzing oracle, where a state change in the target is defined by a sensor (de-)activation or sensor threshold.\n\nMost difficult is certainly the detection of system failure which results in the absence of an observable response from the target. Thus, inputs that lead to failures are easily misinterpreted by a fuzzing tool as inputs that have no effect. For example, our work deals with ECUs that need to regularly receive", "mimetype": "text/plain", "start_char_idx": 3948, "end_char_idx": 5822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2368db4f-c1a2-41d6-a762-a3c79ee118d4": {"__data__": {"id_": "2368db4f-c1a2-41d6-a762-a3c79ee118d4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01e24ca2-a278-40f6-8c6a-3d0ef01e0b02", "node_type": "4", "metadata": {}, "hash": "86ddf1111228739ad90dc105547e5c6212b36a27f635f713a93e560ff26928d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eedcddff-3e26-4c96-8831-a60eec764717", "node_type": "1", "metadata": {}, "hash": "19980326427eee599b674c73fcdb1abff8525269f4e97c56bdffb12cd483f7fb", "class_name": "RelatedNodeInfo"}}, "text": "# Defining CAN Fuzzers\n\nFuzzing is the execution of the target system using input(s) sampled from an input space (the \u201cfuzz input space\u201d) that protrudes the expected input space of the target system [15]. With fuzzing we aim to enumerate and exercise a large subset of this fuzz input space to find system behaviour that triggers an oracle function. ECUs that process CAN messages are an interesting target since the frame size of CAN messages is at most 110 bits. This fuzz input space is certainly huge, but much smaller than, e.g., Ethernet frames, WiFi frames, or multimedia streams. Still, even for CAN networks, this fuzz input space is prohibitively large for being exhaustively exercised. Furthermore, with a maximum bandwidth of 1 MBit, and most ECUs using 500 MBit as a fixed transfer rate, data transmission to a target network of ECUs represents a bottleneck.\n\nStarting with the idea of random fuzzing, where arbitration IDs and message payloads are selected randomly, we devise three additional fuzzing strategies, brute-force fuzzing, mutation fuzzing and identify fuzzing, to narrow down the fuzz input space and explore interesting ECU behaviour more efficiently. These strategies are based on the observation that an ECU typically accepts inputs on a relatively small number of IDs only, that also the number of payload bits that result in an observable state change is limited, and that several consecutive messages may be required to trigger an observable state change. We then integrate these approaches in an automated exploration mode, where inputs from a sensor harness (cf. Sect. IV), which is attached to a target ECU, guide input generation. We have implemented our approach in two modules for CaringCaribou, namely fuzzer and autoFuzz, which can be invoked as ./cc.py &lt;module&gt; &lt;parameters&gt; [-f &lt;file&gt;]. Here, ./cc.py refers to the CaringCaribou main script, &lt;module&gt; to a fuzzer module, and &lt;parameters&gt; to a fuzzer configuration which we discuss below. -f &lt;file&gt; can be used to store a message trail on disk. For example, ./cc.py fuzzer random will generate entirely random messages and dispatch them over the configured CAN interface.\n\n# 1) Brute-Force Fuzzing\n\nThis method aims to exhaustively enumerate selected hexadecimal digits in a message, specifically in the message\u2019s ID field and the payload. For example, the fuzzer can be invoked as ./cc.py fuzzer brute 0x123 12ab..78, where the 5th and 6th octet of the message payload will be enumerated and sent, while the message ID 0x123 and all other payload octets remain constant.\n\n# 2) Mutation Fuzzing\n\nThis strategy can be used to systematically explore a larger fuzz input space through mutating selected hex digits in arbitration ID and message by means of individual random bit flips. An example use for this strategy is ./cc.py fuzzer mutate 7f.. 12ab....; the syntax follows the example given for brute-force fuzzing above.\n\n# 3) Identify Fuzzing\n\nOnce a fuzzing run resulted in an event of interest, e.g., a change of an indicator LED on a target ECU, the identify method can be used to replay and identify a minimal set of messages that caused the event. The syntax for invoking this method is ./cc.py fuzzer identify log.txt, where log.txt refers to a log file previously recorded with the -f parameter. The method relies on human input \u2013 i.e., key presses \u2013 to gather information about the timely occurrence of events, and aims to prune the set of recorded messages in log.txt so that the event still occurs when the pruned set is replayed.\n\n# 4) Automated ECU Exploration\n\nOur autoFuzz module implements the above strategies so that system change can be detected directly through our sensor harness (cf. Sect. IV). Sensor observations can then be used to guide the generation of the next inputs and to automatically identify message bits that lead to observable system change, depending on the fuzzing strategy. The module further features the generation of J1939-compliant messages and the fuzzing of J1939 function group addresses (PGNs and SPNs).\n\nWhen fuzzing an ECU with a single sensor attached to one of the ECU\u2019s actuators, it is possible to immediately run the identify fuzzer when a change in the sensor state is detected without relying on recorded traffic. This requires that the actuator can be triggered with a predictable payload. When using multiple sensors, the log file can be filtered to keep a number of messages preceding the activation of a specific sensor which can then be used as input to the identify fuzzer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eedcddff-3e26-4c96-8831-a60eec764717": {"__data__": {"id_": "eedcddff-3e26-4c96-8831-a60eec764717", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01e24ca2-a278-40f6-8c6a-3d0ef01e0b02", "node_type": "4", "metadata": {}, "hash": "86ddf1111228739ad90dc105547e5c6212b36a27f635f713a93e560ff26928d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2368db4f-c1a2-41d6-a762-a3c79ee118d4", "node_type": "1", "metadata": {}, "hash": "bec7239fc0bce1d8d6060925c567792ec37a42c6a1d75916422615f7a1467a71", "class_name": "RelatedNodeInfo"}}, "text": "Sect. IV). Sensor observations can then be used to guide the generation of the next inputs and to automatically identify message bits that lead to observable system change, depending on the fuzzing strategy. The module further features the generation of J1939-compliant messages and the fuzzing of J1939 function group addresses (PGNs and SPNs).\n\nWhen fuzzing an ECU with a single sensor attached to one of the ECU\u2019s actuators, it is possible to immediately run the identify fuzzer when a change in the sensor state is detected without relying on recorded traffic. This requires that the actuator can be triggered with a predictable payload. When using multiple sensors, the log file can be filtered to keep a number of messages preceding the activation of a specific sensor which can then be used as input to the identify fuzzer.\n\nWhile experimenting with fuzzing strategies, we observed that an ECU\u2019s response to a message is often delayed. During the delay period, other messages are being sent by the fuzzer, which makes identifying the CAN messages specifically responsible for a response more difficult. One possible solution is to increase the delay between sending messages. Yet, this will increase the time required to cover the fuzz input space. Another option is to resend only a subset of the messages preceding a sensor activation with increased delays, which we implemented in our identify method.\n\nOur experiments further revealed that some ECUs expect certain messages to be received regularly. The absence of these messages will lead to a shut-down or render the ECU unresponsive and to indicate a failure. These behaviours prevent our identify method from working as sending the complete traffic log can keep the ECU responsive but sending parts of the recorded traffic will cause the ECU to fault. To address this, we developed an approach that we refer to as omission fuzzing. This strategy sends the complete recorded traffic but omits some messages in order to identify which message cause.", "mimetype": "text/plain", "start_char_idx": 3736, "end_char_idx": 5748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3790ea29-b5b2-4ad7-aab9-63896366bd50": {"__data__": {"id_": "3790ea29-b5b2-4ad7-aab9-63896366bd50", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d5a96b7-487c-441e-b123-806c07f99da9", "node_type": "4", "metadata": {}, "hash": "aea0705d8c3544bc3842f632296da10f02bb5cac7819fd5f018ddc90861bdba3", "class_name": "RelatedNodeInfo"}}, "text": "# C. Target-Specific Fuzzer Configuration\n\nA fuzz configuration of a fuzzer comprises the parameter value(s) that control(s) the fuzz algorithm [15]. In the context of our approach, these parameters involve message generation, message timing, message omission, and the configuration of the sensor harness. As outlined before fuzzing entire CAN messages makes little sense as it results in an extremely large fuzz input space. Thus, configurations will typically restrict the fuzz space to specific octets in (extended) arbitration IDs and message payload. Message timing is typically configured to schedule a new message every 3 ms to 20 ms to avoid message collisions and to leave enough time for actuators to be engaged and sensed. Message omission and baseline traffic are to be set up to simulate typical bus traffic in a car so as to make target ECUs function normally. The sensor harness offers a wide range of configuration options that involve the type of sensors, sampling rates, the number of sensors and their placement on the target ECU.\n\n# IV. A SENSOR HARNESS TO AUTOMATE ECU FUZZING\n\nIn this section we describe an inexpensive and extensible experimental sensor harness to automate the analysis of automotive ECUs. The intuition behind the setup is that fuzzing communication in an automotive control network, or in cyber-physical systems in general, can cause a range of interesting responses beyond network communication. Thus, to use these responses as inputs to fuzzing oracles (cf. Sect. III-A) they must be automatically measured at an appropriate sampling rate. Previous approaches to consider these responses typically rely on human observation and human interaction during the fuzzing process. For example, a fuzzing tool may require the user to press a key if they observe a change in the system, e.g., a flashing indicator light on a control panel. Our work improves over this by detecting physical responses of ECUs automatically, with negligible delays, and at a configurable granularity.\n\nFig. 2 gives an overview of the sensor harness. The system in action is depicted in Sect. V-B. The harness connects multiple sensors together and provides a Universal Serial Bus (USB) interface for a PC to control the setup. The depicted configuration contains only light and colour sensors which can be placed over the various indicators LEDs on a target ECU. No general-purpose microcontroller is used in the harness, which allows the entire setup to be programmed and configured from the PC in a higher-level language, Python in our case.\n\n# Figure 2. Sensor harness connection schema showing (1) the FT232H I2C-to-USB converter, (2) an TCA9548A I2C Multiplexer, and (3) two ISL29125 light & colour sensors.\n\nThe light sensors are connected through an I2C bus, a communication interface which is present on many low-cost sensors. The use of I2C makes the harness extensible with various other sensors, such as sound sensors to monitor auditory alerts, motion sensors to monitor steering wheel movement, or current sensors to detect an engine start. As the I2C bus supports multiplexing, multiple sensors can be connected to the same bus as long as they have a different I2C address. The low-cost sensors used in the harness often have a fixed address, which implies that an I2C multiplexer must be used to connect multiple sensors of the same type in the harness. In order to interface with the sensor harness from a PC an USB to I2C adapter is used.\n\n# 1) Light & Colour Sensors: ISL29125\n\nThe ISL29125 colour sensor is used to measure the status of visual indicators on automotive equipment. The sensor provides a simple RGB light level readout and has a number of configuration options which determine the sensitivity and precision of the sensor. The sensitivity can be configured between 10k lux or 375 lux and the sensor has a built-in infrared light filter which can be configured separately. The precision of the sensor can be configured to be either 12 or 16 bits. Changing the precision also changes the integration time, meaning a higher precision (16 bits) will require the Analogue-Digital Converter (ADC) to sample the sensor longer resulting in slower measurements. In addition to the I2C interface the sensor has an interrupt pin which can be triggered by a configurable light level on either of the red, green or blue channel. Currently the harness does not use the interrupt functionality but works by polling each sensor individually. Our colour sensors have a single fixed I2C address, requiring a multiplexer to connect multiple sensors on the same I2C bus.\n\n# 2) I2C Multiplexer: TCA9548A\n\nAs the colour sensor has a fixed I2C address, the TCA9548A multiplexer is used to connect multiple colour sensors in the harness. The TCA9548A multiplexer has eight I2C channels which allows eight colour sensors to be connected on the same bus. As the multiplexer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97ddde65-51d6-490b-93a1-0efb46f4ed45": {"__data__": {"id_": "97ddde65-51d6-490b-93a1-0efb46f4ed45", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbb2953e-3560-4cf4-9189-a5410ab1af28", "node_type": "4", "metadata": {}, "hash": "9c00ecb6a7f698b851315454bcfee588377dfde2a05c0858de745ac6b93ef3b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f25a1fc-34d0-415d-a2a8-e23ee0713a18", "node_type": "1", "metadata": {}, "hash": "bf865a8bcd507e8ab29909439bf3adcd090879b48c49f2a8d958903b7ad5a04b", "class_name": "RelatedNodeInfo"}}, "text": "# 3) I2C-to-USB: FT232H\n\nIn order to connect the I2C bus to a PC the FTDI FT232H adapter is used. This adapter supports a number of different bus protocols such as UART, SPI and I2C in addition to a GPIO interface which can be used to write to eight digital IO pins.\n\n# 4) Programming, Calibration & Use\n\nTo obtain readings from the light sensors, the Arduino library2 for reading ISL29125 sensors was adapted. The resulting library uses the Adafruit3 library for communicating with the FT232H adapter. Our current sensor library exposes functions for initialising a new sensor, and for reading the sensor\u2019s red, green and blue values.\n\nThe initialisation function resets the sensor and configures it in RGB mode which enables all three colour channels, puts it in 10k-lux mode for bright environments and enables high IR light adjustment. Reading a colour value is done by reading from the relevant device register, which returns a colour value of either 12 or 16 bits depending on the chosen precision. The time the sensor needs to take a reading varies depending on the ADC integration time which in turn depends on the chosen precision: At 16 bit precision, each reading takes about 110 ms while at 12 bit each reading takes only 7 ms. As the sensor\u2019s output registers are double-buffered, reading out these registers between sensing operations will result in outdated readings.\n\nIn order to use multiple light sensors a library that interfaces with the TCA9548A multiplexer was created, this library allows virtual I2C ports to be created for each channel of the multiplexer. These virtual I2C ports can then be used in the sensor library instead of the default FT232H I2C port. In order to switch I2C channels the number of the requested channel is written before any commands, this adds a delay before every I2C command, which is negligible in comparison with the integration time of the light sensor and does not impact fuzzing performance.\n\nIn initial experiments we use the colour light sensors to detect whether the various indicators on an automotive dashboard are changing state, effectively converting the red, green and blue light levels into a binary input signal for the fuzzer. As the sensors are sensitive enough to detect (even reflected) movements behind the sensor while duct-taped to a dashboard in both sensitivity configurations (up to 375 lux and 10k lux) a simple threshold is not sufficient to distinguish state change. We devise a calibration method that involves taking a reading when the indicator is on and when the indicator is off. This results in red, green and blue light level triples to which any new measurement can be compared, if the new measurement is closer to the on-value the indicator is detected as on and vice versa. This method assures that a uniform increase or decrease in ambient light does not change the detected indicator value. The method further requires a calibration with the indicator both on and off, which may not be feasible when the indicator trigger is unknown. When the indicator cannot be triggered during calibration, a simple threshold may be used to detect the indicator state. More elaborate calibration methods may be required to operate the sensor harness in noisy environments.\n\n# V. EVALUATION AND DISCUSSION\n\nWe have applied our fuzzer implementation and the sensor harness to a number of case studies that include the ICSim automotive instrument cluster simulator4, a demo setup for illustrating and implementing message authentication in CAN networks with the VulCAN[10] framework, as well as real instrument clusters. In this section we focus on our experience and lessons learned from the latter two case studies. We compare our findings with earlier manual approaches to discover bugs and explore proprietary functionality in these scenarios.\n\n# A. Case Study 1: VulCAN\n\nWe evaluated the effectiveness of our fuzzer to find implementation bugs and security vulnerabilities on a demo implementation of VulCAN[10], a generic design for CAN message authentication. VulCAN provides efficient and AUTOSAR-compliant[9] authentication plus software component attestation based on lightweight trusted computing technology. We used the same test bench as described in [10] to test the abilities of the fuzzer.\n\nIn brief, the demo consists of a number of ECUs with keypads as input devices and LED displays as actuators. A distributed control application which simulates a traction control system is executing on the ECUs. Application components communicate via cryptographically authenticated CAN messages with freshness guarantees: only messages that are successfully validated to be fresh and to originate from unmodified and integrity-protected remote component should ever be able to trigger output events. The application communicates only a few valid payloads at fixed intervals. Thus, deviation from expected behaviour would be easy to detect.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f25a1fc-34d0-415d-a2a8-e23ee0713a18": {"__data__": {"id_": "9f25a1fc-34d0-415d-a2a8-e23ee0713a18", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cbb2953e-3560-4cf4-9189-a5410ab1af28", "node_type": "4", "metadata": {}, "hash": "9c00ecb6a7f698b851315454bcfee588377dfde2a05c0858de745ac6b93ef3b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97ddde65-51d6-490b-93a1-0efb46f4ed45", "node_type": "1", "metadata": {}, "hash": "172fe42bcad8f8b033cf4f9add0229b5de18ea3de80a7235d76822053e8e988b", "class_name": "RelatedNodeInfo"}}, "text": "VulCAN provides efficient and AUTOSAR-compliant[9] authentication plus software component attestation based on lightweight trusted computing technology. We used the same test bench as described in [10] to test the abilities of the fuzzer.\n\nIn brief, the demo consists of a number of ECUs with keypads as input devices and LED displays as actuators. A distributed control application which simulates a traction control system is executing on the ECUs. Application components communicate via cryptographically authenticated CAN messages with freshness guarantees: only messages that are successfully validated to be fresh and to originate from unmodified and integrity-protected remote component should ever be able to trigger output events. The application communicates only a few valid payloads at fixed intervals. Thus, deviation from expected behaviour would be easy to detect. Yet, since it is unlikely for a random or mutation-based fuzzer to \u201cguess\u201d a valid payload, nonce, and authentication tag triple, and since the system was designed with security in mind, we did not expect the security properties of the system to be broken easily. This part of the evaluation is conducted without using the sensor harness but by relying on visual observation on the demo\u2019s LED displays. The fuzzer is executing on desktop PC, which is connected to the demo setup via a USB to CAN interface.\n\nTo our surprise, with the help of the fuzzer, we detected and traced several unique vulnerabilities in the system in a fairly short period of time. Below we focus on two particularly subtle discoveries.\n\nThe first vulnerability was discovered nearly instantaneously in a fuzzer configuration where messages with extended CAN arbitration IDs are generated. Such messages resulted in system", "mimetype": "text/plain", "start_char_idx": 4043, "end_char_idx": 5819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43b3f599-fc24-4403-a686-7c06efefc68a": {"__data__": {"id_": "43b3f599-fc24-4403-a686-7c06efefc68a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be8eba27-7bc3-47d4-8558-d8523201f14f", "node_type": "4", "metadata": {}, "hash": "e9fddc9ee98750f9fc75f6c34b29b5200d3005dcfd83a7660e84573f265260b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcef5747-a054-4322-a292-ce41ab606a80", "node_type": "1", "metadata": {}, "hash": "1a53a1469db1136ffa86a9fd617905411f2b4421146165e7f0694415f83ca05d", "class_name": "RelatedNodeInfo"}}, "text": "# V. Vulnerabilities in VulCAN\n\nstates where the injected messages could lead to actual display outputs, breaking the security properties of VulCAN entirely. Extended CAN IDs are not being used in the VulCAN demo, and thus, the components were not tested in environments where these messages occur. Most likely, a misconfigured driver for the CAN controller on an ECUs \u2013 \u201cuntrusted\u201d software in VulCAN\u2019s attacker model \u2013 together with an incomplete rejection condition in a secure application module, allowed an attacker to arbitrarily adjust the displays of the test bench without having to pass authenticity checks.\n\nThe second vulnerability was found within the implementation of one of the authentication protocols in VulCAN, specifically VatiCAN [21]. This implementation turned out to be particularly vulnerable to denial-of-service attacks when being flooded with specific traffic patterns, allowing an attacker to desynchronise nonces and render trusted components unresponsive even to dedicated re-synchronisation messages. The bug was discovered in a timespan of several minutes when fuzzing the test bench in a configuration where both, the fuzzer as well as an ECU, are simultaneously attempting to send messages to a target ECU. Interestingly, due to the configuration error in CAN drivers described above, messages with extended CAN IDs are effectively interpreted as broadcast messages. Application components are thus subject to receiving a mix of fuzzer-generated payloads and authenticated messages which results in denial-of-service.\n\n# B. Case Study 2: Instrument Clusters\n\nIn the context of automotive security research and for building demos such as the VulCAN [10] setup, instrument clusters are commonly used as easily accessible off-the-shelf components with many visible indicators (speed needle, turning indicators, display, etc.), most of which can be controlled through CAN messages. Yet, the specific arbitration IDs and payloads to control these functions are not publicly documented. Literature on car hacking (e.g., [13]) suggests manual approaches to reverse-engineer these details, which may require hours or even days of try-and-error, even for a skilled engineer. By using our sensor harness, we expect a substantial speed-up of these processes, on top of being able to largely automate the process.\n\nWe have been experimenting with a number of clusters from passenger cars and commercial vehicles. As illustrated in Fig. 3, components of our sensor harness are duct-taped to indicators of the cluster. The instrument cluster is connected to a desktop PC with the fuzzer via a USB to CAN interface and there are no other ECUs present on the CAN.\n\nIn order to test the instrument cluster\u2019s basic functions, we developed a controller application that would send a number of documented [22] CAN messages to the dashboard. Only some these control messages worked in combination with our dashboard. We then applied our fuzzer in identify mode to filter the traffic data for messages that trigger physical functions, then applied brute-force and mutation mode to explore arbitration IDs and payloads to trigger further functionality.\n\nBy brute-forcing the entire 11-bit arbitration ID fuzz-space with a fixed payload 0xffffffff, most indicators LEDs in the dashboard could be activated. Some of these indicators are switched on by default when the instrument cluster is powered up. These indicators could be triggered using a fixed payload of 0x00000000. Control of the speedometer and engine RPM needles could also be triggered. The fuzzer takes about 30 s to enumerate the 11-bit address space using a delay of 10 ms between messages. Running the identify method after finding a state changes takes an extra 30 s per activation in order to identify the message responsible for an indicator activation.\n\nUsing mutation-based fuzzing on any of the messages identified with the brute-force method, we were able to reverse engineer the semantics of most payload bits. For example, by starting from the messages that activated the left turning indicator, the fuzzer was able to not only identify the bit responsible for triggering the indicator. It was also able to identify the bit responsible for the right turn signal, the headlights indicator and a number of other status LEDs. Enumerating eight payload bits takes about 20 s with a delay of 3 ms between messages. Using the identify method requires an additional 5 s per indicator. The delay between messages is critical when using mutation-based fuzzing. If the delay is too short, the next message, which will have another bit flipped, will overwrite the previous message. This will cause more indicator activations to be missed or some indicators to not even activate.\n\nOmission fuzzing is not necessary for analysing this particular instrument cluster. Yet, we heavily relied on this method when working with more modern clusters from commercial vehicles.\n\n# C. Discussion & Lessons Learned\n\nThe two case studies outlined above show that our fuzzers can efficiently reveal undocumented functionality, intricate bugs and security vulnerabilities in ECUs connected to CAN networks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcef5747-a054-4322-a292-ce41ab606a80": {"__data__": {"id_": "fcef5747-a054-4322-a292-ce41ab606a80", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be8eba27-7bc3-47d4-8558-d8523201f14f", "node_type": "4", "metadata": {}, "hash": "e9fddc9ee98750f9fc75f6c34b29b5200d3005dcfd83a7660e84573f265260b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43b3f599-fc24-4403-a686-7c06efefc68a", "node_type": "1", "metadata": {}, "hash": "41dc22a4f22816595776408abf7369bb1b0e9a49b979e21945cadc7aa4dad584", "class_name": "RelatedNodeInfo"}}, "text": "It was also able to identify the bit responsible for the right turn signal, the headlights indicator and a number of other status LEDs. Enumerating eight payload bits takes about 20 s with a delay of 3 ms between messages. Using the identify method requires an additional 5 s per indicator. The delay between messages is critical when using mutation-based fuzzing. If the delay is too short, the next message, which will have another bit flipped, will overwrite the previous message. This will cause more indicator activations to be missed or some indicators to not even activate.\n\nOmission fuzzing is not necessary for analysing this particular instrument cluster. Yet, we heavily relied on this method when working with more modern clusters from commercial vehicles.\n\n# C. Discussion & Lessons Learned\n\nThe two case studies outlined above show that our fuzzers can efficiently reveal undocumented functionality, intricate bugs and security vulnerabilities in ECUs connected to CAN networks. Ultimately, our experiments provide further evidence for fuzzing to be a useful tool in testing and reverse-engineering, which is due to the technique\u2019s ability to cover an enormous range of possible combinations of system inputs, in our case arbitration IDs and payloads. Many of these inputs may not even occur in normal and benign operation, and are difficult to consider in static test cases.\n\nAbove we describe two critical security vulnerabilities in an experimental system design, which are based on intricate implementation and configuration bugs. Previously undetected, these vulnerabilities became apparent within minutes with the use of a fuzzer, even without relying on a sensor harness. The harness could be used to reduce human interaction and to improve the duration for detecting and tracing these bugs, but non-trivial extensions of the harness would be required to sense the state of actuators such as the attached displays. Alternatively, the demo setup could be modified to feature simpler actuators (i.e., individual LEDs or relays) that allow for an easier detection of conditions that satisfy our bug oracles. We used our fuzzer, specifically the identify and replay.", "mimetype": "text/plain", "start_char_idx": 4174, "end_char_idx": 6357, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1e9170e-2bb9-403c-a6d2-066da1b9bc4e": {"__data__": {"id_": "d1e9170e-2bb9-403c-a6d2-066da1b9bc4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0893cc8e-00ef-42fe-88e6-f6b94837ff1c", "node_type": "4", "metadata": {}, "hash": "67751d0374d91cab5986d3a3e76f5d3f70f3b36c24cde5372dd9e1d3179e9781", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3. An automotive instrument cluster with (part of) our sensor harness attached. The cluster originates from a 2014 Seat Ibiza model.\n\nfunctionality, to trace bugs in source code and fix vulnerabilities. The resulting fixes are not straight-forward as they require consideration of rather involved network states. Our findings highlight the need for thorough testing and verification on top of strong cryptographic primitives and Trusted Computing technology when designing distributed control systems that are potentially exposed to malicious interactions.\n\nWe further described how the sensor harness in combination with our fuzzing techniques can be used to largely automate the process of reverse-engineering communication protocols of proprietary ECUs. Manually reversing a substantial subset of the functionality of, e.g., an instrument cluster, can easily be an effort of several days or even weeks. With our approach, this can be achieved within hours. Additional sensors (e.g., audio, power consumption, vibration) could further extend the harness\u2019 abilities. We believe that our approach can be used to identify bugs and unintended functionality when being applied to components for which a specification is available. This specification could be integrated in a bug oracle such that responses outside of the specified behaviour are detected as errors. In this context, our approach may be useful to automate activities such as integration testing and to achieve a high input-space coverage in these activities.\n\nFuzzing ECUs under realistic conditions, i.e., while being connected to a vehicle\u2019s CAN network with many other \u201cnoisy\u201d ECUs, may also be feasible but requires fuzzing strategies that aim at noise reduction by exploring the effects of individual messages or sequences of messages in different system states. In this context it may be useful to also consider CAN responses of ECUs. We may borrow from recent approaches in anomaly detection in control systems (e.g., [19] and [20]) to define oracle functions that detect changes in the response stream of an individual ECU, or even to detect state change throughout the network.\n\n# VI. RELATED WORK\n\nFuzzing has a long history and is still actively developed, in particular in the domain of security- and penetration testing of software systems [14], [15]. Recent work [16] elaborates on the difficulties of employing fuzzing in embedded systems. Specifically for automotive systems, Smith states in [13] that, while fuzzing can certainly be useful in discovering undocumented services or crashes, it is rarely useful beyond that, e.g., to find and exploit vulnerabilities. Our experience report disagrees slightly with this observation: We discovered that fuzzing is more efficient in finding subtle vulnerabilities and configuration errors than monitoring or reverse engineering the firmware and communications. Fuzzing exposes substantially more of the system\u2019s unintended states than what one would be able to explore manually, due to the sheer amount of pseudo-random message combinations that are generated and dispatched by the fuzzer. This allows testers to focus on tracking down and responding to vulnerability reports instead of having to manually probe the system. With automated oracle function, as discussed in Sect. III-A and Sect. IV, fuzzing becomes even more efficient. While our approach mostly relies on black-box fuzzing where very little knowledge of the system is assumed and oracle functions must rely on system outputs rather than observing the system\u2019s internal state, our approach can certainly be combined with more advanced reverse-engineering and firmware inspection tools. This would lead to more powerful and also much more intricate oracle functions.\n\nRelated research investigates the extent to which fuzzing can be applied be in automotive systems [17], [18], [23]\u2013[25]. Our work aims to improve over this state of the art by not only defining a fuzzer for CAN networks, but by developing an entire methodology that defines fuzzing objectives, oracle functions and fuzzing strategies, and substantially improves the automation of testing cyber-physical systems. We report on experiments and lay out our experience from applying this methodology to two realistic systems, one of which being a prototype for an automotive security system. While related work reports mixed results on the usefulness of fuzzing automotive networks, we judge our results as largely positive since we were able to identify a few subtle vulnerabilities and dramatically.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25b50957-dbac-43bf-a1ce-dd80bbd7bc97": {"__data__": {"id_": "25b50957-dbac-43bf-a1ce-dd80bbd7bc97", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1cd878b3-5851-466f-8859-adbc4ecd5abc", "node_type": "4", "metadata": {}, "hash": "d50be5f7cb0995f211c6f14ea1489cd11b2abd006bb39ccb7dce74bb90eef605", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e79ad8fb-0856-4679-a88e-2e3e4c203205", "node_type": "1", "metadata": {}, "hash": "393fbadf1cd21ab7d3e876faf3f474e515fe6f3f0039f16b4a60627403b552f6", "class_name": "RelatedNodeInfo"}}, "text": "# VII. SUMMARY & CONCLUSIONS\n\nAutomotive control networks are highly complex safety-critical and security-critical systems which have been shown to be vulnerable to adversarial interactions. In this paper we devise a largely automated approach for fuzz-testing these systems. We discuss how bug oracles for automotive ECUs can be described, define a number of fuzzing strategies, and develop a sensor harness to allow oracle functions to detect interesting system behaviour in an automated fashion. We have implemented our fuzzing approach in CaringCaribou, an open-source automotive penetration-testing toolkit, and we report on two sets of experiments where we apply our fuzzer to find vulnerabilities and to reverse-engineer proprietary ECU functions. To the best of our knowledge, our approach is the first to achieve a high degree of automation for these activities, and we see future applications of our fuzzing approach in, e.g., penetration testing but also in integration- and compliance testing. While we have been focusing on CAN networks, we believe that the approach is applicable to other types of control networks and beyond the domain of automotive computing. In the future we will work towards a more rigorous evaluation of our approach, following the methodology of [29].\n\n# 1) Acknowledgements\n\nThis research is partially funded by the Research Fund KU Leuven. This research is partially funded under SErVO, \u201cSecure and Economically Viable V2X Solutions\u201d, by the Flemish Agentschap Innoveren & Ondernemen. We thank the developers of CaringCaribou for their sport and ideas, and for integrating parts of our fuzzer into their platform.\n\n# REFERENCES\n\n|[1]|M. Wolf, A. Weimerskirch, and C. Paar, \u201cSecurity in automotive bus systems,\u201d in Workshop on Embedded Security in Cars, 2004.|\n|---|---|\n|[2]|T. Hoppe, S. Kiltz, and J. Dittmann, \u201cSecurity threats to automotive CAN networks \u2013 practical examples and selected short-term countermeasures,\u201d in Computer Safety, Reliability, and Security (SAFECOMP \u201908). Berlin, Heidelberg: Springer Berlin Heidelberg, 2008, pp. 235\u2013248.|\n|[3]|O. Henniger, L. Apvrille, A. Fuchs, Y. Roudier, A. Ruddle, and B. Weyl, \u201cSecurity requirements for automotive on-board networks,\u201d in 9th International Conference on Intelligent Transport Systems Telecommunications, (ITST), 2009, pp. 641\u2013646.|\n|[4]|K. Koscher, A. Czeskis, F. Roesner, S. Patel, T. Kohno, S. Checkoway, D. McCoy, B. Kantor, D. Anderson, H. Shacham et al., \u201cExperimental security analysis of a modern automobile,\u201d in Security and Privacy, 2010 IEEE Symposium on. IEEE, 2010, pp. 447\u2013462.|\n|[5]|S. Checkoway, D. McCoy, B. Kantor, D. Anderson, H. Shacham, S. Savage, K. Koscher, A. Czeskis, F. Roesner, T. Kohno et al., \u201cComprehensive experimental analyses of automotive attack surfaces,\u201d in USENIX Security Symposium. San Francisco, 2011.|\n|[6]|C. Miller and C. Valasek, \u201cA survey of remote automotive attack surfaces,\u201d Black Hat USA, 2014.|\n|[7]|\u2014\u2014, \u201cRemote exploitation of an unaltered passenger vehicle,\u201d Black Hat USA, 2015.|\n|[8]|SAE International, \u201cJ3061: Cybersecurity guidebook for cyber-physical vehicle systems,\u201d 2016, http://standards.sae.org/j3061 201601/.|\n|[9]|AUTOSAR Specification 4.3, \u201cSpecification of module secure onboard communication,\u201d https://www.autosar.org/standards/classic-platform/release-43/software-architecture/safety-and-security/, 2016.|\n|[10]|J. Van Bulck, J. T. M\u00fchlberg, and F. Piessens, \u201cVulCAN: Efficient component authentication and software isolation for automotive control networks,\u201d in ACSAC \u201917. ACM, 2017, pp. 225\u2013237.|\n|[11]|S. Fr\u00f6schle and A. St\u00fchring, \u201cAnalyzing the capabilities of the CAN attacker,\u201d in ESORICS \u201917, ser. LNCS, vol. 10492. Heidelberg: Springer, 2017, pp. 464\u2013482.|\n|[12]|A.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e79ad8fb-0856-4679-a88e-2e3e4c203205": {"__data__": {"id_": "e79ad8fb-0856-4679-a88e-2e3e4c203205", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1cd878b3-5851-466f-8859-adbc4ecd5abc", "node_type": "4", "metadata": {}, "hash": "d50be5f7cb0995f211c6f14ea1489cd11b2abd006bb39ccb7dce74bb90eef605", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25b50957-dbac-43bf-a1ce-dd80bbd7bc97", "node_type": "1", "metadata": {}, "hash": "ee80ffcf6910b358d8ec75306aeba0493dc78bb612d88b63f6080505f4ab1a32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58a8dc5f-c7d3-43ce-aee1-c13b8812a2d9", "node_type": "1", "metadata": {}, "hash": "55a812c501e0c1d36342da868b084fa3c793f292d21aa0146deeaaf887426117", "class_name": "RelatedNodeInfo"}}, "text": "Van Bulck, J. T. M\u00fchlberg, and F. Piessens, \u201cVulCAN: Efficient component authentication and software isolation for automotive control networks,\u201d in ACSAC \u201917. ACM, 2017, pp. 225\u2013237.|\n|[11]|S. Fr\u00f6schle and A. St\u00fchring, \u201cAnalyzing the capabilities of the CAN attacker,\u201d in ESORICS \u201917, ser. LNCS, vol. 10492. Heidelberg: Springer, 2017, pp. 464\u2013482.|\n|[12]|A. Palanca, E. Evenchick, F. Maggi, and S. Zanero, \u201cA stealth, selective, link-layer denial-of-service attack against automotive networks,\u201d in International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment. Springer, 2017, pp. 185\u2013206.|\n|[13]|C. Smith, The car hacker\u2019s handbook: a guide for the penetration tester. No Starch Press, 2016.|\n|[14]|P. Oehlert, \u201cViolating assumptions with fuzzing,\u201d IEEE Security & Privacy, vol. 3, no. 2, pp. 58\u201362, 2005.|\n|[15]|V. J. M. Man\u00e8s, H. Han, C. Han, S. K. Cha, M. Egele, E. J. Schwartz, and M. Woo, \u201cFuzzing: Art, science, and engineering,\u201d CoRR, vol. abs/1812.00140, 2018. [Online]. Available: http://arxiv.org/abs/1812.00140|\n|[16]|M. Muench, J. Stijohann, F. Kargl, A. Francillon, and D. Balzarotti, \u201cWhat you corrupt is not what you crash: Challenges in fuzzing embedded devices,\u201d in Proceedings of the Network and Distributed System Security Symposium (NDSS), 2018.|\n|[17]|H. Lee, K. Choi, K. Chung, J. Kim, and K. Yim, \u201cFuzzing can packets into automobiles,\u201d in 2015 IEEE 29th International Conference on Advanced Information Networking and Applications. IEEE, 2015, pp. 817\u2013821.|\n|[18]|S. Bayer, T. Enderle, D.-K. Oka, and M. Wolf, \u201cAutomotive security testing \u2013 the digital crash test,\u201d in Energy Consumption and Autonomous Driving. Springer, 2016, pp. 13\u201322.|\n|[19]|A. Taylor, S. Leblanc, and N. Japkowicz, \u201cAnomaly detection in automobile control network data with long short-term memory networks,\u201d in 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA), 2016, pp. 130\u2013139.|\n|[20]|C. Wressnegger, A. Kellner, and K. Rieck, \u201cZoe: Content-based anomaly detection for industrial control systems,\u201d in 2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), 2018, pp. 127\u2013138.|\n|[21]|S. Nurnberger and C. Rossow, \u201c\u2013 vatiCAN \u2013 Vetted, authenticated CAN bus,\u201d in Cryptographic Hardware and Embedded Systems \u2013 CHES \u201916: 18th International Conference, Santa Barbara, CA, USA, August 17-19, 2016, Proceedings. Berlin, Heidelberg: Springer Berlin Heidelberg, 2016, pp. 106\u2013124.|\n|[22]|L. Bataille, \u201cVolkswagen can bus gaming,\u201d URL: https://hackaday.io/project/6288-volkswagen-can-bus-gaming.|\n|[23]|S. Bayer and A. Ptok, \u201cDon\u2019t fuss about fuzzing: Fuzzing controllers in vehicular networks,\u201d 13th escar Europe, p. 88, 2015.|\n|[24]|R. Nishimura, R. Kurachi, K. Ito, T. Miyasaka, M. Yamamoto, and M. Mishima, \u201cImplementation of the can-fd protocol in the fuzzing tool bestorm,\u201d in 2016 IEEE International Conference on Vehicular Electronics and Safety (ICVES). IEEE, 2016, pp. 1\u20136.|\n|[25]|D. S. Fowler, J. Bryans, and S. Shaikh, \u201cAutomating fuzz test generation to improve the security of the controller area network.\u201d|\n|[26]|G.", "mimetype": "text/plain", "start_char_idx": 3388, "end_char_idx": 6509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58a8dc5f-c7d3-43ce-aee1-c13b8812a2d9": {"__data__": {"id_": "58a8dc5f-c7d3-43ce-aee1-c13b8812a2d9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1cd878b3-5851-466f-8859-adbc4ecd5abc", "node_type": "4", "metadata": {}, "hash": "d50be5f7cb0995f211c6f14ea1489cd11b2abd006bb39ccb7dce74bb90eef605", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e79ad8fb-0856-4679-a88e-2e3e4c203205", "node_type": "1", "metadata": {}, "hash": "393fbadf1cd21ab7d3e876faf3f474e515fe6f3f0039f16b4a60627403b552f6", "class_name": "RelatedNodeInfo"}}, "text": "Bayer and A. Ptok, \u201cDon\u2019t fuss about fuzzing: Fuzzing controllers in vehicular networks,\u201d 13th escar Europe, p. 88, 2015.|\n|[24]|R. Nishimura, R. Kurachi, K. Ito, T. Miyasaka, M. Yamamoto, and M. Mishima, \u201cImplementation of the can-fd protocol in the fuzzing tool bestorm,\u201d in 2016 IEEE International Conference on Vehicular Electronics and Safety (ICVES). IEEE, 2016, pp. 1\u20136.|\n|[25]|D. S. Fowler, J. Bryans, and S. Shaikh, \u201cAutomating fuzz test generation to improve the security of the controller area network.\u201d|\n|[26]|G. Banks, M. Cova, V. Felmetsger, K. Almeroth, R. Kemmerer, and G. Vigna, \u201cSnooze: toward a stateful network protocol fuzzer,\u201d in International Conference on Information Security. Springer, 2006, pp. 343\u2013358.|\n|[27]|S. Gorbunov and A. Rosenbloom, \u201cAutofuzz: Automated network protocol fuzzing framework,\u201d IJCSNS, vol. 10, no. 8, p. 239, 2010.|\n|[28]|H. Dantas, Z. Erkin, C. Doerr, R. Hallie, and G. v. d. Bij, \u201cefuzz: A fuzzer for dlms/cosem electricity meters,\u201d in Proceedings of the 2nd Workshop on Smart Energy Grid Security. ACM, 2014, pp. 31\u201338.|\n|[29]|G. Klees, A. Ruef, B. Cooper, S. Wei, and M. Hicks, \u201cEvaluating fuzz testing,\u201d in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2018, pp. 2123\u20132138.|", "mimetype": "text/plain", "start_char_idx": 5985, "end_char_idx": 7260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc24fa4e-1de9-4f81-9956-8aaf42f92e37": {"__data__": {"id_": "fc24fa4e-1de9-4f81-9956-8aaf42f92e37", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8883192f-bd3d-4d39-9db0-3c6b53d2c370", "node_type": "4", "metadata": {}, "hash": "2c2c961fa9e40606b5bfe52bd02b43b536b4ba1e3b9967d39e30c7f0a8bb898b", "class_name": "RelatedNodeInfo"}}, "text": "# Toolformer: Language Models Can Teach Themselves to Use Tools\n\n# Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec\u2020, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom\n\n# Meta AI Research \u2020Universitat Pompeu Fabra\n\n# Abstract\n\nLanguage models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n\n# 1 Introduction\n\nLarge language models achieve impressive zero- and few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent capabilities (Wei et al., 2022). However, all of these models have several inherent limitations that can at best be partially addressed by further scaling. These limitations include an inability to access up-to-date information on recent events (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022).\n\nThe New England Journal of Medicine is a registered trademark of [QA(\u201cWho is the publisher of The New England Journal of Medicine?\u201d) \u2192 Massachusetts Medical Society] the MMS.\n\nOut of 1400 participants, 400 (or [Calculator(400 / 1400) \u2192 0.29] 29%) passed the test.\n\nThe name derives from \u201cla tortuga\u201d, the Spanish word for [MT(\u201ctortuga\u201d) \u2192 turtle] turtle.\n\nThe Brown Act is California\u2019s law [WikiSearch(\u201cBrown Act\u201d) \u2192 The Ralph M. Brown Act is an act of the California State Legislature that guarantees the public's right to attend and participate in meetings of local legislative bodies.] that requires legislative bodies, like city councils, to hold their meetings open to the public.\n\n# Figure 1\n\nExemplary predictions of Toolformer. The model autonomously decides to call different APIs (from top to bottom: a question answering system, a calculator, a machine translation system, and a Wikipedia search engine) to obtain information that is useful for completing a piece of text.\n\nA simple way to overcome these limitations of today\u2019s language models is to give them the ability to use external tools such as search engines, calculators, or calendars. However, existing approaches either rely on large amounts of human annotations (Komeili et al., 2022; Thoppilan et al., 2022) or limit tool use to task-specific settings only (e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs. Therefore, we propose Toolformer, a model that learns to use tools in a novel way, which fulfills the following desiderata:\n\n- The use of tools should be learned in a self-supervised way without requiring large amounts of human annotations. This is impor-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ad2b352-cb24-4dd5-b871-e751654a4bd1": {"__data__": {"id_": "3ad2b352-cb24-4dd5-b871-e751654a4bd1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a57814ee-8ef7-4d20-a9cd-32cebb57863a", "node_type": "4", "metadata": {}, "hash": "ee0945a450bd2685963ee59376cd987173507be198b3022b13066a4d640b6f83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1b3832d-29f2-4e6c-93cf-c7caac1efd18", "node_type": "1", "metadata": {}, "hash": "648bb32a52f80b19806bca9e7ab177ebc6df73f6d70b312a3c25abb57ccfe9cb", "class_name": "RelatedNodeInfo"}}, "text": "# LM Dataset\n\n# Sample API Calls\n\n|Input|API Call|Response|Filtered Result|\n|---|---|---|---|\n|x1:i-1 = Pittsburgh is|ci1 = What other name is Pittsburgh known by?|ri1 = Steel City|L(ci1 Steel City)i\u2192 x* Pittsburgh is= also known as|\n|xi:n = the Steel City|ci2 = Which country is Pittsburgh in?|ri2 = United States|L(ci2 [QA(What \u2026? i \u2192 United States) \u2192 Steel City)]|\n\nFigure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we first sample a position i and corresponding API call candidates ci1, ci2, . . . , ci k. We then execute these API calls and filter out all calls which do not reduce the loss Li over the next tokens. All remaining API calls are interleaved with the original text, resulting in a new text x\u2217.\n\nTant not only because of the costs associated with such annotations, but also because what humans find useful may be different from what a model finds useful.\n\n- The LM should not lose any of its generality and should be able to decide for itself when and how to use which tool. In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to specific tasks.\n\nOur approach for achieving these goals is based on the recent idea of using large LMs with in-context learning (Brown et al., 2020) to generate entire datasets from scratch (Schick and Sch\u00fctze, 2021b; Honovich et al., 2022; Wang et al., 2022): Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, we finetune the LM itself on the API calls that it considers useful. As illustrated in Figure 1, through this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which tool to use when and how.\n\nAs our approach is agnostic of the dataset being used, we can apply it to the exact same dataset that was used to pretrain a model in the first place. This ensures that the model does not lose any of its generality and language modeling abilities. We conduct experiments on a variety of different downstream tasks, demonstrating that after learning to use tools, Toolformer, which is based on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much stronger zero-shot results, clearly outperforming a much larger GPT-3 model (Brown et al., 2020) and several other baselines on various tasks.\n\n# 2 Approach\n\nOur aim is to equip a language model M with the ability to use different tools by means of API calls. We require that inputs and outputs for each API can be represented as text sequences. This allows seamless insertion of API calls into any given text, using special tokens to mark the start and end of each such call. We represent each API call as a tuple c = (ac, ic) where ac is the name of the API and ic is the corresponding input. Given an API call c with a corresponding result r, we denote the linearized sequences of the API call not including and including its result, respectively, as:\n\ne(c) = &lt;API&gt; ac(ic) &lt;/API&gt;\n\ne(c, r) = &lt;API&gt; ac(ic) \u2192 r &lt;/API&gt;\n\nwhere \u201c&lt;API&gt;\u201d, \u201c&lt;/API&gt;\u201d and \u201c\u2192\u201d are special tokens.1 Some examples of linearized API calls inserted into text sequences are shown in Figure 1.\n\nGiven a dataset C = {x1, . . . , x|C|} of plain texts, we first convert this dataset into a dataset C\u2217 augmented with API calls. This is done in three steps, illustrated in Figure 2: First, we exploit the in-context learning ability of M to sample a large number of potential API calls. We then execute these API calls and finally check whether the obtained responses are helpful for predicting future tokens; this is used as a filtering criterion. After filtering, we merge API calls for different tools, resulting in the augmented dataset C\u2217, and finetune.\n\nIn practice, we use the token sequences \u201c [\u201d, \u201c]\u201d and \u201c->\u201d to represent \u201c&lt;API&gt;\u201d, \u201c&lt;/API&gt;\u201d and \u201c\u2192\u201d, respectively. This enables our approach to work without modifying the existing LM\u2019s vocabulary.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1b3832d-29f2-4e6c-93cf-c7caac1efd18": {"__data__": {"id_": "d1b3832d-29f2-4e6c-93cf-c7caac1efd18", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a57814ee-8ef7-4d20-a9cd-32cebb57863a", "node_type": "4", "metadata": {}, "hash": "ee0945a450bd2685963ee59376cd987173507be198b3022b13066a4d640b6f83", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ad2b352-cb24-4dd5-b871-e751654a4bd1", "node_type": "1", "metadata": {}, "hash": "e61f05cc47216b739ce9d4f6229342e5eb588716063e6b76193c3d0fd55d7b31", "class_name": "RelatedNodeInfo"}}, "text": "Given a dataset C = {x1, . . . , x|C|} of plain texts, we first convert this dataset into a dataset C\u2217 augmented with API calls. This is done in three steps, illustrated in Figure 2: First, we exploit the in-context learning ability of M to sample a large number of potential API calls. We then execute these API calls and finally check whether the obtained responses are helpful for predicting future tokens; this is used as a filtering criterion. After filtering, we merge API calls for different tools, resulting in the augmented dataset C\u2217, and finetune.\n\nIn practice, we use the token sequences \u201c [\u201d, \u201c]\u201d and \u201c->\u201d to represent \u201c&lt;API&gt;\u201d, \u201c&lt;/API&gt;\u201d and \u201c\u2192\u201d, respectively. This enables our approach to work without modifying the existing LM\u2019s vocabulary. For reasons of readability, we still refer to them as \u201c&lt;API&gt;\u201d, \u201c&lt;/API&gt;\u201d and \u201c\u2192\u201d throughout this section.", "mimetype": "text/plain", "start_char_idx": 3420, "end_char_idx": 4303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0b658ca-9ab2-42de-b24b-a3d59d920b3c": {"__data__": {"id_": "f0b658ca-9ab2-42de-b24b-a3d59d920b3c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cea75d86-480b-43d7-aeb0-259c84c50a44", "node_type": "4", "metadata": {}, "hash": "9d9359a80f697bd6e202981ed3009e2b6b63af799d4ab102fdb8fa489d5026e1", "class_name": "RelatedNodeInfo"}}, "text": "# Executing API Calls\n\nYour task is to add calls to a Question Answering API to a piece of text. The questions should help you get information required to complete the text. You can call the API by writing \"[QA(question)]\" where \"question\" is the question you want to ask. Here are some examples of API calls:\n\n# Input:\n\nJoe Biden was born in Scranton, Pennsylvania.\n\n# Output:\n\nJoe Biden was born in [QA(\"Where was Joe Biden born?\")] Scranton, [QA(\"In which state is Scranton?\")] Pennsylvania.\n\n# Input:\n\nCoca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola Company.\n\n# Output:\n\nCoca-Cola, or [QA(\"What other name is Coca-Cola known by?\")] Coke, is a carbonated soft drink manufactured by [QA(\"Who manufactures Coca-Cola?\")] the Coca-Cola Company.\n\n# Input:\n\nx\n\n# Output:\n\n# Filtering API Calls\n\nLet i be the position of the API call ci in the sequence x = x1, . . . , xn, and let ri be the response from the API. Further, given a sequence (wi | i \u2208 N) of weights, let\n\nL(z) = \u2212\u2211wj\u2212i \u00b7 log pM (xj | z, x1:j\u22121)n\n\nbe the weighted cross entropy loss for M over the tokens x, . . . , xn if the model is prefixed with z.\n\nWe compare two different instantiations of this loss:\n\nLi = Li(e(ci, ri))\n\nLi = min (Li(\u03b5), L(e(ci, \u03b5)))\n\nwhere \u03b5 denotes an empty sequence. The former is the weighted loss over all tokens xi, . . . , xn if the API call and its result are given to M as a prefix; the latter is the minimum of the losses obtained from (i) doing no API call at all and (ii) doing an API call, but not providing the response. Intuitively, an API call is helpful to M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens, compared to not receiving the API call at all, or receiving only its input.\n\n# Sampling API Calls\n\nFor each API, we write a prompt P (x) that encourages the LM to annotate an example x = x1, . . . , xn with API calls. An example of such a prompt for a question answering tool is shown in Figure 3; all prompts used are shown in Appendix A.2. Let pM (zn+1 | z1, . . . , zn) be the probability that M assigns to token zn+1 as a continuation for the sequence z1, . . . , zn. We first sample up to k candidate positions for doing API calls by computing, for each i \u2208 {1, . . . , n}, the probability\n\npi = pM (<API> | P (x), x1:i\u22121)\n\nthat M assigns to starting an API call at position i. Given a sampling threshold \u03c4s, we keep all positions I = {i | pi > \u03c4s}; if there are more than k such positions, we only keep the top k.\n\nFor each position i \u2208 I, we then obtain up to m1, . . . , ci m by sampling from M given the sequence [P (x), x1, . . . , xi\u22121, <API>] as a prefix and </API> as an end-of-sequence token.\n\nWe provide e(c, r) as a prefix instead of inserting it at position i because M is not yet finetuned on any examples containing API calls, so inserting it in the middle of x would interrupt the flow and not align with patterns in the pretraining corpus, thus hurting perplexity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "160351b5-0d0a-49f6-b976-42352f557a6f": {"__data__": {"id_": "160351b5-0d0a-49f6-b976-42352f557a6f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13207cf7-80cc-45b9-8997-079d14cc8bd7", "node_type": "4", "metadata": {}, "hash": "0db40346e4092cb54ae318e0c89a80cc72035b80e890568e2ef0021fc695153f", "class_name": "RelatedNodeInfo"}}, "text": "# 3 Tools\n\nWe explore a variety of tools to address different shortcomings of regular LMs. The only constraints we impose on these tools is that (i) both their inputs and outputs can be represented as text sequences, and (ii) we can obtain a few demonstrations of their intended use. Concretely, we explore the following five tools: a question answering system, a Wikipedia search engine, a calculator, a calendar, and a machine translation system. Some examples of potential calls and return strings for the APIs associated with each of these tools are shown in Table 1. We briefly discuss all tools below; further details can be found in Appendix A.\n\n# Question Answering\n\nOur first tool is a question answering system based on another LM that can answer simple factoid questions. Specifically, we use Atlas (Izacard et al., 2022), a retrieval-augmented LM finetuned on Natural Questions (Kwiatkowski et al., 2019).\n\n# Calculator\n\nAs a second tool, we use a calculator that can perform simple numeric calculations; we only support the four basic arithmetic operations. Results are always rounded to two decimal places.\n\n# Wikipedia Search\n\nOur third tool is a search engine that, given a search term, returns short text snippets from Wikipedia. Compared to our question answering tool, this search enables a model to get more comprehensive information on a subject, but requires it to extract the relevant parts by itself. As our search engine, we use a BM25 retriever (Robertson et al., 1995; Baeza-Yates et al., 1999) that indexes the Wikipedia dump from KILT (Petroni et al., 2021).\n\n# Machine Translation System\n\nOur fourth tool is a machine translation system based on a LM that can translate a phrase from any language into English. More concretely, we use the 600M parameter NLLB (Costa-juss\u00e0 et al., 2022) as our multilingual machine translation model that works for 200 languages (including low-resource ones). The source language is automatically detected using the fastText classifier (Joulin et al., 2016), while the target language is always set to English.\n\n# Calendar\n\nOur final tool is a calendar API that, when queried, returns the current date without taking any input. This provides temporal context for predictions that require some awareness of time.\n\n# 4 Experiments\n\nWe investigate whether our approach enables a model to use tools without any further supervision and to decide for itself when and how to call which of the available tools. To test this, we select a variety of downstream tasks where we assume at least one of the considered tools to be useful, and evaluate performance in zero-shot settings (Section 4.2). Beyond that, we also ensure that our approach does not hurt the model\u2019s core language modeling abilities; we verify this by looking at perplexity on two language modeling datasets (Section 4.3). Finally, we investigate how the ability to learn using tools is affected by model size (Section 4.4).\n\n# 4.1 Experimental Setup\n\nDataset Generation Throughout all of our experiments, we use a subset of CCNet (Wenzek et al., 2020) as our language modeling dataset C and GPT-J (Wang and Komatsuzaki, 2021) as our language model M. To reduce the computational cost of annotating C with API calls, we define heuristics for some APIs to get a subset of C for which API calls are more likely to be helpful than for an average text. For example, we only consider texts for the calculator tool if they contain at least three numbers. Details of the heuristics used are given in", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cc6ad34-a930-41e9-9e98-1733168b1f41": {"__data__": {"id_": "6cc6ad34-a930-41e9-9e98-1733168b1f41", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27d57489-58df-48c0-a64d-784b7f0f3bd0", "node_type": "4", "metadata": {}, "hash": "95b68d9b6cdb3f0a49b38d5e7e42a72acb111d735c9b87ffe936c2365a27bffb", "class_name": "RelatedNodeInfo"}}, "text": "# API Name\n\n|API|Example Input|\n|---|---|\n|Question Answering|Where was the Knights of Columbus founded?|\n|Wikipedia Search|Fishing Reel Types|\n|Calculator|27 + 4 * 2|\n|Calendar|\u03b5|\n|Machine Translation|s\u00fbret\u00e9 nucl\u00e9aire|\n\n# Example Output\n\nNew Haven, Connecticut\n\nSpin fishing > Spin fishing is distinguished between fly fishing and bait cast fishing by the type of rod and reel used. There are two types of reels used when spin fishing, the open faced reel and the closed faced reel.\n\nToday is Monday, January 30, 2023.\n\nnuclear safety\n\n# Table 1: Examples of inputs and outputs for all APIs used.\n\n# Table 2: Number of examples with API calls in C\u2217 for different values of our filtering threshold \u03c4f.\n\n|API|\u03c4f = 0.5|\u03c4f = 1.0|\u03c4f = 2.0|\n|---|---|---|---|\n|Question Answering|51,987|18,526|5,135|\n|Wikipedia Search|207,241|60,974|13,944|\n|Calculator|3,680|994|138|\n|Calendar|61,811|20,587|3,007|\n|Machine Translation|3,156|1,034|229|\n\n# Appendix A\n\nFor obtaining C\u2217 from C, we perform all steps described in Section 2 and additionally filter out all examples for which all API calls were eliminated in the filtering step.\n\nFor the weighting function, we use wt = \u2211 w\u02dct with \u02dct = max(0, 1 \u2212 0.2 \u00b7 t)w to make sure that API calls happen close to where the information provided by the API is actually helpful for the model. The thresholds \u03c4s and \u03c4f are chosen individually for each tool to ensure a sufficiently larger number of examples; see Appendix A for details.\n\n# Model Finetuning\n\nWe finetune M on C\u2217 using a batch size of 128 and a learning rate of 1 \u00b7 10\u22125 with linear warmup for the first 10% of training. Details of our finetuning procedure are given in Appendix B.\n\n# Baseline Models\n\nThroughout the remainder of this section, we mainly compare the following models:\n\nFor most tasks, we additionally compare to OPT (66B) (Zhang et al., 2022) and GPT-36 (175B) (Brown et al., 2020), two models that are about 10 and 25 times larger than our other baseline models, respectively.\n\n# 4.2 Downstream Tasks\n\nWe evaluate all models on a variety of downstream tasks. In all cases, we consider a prompted zero-shot setup \u2013 i.e., models are instructed to solve each task in natural language, but we do not provide any in-context examples. This is in contrast to prior work on tool use (e.g., Gao et al., 2022; Parisi et al., 2022), where models are provided with dataset-specific examples of how a tool can be used to solve a concrete task.\n\nWe choose the more challenging zero-shot setup as we are interested in seeing whether Toolformer works in precisely those cases where a user does not specify in advance which tools should be used in which way for solving a specific problem.\n\nWe use standard greedy decoding, but with one modification for Toolformer: We let the model start an API call not just when &lt;API&gt; is the most likely.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb5e8f63-1d9c-4bcf-bbcd-da255b66ada3": {"__data__": {"id_": "fb5e8f63-1d9c-4bcf-bbcd-da255b66ada3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ffef089-a4de-4b8f-a525-488eee533914", "node_type": "4", "metadata": {}, "hash": "99e226e0fa99df284ae15f3020c20d279345ae1541dc01878f55801b3ca7ec65", "class_name": "RelatedNodeInfo"}}, "text": "# 4.2.1 LAMA\n\nWe evaluate our models on the SQuAD, Google-RE and T-REx subsets of the LAMA benchmark (Petroni et al., 2019). For each of these subsets, the task is to complete a short statement with a missing fact (e.g., a date or a place). As LAMA was originally designed to evaluate masked language models (e.g., Devlin et al., 2019), we filter out examples where the mask token is not the final token, so that the remaining examples can be processed in a left-to-right fashion. To account for different tokenizations and added complexity from not informing the model that a single word is required, we use a slightly more lenient evaluation criterion than exact match and simply check whether the correct word is within the first five words predicted by the model. As LAMA is based on statements obtained directly from Wikipedia, we prevent Toolformer from using the Wikipedia Search API to avoid giving it an unfair advantage.\n\nResults for all models can be seen in Table 3. All GPT-J models without tool use achieve similar performance. Crucially, Toolformer clearly outperforms these baseline models, improving upon the best baseline by 11.7, 5.2 and 18.6 points, respectively. It also clearly outperforms OPT (66B) and GPT-3 (175B), despite both models being much larger. This is achieved because the model independently decides to ask the question answering tool for the required information in almost all cases (98.1%); for only very few examples, it uses a different tool (0.7%) or no tool at all (1.2%).\n\n# 4.2.2 Math Datasets\n\nWe test mathematical reasoning abilities on ASDiv (Miao et al., 2020), SVAMP (Patel et al., 2021) and the MAWPS benchmark (Koncel-Kedziorski et al., 2016). We again account for the fact that we test all models in a zero-shot setup by using a more lenient evaluation criterion: As the required output is always a number, we simply check for the first number predicted by the model.\n\n**Table 3: Results on subsets of LAMA. Toolformer uses the question answering tool for most examples, clearly outperforming all baselines of the same size and achieving results competitive with GPT-3 (175B).**\n|Model|SQuAD|Google-RE|T-REx|\n|---|---|---|---|\n|GPT-J|17.8|4.9|31.9|\n|GPT-J + CC|19.2|5.6|33.2|\n|Toolformer (disabled)|22.1|6.3|34.9|\n|Toolformer|33.8|11.5|53.5|\n|OPT (66B)|21.6|2.9|30.1|\n|GPT-3 (175B)|26.8|7.0|39.8|\n\n**Table 4: Results for various benchmarks requiring mathematical reasoning. Toolformer makes use of the calculator tool for most examples, clearly outperforming even OPT (66B) and GPT-3 (175B).**\n|Model|ASDiv|SVAMP|MAWPS|\n|---|---|---|---|\n|GPT-J|7.5|5.2|9.9|\n|GPT-J + CC|9.6|5.0|9.3|\n|Toolformer (disabled)|14.8|6.3|15.0|\n|Toolformer|40.4|29.4|44.0|\n|OPT (66B)|6.0|4.9|7.9|\n|GPT-3 (175B)|14.0|10.0|19.8|\n\n# 4.2.3 Question Answering\n\nWe look at Web Questions (Berant et al., 2013), Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017), the three question answering datasets considered by Brown et al. (2020). For evaluation, we check whether the first 20 words predicted by a model contain the correct answer instead of requiring an exact match. For Toolformer, we disable the question answering tool as an exception to this is if the model\u2019s prediction contains an equation (e.g., \u201cThe correct answer is 5+3=8\u201d), in which case we consider the first number after the \u201c=\u201d sign to be its prediction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1caa19d0-3bc2-4a72-98ff-d5378e7ced30": {"__data__": {"id_": "1caa19d0-3bc2-4a72-98ff-d5378e7ced30", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4282d3af-4e60-4356-8ad2-9ec5b836bd06", "node_type": "4", "metadata": {}, "hash": "bce4c02837a8ff6d51c138a54343ba3916ae72c6a749f0adf09e38afb03c0605", "class_name": "RelatedNodeInfo"}}, "text": "# 4.2.4 Multilingual Question Answering\n\nWe evaluate Toolformer and all baseline models on MLQA (Lewis et al., 2019), a multilingual question-answering benchmark. A context paragraph for each question is provided in English, while the question can be in Arabic, German, Spanish, Hindi, Vietnamese, or Simplified Chinese. In order to solve the task, the model needs to be able to understand both the paragraph and the question, so it may benefit from translating the question into English. Our evaluation metric is the percentage of times the model\u2019s generation, capped at 10 words, contains the correct answer.\n\nResults are shown in Table 6. Using API calls consistently improves Toolformer\u2019s performance for all languages, suggesting that it has learned to make use of the machine translation tool. Depending on the language, this tool is used for 63.8% to 94.9% of all examples; the only exception to this is Hindi, for which the machine translation tool is used in only 7.3% of cases. However, Toolformer does not consistently outperform vanilla GPT-J. This is mainly because for some languages, finetuning on CCNet deteriorates performance; this might be due to a distribution shift compared to GPT-J\u2019s original pretraining data.\n\n|Model|Es|De|Hi|Vi|Zh|Ar| |\n|---|---|---|---|---|---|---|---|\n|GPT-J|15.2|16.5|1.3|8.2|18.2|8.2| |\n|GPT-J + CC|15.7|14.9|0.5|8.3|13.7|4.6| |\n|Toolformer (disabled)|19.8|11.9| |1.2|10.1|15.0|3.1|\n|Toolformer|20.6|13.5|1.4|10.6|16.8|3.7| |\n|OPT (66B)|0.3|0.1|1.1|0.2|0.7|0.1| |\n|GPT-3 (175B)|3.4|1.1|0.1|1.7|17.7|0.1| |\n|GPT-J (All En)|24.3|27.0|23.9|23.3|23.1|23.6| |\n|GPT-3 (All En)|24.7|27.2|26.1|24.9|23.6|24.0| |\n\nTable 6: Results on MLQA for Spanish (Es), German (De), Hindi (Hi), Vietnamese (Vi), Chinese (Zh) and Arabic (Ar). While using the machine translation tool to translate questions is helpful across all languages, further pretraining on CCNet deteriorates performance; consequently, Toolformer does not consistently outperform GPT-J. The final two rows correspond to models that are given contexts and questions in English.\n\nOPT and GPT-3 perform surprisingly weak across all languages, mostly because they fail to provide an answer in English despite being instructed to do so. A potential reason for GPT-J not suffering from this problem is that it was trained on more multilingual data than both OPT and GPT-3, including the EuroParl corpus (Koehn, 2005; Gao et al., 2020). As an upper bound, we also evaluate GPT-J and GPT-3 on a variant of MLQA where both the context and the question are provided in English. In this setup, GPT-3 performs better than all other models, supporting our hypothesis that its subpar performance on MLQA is due to the multilingual aspect of the task.\n\n# 4.2.5 Temporal Datasets\n\nTo investigate the calendar API\u2019s utility, we evaluate all models on TEMPLAMA (Dhingra et al., 2022) and a new dataset that we call DATESET. TEMPLAMA is a dataset built from Wikidata that contains cloze queries about facts that change with time (e.g., \u201cCristiano Ronaldo plays for ___\u201d) as well as the correct answer for the years between 2010 and 2020. DATESET, described in Appendix D, is also generated through a series of templates, but populated using a combination of random dates/durations (e.g., \u201cWhat day of the week was it 30 days ago?\u201d). Critically, knowing the current date is required to answer these questions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31aec8b5-35a1-4ca5-8be4-606298653879": {"__data__": {"id_": "31aec8b5-35a1-4ca5-8be4-606298653879", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc275abf-a109-47c0-b1d2-15d0d38f05db", "node_type": "4", "metadata": {}, "hash": "a86415da615d60465fa29ad36e4b15e9c5bea5e6acf570f38bac654df4db5a95", "class_name": "RelatedNodeInfo"}}, "text": "# Table 7: Results for the temporal datasets.\n\n|Model|TEMPLAMA|DATESET|\n|---|---|---|\n|GPT-J|13.7|3.9|\n|GPT-J + CC|12.9|2.9|\n|Toolformer (disabled)|12.7|5.9|\n|Toolformer|16.3|27.3|\n|OPT (66B)|14.5|1.3|\n|GPT-3 (175B)|15.5|0.8|\n\nToolformer outperforms all baselines, but does not make use of the calendar tool for TEMPLAMA.\n\nFor both tasks, we use the same evaluation as for the original LAMA dataset. Results shown in Table 7 illustrate that Toolformer outperforms all baselines for both TEMPLAMA and DATESET. However, closer inspection shows that improvements on TEMPLAMA cannot be attributed to the calendar tool, which is only used for 0.2% of all examples, but mostly to the Wikipedia search and question answering tools, which Toolformer calls the most. This makes sense given that named entities in TEMPLAMA are often so specific and rare that even knowing the exact date alone would be of little help. The best course of action for this dataset \u2013 first querying the calendar API to get the current date, and then querying the question answering system with this date \u2013 is not only prohibited by our restriction of using at most one API call per example, but also hard to learn for Toolformer given that all API calls in its training data are sampled independently.\n\nFor DATESET, on the other hand, the considerable improvement of Toolformer compared to other models can be fully accredited to the calendar tool, which it makes use of for 54.8% of all examples.\n\n# 4.3 Language Modeling\n\nIn addition to verifying improved performance on various downstream tasks, we also want to ensure that language modeling performance of Toolformer does not degrade through our finetuning with API calls. To this end, we evaluate our models on two language modeling datasets: WikiText (Merity et al., 2017) and a subset of 10,000 randomly selected documents from CCNet (Wenzek et al., 2020) that were not used during training. Perplexities of various models are shown in Table 8. As one would expect, finetuning on CCNet leads to slightly improved performance on a different CCNet subset, but it slightly deteriorates performance on WikiText, presumably because the original pre-training data for GPT-J is more similar to WikiText than our randomly selected subset of CCNet. Most importantly, however, training on C* (our dataset annotated with API calls) does not lead to an increase in perplexity compared to training on C when API calls are disabled at inference time.\n\n# Table 8: Perplexities of different models on WikiText and our validation subset of CCNet.\n\n|Model|WikiText|CCNet|\n|---|---|---|\n|GPT-J|9.9|10.6|\n|GPT-J + CC|10.3|10.5|\n|Toolformer (disabled)|10.3|10.5|\n\nAdding API calls comes without a cost in terms of perplexity for language modeling without any API calls.\n\n# 4.4 Scaling Laws\n\nWe investigate how the ability to ask external tools for help affects performance as we vary the size of our LM. To this end, we apply our approach not just to GPT-J, but also to four smaller models from the GPT-2 family (Radford et al., 2019), with 124M, 355M, 775M and 1.6B parameters, respectively. We do so using only a subset of three tools: the question answering system, the calculator, and the Wikipedia search engine. Apart from this, we follow the experimental setup described in Section 4.1.\n\nFigure 4 shows that the ability to leverage the provided tools only emerges at around 775M parameters: smaller models achieve similar performance both with and without tools. An exception to this is the Wikipedia search engine used mostly for QA benchmarks; we hypothesize that this is because the API is comparably easy to use. While models become better at solving tasks without API calls as they grow in size, their ability to make good use of the provided API improves at the same time. As a consequence, there remains a large gap between predictions with and without API calls even for our biggest model.\n\n# 5 Analysis\n\nDecoding Strategy We investigate the effect of our modified decoding strategy introduced in Section 4.2, where instead of always generating the token xt given x1, . . . , xt\u22121 would require marginalizing over all potential API calls that the model could make at position t, which is intractable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e32392f5-2c26-48b8-aa48-4373917f9ad5": {"__data__": {"id_": "e32392f5-2c26-48b8-aa48-4373917f9ad5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51209ac6-cbd7-4a11-909a-bd7205f89928", "node_type": "4", "metadata": {}, "hash": "00ac25586f113133cc06d4fb2eb66132d05ea52181cfba0e2689ed58d2d6b406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d38e6b77-cc4a-468d-9f68-fa67e3e83c87", "node_type": "1", "metadata": {}, "hash": "e7a3778d49620a14ab3b9f76c8792f11b16f377dbd62cc53eaa22692025ed6d8", "class_name": "RelatedNodeInfo"}}, "text": "# LAMA\n\n# Math Benchmarks\n\n# QA Benchmarks\n\n| | | |Math Benchmarks| |QA Benchmarks|\n|---|---|---|---|---|---|\n|30|30| | |40| |\n|25|25| | |35| |\n|20|20|30| | | |\n|15|15| | |20| |\n|10|10| | |15| |\n| |Toolformer| | |10| |\n|5|Toolformer (disabled)|5| | | |\n|GPT3| |0| | | |\n|0|2000|4000|6000| | |\n|0|2000|4000|6000| | |\n|0|2000|4000|6000| | |\n\nFigure 4: Average performance on LAMA, our math benchmarks and our QA benchmarks for GPT-2 models of different sizes and GPT-J finetuned with our approach, both with and without API calls. While API calls are not helpful to the smallest models, larger models learn how to make good use of them. Even for bigger models, the gap between model predictions with and without API calls remains high.\n\nMost likely token, we generate the &lt;API&gt; token if it is one of the k most likely tokens. Table 9 shows performance on the T-REx subset of LAMA and on WebQS for different values of k. As expected, increasing k leads to the model doing API calls for more examples \u2013 from 40.3% and 8.5% with k = 1 (i.e., regular greedy decoding) to 98.1% and 100% for k = 10. While for T-REx, there is already a clear improvement in performance with greedy decoding, on WebQS our model only starts to make a substantial number of API calls as we slightly increase k. Interestingly, for k = 1 the model is calibrated to some extent: It decides to call APIs for examples that it would perform particularly badly on without making API calls. This can be seen from the fact that performance on examples where it decides not to make an API call (44.3 and 19.9) is higher than average performance if no API calls are made at all (34.9 and 18.9). However, this calibration is lost for higher values of k.\n\n# Data Quality\n\nWe qualitatively analyze some API calls generated with our approach for different APIs. Table 10 shows some examples of texts from CCNet augmented with API calls, as well as the corresponding score Li \u2212\u2212 Li + that is used as a filtering criterion, and whether the API calls made by the model are intuitively useful in the given context. As can be seen, high values of Li \u2212\u2212 Li typically correspond to useful API calls, whereas low values correspond to API calls that do not provide any information that is useful for predicting future tokens. There are some exceptions, e.g., an API call for \u201cFast train success\u201d in the fourth example that does not give any relevant information but still reduces perplexity. However, some amount of noise in the API calls that are not filtered can actually be useful as it forces the model finetuned on C\u2217 to not always blindly follow the results of each call it makes.\n\n| |T-REx|WebQS| | | | | | |\n|---|---|---|---|---|---|---|---|---|\n|k|All|AC|NC|%|All|AC|NC|%|\n|0|34.9|\u2013|34.9|0.0|18.9|\u2013|18.9|0.0|\n|1|47.8|53.0|44.3|40.3|19.3|17.1|19.9|8.5|\n|3|52.9|58.0|29.0|82.8|26.3|26.5|6.6|99.3|\n|10|53.5|54.0|22.5|98.1|26.3|26.4|\u2013|100.0|\n\nTable 9: Toolformer results on the T-REx subset of LAMA and on WebQS for different values of k used during decoding. Numbers shown are overall performance (All), performance on the subset where the model decides to make an API call (AC) and all remaining examples (NC), as well as the percentage of examples for which the model decides to call an API (%).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d38e6b77-cc4a-468d-9f68-fa67e3e83c87": {"__data__": {"id_": "d38e6b77-cc4a-468d-9f68-fa67e3e83c87", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51209ac6-cbd7-4a11-909a-bd7205f89928", "node_type": "4", "metadata": {}, "hash": "00ac25586f113133cc06d4fb2eb66132d05ea52181cfba0e2689ed58d2d6b406", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e32392f5-2c26-48b8-aa48-4373917f9ad5", "node_type": "1", "metadata": {}, "hash": "d9273460b912981f9a86228a74bf99a55bde4f17a69f63e4170734af3f302c88", "class_name": "RelatedNodeInfo"}}, "text": "Numbers shown are overall performance (All), performance on the subset where the model decides to make an API call (AC) and all remaining examples (NC), as well as the percentage of examples for which the model decides to call an API (%).\n\n# Related Work\n\n# Language Model Pretraining\n\nThere are various approaches that augment language models with some form of additional textual information during pretraining, including various forms of metadata (Keskar et al., 2019), HTML tags (Aghajanyan et al., 2021), Wikipedia markup (Schick et al., 2022), or related texts obtained from an information retrieval system (Guu et al., 2020; Borgeaud et al., 2021; Izacard et al., 2022). For all of these", "mimetype": "text/plain", "start_char_idx": 3019, "end_char_idx": 3712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05181800-79a3-481f-ab2a-fe3d08d2e5b0": {"__data__": {"id_": "05181800-79a3-481f-ab2a-fe3d08d2e5b0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ddaad7f-9806-4bdb-894b-505a9065a993", "node_type": "4", "metadata": {}, "hash": "0ff47a7a558a3e4bf49e1ba57039e76451cad599b67017da21bdbb0bd616c2f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97bbba19-6b25-4c7f-aa99-a7c329a705f8", "node_type": "1", "metadata": {}, "hash": "e5013c3cc2d59ad20308d4ab2346f3c5cbee47d711f4acd5671413a9beeaeb17", "class_name": "RelatedNodeInfo"}}, "text": "# Example\n\n# Useful\n\nThe Flodden Window (a war memorial dedicated to The Middleton Archers), in the Grade I-listed Church of St Leonard in Middleton is said to be the oldest war memorial in the United Kingdom. <API> WikiSearch(War memorial Flodden) \u2192 Battle of Flodden > Commemoration > The stained-glass Flodden Window in Middleton Parish Church [. . . ] was constructed by Sir Richard Assheton in memory of the Battle of Flodden and the archers from Middleton who fought in it. </API> Sir Richard Assheton of Middleton (who built St Leonard) was granted knighthood [. . . ]\n\nNote: The WL will be open on Friday, <API> Calendar() \u2192 Today is Thursday, March 9, 2017. </API> March 10, and Sunday, March 19 for regular hours.\n\nThe Nile has an approximate length of <API> QA(What is the approximate length of the Nile?) \u2192 6,853 km </API> 6,853 kilometers, the White Nile being its main source.\n\nIf Venus had an atmosphere similar to Earth\u2019s then you would expect Venus\u2019 mean temperature to be 499 K (1.74 x 287) rather than 735 K which is <API> Calculator(735 / 499) \u2192 1.47 </API> 1.47 (735 / 499) times hotter than it should be.\n\nYou are here: Home / Featured / Catch this fast train to success! <API> WikiSearch(Fast train success) \u2192 Fast Train > It also peaked at #23 on the Canadian CHUM singles chart, on June 26, 1971. The success of this single established Myles Goodwyn as the band\u2019s main songwriter, and made it possible for April Wine to record a second album. </API> Don\u2019t wait weeks and [. . .]\n\nOs Melhores Escolas em Jersey 2020 <API> MT(Os Melhores Escolas em Jersey) \u2192 The Best Schools in Jersey </API> On this page you can search for Universities, Colleges and Business schools in Jersey.\n\nEnjoy these pictures from the <API> Calendar() \u2192 Today is Friday, April 19, 2013. </API> Easter Egg Hunt.\n\n85 patients (23%) were hospitalised alive and admitted to a hospital ward. Of them, <API> Calculator(85 / 23) \u2192 3.70 </API> 65% had a cardiac aetiology [. . .]\n\nBut hey, after the <API> Calendar() \u2192 Today is Saturday, June 25, 2011. </API> Disneyland fiasco with the fire drill, I think it\u2019s safe to say Chewey won\u2019t let anyone die in a fire.\n\nThe last time I was with <API> QA(Who was last time I was with?) \u2192 The Last Time </API> him I asked what he likes about me and he said he would tell me one day.\n\n# Table 10: Examples of API calls for different tools, sorted by the value of Li \u2212\u2212 Li that is used as a filtering criterion.\n\nHigh values typically correspond to API calls that are intuitively useful for predicting future tokens.\n\n# Tool Use\n\nSeveral approaches aim to equip LMs with the ability to use external tools such as search engines (Komeili et al., 2022; Thoppilan et al., 2022; Lazaridou et al., 2022; Shuster et al., 2022; Yao et al., 2022), web browsers (Nakano et al., 2021), calculators (Cobbe et al., 2021; Thoppilan et al., 2022), translation systems (Thoppilan et al., 2022) and Python interpreters (Gao et al., 2022). The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022). In contrast, the self-supervised nature of Toolformer enables it to learn how and when to use tools without requiring a specific prompt that shows task-specific examples of how a tool could be used.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97bbba19-6b25-4c7f-aa99-a7c329a705f8": {"__data__": {"id_": "97bbba19-6b25-4c7f-aa99-a7c329a705f8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ddaad7f-9806-4bdb-894b-505a9065a993", "node_type": "4", "metadata": {}, "hash": "0ff47a7a558a3e4bf49e1ba57039e76451cad599b67017da21bdbb0bd616c2f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05181800-79a3-481f-ab2a-fe3d08d2e5b0", "node_type": "1", "metadata": {}, "hash": "feedbd39a3140d75773689b39d8a55354d7434525a4e34c77631ed9bc51c1d48", "class_name": "RelatedNodeInfo"}}, "text": "The way these models learn to use tools can roughly be divided into two approaches: Either they rely on large amounts of human supervision (Komeili et al., 2022; Nakano et al., 2021; Thoppilan et al., 2022) or they work by prompting the language model in a few-shot setup tailored towards a specific task where it is known a priori which tools needs to be used (Gao et al., 2022; Lazaridou et al., 2022; Yao et al., 2022). In contrast, the self-supervised nature of Toolformer enables it to learn how and when to use tools without requiring a specific prompt that shows task-specific examples of how a tool could be used. Perhaps most closely related to our work is TALM (Parisi et al., 2022), an approach that uses a similar self-supervised objective for teaching a model to use a calculator and a search engine, but explores this only in settings where a model is finetuned for downstream tasks.\n\n# Bootstrapping\n\nThe idea of using self-training and bootstrapping techniques to improve models has been investigated in various contexts, ranging from word sense disambiguation (Yarowsky, 1995), relation extraction (Brin, 1999; Agichtein and Gravano, 2000), parsing (McClosky et al., 2006; Reichart and Rappoport, 2007), sequence generation (He et al., 2020), few-shot text classification.", "mimetype": "text/plain", "start_char_idx": 2954, "end_char_idx": 4243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfafd0f9-1b2a-4fbe-83d3-769c2291dc09": {"__data__": {"id_": "dfafd0f9-1b2a-4fbe-83d3-769c2291dc09", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "909bdcc3-15c2-439f-bd37-95be22277e45", "node_type": "4", "metadata": {}, "hash": "e059c3a91d43aae3151f6abcb0e11af5636a5a9560d0811d317180be5355648d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "076d8f6b-83ba-45be-8157-b0b6e0d740a0", "node_type": "1", "metadata": {}, "hash": "89e00e303d6c5825d11091575977e7775100d7fcd099f9e93dc1c06c964f953f", "class_name": "RelatedNodeInfo"}}, "text": "# 6 Limitations\n\nWhile our approach enables LMs to learn how to use a variety of tools in a self-supervised way, there are some clear limitations to what can be achieved with our method in its current form. One such limitation is the inability of Toolformer to use tools in a chain (i.e., using the output of one tool as an input for another tool). This is due to the fact that API calls for each tool are generated independently; as a consequence, there are no examples of chained tool use in the finetuning dataset. Our current approach also does not allow the LM to use a tool in an interactive way \u2013 especially for tools such as search engines, that could potentially return hundreds of different results, enabling a LM to browse through these results or to refine its search query in a similar spirit to Nakano et al. (2021) can be crucial for certain applications. Beyond this, we found models trained with Toolformer to often be sensitive to the exact wording of their input when deciding whether or not to call an API; this is perhaps unsurprising given that LMs are known to be very sensitive to the prompt they are provided with in both zero- and few-shot settings (Jiang et al., 2020; Schick and Sch\u00fctze, 2021a). Depending on the tool, our method is also very sample-inefficient; for example, processing more than a million documents results in only a few thousand examples of useful calls to the calculator API. A potential solution to this problem might be to iteratively apply our approach, similar to how this is done in related bootstrapping approaches (Schick and Sch\u00fctze, 2021a; Izacard and Grave, 2021; Parisi et al., 2022). Finally, when deciding whether or not to make an API call, Toolformer currently does not take into account the tool-dependent, computational cost incurred from making an API call.\n\n# 8 Conclusion\n\nWe have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, calculators, and translation systems via simple API calls. This is done by finetuning on a large number of sampled API calls that are filtered based on whether they\n\n# References\n\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettlemoyer. 2021. Htlm: Hyper-text pre-training and prompting of language models.\n\nEugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the Fifth ACM Conference on Digital Libraries, DL \u201900, page 85\u201394, New York, NY, USA. Association for Computing Machinery.\n\nRicardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999. Modern information retrieval, volume 463. ACM press New York.\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA. Association for Computational Linguistics.\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hen nigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2021. Improving language models by retrieving from trillions of tokens.\n\nSergey Brin. 1999. Extracting patterns and relations from the world wide web. In The World Wide Web and Databases, pages 172\u2013183, Berlin, Heidelberg. Springer Berlin Heidelberg.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "076d8f6b-83ba-45be-8157-b0b6e0d740a0": {"__data__": {"id_": "076d8f6b-83ba-45be-8157-b0b6e0d740a0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "909bdcc3-15c2-439f-bd37-95be22277e45", "node_type": "4", "metadata": {}, "hash": "e059c3a91d43aae3151f6abcb0e11af5636a5a9560d0811d317180be5355648d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfafd0f9-1b2a-4fbe-83d3-769c2291dc09", "node_type": "1", "metadata": {}, "hash": "0803a83c613879d8cd3dd68afdd2add098e6ff752bc3d46d5e871e9b23797a7c", "class_name": "RelatedNodeInfo"}}, "text": "2021. Improving language models by retrieving from trillions of tokens.\n\nSergey Brin. 1999. Extracting patterns and relations from the world wide web. In The World Wide Web and Databases, pages 172\u2013183, Berlin, Heidelberg. Springer Berlin Heidelberg.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.", "mimetype": "text/plain", "start_char_idx": 3447, "end_char_idx": 4322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b99e58f-ccb2-49ef-88ef-0ba35369304e": {"__data__": {"id_": "4b99e58f-ccb2-49ef-88ef-0ba35369304e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "606a5e5e-6633-4800-9cc1-0635f0abfa8e", "node_type": "4", "metadata": {}, "hash": "6649186643eeb19ad4da16b9c8c282b7747476ba76d38f3aa174da1e71fe95ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2debd237-4d76-4f46-888b-1db1e41e731d", "node_type": "1", "metadata": {}, "hash": "e7ebaa7d5ee5dbb61685331b49fef53aa0539198badc20eefee516c1cb539e2e", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre-training.\n\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio Ranzato. 2020. Revisiting self-training for neural sequence generation. In International Conference on Learning Representations.\n\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor.\n\nGautier Izacard and Edouard Grave. 2021. Distilling knowledge from reader to retriever for question answering. In International Conference on Learning Representations.\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval augmented language models.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022. Survey of hallucination in natural language generation. ACM Computing Surveys.\n\nMarta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10:257\u2013273.\n\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2debd237-4d76-4f46-888b-1db1e41e731d": {"__data__": {"id_": "2debd237-4d76-4f46-888b-1db1e41e731d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "606a5e5e-6633-4800-9cc1-0635f0abfa8e", "node_type": "4", "metadata": {}, "hash": "6649186643eeb19ad4da16b9c8c282b7747476ba76d38f3aa174da1e71fe95ab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b99e58f-ccb2-49ef-88ef-0ba35369304e", "node_type": "1", "metadata": {}, "hash": "7d576f9ff0b5cfa69644bf0709dbe4f6d03f85da7b92ceaee5bedeadd154038d", "class_name": "RelatedNodeInfo"}}, "text": "2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10:257\u2013273.\n\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438.\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611, Vancouver, Canada. Association for Computational Linguistics.\n\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov. 2016. Fasttext. zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.\n\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.\n\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models.\n\nPhilipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of machine translation summit x: papers, pages 79\u201386.\n\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8460\u20138478, Dublin, Ireland. Association for Computational Linguistics.", "mimetype": "text/plain", "start_char_idx": 2741, "end_char_idx": 5089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "605d7da6-ac98-4b03-bbe9-0613cb404ac4": {"__data__": {"id_": "605d7da6-ac98-4b03-bbe9-0613cb404ac4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1140389-bb3a-478e-a038-e98a3e801170", "node_type": "4", "metadata": {}, "hash": "46c65aa606d876c72ddce9aa3952f49e0ff3dac28b230a349aa070e5826f948c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6e8f320-c088-4606-bb5a-d8f5a0111dc7", "node_type": "1", "metadata": {}, "hash": "aeb2c50a7f5c321ace5d2aa50457b710bdf7f6ac3ef875f9fb4fc1f82eb022fa", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152\u20131157, San Diego, California. Association for Computational Linguistics.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466.\n\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.\n\nPatrick Lewis, Barlas O\u011fuz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2019. Mlqa: Evaluating cross-lingual extractive question answering. arXiv preprint arXiv:1910.07475.\n\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2021. Few-shot learning with multilingual language models.\n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization.\n\nDavid McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152\u2013159, New York City, USA. Association for Computational Linguistics.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In International Conference on Learning Representations.\n\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975\u2013984, Online. Association for Computational Linguistics.\n\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. Webgpt: Browser-assisted question-answering with human feedback.\n\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models.\n\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080\u20132094, Online. Association for Computational Linguistics.\n\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523\u20132544, Online. Association for Computational Linguistics.\n\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6e8f320-c088-4606-bb5a-d8f5a0111dc7": {"__data__": {"id_": "b6e8f320-c088-4606-bb5a-d8f5a0111dc7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1140389-bb3a-478e-a038-e98a3e801170", "node_type": "4", "metadata": {}, "hash": "46c65aa606d876c72ddce9aa3952f49e0ff3dac28b230a349aa070e5826f948c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "605d7da6-ac98-4b03-bbe9-0613cb404ac4", "node_type": "1", "metadata": {}, "hash": "831051f99c468ae1691c851455449653fc8da9b343aa4ab8f352b6cc9fe472be", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational Linguistics.\n\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523\u20132544, Online. Association for Computational Linguistics.\n\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China. Association for Computational Linguistics.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\nRoi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616\u2013623, Prague, Czech Republic. Association for Computational Linguistics.\n\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at trec-3. Nist Special Publication Sp, 109:109.\n\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. Peer: A collaborative language model.\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Timo Schick and Hinrich Sch\u00fctze. 2021a. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the", "mimetype": "text/plain", "start_char_idx": 3007, "end_char_idx": 5028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76c45c39-fefd-41f1-9c5f-b39e23834c3d": {"__data__": {"id_": "76c45c39-fefd-41f1-9c5f-b39e23834c3d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f90606e5-a9bf-4a1d-a445-5b774319de09", "node_type": "4", "metadata": {}, "hash": "f4174bf47dc1c896fe8ed232a689c4488747c76d46812588f58870ed107125cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34712188-7980-449e-b225-0bbf29eba6f1", "node_type": "1", "metadata": {}, "hash": "2edbb9ea8c392c95b9379193a477c4ed77ac61b7d11c44db3c7e158263903d36", "class_name": "RelatedNodeInfo"}}, "text": "# 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume\n\npages 255\u2013269, Online. Association for Computational Linguistics.\n\n# References\n\nTimo Schick and Hinrich Sch\u00fctze. 2021b. Generating datasets with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6943\u20136951, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. 2022. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage.\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications.\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models.\n\nDavid Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 189\u2013196, Cambridge, Massachusetts, USA. Association for Computational Linguistics.\n\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models.\n\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.\n\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models.\n\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34712188-7980-449e-b225-0bbf29eba6f1": {"__data__": {"id_": "34712188-7980-449e-b225-0bbf29eba6f1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f90606e5-a9bf-4a1d-a445-5b774319de09", "node_type": "4", "metadata": {}, "hash": "f4174bf47dc1c896fe8ed232a689c4488747c76d46812588f58870ed107125cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76c45c39-fefd-41f1-9c5f-b39e23834c3d", "node_type": "1", "metadata": {}, "hash": "2146a342b23647c16227a3c649c4640749095ed8c0bab0c27def2e904e6aec18", "class_name": "RelatedNodeInfo"}}, "text": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models.\n\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4003\u20134012, Marseille, France. European Language Resources Association.", "mimetype": "text/plain", "start_char_idx": 2807, "end_char_idx": 3639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd3179a6-e607-47c4-a0e6-c2ea8407a2e5": {"__data__": {"id_": "fd3179a6-e607-47c4-a0e6-c2ea8407a2e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c3e870f-d94e-47cc-b7ba-8706db2f3342", "node_type": "4", "metadata": {}, "hash": "519ae1b7a78c6798f88c719c3d05fa97dd9037a62829a3328a8ea8f984984bfc", "class_name": "RelatedNodeInfo"}}, "text": "# API Details\n\nWhen sampling and filtering API calls, by default we use values of \u03c4s = 0.05 and \u03c4f = 1.0 \u2013 i.e., we only make API calls at positions where the probability of the &lt;API&gt; token is at least 5%, and we keep API calls if they reduce the loss by at least 1.0. We only keep the top k = 5 such positions and sample up to m = 5 API calls for each position identified in a piece of text. Due to the heuristic filtering described below, we generate API calls for the calculator and machine translation system on only a small subset of C; to compensate for this, we set \u03c4s = 0.0, k = 20 and m = 10 for these tools. As the resulting sets of API calls are still comparably small, we additionally set \u03c4f = 0.5.\n\n# A.1 Implementation\n\n# Question Answering\n\nWe use the Atlas model of Izacard et al. (2022) finetuned on Natural Questions (Kwiatkowski et al., 2019) as our question answering system. For creating C* we use Atlas-large, enabling us to efficiently process millions of API calls; during inference, we use the larger Atlas-xxl model.\n\n# Calculator\n\nOur calculator is based on a simple Python script and only supports the operators \u201c+\u201d, \u201c\u2212\u201d, \u201c\u2217\u201d, and \u201c/\u201d. It does not return any result for syntactically invalid equations. For sampling API calls, we apply heuristic filters to our subset of CCNet and only process documents that either (i) contain at least three numbers within a window of 100 tokens, where one of these numbers is the result of applying a mathematical operation to the other two, (ii) contain one of the sequences \u201c=\u201d, \u201cequals\u201d, \u201cequal to\u201d, \u201ctotal of\u201d, \u201caverage of\u201d followed by a number, or (iii) contain at least three numbers; for texts that only match the last criterion, we only keep a random subset of 1%.\n\n# Calendar\n\nFor creating our dataset C*, we operate under the assumption that the calendar date in such cases should be the date that the document was created. We approximate this by extracting the date from the URL, if it is present. We filter out texts for which a date cannot be extracted, leaving around 18% of the documents.\n\n# Machine Translation\n\nFor both training and inference, we use the 600M parameter NLLB (Costa-juss\u00e0 et al., 2022) as our machine translation (MT) model. The source language is automatically detected using the fastText classifier (Joulin et al., 2016), while the target language is always set to English. Since most of the CCNet dataset is in English, we filter out the parts that contain only English text before generating API calls. More specifically, we only keep those paragraphs which contain text chunks in a language other than English preceded and followed by English text. We use text chunks of size 10 tokens. To determine whether the middle text chunk is in a language different than English we again use the fastText classifier with a confidence greater than 0.8. We also filter out any text chunks that contain only numbers or special symbols. This filtering mechanism allows us to generate data more efficiently by focusing our API call generations in places where the MT tool is likely to be helpful. After generating the MT API calls, we additionally remove from our training set those where the input to the MT tool appears after the API call but not before it. While during data generation the model can look ahead to generate API calls, this is not possible at inference time, so we want to dissuade the model from calling the API in such cases.\n\n# A.2 Prompts\n\nBelow, we list the prompts used to sample API calls for each tool considered.\n\n# Question Answering\n\nWe use the following prompt for the question answering tool:\n\nYour task is to add calls to a Question Answering API to a piece of text. The questions should help you get information required to complete the text. You can call the API by writing \"[QA(question)]\" where \"question\" is the question you want to ask. Here are some examples of API calls:\n\nInput: Joe Biden was born in Scranton, Pennsylvania.\n\nOutput: Joe Biden was born in [QA(\"Where was Joe Biden born?\")] Scranton, [QA(\"In which state is Scranton?\")] Pennsylvania.\n\nInput: Coca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola Company.\n\nOutput: Coca-Cola, or [QA(\"What other name is Coca-Cola known by?\")] Coke, is a carbonated soft drink manufactured by [QA(\"Who manufactures Coca-Cola?\")] the Coca-Cola Company.\n\n# Calculator\n\nWe use the following prompt for the calculator:\n\nYour task is to add calls to a Calculator API to a piece of text.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a0c09e1-86f3-42f2-adb3-63023afe5d99": {"__data__": {"id_": "2a0c09e1-86f3-42f2-adb3-63023afe5d99", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78b896a5-8c17-485f-acca-a5632ef5469f", "node_type": "4", "metadata": {}, "hash": "af4a93866ff402c20878b35162a447e10358994cf77b6a3958266e85eee7d61d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48d16606-c594-43de-95b7-3b79dc53d267", "node_type": "1", "metadata": {}, "hash": "bf2602b2b7fe82a85d4f9faf7d758e94eff35cea82a9ea247d63435b39d85c21", "class_name": "RelatedNodeInfo"}}, "text": "# API Calls Examples\n\n# Calculator API\n\nThe calls should help you get information required to complete the text. You can call the API by writing \"[Calculator(expression)]\" where \"expression\" is the expression to be computed. Here are some examples of API calls:\n\n# Input:\n\nThe number in the next term is 18 + 12 x 3 = 54.\n\n# Output:\n\nThe number in the next term is 18 + 12 x 3 = [Calculator(18 + 12 * 3)] 54.\n\n# Input:\n\nThe population is 658,893 people. This is 11.4% of the national average of 5,763,868 people.\n\n# Output:\n\nThe population is 658,893 people. This is 11.4% of the national average of [Calculator(658,893 / 11.4%)] 5,763,868 people.\n\n# Input:\n\nA total of 252 qualifying matches were played, and 723 goals were scored (an average of 2.87 per match). This is three times less than the 2169 goals last year.\n\n# Output:\n\nA total of 252 qualifying matches were played, and 723 goals were scored (an average of [Calculator(723 / 252)] 2.87 per match). This is twenty goals more than the [Calculator(723 - 20)] 703 goals last year.\n\n# Input:\n\nI went to Paris in 1994 and stayed there until 2011, so in total, it was 17 years.\n\n# Output:\n\nI went to Paris in 1994 and stayed there until 2011, so in total, it was [Calculator(2011 - 1994)] 17 years.\n\n# Input:\n\nFrom this, we have 4 * 30 minutes = 120 minutes.\n\n# Output:\n\nFrom this, we have 4 * 30 minutes = [Calculator(4 * 30)] 120 minutes.\n\n# Wikipedia Search API\n\nWe use the following prompt for the Wikipedia search tool:\n\nYour task is to complete a given piece of text. You can use a Wikipedia Search API to look up information. You can do so by writing \"[WikiSearch(term)]\" where \"term\" is the search term you want to look up. Here are some examples of API calls:\n\n# Input:\n\nThe colors on the flag of Ghana have the following meanings: red is for the blood of martyrs, green for forests, and gold for mineral wealth.\n\n# Output:\n\nThe colors on the flag of Ghana have the following meanings: red is for [WikiSearch(\"Ghana flag red meaning\")] the blood of martyrs, green for forests, and gold for mineral wealth.\n\n# Input:\n\nBut what are the risks during production of nanomaterials? Some nanomaterials may give rise to various kinds of lung damage.\n\n# Output:\n\nBut what are the risks during production of nanomaterials? [WikiSearch(\"nanomaterial production risks\")] Some nanomaterials may give rise to various kinds of lung damage.\n\n# Input:\n\nMetformin is the first-line drug for patients with type 2 diabetes and obesity.\n\n# Output:\n\nMetformin is the first-line drug for [WikiSearch(\"Metformin first-line drug\")] patients with type 2 diabetes and obesity.\n\n# Machine Translation API\n\nWe use the following prompt for the machine translation tool:\n\nYour task is to complete a given piece of text by using a Machine Translation API.\n\nYou can do so by writing \"[MT(text)]\" where text is the text to be translated into English. Here are some examples:\n\n# Input:\n\nHe has published one book: O homem suprimido (\u201cThe Supressed Man\u201d)\n\n# Output:\n\nHe has published one book: O homem suprimido [MT(O homem suprimido)] (\u201cThe Supressed Man\u201d)\n\n# Input:\n\nIn Morris de Jonge\u2019s Jeschuah, der klassische j\u00fcdische Mann, there is a description of a Jewish writer\n\n# Output:\n\nIn Morris de Jonge\u2019s Jeschuah, der klassische j\u00fcdische Mann [MT(der klassische j\u00fcdische Mann)], there is a description of a Jewish writer.\n\n# Input:\n\n\u5357 \u4eac \u9ad8 \u6df3 \u53bf \u4f4f \u623f \u548c \u57ce \u4e61 \u5efa \u8bbe \u5c40 \u57ce \u5e02 \u65b0 \u533a \u8bbe \u8ba1 a plane of reference Gaochun is one of seven districts of the provincial capital Nanjing\n\n# Output:\n\n[MT(\u5357\u4eac\u9ad8\u6df3\u53bf\u4f4f\u623f\u548c\u57ce\u4e61\u5efa\u8bbe\u5c40 \u57ce\u5e02\u65b0\u533a \u8bbe\u8ba1)] a plane of reference Gaochun is one of seven districts of the provincial capital Nanjing.\n\n# Calendar API\n\nWe use the following prompt for the calendar tool:\n\nYour task is to add calls to a Calendar API to a piece of text.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48d16606-c594-43de-95b7-3b79dc53d267": {"__data__": {"id_": "48d16606-c594-43de-95b7-3b79dc53d267", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78b896a5-8c17-485f-acca-a5632ef5469f", "node_type": "4", "metadata": {}, "hash": "af4a93866ff402c20878b35162a447e10358994cf77b6a3958266e85eee7d61d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a0c09e1-86f3-42f2-adb3-63023afe5d99", "node_type": "1", "metadata": {}, "hash": "229d9b43ea89147a770cf3522aafe7eb4d9c4ea2e53e8ccda39d97d393632ab1", "class_name": "RelatedNodeInfo"}}, "text": "# Input:\n\n\u5357 \u4eac \u9ad8 \u6df3 \u53bf \u4f4f \u623f \u548c \u57ce \u4e61 \u5efa \u8bbe \u5c40 \u57ce \u5e02 \u65b0 \u533a \u8bbe \u8ba1 a plane of reference Gaochun is one of seven districts of the provincial capital Nanjing\n\n# Output:\n\n[MT(\u5357\u4eac\u9ad8\u6df3\u53bf\u4f4f\u623f\u548c\u57ce\u4e61\u5efa\u8bbe\u5c40 \u57ce\u5e02\u65b0\u533a \u8bbe\u8ba1)] a plane of reference Gaochun is one of seven districts of the provincial capital Nanjing.\n\n# Calendar API\n\nWe use the following prompt for the calendar tool:\n\nYour task is to add calls to a Calendar API to a piece of text. The API calls should help you get information required to complete the text. You can call the API by writing \"[Calendar()]\". Here are some examples of API calls:\n\n# Input:\n\nToday is the first Friday of the year.\n\n# Output:\n\nToday is the first [Calendar()] Friday of the year.", "mimetype": "text/plain", "start_char_idx": 3353, "end_char_idx": 4028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9eafdbdd-1374-487d-8d9f-85c99153a83f": {"__data__": {"id_": "9eafdbdd-1374-487d-8d9f-85c99153a83f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91942c04-768a-4427-af6b-f5766072669e", "node_type": "4", "metadata": {}, "hash": "54dfd43c2ba4ee91560ad4d9a52696292f6e69cc2d869e8883759cede42c1e77", "class_name": "RelatedNodeInfo"}}, "text": "# Input\n\nThe president of the United States is Joe Biden.\n\n# Output\n\nThe president of the United States is [Calendar()] Joe Biden.\n\n# Input\n\nThe current day of the week is Wednesday.\n\n# Output\n\nThe current day of the week is [Calendar()] Wednesday.\n\n# Input\n\nThe number of days from now until Christmas is 30.\n\n# Output\n\nThe number of days from now until Christmas is [Calendar()] 30.\n\n# Input\n\nThe store is never open on the weekend, so today it is closed.\n\n# Output\n\nThe store is never open on the weekend, so today [Calendar()] it is closed.\n\n# Input\n\nx\n\n# Output\n\n|Template|Size|\n|---|---|\n|How many days {ago was, are there until}|400|\n|{past_date, future_date}?| |\n|What {day of the week, day of the month, month, year} was it (current_date \u2013 past_date) {days, weeks, months, years} ago?|800|\n|What {day of the week, day of the month, month, year} will it be in (future_date \u2013 current_date) days?|800|\n|What day of the week {is, was} it on {past_date, future_date}?|400|\n|What {day of the week, day of the month, month, year} {is, was} it {the day before yesterday, yesterday, today, tomorrow, the day after tomorrow}?|4,000|\n|What {day of the week, day of the month, month} {is, was} holiday this year?|1,800|\n|How many {days, weeks, months, years} {ago was, are there until} holiday this year?|1,200|\n|Total|9,400|\n\n# B Toolformer Training\n\nWe use up to 25k examples per API. Max sequence length 1,024. Effective batch size of 128. All models are trained using DeepSpeed\u2019s ZeRO-3 (Rasley et al., 2020). We used 8 NVIDIA A100 40GB GPUs with BF16. Training up to 2k steps, where we evaluate PPL on a small development set from CCNet containing 1,000 examples every 500 steps. We pick the checkpoint that performs best.\n\n# C Zero-Shot Prompts\n\n# C.1 LAMA and TEMPLAMA\n\nFor both LAMA and TEMPLAMA, given an input text x, we use the following prompt: Please complete the following text so that it is factually correct: x.\n\n# C.2 Math Benchmarks\n\nFor all math benchmarks, given a context x and a question q, our prompt is: x q The answer is.\n\n# C.3 Question Answering\n\nFor all question answering datasets, including DATESET, we simply prefix the question with Answer the following question:. We append a question mark if the question does not already end with one.\n\n# C.4 Multilingual Question Answering\n\nFor MLQA, given a context x and a question q, our prompt is: Your task is.\n\n# D DATESET\n\nDATESET is created by first randomly selecting 500 \u201ccurrent dates\u201d. For each current date, another relatively past/future date is randomly selected within a four-year range, and the two dates are used to fill the query templates in Table 11. An example of one such query using the first template would be, \u201cHow many days ago was August 14, 2020?\u201d If called, the Calendar tool would return the presumed current date (e.g., \u201cToday is Sunday, November 20, 2020\u201d).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8762bcd4-a342-485a-88bb-5d515840b091": {"__data__": {"id_": "8762bcd4-a342-485a-88bb-5d515840b091", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "247bcc3c-166c-4e1d-b7bf-b89b12432dbb", "node_type": "4", "metadata": {}, "hash": "fe5f6dbb5d74c672e67254ff328756778cc6fc5023b4054cc8489596bfc38b0c", "class_name": "RelatedNodeInfo"}}, "text": "# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\n\nEdward Hu\u2217, Yelong Shen\u2217, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen\n\nMicrosoft Corporation\n\n{edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com\n\nyuanzhil@andrew.cmu.edu\n\n(Version 2)\n\n# ABSTRACT\n\nAn important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example \u2013 deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\n\n# 1 INTRODUCTION\n\nMany applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning, which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere \u201cinconvenience\u201d for GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a critical deployment challenge for GPT-3 (Brown et al., 2020) with 175 billion trainable parameters.1\n\nMany sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques\n\n\u2217Equal contribution.\n\n0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\n\n1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.\n\n# Figure 1: Our reparametrization.\n\nWe only train A and B", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "478082de-98b2-46b9-be0f-95c59654c1b8": {"__data__": {"id_": "478082de-98b2-46b9-be0f-95c59654c1b8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "504d03f2-f63a-4a78-ba26-f2174a4d0dad", "node_type": "4", "metadata": {}, "hash": "e76739623baa5fcf682b9c33a5511948c6d9203a024726179a105cfadb2ad1ae", "class_name": "RelatedNodeInfo"}}, "text": "# PROBLEM STATEMENT\n\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivating use case. Below is a brief description of the language modeling problem and, in particular, the maximization of conditional probabilities given a task-specific prompt.\n\nSuppose we are given a pre-trained autoregressive language model P\u03a6(y|x) parametrized by \u03a6. For instance, P\u03a6(y|x) can be a generic multi-task learner such as GPT (Radford et al., b; Brown et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this pre-trained model to downstream conditional text generation tasks, such as summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is represented by a training dataset of context-target pairs: Z = {(xi, yi)}i=1,..,N, where both xi and yi are sequences of tokens. For example, in NL2SQL, xi is a natural language query and yi its corresponding SQL command; for summarization, xi is the content of an article and yi its summary.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "180c4401-7074-49ab-9b0c-06c1aba6b6fc": {"__data__": {"id_": "180c4401-7074-49ab-9b0c-06c1aba6b6fc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1df3586a-38fb-48fd-afdc-238e74bea111", "node_type": "4", "metadata": {}, "hash": "99b945e16d01fe063dbe12c46fb117f78475820bbcf6c0527f290d430c5d5b9d", "class_name": "RelatedNodeInfo"}}, "text": "During full fine-tuning, the model is initialized to pre-trained weights \u03a60 and updated to \u03a60 + \u2206\u03a6 by repeatedly following the gradient to maximize the conditional language modeling objective:\n\n\u2211       \u2211log (P\u03a6(yt|x, y<t))\n\n|y|\n\nmax\u03a6                                                                       (1)\n\n(x,y)\u2208Zt=1\n\nOne of the main drawbacks for full fine-tuning is that for each downstream task, we learn a different set of parameters \u2206\u03a6 whose dimension |\u2206\u03a6| equals |\u03a60|. Thus, if the pre-trained model is large (such as GPT-3 with |\u03a60| \u2248 175 Billion), storing and deploying many independent instances of fine-tuned models can be challenging, if at all feasible.\n\nIn this paper, we adopt a more parameter-efficient approach, where the task-specific parameter increment \u2206\u03a6 = \u2206\u03a6(\u0398) is further encoded by a much smaller-sized set of parameters \u0398 with |\u0398| << |\u03a60|. The task of finding \u2206\u03a6 thus becomes optimizing over \u0398:\n\n\u2211      \u2211log (p\u03a60+\u2206\u03a6(\u0398)(yt|x, y<t))\n\n|y|\n\nmax\u0398                                                                            (2)\n\n(x,y)\u2208Zt=1\n\nIn the subsequent sections, we propose to use a low-rank representation to encode \u2206\u03a6 that is both compute- and memory-efficient. When the pre-trained model is GPT-3 175B, the number of trainable parameters |\u0398| can be as small as 0.01% of |\u03a60|.\n\n# 3     AREN\u2019T EXISTING SOLUTIONS GOOD ENOUGH?\n\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter- and compute-efficient. See Section 6 for a survey of some of the well-known works. Using language modeling as an example, there are two prominent strategies when it comes to efficient adaptations: adding adapter layers (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; R\u00a8uckl\u00b4e et al., 2020) or optimizing some forms of the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). However, both strategies have their limitations, especially in a large-scale and latency-sensitive production scenario.\n\n# Adapter Layers Introduce Inference Latency\n\nThere are many variants of adapters. We focus on the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block and a more recent one by Lin et al. (2020) which has only one per block but with an additional LayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploiting multi-task settings (R\u00a8uckl\u00b4e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass the extra compute in adapter layers. This seems like a non-issue since adapter layers are designed to have few parameters (sometimes <1% of the original model) by having a small bottleneck dimension, which limits the FLOPs they can add. However, large neural networks rely on hardware parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes a difference in the online inference setting where the batch size is typically as small as one. In a generic scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b) medium on a single GPU, we see a noticeable increase in latency when using adapters, even with a very small bottleneck dimension (Table 1).\n\nThis problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lepikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as AllReduce and Broadcast, unless we store the adapter parameters redundantly many times.\n\n# Directly Optimizing the Prompt is Hard\n\nThe other direction, as exemplified by prefix tuning (Li & Liang, 2021), faces a different challenge. We observe that prefix tuning is difficult to optimize and that its performance changes non-monotonically in trainable parameters, confirming similar observations in the original paper. More fundamentally, reserving a part of the sequence length for adaptation necessarily reduces the sequence length available to process a downstream task, which we suspect makes tuning the prompt less performant compared to other methods. We defer the study on task performance to Section 5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2217b17-f678-461b-a9bc-4e22334a20e8": {"__data__": {"id_": "d2217b17-f678-461b-a9bc-4e22334a20e8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3edafba9-7c7c-4bcb-b1da-05614d434e70", "node_type": "4", "metadata": {}, "hash": "044201f20dd9b5d29809017320db76e3581396b8ed18030e7e3802293fbbfb84", "class_name": "RelatedNodeInfo"}}, "text": "|Batch Size|32|16|1|\n|---|---|---|---|\n|Sequence Length|512|256|128|\n||\u0398||0.5M|11M|11M|\n|Fine-Tune/LoRA|1449.4\u00b10.8|338.0\u00b10.6|19.8\u00b12.7|\n|AdapterL|1482.0\u00b11.0 (+2.2%)|354.8\u00b10.5 (+5.0%)|23.9\u00b12.1 (+20.7%)|\n|AdapterH|1492.2\u00b11.0 (+3.0%)|366.3\u00b10.5 (+8.4%)|25.8\u00b12.2 (+30.3%)|\n\nTable 1: Inference latency of a single forward pass in GPT-2 medium measured in milliseconds, averaged over 100 trials. We use an NVIDIA Quadro RTX8000. \u201c|\u0398|\u201d denotes the number of trainable parameters in adapter layers. AdapterL and AdapterH are two variants of adapter tuning, which we describe in Section 5.1. The inference latency introduced by adapter layers can be significant in an online, short-sequence-length scenario. See the full study in Appendix B.\n\n# 4 OUR METHOD\n\nWe describe the simple design of LoRA and its practical benefits. The principles outlined here apply to any dense layers in deep learning models, though we only focus on certain weights in Transformer language models in our experiments as the motivating use case.\n\n# 4.1 LOW-RANK-PARAMETRIZED UPDATE MATRICES\n\nA neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a specific task, Aghajanyan et al. (2020) shows that the pre-trained language models have a low \u201cintrinsic dimension\u201d and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, we hypothesize the updates to the weights also have a low \u201cintrinsic rank\u201d during adaptation. For a pre-trained weight matrix W0 \u2208 Rd\u00d7k, we constrain its update by representing the latter with a low-rank decomposition W0 + \u2206W = W0 + BA, where B \u2208 Rd\u00d7r, A \u2208 Rr\u00d7k, and the rank r min(d, k).\n\nDuring training, W0 is frozen and does not receive gradient updates, while A and B contain trainable parameters. Note both W0 and \u2206W = BA are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For h = W0x, our modified forward pass yields:\n\nh = W0x + \u2206W x = W0x + BAx\n\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for A and \u03b1, where \u03b1 is zero for B, so \u2206W = BA is zero at the beginning of training. We then scale \u2206W x by r is a constant in r. When optimizing with Adam, tuning \u03b1 is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set \u03b1 to the first r we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary r (Yang & Hu, 2021).\n\nA Generalization of Full Fine-tuning. A more general form of fine-tuning allows the training of a subset of the pre-trained parameters. LoRA takes a step further and does not require the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that when applying LoRA to all weight matrices and training all biases, we roughly recover the expressiveness of full fine-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices. In other words, as we increase the number of trainable parameters, training LoRA roughly converges to training the original model, while adapter-based methods converge to an MLP and prefix-based methods to a model that cannot take long input sequences.\n\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and store W = W0 + BA and perform inference as usual. Note that both W0 and BA are in Rd\u00d7k. When we need to switch to another downstream task, we can recover W0 by subtracting BA and then adding a different B\u2032A, a quick operation with very little memory overhead. Critically, this represents a negligible number of parameters compared to weights.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7f8599a-7dc5-43be-bed7-1aa35ed60e4e": {"__data__": {"id_": "a7f8599a-7dc5-43be-bed7-1aa35ed60e4e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c48223cc-c3ad-4000-9e21-920f781c5716", "node_type": "4", "metadata": {}, "hash": "38a8eece9967cc965b937d21e27677caf7ee82ca9c24d47ee28fcde4ac7f7913", "class_name": "RelatedNodeInfo"}}, "text": "# 4.2 APPLYING LORA TO TRANSFORMER\n\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the number of trainable parameters. In the Transformer architecture, there are four weight matrices in the self-attention module (Wq, Wk, Wv, Wo) and two in the MLP module. We treat Wq (or Wk, Wv) as a single matrix of dimension dmodel \u00d7 dmodel, even though the output dimension is usually sliced into attention heads. We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency. We further study the effect on adapting different types of attention weight matrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP layers, LayerNorm layers, and biases to a future work.\n\n# Practical Benefits and Limitations.\n\nThe most significant benefit comes from the reduction in memory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM usage by up to 2/3 if r &lt; dmodel as we do not need to store the optimizer states for the frozen parameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to 350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000\u00d7 (from 350GB to 35MB)4. This allows us to train with significantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup during training on GPT-3 175B compared to full fine-tuning5 as we do not need to calculate the gradient for the vast majority of the parameters.\n\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks with different A and B in a single forward pass, if one chooses to absorb A and B into W to eliminate additional inference latency. Though it is possible to not merge the weights and dynamically choose the LoRA modules to use for samples in a batch for scenarios where latency is not critical.\n\n# 5 EMPIRICAL EXPERIMENTS\n\nWe evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown et al., 2020). Our experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG). Specifically, we evaluate on the GLUE (Wang et al., 2019) benchmark for RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct comparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al., 2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for more details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\n\n# 5.1 BASELINES\n\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible. This, however, means that some baselines might only appear in certain experiments. Fine-Tuning (FT) is a common approach for adaptation. During fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates. A simple variant is to update only some layers while freezing others. We include one such baseline reported in prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers (FTTop2).\n\n4 We still need the 350GB model during deployment; however, storing 100 adapted models only requires 350GB + 35MB * 100 \u2248 354GB as opposed to 100 * 350GB \u2248 35TB.\n\n5 For GPT-3 175B, the training throughput for full fine-tuning is 32.5 tokens/s per V100 GPU; with the same number of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a370f997-4a72-4679-b584-230c58db6314": {"__data__": {"id_": "a370f997-4a72-4679-b584-230c58db6314", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12d43681-7207-4c64-9271-ca3c8011311b", "node_type": "4", "metadata": {}, "hash": "0fe84c722e09c7b38e5b4d743b5aa459370c73bbbae4629714a4bf0c6beba565", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f924674e-40ee-4e62-a586-b97e6a9dec19", "node_type": "1", "metadata": {}, "hash": "d67b1121ae2a3ff08f7de851a368029e0c8fb3504b2c22c79ba4ae49ad1bf427", "class_name": "RelatedNodeInfo"}}, "text": "# Model & Method\n\n|Trainable Parameters|MNLI|SST-2|MRPC|CoLA|QNLI|QQP|RTE|STS-B|Avg.|\n|---|---|---|---|---|---|---|---|---|---|\n|RoBbase (FT)*|125.0M|87.6|94.8|90.2|63.6|92.8|91.9|78.7|91.2|\n|RoBbase (BitFit)*| | | | | | | | | |\n|RoBbase (AdptD)*|0.1M|84.7|93.7|92.7|62.0|91.8|84.0|81.5|90.8|\n|RoBbase (AdptD)*|0.3M|87.1\u00b1.0|94.2\u00b1.1|88.5\u00b11.1|60.8\u00b1.4|93.1\u00b1.1|90.2\u00b1.0|71.5\u00b12.7|89.7\u00b1.3|\n| |0.9M|87.3\u00b1.1|94.7\u00b1.3|88.4\u00b1.1|62.6\u00b1.9|93.0\u00b1.2|90.6\u00b1.0|75.9\u00b12.2|90.3\u00b1.1|\n|RoBbase (LoRA)|0.3M|87.5\u00b1.3|95.1\u00b1.2|89.7\u00b1.7|63.4\u00b11.2|93.3\u00b1.3|90.8\u00b1.1|86.6\u00b1.7|91.5\u00b1.2|\n|RoBlarge (FT)*|355.0M|90.2|96.4|90.9|68.0|94.7|92.2|86.6|92.4|\n|RoBlarge (LoRA)|0.8M|90.6\u00b1.2|96.2\u00b1.5|90.9\u00b11.2|68.2\u00b11.9|94.9\u00b1.3|91.6\u00b1.1|87.4\u00b12.5|92.6\u00b1.2|\n|RoBlarge (AdptP)\u2020|3.0M|90.2\u00b1.3|96.1\u00b1.3|90.2\u00b1.7|68.3\u00b11.0|94.8\u00b1.2|91.9\u00b1.1|83.8\u00b12.9|92.1\u00b1.7|\n|RoBlarge (AdptP)\u2020|0.8M|90.5\u00b1.3|96.6\u00b1.2|89.7\u00b11.2|67.8\u00b12.5|94.8\u00b1.3|91.7\u00b1.2|80.1\u00b12.9|91.9\u00b1.4|\n|RoBlarge (AdptH)\u2020|6.0M|89.9\u00b1.5|96.2\u00b1.3|88.7\u00b12.9|66.5\u00b14.4|94.7\u00b1.2|92.1\u00b1.1|83.4\u00b11.1|91.0\u00b11.7|\n|RoBlarge (AdptH)\u2020|0.8M|90.3\u00b1.3|96.3\u00b1.5|87.7\u00b11.7|66.3\u00b12.0|94.7\u00b1.2|91.5\u00b1.1|72.9\u00b12.9|91.5\u00b1.5|\n|RoBlarge (LoRA)\u2020|0.8M|90.6\u00b1.2|96.2\u00b1.5|90.2\u00b11.0|68.2\u00b11.9|94.8\u00b1.3|91.6\u00b1.2|85.2\u00b11.1|92.3\u00b1.5|\n|DeBXXL (FT)*|1500.0M|91.8|97.2|92.0|72.0|96.0|92.7|93.9|92.9|\n|DeBXXL (LoRA)|4.7M|91.9\u00b1.2|96.9\u00b1.2|92.6\u00b1.6|72.4\u00b11.1|96.0\u00b1.1|92.9\u00b1.1|94.9\u00b1.4|93.0\u00b1.2|\n\nTable 2: RoBERTabase, RoBERTalarge, and DeBERTaXXL with different adaptation methods on the GLUE benchmark.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f924674e-40ee-4e62-a586-b97e6a9dec19": {"__data__": {"id_": "f924674e-40ee-4e62-a586-b97e6a9dec19", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12d43681-7207-4c64-9271-ca3c8011311b", "node_type": "4", "metadata": {}, "hash": "0fe84c722e09c7b38e5b4d743b5aa459370c73bbbae4629714a4bf0c6beba565", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a370f997-4a72-4679-b584-230c58db6314", "node_type": "1", "metadata": {}, "hash": "aa7c8eb8dc05faf0fbb78afbec4cf457260e17e15cae3cc47656b6c87969a004", "class_name": "RelatedNodeInfo"}}, "text": "We report the overall (matched and mismatched) accuracy for MNLI, Matthew\u2019s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. * indicates numbers published in prior works. \u2020 indicates runs configured in a setup similar to Houlsby et al. (2019) for a fair comparison.\n\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else. Contemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\n\nPrefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These special tokens have trainable word embeddings and are generally not in the model\u2019s vocabulary. Where to place such tokens can have an impact on performance. We focus on \u201cprefixing\u201d, which prepends such tokens to the prompt, and \u201cinfixing\u201d, which appends to the prompt; both are discussed in Li & Liang (2021). We use lp (resp. l) denote the number of prefix (resp. infix) tokens. The number of trainable parameters is |\u0398| = dmodel \u00d7 (lp + l).\n\nPrefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning. Instead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer. The activations computed from previous layers are simply replaced by trainable ones. The resulting number of trainable parameters is |\u0398| = L \u00d7 dmodel \u00d7 (lp + l), where L is the number of Transformer layers.\n\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-attention module (and the MLP module) and the subsequent residual connection. There are two fully connected layers with biases in an adapter layer with a nonlinearity in between. We call this original design AdapterH. Recently, Lin et al. (2020) proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This is very similar to another design proposed in Pfeiffer et al. (2021), which we call AdapterP. We also include another baseline called AdapterDrop (R\u00fcckl\u00e9 et al., 2020) which drops some adapter layers for greater efficiency (AdapterD). We cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column.\n\nIn all cases, we have |\u0398| = L\u02c6Adpt \u00d7 (2 \u00d7 dmodel \u00d7 r + r + dmodel) + 2 \u00d7 L\u02c6LN \u00d7 dmodel where L\u02c6Adpt is the number of adapter layers and L\u02c6LN the number of trainable LayerNorms (e.g., in Adapter LoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices. As mentioned in Section 4.2, we only apply LoRA to Wq and Wv in most experiments for simplicity. The number of trainable parameters is determined by the rank r and the shape of the original weights: |\u0398| = 2 \u00d7 L\u02c6LoRA \u00d7 dmodel \u00d7 r, where L\u02c6LoRA is the number of weight matrices we apply LoRA to.", "mimetype": "text/plain", "start_char_idx": 1415, "end_char_idx": 4401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f94073b5-a48d-414c-a361-f450ac221235": {"__data__": {"id_": "f94073b5-a48d-414c-a361-f450ac221235", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df9266e1-aed0-40e8-baa0-50c1176bc9ac", "node_type": "4", "metadata": {}, "hash": "7a3bf74e9c71e5a8e5b377364b090db3ec3221e4bdefdeb049f04bb314bbd5c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc99360c-5dbe-44cd-b6f8-3da4bee0da3d", "node_type": "1", "metadata": {}, "hash": "e69b8705d956dbc76040ed25903de3809a9661fb368d96f6491756888ed893cc", "class_name": "RelatedNodeInfo"}}, "text": "# 5.2 ROBERTA BASE/LARGE\n\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin et al., 2019a) and boosted the latter\u2019s task performance without introducing many more trainable parameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards such as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and popular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base (125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020) and evaluate the performance of different efficient adaptation approaches on tasks from the GLUE benchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when comparing with adapters. First, we use the same batch size for all tasks and use a sequence length of 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for MRPC, RTE, and STS-B, not a model already adapted to MNLI like the fine-tuning baseline. Runs following this more restricted setup from Houlsby et al. (2019) are labeled with \u2020. The result is presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\n\n# 5.3 DEBERTA XXL\n\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger scale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully fine-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section). See Section D.2 for details on the hyperparameters used.\n\n# 5.4 GPT-2 MEDIUM/LARGE\n\nHaving shown that LoRA can be a competitive alternative to full fine-tuning on NLU, we hope to answer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al., b). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due to space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section. See Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We include a list of the hyperparameters used in Section D.3.\n\n# Table 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG Challenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable or fewer trainable parameters. Confidence intervals are shown for experiments we ran. * indicates numbers published in prior works.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc99360c-5dbe-44cd-b6f8-3da4bee0da3d": {"__data__": {"id_": "cc99360c-5dbe-44cd-b6f8-3da4bee0da3d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df9266e1-aed0-40e8-baa0-50c1176bc9ac", "node_type": "4", "metadata": {}, "hash": "7a3bf74e9c71e5a8e5b377364b090db3ec3221e4bdefdeb049f04bb314bbd5c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f94073b5-a48d-414c-a361-f450ac221235", "node_type": "1", "metadata": {}, "hash": "e690ae3682bd762e08fe1e861b3c61a143393a45f4a9a7be9b16b289595843e6", "class_name": "RelatedNodeInfo"}}, "text": "We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due to space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section. See Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We include a list of the hyperparameters used in Section D.3.\n\n# Table 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG Challenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable or fewer trainable parameters. Confidence intervals are shown for experiments we ran. * indicates numbers published in prior works.\n\n|Model & Method|# Trainable Parameters|BLEU|NIST|MET|ROUGE-L|CIDEr|\n|---|---|---|---|---|---|---|\n|GPT-2 M (FT)*|354.92M|68.2|8.62|46.2|71.0|2.47|\n|GPT-2 M (AdapterL)*|0.37M|66.3|8.41|45.0|69.8|2.40|\n|GPT-2 M (AdapterL)*|11.09M|68.9|8.71|46.1|71.3|2.47|\n|GPT-2 M (AdapterH)|11.09M|67.3\u00b1.6|8.50\u00b1.07|46.0\u00b1.2|70.7\u00b1.2|2.44\u00b1.01|\n|GPT-2 M (FTTop2)*|25.19M|68.1|8.59|46.0|70.8|2.41|\n|GPT-2 M (PreLayer)*|0.35M|69.7|8.81|46.1|71.4|2.49|\n|GPT-2 M (LoRA)|0.35M|70.4\u00b1.1|8.85\u00b1.02|46.8\u00b1.2|71.8\u00b1.1|2.53\u00b1.02|\n|GPT-2 L (FT)*|774.03M|68.5|8.78|46.0|69.9|2.45|\n|GPT-2 L (AdapterL)|0.88M|69.1\u00b1.1|8.68\u00b1.03|46.3\u00b1.0|71.4\u00b1.2|2.49\u00b1.0|\n|GPT-2 L (AdapterL)|23.00M|68.9\u00b1.3|8.70\u00b1.04|46.1\u00b1.1|71.3\u00b1.2|2.45\u00b1.02|\n|GPT-2 L (PreLayer)*|0.77M|70.3|8.85|46.2|71.7|2.47|\n|GPT-2 L (LoRA)|0.77M|70.4\u00b1.1|8.89\u00b1.02|46.8\u00b1.2|72.0\u00b1.2|2.47\u00b1.02|", "mimetype": "text/plain", "start_char_idx": 2032, "end_char_idx": 3489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c000852-de47-4fd2-b35d-eceafb2c40ba": {"__data__": {"id_": "3c000852-de47-4fd2-b35d-eceafb2c40ba", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dafd937-f703-40eb-9b35-fdfa06374640", "node_type": "4", "metadata": {}, "hash": "ea94901b6c655470c8066e3ceafb039c304cb455a0529f37e8e75ac2f4e6c1dd", "class_name": "RelatedNodeInfo"}}, "text": "# 4 Performance of different adaptation methods on GPT-3 175B\n\n|Model & Method|# Trainable Parameters|WikiSQL Acc. (%)|MNLI-m Acc. (%)|SAMSum R1/R2/RL|\n|---|---|---|---|---|\n|GPT-3 (FT)|175,255.8M|73.8|89.5|52.0/28.0/44.5|\n|GPT-3 (BitFit)|14.2M|71.3|91.0|51.3/27.4/43.5|\n|GPT-3 (PreEmbed)|3.2M|63.1|88.6|48.3/24.2/40.5|\n|GPT-3 (PreLayer)|20.2M|70.1|89.5|50.8/27.3/43.5|\n|GPT-3 (AdapterH)|7.1M|71.9|89.8|53.0/28.9/44.8|\n|GPT-3 (AdapterH)|40.1M|73.2|91.5|53.2/29.0/45.1|\n|GPT-3 (LoRA)|4.7M|73.4|91.7|53.8/29.8/45.9|\n|GPT-3 (LoRA)|37.7M|74.0|91.6|53.4/29.2/45.1|\n\nWe report the logical form validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on SAMSum. LoRA performs better than prior approaches, including full fine-tuning. The results on WikiSQL have a fluctuation around \u00b10.5%, MNLI-m around \u00b10.1%, and SAMSum around \u00b10.2/\u00b10.2/\u00b10.1 for the three metrics.\n\n# 5.5 SCALING UP TO GPT-3 175B\n\nAs a final stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high training cost, we only report the typical standard deviation for a given task over random seeds, as opposed to providing one for every entry. See Section D.4 for details on the hyperparameters used. As shown in Table 4, LoRA matches or exceeds the fine-tuning baseline on all three datasets. Note that not all methods benefit monotonically from having more trainable parameters, as shown in Figure 2. We observe a significant performance drop when we use more than 256 special tokens for prefix-embedding tuning or more than 32 special tokens for prefix-layer tuning. This corroborates similar observations in Li & Liang (2021). While a thorough investigation into this phenomenon is out-of-scope for this work, we suspect that having more special tokens causes the input distribution to shift further away from the pre-training data distribution. Separately, we investigate the performance of different adaptation approaches in the low-data regime in Section F.3.\n\n# Figure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation methods on WikiSQL and MNLI-matched.\n\n|WikiSQL|MultiNLI-matched|\n|---|---|\n|0.75| |\n|0.70| |\n|0.65|Fine-Tune|\n|0.60|PrefixEmbed|\n|0.55|PrefixLayer|\n| |Adapter(H)|\n| |LoRA|\n\n# 6 RELATED WORKS\n\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence architecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive language modeling by using a stack of Transformer decoders. Since then, Transformer-based language models have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged with BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) \u2013 both are large Transformer language models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea7433e7-3b31-46a7-8b7d-49dd52a67092": {"__data__": {"id_": "ea7433e7-3b31-46a7-8b7d-49dd52a67092", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b115dac-cb32-4bae-99fe-9cc1bd788c9c", "node_type": "4", "metadata": {}, "hash": "81319cc83f5a56d3ca1858fbb2e4ba0c59eeeff70b55f0c067c41b0eca04e90f", "class_name": "RelatedNodeInfo"}}, "text": "# 7 UNDERSTANDING THE LOW-RANK UPDATES\n\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank adaptation learned from downstream tasks. Note that the low-rank structure not only lowers the hardware barrier to entry which allows us to run multiple experiments in parallel, but also gives better interpretability of how the update weights are correlated with the pre-trained weights. We focus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters (up to 10,000\u00d7) without adversely affecting task performances.\n\nWe perform a sequence of empirical studies to answer the following questions: 1) Given a parameter budget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23671350-d939-421a-a34c-3f554d4b7732": {"__data__": {"id_": "23671350-d939-421a-a34c-3f554d4b7732", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62d3220a-2116-4a29-bcf9-e84cd6910dc3", "node_type": "4", "metadata": {}, "hash": "c5aeee8e51fff1bcb6cb99241f1e893c728396333a4d24a0efab1fc403c2cff7", "class_name": "RelatedNodeInfo"}}, "text": "# 7.1 WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?\n\nGiven a limited parameter budget, which types of weights should we adapt with LoRA to obtain the best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight matrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored in FP16) on GPT-3 175B, which corresponds to r = 8 if we adapt one type of attention weights or r = 4 if we adapt two types, for all 96 layers. The result is presented in Table 5.\n\n|Weight Type|Wq|Wk|Wv|Wo|Wq, Wk|Wq, Wv|Wq, Wk, Wv, Wo|\n|---|---|---|---|---|---|---|---|\n|Rank r|8|8|8|8|4|4|2|\n|WikiSQL (\u00b10.5%)|70.4|70.0|73.0|73.2|71.4|73.7|73.7|\n|MultiNLI (\u00b10.1%)|91.0|90.8|91.0|91.3|91.3|91.3|91.7|\n\nTable 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of attention weights in GPT-3, given the same number of trainable parameters. Adapting both Wq and Wv gives the best performance overall. We find the standard deviation across random seeds to be consistent for a given dataset, which we report in the first column.\n\nNote that putting all the parameters in \u2206Wq or \u2206Wk results in significantly lower performance, while adapting both Wq and Wv yields the best result. This suggests that even a rank of four captures enough information in \u2206W such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank.\n\n# 7.2 WHAT IS THE OPTIMAL RANK r FOR LORA?\n\nWe turn our attention to the effect of rank r on model performance. We adapt {Wq, Wv}, {Wq, Wk, Wv, W}, and just Wq for a comparison.\n\n|Weight Type|r = 1|r = 2|r = 4|r = 8|r = 64|\n|---|---|---|---|---|---|\n|WikiSQL (\u00b10.5%) Wq|68.8|69.6|70.5|70.4|70.0|\n|Wq, Wv|73.4|73.3|73.7|73.8|73.5|\n|Wq, Wk, Wv, Wo|74.1|73.7|74.0|74.0|73.9|\n|MultiNLI (\u00b10.1%) Wq|90.7|90.9|91.1|90.7|90.7|\n|Wq, Wk, Wv, WoWq, Wv|91.3|91.4|91.3|91.6|91.4|\n| |91.2|91.7|91.7|91.5|91.4|\n\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a rank as small as one suffices for adapting both Wq and Wv on these datasets while training Wq alone needs a larger r. We conduct a similar experiment on GPT-2 in Section H.2.\n\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small r (more so for {Wq, Wv} than just Wq). This suggests the update matrix \u2206W could have a very small different choices of r and by different random seeds. We argue that increasing r does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufficient.\n\nHowever, we do not expect a small r to work for every task or dataset. Consider the following thought experiment: if the downstream task were in a different language than the one used for pre-training, retraining the entire model (similar to LoRA with r = dmodel) could certainly outperform LoRA with a small r.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e053b369-8a3e-46cf-aa99-440feb5677fe": {"__data__": {"id_": "e053b369-8a3e-46cf-aa99-440feb5677fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d51054d-6664-4ee0-ba71-88f5bacfdcd8", "node_type": "4", "metadata": {}, "hash": "795d4e6147d7ef0a264cf8edf49a090b0e67ecc5b52dfdac621d8658da66a5aa", "class_name": "RelatedNodeInfo"}}, "text": "# Subspace similarity between different r.\n\nGiven Ar=8 and Ar=64 which are the learned adaptation matrices with rank r = 8 and 64 using the same pre-trained model, we perform singular value decomposition and obtain the right-singular unitary matrices UAr=8 and UAr=64. We hope to answer: how much of the subspace spanned by the top i singular vectors in UAr=8 (for 1 \u2264 i \u2264 8) is contained in the subspace spanned by top j singular vectors of UAr=64 (for 1 \u2264 j \u2264 64)? We measure this quantity with a normalized subspace similarity based on the Grassmann distance (See Appendix G for a more formal discussion).\n\n\u03c6(Ar=8, Ar=64, i, j) = ||UAr=8 Uj=64||2i A F35\u2208 [0, 1] (4)\n\n\u03c6(\u00b7) has a range of [0, 1], where 1 represents a complete overlap of subspaces and 0 a complete separation. See Figure 3 for how \u03c6 changes as we vary i and j. We only look at the 48th layer (out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown in Section H.1.\n\n# (Ar = 64, Ar = 8, i, j)\n\n|Wq|Wv|Wq|Wv|\n|---|---|---|---|\n|1.0| |1.0| |\n|0.8| |0.8| |\n|0.6| |0.6| |\n|0.4| |0.4| |\n|0.2| |0.2| |\n|0.0| |0.0| |\n\nFigure 3: Subspace similarity between column vectors of Ar=8 and Ar=64 for both \u2206Wq and \u2206Wv. The third and the fourth figures zoom in on the lower-left triangle in the first two figures. The top directions in r = 8 are included in r = 64, and vice versa.\n\nWe make an important observation from Figure 3. Directions corresponding to the top singular vector overlap significantly between Ar=8 and Ar=64, while others do not. Specifically, \u2206Wv (resp. \u2206Wq) of Ar=8 and \u2206Wv (resp. \u2206Wq) of Ar=64 share a subspace of dimension 1 with normalized similarity > 0.5, providing an explanation of why r = 1 performs quite well in our downstream tasks for GPT-3.\n\nSince both Ar=8 and Ar=64 are learned using the same pre-trained model, Figure 3 indicates that the top singular-vector directions of Ar=8 and Ar=64 are the most useful, while other directions potentially contain mostly random noises accumulated during training. Hence, the adaptation matrix can indeed have a very low rank.\n\n# Subspace similarity between different random seeds.\n\nWe further confirm this by plotting the normalized subspace similarity between two randomly seeded runs with r = 64, shown in Figure 4. \u2206Wq appears to have a higher \u201cintrinsic rank\u201d than \u2206Wv, since more common singular value directions are learned by both runs for \u2206Wq, which is in line with our empirical observation in Table 6. As a comparison, we also plot two random Gaussian matrices, which do not share any common singular value directions with each other.\n\n# HOW DOES THE ADAPTATION MATRIX \u2206W COMPARE TO W?\n\nWe further investigate the relationship between \u2206W and W. In particular, does \u2206W highly correlate with W? (Or mathematically, is \u2206W mostly contained in the top singular directions of W?) Also, note that a similar analysis can be carried out with B and the left-singular unitary matrices \u2013 we stick with A for our experiments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11523e6e-5fd3-4123-b275-375c917397d1": {"__data__": {"id_": "11523e6e-5fd3-4123-b275-375c917397d1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4690e286-763b-4f23-b1b3-009c74df0c5e", "node_type": "4", "metadata": {}, "hash": "ec8f3e92d03f212e15fc1ee093304c5c56a59057d1e24d28fb6995c53857284b", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 4\n\nLeft and Middle: Normalized subspace similarity between the column vectors of Ar=645459 from two random seeds, for both \u2206Wq and \u2206Wv in the 48-th layer. Right: the same heat-map between the column vectors of two random Gaussian matrices. See Section H.1 for other layers.\n\n# Table 7\n\n|r|\u2206Wq|Wq|Random|\u2206Wq|Wq|Random|\n|---|---|---|---|---|---|---|\n|4|||U >Wq V>||F = 0.32|21.67|0.02|1.90|37.71|0.33|\n|64|||Wq ||F = 61.95|||\u2206Wq ||F = 6.91|||\u2206Wq ||F = 3.57| | | |\n\nWe draw several conclusions from Table 7. First, \u2206W has a stronger correlation with W compared to a random matrix, indicating that \u2206W amplifies some features that are already in W. Second, instead of repeating the top singular directions of W, \u2206W only amplifies directions that are not emphasized in W. Third, the amplification factor is rather huge: 21.5 \u2248 6.91/0.32 for r = 4. See Section H.4 for why r = 64 has a smaller amplification factor. We also provide a visualization in Section H.3 for how the correlation changes as we include more top singular directions from Wq. This suggests that the low-rank adaptation matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model.\n\n# 8 CONCLUSION AND FUTURE WORK\n\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required and the storage/switching cost for hosting independent instances for different tasks. We propose LoRA, an efficient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality. Importantly, it allows for quick task-switching when deployed as a service by sharing the vast majority of the model parameters. While we focused on Transformer language models, the proposed principles are generally applicable to any neural networks with dense layers.\n\nThere are many directions for future works. 1) LoRA can be combined with other efficient adaptation methods, potentially providing orthogonal improvement. 2) The mechanism behind fine-tuning or LoRA is far from clear \u2013 how are features learned during pre-training transformed to do well on downstream tasks? We believe that LoRA makes it more tractable to answer this than full fine-tuning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "013a6a6e-d551-4110-8f1f-950c49371b8c": {"__data__": {"id_": "013a6a6e-d551-4110-8f1f-950c49371b8c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5bf4f04-3d68-4d59-be14-3da4c8094397", "node_type": "4", "metadata": {}, "hash": "cd963e880376a43878aa41ffc00a7def3bb305f7f5d609c224f01f6e61aef383", "class_name": "RelatedNodeInfo"}}, "text": "# REFERENCES\n\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.\n\nZeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efficiently, Going Beyond Kernels? In NeurIPS, 2019. Full version available at http://arxiv.org/abs/1905.10337.\n\nZeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep learning. arXiv preprint arXiv:2001.04413, 2020a.\n\nZeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust deep learning. arXiv preprint arXiv:2005.10190, 2020b.\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811.03962.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs], July 2020. URL http://arxiv.org/abs/2005.14165.\n\nJian-Feng Cai, Emmanuel J Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956\u20131982, 2010.\n\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001.\n\nRonan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, ICML \u201908, pp. 160\u2013167, New York, NY, USA, July 2008. Association for Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL https://doi.org/10.1145/1390156.1390177.\n\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc\u2019Aurelio Ranzato, and Nando de Freitas. Predicting parameters in deep learning, 2014.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019a.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019b. URL http://arxiv.org/abs/1810.04805.\n\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://aclanthology.org/I05-5002.\n\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pp. 124\u2013133, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bf5c54d-8ba5-4445-a070-6b43818060c8": {"__data__": {"id_": "3bf5c54d-8ba5-4445-a070-6b43818060c8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0749e919-af60-4d3b-8b3c-333a623117a3", "node_type": "4", "metadata": {}, "hash": "e49f8c1dfe1d4f9996858869422b900bc1efa500b53de06a0bf09247d1c1d72a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5105cbbc-f838-4a7d-80cb-4364636a05fe", "node_type": "1", "metadata": {}, "hash": "717db3dc80ab17989cbb83a074eeb8b7ef4cbf4f63240c7b907d760ca4209e7c", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nBehrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020.\n\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URL http://arxiv.org/abs/1911.12237.\n\nLars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor approximation techniques. GAMM-Mitteilungen, 36(1):53\u201378, 2013.\n\nJihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based learning. In ICML, pp. 376\u2013383, 2008. URL https://doi.org/10.1145/1390156.1390204.\n\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial ReProgramming. arXiv:2101.00121 [cs], December 2020. URL http://arxiv.org/abs/2101.00121. arXiv: 2101.00121.\n\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention, 2021.\n\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], June 2019. URL http://arxiv.org/abs/1902.00751.\n\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.\n\nMikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol\u00f2 Fusi. Initialization and regularization of factorized neural layers, 2021.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.\n\nBrian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. arXiv:2104.08691 [cs], April 2021. URL http://arxiv.org/abs/2104.08691. arXiv: 2104.08691.\n\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Dimension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http://arxiv.org/abs/1804.08838. arXiv: 1804.08838.\n\nXiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv:2101.00190 [cs], January 2021. URL http://arxiv.org/abs/2101.00190.\n\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, 2018.\n\nYuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank approximation via alternating minimization. In International Conference on Machine Learning, pp. 2358\u20132367. PMLR, 2016.\n\nYuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pp. 2\u201347. PMLR, 2018b.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5105cbbc-f838-4a7d-80cb-4364636a05fe": {"__data__": {"id_": "5105cbbc-f838-4a7d-80cb-4364636a05fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0749e919-af60-4d3b-8b3c-333a623117a3", "node_type": "4", "metadata": {}, "hash": "e49f8c1dfe1d4f9996858869422b900bc1efa500b53de06a0bf09247d1c1d72a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf5c54d-8ba5-4445-a070-6b43818060c8", "node_type": "1", "metadata": {}, "hash": "e75ca82b7ad9fb23556941b75a0c12acc7cd33dd5d500f49f78d262c8b5865af", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:2101.00190 [cs], January 2021. URL http://arxiv.org/abs/2101.00190.\n\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, 2018.\n\nYuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank approximation via alternating minimization. In International Conference on Machine Learning, pp. 2358\u20132367. PMLR, 2016.\n\nYuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pp. 2\u201347. PMLR, 2018b.\n\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 441\u2013459, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.41. URL https://aclanthology.org/2020.findings-emnlp.41.", "mimetype": "text/plain", "start_char_idx": 2432, "end_char_idx": 3497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a945534c-7420-448d-8e87-c6d33d2c3cfa": {"__data__": {"id_": "a945534c-7420-448d-8e87-c6d33d2c3cfa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab5c9b84-0d38-4cbd-bf8e-62a83f09b1d1", "node_type": "4", "metadata": {}, "hash": "c4f1422a73f3c511df7117a5a04fce95247494466d1b873187e7a17708e98acb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9e90fea-f607-4f35-9541-9f0482a78a9b", "node_type": "1", "metadata": {}, "hash": "a7bc7b5f21911d462f67bdd612b7379ad858110d63c55881eba390c21d07dc6b", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT Understands, Too. arXiv:2103.10385 [cs], March 2021. URL http://arxiv.org/abs/2103.10385. arXiv: 2103.10385.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\n\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers, 2021.\n\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871, 2020.\n\nJekaterina Novikova, Ond\u02c7rej Du\u02c7sek, and Verena Rieser. The e2e dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254, 2017.\n\nSamet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint arXiv:1906.05392, 2019.\n\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00a8uckl\u00b4e, Kyunghyun Cho, and Iryna Gurevych. Adapter-fusion: Non-destructive task composition for transfer learning, 2021.\n\nDaniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and Sanjeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, pp. 3743\u20133747, 2018.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training. pp. 12, a.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. pp. 24, b.\n\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad. CoRR, abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822.\n\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv:1705.08045 [cs, stat], November 2017. URL http://arxiv.org/abs/1705.08045. arXiv: 1705.08045.\n\nAndreas R\u00a8uckl\u00b4e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers, 2020.\n\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655\u20136659. IEEE, 2013.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9e90fea-f607-4f35-9541-9f0482a78a9b": {"__data__": {"id_": "a9e90fea-f607-4f35-9541-9f0482a78a9b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab5c9b84-0d38-4cbd-bf8e-62a83f09b1d1", "node_type": "4", "metadata": {}, "hash": "c4f1422a73f3c511df7117a5a04fce95247494466d1b873187e7a17708e98acb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a945534c-7420-448d-8e87-c6d33d2c3cfa", "node_type": "1", "metadata": {}, "hash": "0429d99943f9d6d8c094800a690026cdc927533d81075ff36f8499a93d307c9d", "class_name": "RelatedNodeInfo"}}, "text": "arXiv: 1705.08045.\n\nAndreas R\u00a8uckl\u00b4e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers, 2020.\n\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655\u20136659. IEEE, 2013.\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170.", "mimetype": "text/plain", "start_char_idx": 2387, "end_char_idx": 3496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "996b2f85-5c85-455c-8ace-695a13f8abc9": {"__data__": {"id_": "996b2f85-5c85-455c-8ace-695a13f8abc9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdd9df16-251a-4233-8269-5b21371850ae", "node_type": "4", "metadata": {}, "hash": "8d02807ac3dc858dcf07e96b21aa722b4b369f4a50dc5374b42535985e366811", "class_name": "RelatedNodeInfo"}}, "text": "# A LARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES\n\nFew-shot learning, or prompt engineering, is very advantageous when we only have a handful of training samples. However, in practice, we can often afford to curate a few thousand or more training examples for performance-sensitive applications. As shown in Table 8, fine-tuning improves the model performance drastically compared to few-shot learning on datasets large and small. We take the GPT-3 few-shot result on RTE from the GPT-3 paper (Brown et al., 2020). For MNLI-matched, we use two demonstrations per class and six in-context examples in total.\n\n|Reference|Details|\n|---|---|\n|Ashish Vaswani et al. (2017)|Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 6000\u20136010.|\n|Alex Wang et al. (2019)|Glue: A multi-task benchmark and analysis platform for natural language understanding.|\n|Alex Wang et al. (2020)|Superglue: A stickier benchmark for general-purpose language understanding systems.|\n|Alex Warstadt et al. (2018)|Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.|\n|Adina Williams et al. (2018)|A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112\u20131122, New Orleans, Louisiana, June 2018. doi: 10.18653/v1/N18-1101. URL: https://www.aclweb.org/anthology/N18-1101.|\n|Thomas Wolf et al. (2020)|Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. URL: https://www.aclweb.org/anthology/2020.emnlp-demos.6.|\n|Greg Yang and Edward J. Hu (2021)|Feature Learning in Infinite-Width Neural Networks. arXiv:2011.14522 [cond-mat]. URL: http://arxiv.org/abs/2011.14522.|\n|Elad Ben Zaken et al. (2021)|Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.|\n|Yu Zhang et al. (2014)|Extracting deep neural network bottleneck features using low-rank matrix factorization. In 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 185\u2013189. IEEE.|\n|Yong Zhao et al. (2016)|Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5005\u20135009. IEEE.|\n|Victor Zhong et al. (2017)|Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs/1709.00103. URL: http://arxiv.org/abs/1709.00103.|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52fbd477-65f5-481e-b6b1-62ae318a58d6": {"__data__": {"id_": "52fbd477-65f5-481e-b6b1-62ae318a58d6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6278ccd-0a3b-4828-be53-7a715d1aa418", "node_type": "4", "metadata": {}, "hash": "1fa162292cb36c68142847ed8a0de8094212b13aba7abeb3a16213bbc7a598dd", "class_name": "RelatedNodeInfo"}}, "text": "# Method\n\n| |MNLI-m (Val. Acc./%)|RTE (Val. Acc./%)|\n|---|---|---|\n|GPT-3 Few-Shot|40.6|69.0|\n|GPT-3 Fine-Tuned|89.5|85.4|\n\nTable 8: Fine-tuning significantly outperforms few-shot learning on GPT-3 (Brown et al., 2020).\n\n# B INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\n\nAdapter layers are external modules added to a pre-trained model in a sequential manner, whereas our proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently, adapter layers must be computed in addition to the base model, inevitably introducing additional latency. While as pointed out in R\u00fcckl\u00e9 et al. (2020), the latency introduced by adapter layers can be mitigated when the model batch size and/or sequence length is large enough to fully utilize the hardware parallelism. We confirm their observation with a similar latency study on GPT-2 medium and point out that there are scenarios, notably online inference where the batch size is small, where the added latency can be significant.\n\nWe measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging over 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension r. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH, and a recent, more efficient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1 for more details on the designs. We plot the slow-down in percentage compared to the no-adapter baseline in Figure 5.\n\n# Seq Len = 128\n\n# Seq Len = 256\n\n# Seq Len = 512\n\nFigure 5: Percentage slow-down of inference latency compared to the no-adapter (r = 0) baseline. The top row shows the result for AdapterH and the bottom row AdapterL. Larger batch size and sequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an online, short-sequence-length scenario. We tweak the colormap for better visibility.\n\n# C DATASET DETAILS\n\nGLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes MNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC (paraphrase detection, Dolan & Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al. (2018)), QNLI (inference, Rajpurkar et al. (2018)), QQP (question-answering), RTE (inference).\n\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2839109-e68f-48d8-a71d-0bd82f5fc770": {"__data__": {"id_": "a2839109-e68f-48d8-a71d-0bd82f5fc770", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c276e4dd-7eb0-47ab-9fe1-01c8d3880ea0", "node_type": "4", "metadata": {}, "hash": "2cba3219eb9f2c4f83acadb3bc4957d8f6690f1eab027d9d20224fc1a478ff66", "class_name": "RelatedNodeInfo"}}, "text": "# D     HYPERPARAMETERS USED IN EXPERIMENTS\n\n# D.1     ROBERTA\n\nWe train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number of training epochs, and batch size for LoRA. Following Liu et al. (2019), we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. For a fair comparison with the setup in Houlsby et al. (2019) and Pfeiffer et al. (2021), we restrict the model sequence length to 128 and used a fixed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large model when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI. The runs with this restricted setup are marked with \u2020. See the hyperparameters used in our runs in Table 9.\n\n# D.2     DEBERTA\n\nWe again train using AdamW with a linear learning rate decay schedule. Following He et al. (2021), we tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model sequence length used by (He et al., 2021) to keep our comparison fair. Following He et al. (2021), we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. See the hyperparameters used in our runs in Table 10.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bb4d31e-11ae-420a-9df9-c37af4cef012": {"__data__": {"id_": "6bb4d31e-11ae-420a-9df9-c37af4cef012", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4429fcb9-241a-44ce-bbbb-28be8a228e9c", "node_type": "4", "metadata": {}, "hash": "3c3fbaed5c0bf02f710557e8432698c6406afc08a909d0b205c1a0f90d8ea953", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bba56623-699f-4a36-89ce-8558b05ab7dd", "node_type": "1", "metadata": {}, "hash": "63d2699393c6a015a3ffdfb16f33eaa9cc7c09dfa8ebf6ffdc0bdd49bddfdb3c", "class_name": "RelatedNodeInfo"}}, "text": "# Table 9: The hyperparameters we used for RoBERTa on the GLUE benchmark.\n\n|Method|Dataset|MNLI|SST-2|MRPC|CoLA|QNLI|QQP|RTE|STS-B| | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|RoBERTa base|Optimizer| | | |AdamW| | | | | | | |\n| |Warmup Ratio| | | |0.06| | | | | | | |\n|LoRA|LR Schedule| | | |Linear| | | | | | | |\n| |Batch Size|16|16|16|32|32|16|32|16| | | |\n|RoBERTa large|# Epochs|30|60|30|80|25|25|80|40| | | |\n| |Learning Rate|5E-04|5E-04|4E-04|4E-04|4E-04|5E-04|5E-04|4E-04| | | |\n|RoBERTa large|LoRA Config.| | | |rq = rv = 8| | | | | | | |\n| |LoRA \u03b1| | | |16| | | | | | | |\n| |Max Seq. Len.|128|128|512|128|512|512|512|512| | | |\n| |Batch Size|4| | | | | | | | | | |\n|LoRA\u2020|# Epochs|10|10|20|20|10|20|20|30| | | |\n| |Learning Rate|3E-04|4E-04|3E-04|2E-04|2E-04|3E-04|4E-04|2E-04| | | |\n|RoBERTa largeAdptP (3M)\u2020|# Epochs|10|20|20|20|10|20|20|20| | | |\n| |Learning Rate|3E-05|3E-05|3E-04|3E-04|3E-04|3E-04|3E-04|3E-04| | | |\n| |Bottleneck r| | | |64| | | | | | | |\n| |Max Seq. Len.|128| | | | | | | | | | |\n| |Batch Size| | |32| | | | | | | | |\n|RoBERTa large AdptP (0.8M)\u2020|# Epochs|5|20|20|20|10|20|20|20| | | |\n| |Learning Rate|3E-04|3E-04|3E-04|3E-04|3E-04|3E-04|3E-04|3E-04| | | |\n| |Bottleneck r| | | |16| | | | | | | |\n| |Max Seq. Len.|128| | | | | | | | | | |\n| |Batch Size| | |32| | | | | | | | |\n|RoBERTa largeAdptH (6M)\u2020|# Epochs|10|5|10|10|5|20|20|10| | | |\n| |Learning Rate|3E-05| |3E-04|3E-04|3E-04|3E-04|3E-04|3E-04|3E-04| | |\n| |Bottleneck r| | | |64| | | | | | | |\n| |Max Seq. Len.|128| | | | | | | | | | |\n| |Batch Size| | |32| | | | | | | | |\n|RoBERTa large AdptH (0.8M)\u2020|# Epochs| | | |10|5|10|10|5|20|20|10|\n| |Learning Rate|3E-04|3E-04|3E-04|3E-04|3E-04|3E-04|3E-04|3E-04| | | |\n| |Bottleneck r| | | |8| | | | | | | |\n| |Max Seq. Len.|128| | | | | | | | | | |\n\n# D.3 GPT-2\n\nWe train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described in Li & Liang (2021).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bba56623-699f-4a36-89ce-8558b05ab7dd": {"__data__": {"id_": "bba56623-699f-4a36-89ce-8558b05ab7dd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4429fcb9-241a-44ce-bbbb-28be8a228e9c", "node_type": "4", "metadata": {}, "hash": "3c3fbaed5c0bf02f710557e8432698c6406afc08a909d0b205c1a0f90d8ea953", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bb4d31e-11ae-420a-9df9-c37af4cef012", "node_type": "1", "metadata": {}, "hash": "776bc26180beac1d5565122c13be264d6291da5fec37ed2ef132b5719e7129e7", "class_name": "RelatedNodeInfo"}}, "text": "Len.|128| | | | | | | | | | |\n\n# D.3 GPT-2\n\nWe train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described in Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the mean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters used for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang (2021).\n\n# D.4 GPT-3\n\nFor all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with a batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for", "mimetype": "text/plain", "start_char_idx": 1774, "end_char_idx": 2525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40825576-2e7f-43cf-88cc-408564d9c22d": {"__data__": {"id_": "40825576-2e7f-43cf-88cc-408564d9c22d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de6ba9c8-ec35-456c-a781-e8ecc34e7bec", "node_type": "4", "metadata": {}, "hash": "7927cac5166dfe58c016c346eeec84baac0acacf2af8ac8d825efb0a685da18a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af5df226-9f09-4a63-97b3-4c3512500483", "node_type": "1", "metadata": {}, "hash": "6aaca95c81d5aefc82f1fac2d4dc00602e4df9a67da3560c1fe580be2a70bd56", "class_name": "RelatedNodeInfo"}}, "text": "# Method\n\n|Dataset|MNLI|SST-2|MRPC|CoLA|QNLI|QQP|RTE|STS-B| |\n|---|---|---|---|---|---|---|---|---|---|\n|Optimizer|AdamW|AdamW|AdamW|AdamW|AdamW|AdamW|AdamW|AdamW| | | | | | | | |\n|Warmup Ratio|0.1|0.1|0.1|0.1|0.1|0.1|0.1|0.1| | | | | | | | |\n|LR Schedule|Linear|Linear|Linear|Linear|Linear|Linear|Linear|Linear| | | | | | | | |\n|Batch Size|8|8|32|4|6|8|4|4| |\n|DeBERTa XXL|# Epochs|5|16|30|10|8|11|11|10|\n|LoRA|Learning Rate|1E-04|6E-05|2E-04|1E-04|1E-04|1E-04|2E-04|2E-04|\n| |Weight Decay|0|0.01|0.01|0|0.01|0.01|0.01|0.1|\n| |CLS Dropout|0.15|0|0|0.1|0.1|0.2|0.2|0.2|\n| |LoRA Config.|rq = rv = 8|rq = rv = 8|rq = rv = 8|rq = rv = 8|rq = rv = 8|rq = rv = 8|rq = rv = 8| | | | | | | |\n| |LoRA \u03b1|8|8|8|8|8|8|8| | | | | | | |\n| |Max Seq. Len.|256|128|128|64|512|320|320|128|\n\nTable 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark.\n\n# Dataset\n\n| |E2E|WebNLG|DART|\n|---|---|---|---|\n|Optimizer|AdamW|AdamW|AdamW|\n|Weight Decay|0.01|0.01|0.0|\n|Dropout Prob|0.1|0.1|0.0|\n|Batch Size|8|8|8|\n|# Epoch|5|5|5|\n|Warmup Steps|500|500|500|\n|Learning Rate Schedule|Linear|Linear|Linear|\n|Label Smooth|0.1|0.1|0.0|\n|Learning Rate|0.0002|0.0002|0.0002|\n|Adaptation|rq = rv = 4|rq = rv = 4|rq = rv = 4|\n|LoRA \u03b1|32|32|32|\n|Inference| | | |\n|Beam Size|10|10|10|\n|Length Penalty|0.9|0.8|0.8|\n|no repeat ngram size| | | |\n\nTable 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART.\n\nWikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa et al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more details on the hyperparameters used. For prefix-embedding tuning, we find the optimal lp and li to be 256 and 8, respectively, totalling 3.2M trainable parameters. We use lp = 8 and li = 8 for prefix-layer tuning with 20.2M trainable parameters to obtain the overall best performance. We present two parameter budgets for LoRA: 4.7M (rq = rv = 1 or rv = 2) and 37.7M (rq = rv = 8 or rq = rk = rv = ro = 2). We report the best validation performance from each run. The training hyperparameters used in our GPT-3 experiments are listed in Table 12.\n\n# E COMBINING LORA WITH PREFIX TUNING\n\nLoRA can be naturally combined with existing prefix-based approaches. In this section, we evaluate two combinations of LoRA and variants of prefix-tuning on WikiSQL and MNLI.\n\nLoRA+PrefixEmbed (LoRA+PE) combines LoRA with prefix-embedding tuning, where we insert lp + li special tokens whose embeddings are treated as trainable parameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af5df226-9f09-4a63-97b3-4c3512500483": {"__data__": {"id_": "af5df226-9f09-4a63-97b3-4c3512500483", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de6ba9c8-ec35-456c-a781-e8ecc34e7bec", "node_type": "4", "metadata": {}, "hash": "7927cac5166dfe58c016c346eeec84baac0acacf2af8ac8d825efb0a685da18a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40825576-2e7f-43cf-88cc-408564d9c22d", "node_type": "1", "metadata": {}, "hash": "157afc320a23c8ef0428896138c1e20581c840434919dbc4c8b405ff974ea0cf", "class_name": "RelatedNodeInfo"}}, "text": "We present two parameter budgets for LoRA: 4.7M (rq = rv = 1 or rv = 2) and 37.7M (rq = rv = 8 or rq = rk = rv = ro = 2). We report the best validation performance from each run. The training hyperparameters used in our GPT-3 experiments are listed in Table 12.\n\n# E COMBINING LORA WITH PREFIX TUNING\n\nLoRA can be naturally combined with existing prefix-based approaches. In this section, we evaluate two combinations of LoRA and variants of prefix-tuning on WikiSQL and MNLI.\n\nLoRA+PrefixEmbed (LoRA+PE) combines LoRA with prefix-embedding tuning, where we insert lp + li special tokens whose embeddings are treated as trainable parameters. For more on prefix-embedding tuning, see Section 5.1.\n\nLoRA+PrefixLayer (LoRA+PL) combines LoRA with prefix-layer tuning. We also insert lp + li special tokens; however, instead of letting the hidden representations of these tokens evolve naturally.", "mimetype": "text/plain", "start_char_idx": 1884, "end_char_idx": 2775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b622cef4-8291-4992-8708-e2387de255ec": {"__data__": {"id_": "b622cef4-8291-4992-8708-e2387de255ec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c57bff8-dbe3-4a5c-b732-d9d5217d4aab", "node_type": "4", "metadata": {}, "hash": "5c84d4a6bc155dde2a41552f7347960c40a875bbb095ae5f21048a67b54ad305", "class_name": "RelatedNodeInfo"}}, "text": "# Hyperparameters\n\n|Hyperparameters|Fine-Tune|PreEmbed|PreLayer|BitFit|AdapterH|LoRA|\n|---|---|---|---|---|---|---|\n|Optimizer|AdamW|AdamW|AdamW|AdamW|AdamW|AdamW|\n|Batch Size|128|128|128|128|128|128|\n|# Epoch|2|2|2|2|2|2|\n|Warmup Tokens|250,000|250,000|250,000|250,000|250,000|250,000|\n|LR Schedule|Linear|Linear|Linear|Linear|Linear|Linear|\n|Learning Rate|5.00E-06|5.00E-04|1.00E-04|1.6E-03|1.00E-04|2.00E-04|\n\nTable 12: The training hyperparameters used for different GPT-3 adaption methods. We use the same hyperparameters for all datasets after tuning learning rate.\n\nRally, we replace them after every Transformer block with an input agnostic vector. Thus, both the embeddings and subsequent Transformer block activations are treated as trainable parameters. For more on prefix-layer tuning, see Section 5.1.\n\nIn Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI. First of all, LoRA+PE significantly outperforms both LoRA and prefix-embedding tuning on WikiSQL, which indicates that LoRA is somewhat orthogonal to prefix-embedding tuning. On MultiNLI, the combination of LoRA+PE doesn\u2019t perform better than LoRA, possibly because LoRA on its own already achieves performance comparable to the human baseline. Secondly, we notice that LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We attribute this to the fact that prefix-layer tuning is very sensitive to the choice of learning rate and thus makes the optimization of LoRA weights more difficult in LoRA+PL.\n\n# F ADDITIONAL EMPIRICAL EXPERIMENTS\n\n# F.1 ADDITIONAL EXPERIMENTS ON GPT-2\n\nWe also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017) following the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result on E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with prefix-based approaches given the same number of trainable parameters.\n\n|Method|# Trainable Parameters|DART BLEU\u2191|DART MET\u2191|DART TER\u2193|\n|---|---|---|---|---|\n|GPT-2 Medium| | | | |\n|Fine-Tune|354M|46.2|0.39|0.46|\n|AdapterLL|0.37M|42.4|0.36|0.48|\n|Adapter|11M|45.2|0.38|0.46|\n|FTTop2|24M|41.0|0.34|0.56|\n|PrefLayer|0.35M|46.4|0.38|0.46|\n|LoRA|0.35M|47.1\u00b1.2|0.39|0.46|\n|GPT-2 Large| | | | |\n|Fine-Tune|774M|47.0|0.39|0.46|\n|AdapterL|0.88M|45.7\u00b1.1|0.38|0.46|\n|AdapterL|23M|47.1\u00b1.1|0.39|0.45|\n|PrefLayer|0.77M|46.7|0.38|0.45|\n|LoRA|0.77M|47.5\u00b1.1|0.39|0.45|\n\nTable 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are less than 0.01 for all adaption approaches.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de61dcce-8a06-4fb5-bf5a-90071c8c0101": {"__data__": {"id_": "de61dcce-8a06-4fb5-bf5a-90071c8c0101", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1239de6d-e7d8-4883-be0f-190f81e639b1", "node_type": "4", "metadata": {}, "hash": "4f69b086aa0701eaaa1fe7947d096c725753a3a727f5b3a6c11a1e7c3de4a9a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fcbd6b2-9a38-45e8-a1d5-8cdfd676801c", "node_type": "1", "metadata": {}, "hash": "2ec9ece8afc5c5d8abddd879efa40e88ac1741063503f3f03e2f860b6cff1248", "class_name": "RelatedNodeInfo"}}, "text": "# Method\n\n# WebNLG\n\n|Method|BLEU\u2191|BLEU\u2191|BLEU\u2191|MET\u2191|MET\u2191|MET\u2191|TER\u2193|TER\u2193|TER\u2193| | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n| |U|S|A|U|S|A|U|S|A| |\n|GPT-2 Medium|27.7|64.2|46.5|.30|.45|.38|.76|.33|.53| |\n|Fine-Tune (354M)|45.1|54.5|50.2|.36|.39|.38|.46|.40|.43| |\n|AdapterL (0.37M)|48.3|60.4|54.9|.38|.43|.41|.45|.35|.39| |\n|FTTop2 (24M)|18.9|53.6|36.0|.23|.38|.31|.99|.49|.72| |\n|Prefix (0.35M)|45.6|62.9|55.1|.38|.44|.41|.49|.35|.40| |\n|LoRA (0.35M)|46.7\u00b1.4|62.1\u00b1.2|55.3\u00b1.2|.38|.44|.41|.46|.33|.39| |\n|Fine-Tune (774M)|43.1|65.3|55.5|GPT-2 Large|.38|.46|.42|.53|.33|.42|\n|AdapterL (0.88M)|49.8\u00b1.0|61.1\u00b1.0|56.0\u00b1.0|.38|.43|.41|.44|.35|.39| |\n|AdapterL (23M)|49.2\u00b1.1|64.7\u00b1.2|57.7\u00b1.1|.39|.46|.43|.46|.33|.39| |\n|Prefix (0.77M)|47.7|63.4|56.3|.39|.45|.42|.48|.34|.40| |\n|LoRA (0.77M)|48.4\u00b1.3|64.0\u00b1.3|57.0\u00b1.1|.39|.45|.42|.45|.32|.38| |\n\nTable 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER are less than 0.01 for all the experiments we ran. \u201cU\u201d indicates unseen categories, \u201cS\u201d indicates seen categories, and \u201cA\u201d indicates all categories in the test set of WebNLG.\n\n# F.2 ADDITIONAL EXPERIMENTS ON GPT-3\n\nWe present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on identifying the trade-off between performance and the number of trainable parameters.\n\n# F.3 LOW-DATA REGIME\n\nTo evaluate the performance of different adaptation approaches in the low-data regime, we randomly sample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data MNLI-n tasks. In Table 16, we show the performance of different adaptation approaches on MNLI-n. To our surprise, PrefixEmbed and PrefixLayer perform very poorly on MNLI-100 dataset, with PrefixEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PrefixLayer performs better than PrefixEmbed but is still significantly worse than Fine-Tune or LoRA on MNLI-100. The gap between prefix-based approaches and LoRA/Fine-tuning becomes smaller as we increase the number of training examples, which might suggest that prefix-based approaches are not suitable for low-data tasks in GPT-3. LoRA achieves better performance than fine-tuning on both MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the (\u00b10.3) variance due to random seeds. The training hyperparameters of different adaptation approaches on MNLI-n are reported in Table 17. We use a smaller learning rate for PrefixLayer on the MNLI-100 set, as the training loss does not decrease with a larger learning rate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fcbd6b2-9a38-45e8-a1d5-8cdfd676801c": {"__data__": {"id_": "2fcbd6b2-9a38-45e8-a1d5-8cdfd676801c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1239de6d-e7d8-4883-be0f-190f81e639b1", "node_type": "4", "metadata": {}, "hash": "4f69b086aa0701eaaa1fe7947d096c725753a3a727f5b3a6c11a1e7c3de4a9a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de61dcce-8a06-4fb5-bf5a-90071c8c0101", "node_type": "1", "metadata": {}, "hash": "6ad968ffead6e7dd19f6190a8f7da0d3402090de8a4cbe76f4bf9db55db44ea6", "class_name": "RelatedNodeInfo"}}, "text": "33.3%). PrefixLayer performs better than PrefixEmbed but is still significantly worse than Fine-Tune or LoRA on MNLI-100. The gap between prefix-based approaches and LoRA/Fine-tuning becomes smaller as we increase the number of training examples, which might suggest that prefix-based approaches are not suitable for low-data tasks in GPT-3. LoRA achieves better performance than fine-tuning on both MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the (\u00b10.3) variance due to random seeds. The training hyperparameters of different adaptation approaches on MNLI-n are reported in Table 17. We use a smaller learning rate for PrefixLayer on the MNLI-100 set, as the training loss does not decrease with a larger learning rate.\n\n# G MEASURING SIMILARITY BETWEEN SUBSPACES\n\nIn this paper we use the measure \u03c6(A, B, i, j) = \u03c8(U A i, U B j) = \u2016U A i>U B \u20162F min{i,j} to measure the subspace similarity between two column orthonormal matrices U Ai \u2208 Rd\u00d7i and U Bj \u2208 Rd\u00d7j, obtained by taking columns of the left singular matrices of A and B. We point out that this similarity is simply a reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee (2008).", "mimetype": "text/plain", "start_char_idx": 1818, "end_char_idx": 3033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c77a97ec-8bdc-407f-97c5-63f6a09cce52": {"__data__": {"id_": "c77a97ec-8bdc-407f-97c5-63f6a09cce52", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b56ed615-ceeb-4964-bc9e-bb355b4355a4", "node_type": "4", "metadata": {}, "hash": "cf6ead36d44806d9830ee503ac3909459b1371fa4eebb7c9f5161a7db7f57e22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d6de4ff-b77e-4757-826c-0492e208a5d2", "node_type": "1", "metadata": {}, "hash": "f82795d07466fc6acffefde58102ac77f1ad88d9d33ea3b21c8088e56e8e7107", "class_name": "RelatedNodeInfo"}}, "text": "# Hyperparameter Analysis of Different Adaptation Approaches\n\n# Table 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI.\n\n|Method|Hyperparameters|# Trainable Parameters|WikiSQL|MNLI-m|\n|---|---|---|---|---|\n|Fine-Tune|-|175B|73.8|89.5|\n| |lp = 32, li = 8|0.4 M|55.9|84.9|\n| |lp = 64, li = 8|0.9 M|58.7|88.1|\n|PrefixEmbed|lp = 128, li = 8|1.7 M|60.6|88.0|\n| |lp = 256, li = 8|3.2 M|63.1|88.6|\n| |lp = 512, li = 8|6.4 M|55.9|85.8|\n| |lp = 2, li = 2|5.1 M|68.5|89.2|\n| |lp = 8, li = 0|10.1 M|69.8|88.2|\n|PrefixLayer|lp = 8, li = 8|20.2 M|70.1|89.5|\n| |lp = 32, li = 4|44.1 M|66.4|89.6|\n| |lp = 64, li = 0|76.1 M|64.9|87.9|\n| |r = 1|7.1 M|71.9|89.8|\n| |r = 4|21.2 M|73.2|91.0|\n|AdapterH|r = 8|40.1 M|73.2|91.5|\n| |r = 16|77.9 M|73.2|91.5|\n| |r = 64|304.4 M|72.6|91.5|\n| |rv = 2|4.7 M|73.4|91.7|\n| |rq = rv = 1|4.7 M|73.4|91.3|\n|LoRA|rq = rv = 2|9.4 M|73.3|91.4|\n| |rq = rk = rv = ro = 1|9.4 M|74.1|91.2|\n| |rq = rv = 4|18.8 M|73.7|91.3|\n| |rq = rk = rv = ro = 2|18.8 M|73.7|91.7|\n| |rq = rv = 8|37.7 M|73.8|91.6|\n| |rq = rk = rv = ro = 4|37.7 M|74.0|91.7|\n| |rq = rv = 64|301.9 M|73.6|91.4|\n| |rq = rk = rv = ro = 64|603.8 M|73.9|91.4|\n| |rq = rv = 8, lp = 8, li = 4|37.8 M|75.0|91.4|\n|LoRA+PE|rq = rv = 32, lp = 8, li = 4|151.1 M|75.9|91.1|\n| |rq = rv = 64, lp = 8, li = 4|302.1 M|76.2|91.3|\n|LoRA+PL|rq = rv = 8, lp = 8, li = 4|52.8 M|72.9|90.2|\n\n# Table 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B.\n\n|Method|MNLI(m)-100|MNLI(m)-1k|MNLI(m)-10k|MNLI(m)-392K|\n|---|---|---|---|---|\n|GPT-3 (Fine-Tune)|60.2|85.8|88.9|89.5|\n|GPT-3 (PrefixEmbed)|37.6|75.2|79.5|88.6|\n|GPT-3 (PrefixLayer)|48.3|82.5|85.9|89.6|\n|GPT-3 (LoRA)|63.8|85.6|89.2|91.7|\n\n# Projection Metric\n\nTo be concrete, let the singular values of U A i>U B to be \u03c31, \u03c32, \u00b7 \u00b7 \u00b7 , \u03c3p where p = min{i, j}.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d6de4ff-b77e-4757-826c-0492e208a5d2": {"__data__": {"id_": "1d6de4ff-b77e-4757-826c-0492e208a5d2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b56ed615-ceeb-4964-bc9e-bb355b4355a4", "node_type": "4", "metadata": {}, "hash": "cf6ead36d44806d9830ee503ac3909459b1371fa4eebb7c9f5161a7db7f57e22", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c77a97ec-8bdc-407f-97c5-63f6a09cce52", "node_type": "1", "metadata": {}, "hash": "93d897ceb1935907b516c7e1d4a07063e9b0c49a7e4ba0d20d864f45be7e757e", "class_name": "RelatedNodeInfo"}}, "text": "|Method|MNLI(m)-100|MNLI(m)-1k|MNLI(m)-10k|MNLI(m)-392K|\n|---|---|---|---|---|\n|GPT-3 (Fine-Tune)|60.2|85.8|88.9|89.5|\n|GPT-3 (PrefixEmbed)|37.6|75.2|79.5|88.6|\n|GPT-3 (PrefixLayer)|48.3|82.5|85.9|89.6|\n|GPT-3 (LoRA)|63.8|85.6|89.2|91.7|\n\n# Projection Metric\n\nTo be concrete, let the singular values of U A i>U B to be \u03c31, \u03c32, \u00b7 \u00b7 \u00b7 , \u03c3p where p = min{i, j}. We know that the Projection Metric Ham & Lee (2008) is defined as:\n\nd(U Ai, U B) = \u221ap - \u2211\u03c3i\u00b2, i=1 to p, where d \u2208 [0, \u221ap]", "mimetype": "text/plain", "start_char_idx": 1469, "end_char_idx": 1949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84b0b2eb-0474-4cfa-92e4-6b2a309e16a6": {"__data__": {"id_": "84b0b2eb-0474-4cfa-92e4-6b2a309e16a6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5b49194-ea72-4320-91dc-edc64de669a3", "node_type": "4", "metadata": {}, "hash": "915a8525568c07096ce9701c7dfa85d0c4d1b3f6d2042e6a5ace4b65159b8761", "class_name": "RelatedNodeInfo"}}, "text": "# Hyperparameters\n\n|Adaptation| |MNLI-100|MNLI-1k|MNLI-10K|MNLI-392K| |\n|---|---|---|---|---|---|---|\n|Optimizer|-| | |AdamW|-|-|\n|Warmup Tokens|-| | |250,000|-|-|\n|LR Schedule|-| | |Linear|-|-|\n|Batch Size|-|20|20|100|128| |\n|# Epoch|-|40|40|4|2| |\n|Learning Rate|PrefixEmbed|2.00E-04|2.00E-04|4.00E-04|5.00E-04| |\n| |PrefixLayer|5.00E-05|5.00E-05|5.00E-05|1.00E-04| |\n| |LoRA|-|-|-|2.00E-4| |\n| |PrefixEmbed lp|16|32|64|256| |\n|Adaptation-Specific|PrefixEmbed li|-|-|-|8| |\n| |PrefixTune|rlp = rv = 8 = li = 8|-|-|-| |\n| |LoRA|-|-|-|q| |\n\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)-n.\n\nwhere our similarity is defined as:\n\n\u03c6(A, B, i, j) = \u03c8(U A, U B, i, j) = \u2211p=1 \u03c32i i = 1(1 \u2212 d(U Ai, U B j)\u00b2)\n\nThis similarity satisfies that if U Ai and U Bj share the same column span, then \u03c6(A, B, i, j) = 1. If they are completely orthogonal, then \u03c6(A, B, i, j) = 0. Otherwise, \u03c6(A, B, i, j) \u2208 (0, 1).\n\n# ADDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\n\nWe present additional results from our investigation into the low-rank update matrices.\n\n# CORRELATION BETWEEN LORA MODULES\n\nSee Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other layers.\n\n# EFFECT OF r ON GPT-2\n\nWe repeat our experiment on the effect of r (Section 7.2) in GPT-2. Using the E2E NLG Challenge dataset as an example, we report the validation loss and test metrics achieved by different choices of r after training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2 Medium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B. Note that the relationship between model size and the optimal rank for adaptation is still an open question.\n\n# CORRELATION BETWEEN W AND \u2206W\n\nSee Figure 8 for the normalized subspace similarity between W and \u2206W with varying r. Note again that \u2206W does not contain the top singular directions of W, since the similarity between the top 4 directions in \u2206W and the top-10% of those in W barely exceeds 0.2. This gives evidence that \u2206W contains those \u201ctask-specific\u201d directions that are otherwise not emphasized in W. An interesting next question to answer, is how \u201cstrong\u201d do we need to amplify those task-specific directions, in order for the model adaptation to work well?\n\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2309, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0267f680-2bfa-42ef-9f4a-7d8f0a609a22": {"__data__": {"id_": "0267f680-2bfa-42ef-9f4a-7d8f0a609a22", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c28fea0f-0f26-468f-9e3a-cab565d71d0a", "node_type": "4", "metadata": {}, "hash": "0ba4a599132e93700236a9c83df4465840babe561be31cc955948dbe9d6864d6", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 6: Normalized subspace similarity between the column vectors of Ar=8 and Ar=64 for both \u2206Wq and \u2206Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\n\n|Layer|Wq|Wv|\n|---|---|---|\n|1|11.0|0.2|\n|6|6|0.0|\n|12|12| |\n|18|180.8| |\n|23|23| |\n|29|29| |\n|35|35| |\n|40|40| |\n|46|46| |\n|52|52| |\n|58|58| |\n\n# AMPLIFICATION FACTOR\n\nOne can naturally consider a feature amplification factor as the ratio \u2016U >W V >\u2016F , where U and V\u2016\u2206W \u2016F are the left- and right-singular matrices of the SVD decomposition of \u2206W . (Recall U U >W V >V gives the \u201cprojection\u201d of W onto the subspace spanned by \u2206W .)\n\nIntuitively, when \u2206W mostly contains task-specific directions, this quantity measures how much of them are amplified by \u2206W . As shown in Section 7.3, for r = 4, this amplification factor is as large as 20. In other words, there are (generally speaking) four feature directions in each layer (out of the entire feature space from the pre-trained model W ), that need to be amplified by a very large factor 20, in order to achieve our reported accuracy for the downstream specific task. And, one should expect a very different set of feature directions to be amplified for each different downstream task.\n\nOne may notice, however, for r = 64, this amplification factor is only around 2, meaning that most directions learned in \u2206W with r = 64 are not being amplified by much. This should not be surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent the \u201ctask-specific directions\u201d (thus for model adaptation) is low. In contrast, those directions in the rank-4 version of \u2206W (corresponding to r = 4) are amplified by a much larger factor 20.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3e7fafd-088c-4f6e-a8c6-1a9b3ed09427": {"__data__": {"id_": "f3e7fafd-088c-4f6e-a8c6-1a9b3ed09427", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a1d3396a-359c-4e00-acc0-82ff635a98ab", "node_type": "4", "metadata": {}, "hash": "b9db3ce904fc8c8f599d57b3012640ea2856f4e2dafddabaaabf0f696309c2ad", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 7: Normalized subspace similarity between the column vectors of Ar=64 from two randomly seeded runs, for both \u2206Wq and \u2206Wv from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\n\n|Layer|Rank r|val loss|BLEU|NIST|METEOR|ROUGE L|CIDEr|\n|---|---|---|---|---|---|---|---|\n|1|211|1.23|68.72|8.72|0.4565|0.7052|2.4329|\n|32|414|1.18|70.38|8.84|0.4689|0.7186|2.5349|\n|64|8|1.17|69.57|8.74|0.4636|0.7196|2.5196|\n|96|32|1.16|69.33|8.77|0.4642|0.7105|2.5255|\n| |11128|1.16|68.73|8.67|0.4628|0.7127|2.5030|\n| |31512|1.16|68.78|8.68|0.4637|0.7128|2.5025|\n\n# Table 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with different rank r using GPT-2 Medium.\n\nUnlike on GPT-3 where r = 1 suffices for many tasks, here the performance peaks at r = 16 for validation loss and r = 4 for BLEU, suggesting the GPT-2 Medium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our hyperparameters are tuned on r = 4, which matches the parameter count of another baseline, and thus might not be optimal for other choices of r.\n\n# Figure 8: Normalized subspace similarity between the singular directions of Wq and those of \u2206Wq with varying r and a random baseline.\n\n|(Wq, Ar = 4, i, j)|(Wq, Ar = 8, i, j)|(Wq, Ar = 64, i, j)|(Wq, Arand, i, j)|\n|---|---|---|---|\n|451|555|0.200| |\n|658|762|0.175| |\n|865|969|0.150| |\n|1072|1176|0.125| |\n| | |0.100| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b30fde4-09c2-4aec-ac2f-b8b391196e19": {"__data__": {"id_": "3b30fde4-09c2-4aec-ac2f-b8b391196e19", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5f9c112-b2e1-475f-b844-66038abff1f2", "node_type": "4", "metadata": {}, "hash": "532330e410fc1189c39c3d54ba237e42dd9faf016adb6a08e73d5b98d3e212a5", "class_name": "RelatedNodeInfo"}}, "text": "# A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph\n\nCheonsu Jeong1\n\n1 Dr. Jeong is Principal Consultant & the Technical Leader for AI Automation at SAMSUNG SDS;\n\n# Abstract\n\nThis study aims to improve knowledge-based question-answering (QA) systems by overcoming the limitations of existing Retrieval-Augmented Generation (RAG) models and implementing an advanced RAG system based on Graph technology to develop high-quality generative AI services. While existing RAG models demonstrate high accuracy and fluency by utilizing retrieved information, they may suffer from accuracy degradation as they generate responses using pre-loaded knowledge without reprocessing. Additionally, they cannot incorporate real-time data after the RAG configuration stage, leading to issues with contextual understanding and biased information. To address these limitations, this study implemented an enhanced RAG system utilizing Graph technology. This system is designed to efficiently search and utilize information. Specifically, it employs LangGraph to evaluate the reliability of retrieved information and synthesizes diverse data to generate more accurate and enhanced responses. Furthermore, the study provides a detailed explanation of the system's operation, key implementation steps, and examples through implementation code and validation results, thereby enhancing the understanding of advanced RAG technology. This approach offers practical guidelines for implementing advanced RAG systems in corporate services, making it a valuable resource for practical application.\n\n# Keywords\n\nAdvance RAG; Agent RAG; LLM; Generative AI; LangGraph\n\n# I. Introduction\n\nRecent advancements in AI technology have brought significant attention to Generative AI. Generative AI, a form of artificial intelligence that can create new content such as text, images, audio, and video based on vast amounts of trained data models (Jeong, 2023d), is being applied in various fields, including daily conversations, finance, healthcare, education, and entertainment (Ahn & Park, 2023). As generative AI services become more accessible to the general public, the role of generative AI-based chatbots is becoming increasingly important (Adam et al., 2021; Przegalinska et al., 2019; Park, 2024). A chatbot is an intelligent agent that allows users to have conversations typically through text or voice (S\u00e1nchez-D\u00edaz et al., 2018; Jeong & Jeong, 2020). Recently, generative AI chatbots have advanced to the level of analyzing human emotions and intentions to provide responses (Jeong, 2023a). With the advent of large language models (LLMs), these chatbots can now be utilized for automatic dialogue generation and translation (Jeong, 2023b). However, they may generate responses\n\n* Corresponding Author: Cheonsu Jeong; paripal@korea.ac.kr", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f917a2d9-acc0-4957-ac3c-cbf5e0ab70c5": {"__data__": {"id_": "f917a2d9-acc0-4957-ac3c-cbf5e0ab70c5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07f910cd-30a7-4e0a-9609-1f16f392929a", "node_type": "4", "metadata": {}, "hash": "3737ba1df978209266acc85ad6746d4288097ea36fbd672d1a760d7901d636f4", "class_name": "RelatedNodeInfo"}}, "text": "# Current Trends in Generative AI and RAG Models\n\nthat conflict with the latest information and have a low understanding of new problems or domains as they rely on previously trained data (Jeong, 2023c). While 2023 was marked by the release of foundational large language models (LLMs) like ChatGPT and Llama-2, experts predict that 2024 will be the year of Retrieval Augmented Generation (RAG) and AI Agents (Skelter Labs, 2024). However, there are several considerations for companies looking to adopt generative AI services. Companies must address concerns such as whether the AI can provide accurate responses based on internal data, the potential risk of internal data leakage, and how to integrate generative AI with corporate systems. Solutions include using domain-specific fine-tuned LLMs and enhancing reliability with RAG that utilizes internal information (Jung, 2024). When domain-specific information is fine-tuned on GPT-4 LLM, accuracy improves from 75% to 81%, and adding RAG can further increase accuracy to 86% (Angels et al., 2024). RAG models are known for effectively combining internal knowledge retrieval and generation to produce more accurate responses. They offer the advantages of source-based fact provision and addressing data freshness issues through the integration of internal and external knowledge bases. However, the effectiveness of RAG models heavily depends on the quality of the database, directly impacting model performance (Kim, 2024). Traditional RAG models load knowledge once and generate responses without reprocessing, leading to accuracy degradation and an inability to reflect real-time data after the RAG configuration. This process can result in inaccurate responses, particularly when generating answers to complex questions, as the initial vectorized knowledge is used without updating with new information. Furthermore, traditional RAG models struggle to handle various types of questions and may suffer from unrelated documents being used in response due to poor retrieval strategies, along with the hallucination issues observed in LLMs.\n\nThe purpose of this study is to improve the traditional RAG model-based knowledge-based QA system (Jeon et al., 2024) and overcome its limitations by accessing real-time data and verifying whether the retrieved documents are genuinely relevant to the questions. By implementing an enhanced RAG system capable of addressing questions about recent events and real-time data, and being less susceptible to hallucinations, this study aims to improve the quality and performance of generative AI services.\n\nThe introduction of this paper explains the research background and objectives, the limitations of existing RAG models, the importance and contributions of the study, and the structure of the paper. The theoretical background reviews the overview of RAG models, advanced RAG approaches, and case studies of existing research improvements. The design of the advanced RAG model covers the composition flow of advanced RAG, the configuration of Agent RAG, and other enhanced features. The implementation of the advanced RAG system details the overview and application of LangGraph, the system implementation process, and the results. The testing section presents the improved results of the implemented code. Finally, the conclusion summarizes the research findings, discusses the limitations, and outlines directions for future research.\n\n# II. Related Work\n\nFor this study, recent key research papers, journals, and articles related to the RAG model were reviewed. This section provides an overview of the RAG model and describes the advancements leading to the development of the Advanced RAG.\n\n# 2.1. Overview of the RAG Model\n\nThe RAG (Retrieval-Augmented Generation) model combines retrieval and generation to produce answers by integrating document retrieval and generation models (Lewis et al., 2020). To generate an answer to a question, the model first retrieves relevant documents and then uses them to produce the response. This process helps in generating accurate answers to questions. The RAG model can handle various types of questions effectively, even when there is a lack of specific domain knowledge. Consequently, it enhances the accuracy and consistency of information compared to traditional generative models.\n\nThe RAG model consists of two main stages:\n\n- Retrieval Stage: Information relevant to the given question is retrieved through a search engine.\n- Generation Stage: Answers are generated based on the retrieved information.\n\n# 2.1.1. RAG Model Implementation Flow\n\nThe RAG model performs text generation tasks by retrieving information from a given source data and using that information to generate the desired text. The data processing for using RAG involves dividing the original data into smaller chunks, embedding the text data by converting it into numerical vectors, and storing these vectors in a vector store (Microsoft, 2023). The implementation flow of a generative AI service based on the RAG model is depicted in Figure 1 (Jeong, 2023e).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d1016ca-1c49-4391-83dc-ecd0f24fbfef": {"__data__": {"id_": "2d1016ca-1c49-4391-83dc-ecd0f24fbfef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e9b9781-5e82-456a-80e0-5c8d45b01b1e", "node_type": "4", "metadata": {}, "hash": "cfcf10e14223db6ee0e854a5a22a99d51a4f1485539a50ffdc3a8ec60fed16f0", "class_name": "RelatedNodeInfo"}}, "text": "# Gen = plllorm\n\n# PDF; Website, Vcclor LLM\n\n# Youtube Store\n\n# Source DB\n\n# Gathenno Euaclon sputhltocnuniEmbuddina: VeclatStotuPrexnnirShalci\n\n# Figure 1: Implementation Flow of a RAG-based Generative AI Service\n\n# 2.1.2. RAG-Based Vector Store Types\n\nTo establish a RAG (Retrieval-Augmented Generation) system, a vector database is utilized to store knowledge. A typical vector pipeline for vector databases involves three stages: Indexing, Querying, and Post Processing (Devtorium, 2023). Specifically, RAG-based vector store configurations can be categorized into two types as illustrated in Figure 2: one where all source data is pre-stored in the vector store and another where data is dynamically inserted at query time.\n\n# Figure 2: Configuration Types and Processing Procedures of RAG-Based Vector Stores\n\nWhen a company offers internal knowledge through an Open LLM (Large Language Model), ensuring security is a critical issue, making the use of Local LLMs essential. In this scenario, it is effective to employ multiple Local LLMs, each optimized for different tasks. For instance, one LLM might be specialized in generating database queries to retrieve relevant data from multiple source databases, while another LLM could be developed to provide answers based on specific domain knowledge (Jeong, 2023e).\n\n# 2.2. Prior Research on Advanced RAG\n\n# 2.2.1. Methods to Enhance RAG Performance\n\nThe performance of RAG (Retrieval-Augmented Generation) is influenced by the quality of the data that can be composed into prompts based on the results of question processing from external repositories. Recently, various Advanced RAG (Advanced Retrieval-Augmented Generation) methods have been proposed to address the limitations of conventional RAG. Advanced RAG represents an evolved form of the traditional RAG technique, incorporating various optimization methods to overcome its limitations. Recent research by Yunfan G. et al. introduces an optimization strategy that divides the retrieval process into Pre-Retrieval, Retrieval, and Post-Retrieval stages, significantly enhancing information accuracy and processing efficiency through optimization at each stage (Yunfan G. et al., 2024). Additionally, various improvement methods, such as re-ranking based on relevance to enhance accuracy, have been proposed as strategies for improving the quality of RAG systems (Jang Dong-jin, 2024), as outlined in Table 1 (Matt A., 2023). Frameworks like LangChain or LlamaIndex provide libraries for implementing these strategies, making the implementation process more straightforward.\n\n# Table 1: Methods to Enhance RAG Performance\n\n|Method|Descriptions|\n|---|---|\n|Clean your data|When dealing with conflicting or redundant information, it becomes challenging to find the correct context during retrieval. To ensure accurate responses to queries, it is essential to properly structure the documents themselves. One approach is to create summaries for all documents and use these summaries as context.|\n|Explore different index types|While embedding-based similarity search methods generally perform well, they are not always the best solution. For example, in e-commerce, keyword-based search methods may be more suitable for finding specific items such as products. Many systems employ hybrid approaches, where keyword-based searches are used for specific products, and embedding-based searches are applied for general customer information and support.|\n|Experiment with your chunking approach|Chunk size is critically important, with smaller chunks typically yielding better performance; however, they may also lead to issues related to insufficient surrounding context. Generally, smaller chunk sizes aid search systems in identifying relevant contextual information more effectively.|\n|Play around with your base prompt|To reduce hallucinations, prompting should be designed to ensure that responses are based solely on the given contextual information. For example: \"You are a customer support representative, designed to provide assistance based on factual information only. Please answer queries based on the given context information, not.|", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59f71801-587b-40eb-8617-935de9c539ce": {"__data__": {"id_": "59f71801-587b-40eb-8617-935de9c539ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d1ec2ad-747e-4253-bd91-4f9272e45223", "node_type": "4", "metadata": {}, "hash": "cbdc7b352db77d46117fea7a20c46331c966237148634735c2af018d3c9d8e10", "class_name": "RelatedNodeInfo"}}, "text": "# III. Design of Advanced RAG\n\n# Models\n\nIn this chapter, various Advanced RAG approaches proposed in previous research are reviewed, and an enhanced RAG system is designed based on these findings. Specifically, we closely analyze methods such as Self-RAG, Corrective RAG, and Adaptive RAG, and present an implementation model as shown in Figure 3 based on the improvements derived from these analyses. The implementation of the Agent RAG system primarily builds on Corrective RAG, while referencing Self-RAG and Adaptive RAG. The workflow to enhance a typical RAG system involves retrieving document chunks from a vector database and then using an LLM to verify the relevance of each retrieved document chunk to the input query. If all retrieved document chunks are relevant, the responses can be refined to find relevant information and using it to refine the answers. This approach can enhance the accuracy and fluency of the responses (Asai A. et al., 2023).\n\n# Notable advanced RAG approaches currently being researched include the following:\n\n- Self-RAG: This method involves re-searching generated responses to find relevant information.\n- Corrective RAG: This approach employs a Corrective Agent to rectify errors in generated responses. The Corrective Agent identifies errors in the responses and retrieves information to correct them, thereby improving the reliability of the answers (Yan, S.Q. et al., 2024).\n- Adaptive RAG: This method involves selecting the appropriate RAG approach based on the type of question. For instance, Self-RAG may be used for factual questions, while Corrective RAG could be employed for opinion-based questions. By choosing the appropriate method according to the question type, the accuracy of the responses can be improved (Jeong, S. et al., 2024).\n\n# Consider query transformations\n\nWhen the context or domain does not align, fine-tuning the embedding model can enhance performance, particularly for domain-specific terminology. For example, this can involve adapting the model to better handle specialized vocabulary pertinent to a specific domain.\n\n# Start using LLM dev. tools\n\nWhen building a RAG system using LlamaIndex or LangChain, debugging tools can be utilized to identify the sources of documents and context.\n\n# Figure 3: Agent-based advanced RAG workflow\n\n|Edge|Nd|Not Uzefub|\n|---|---|---|\n|Mxk|Edge|Useful|\n|Not Support| | |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "960b7b58-d6f1-41fe-ae5a-e5cf598e6725": {"__data__": {"id_": "960b7b58-d6f1-41fe-ae5a-e5cf598e6725", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8b5862e5-b71f-42f5-8b94-01fca00edd3f", "node_type": "4", "metadata": {}, "hash": "75d41e4ec5f6884fb49732d810a5cd26a8043802ad30496f1b7c2e3ca735ec42", "class_name": "RelatedNodeInfo"}}, "text": "# Advanced RAG System\n\n# 3.1. Design of Advanced RAG Execution Procedures\n\nThe Advanced RAG model maintains the basic flow of a traditional RAG model while incorporating additional processes after the search stage to enhance the accuracy and consistency of responses. The composition and flow of the Advanced RAG model are designed as follows:\n\n1. Input the question.\n2. Search for information related to the question using a search engine.\n3. Extract relevant information related to the question from the search results using an information extractor.\n4. Generate an answer based on the extracted information.\n5. The Agent refines the answer to enhance its accuracy and fluency.\n6. Output the final answer.\n\n# 3.2. Application of Agent RAG\n\nTo apply the enhanced execution procedures, the Agent RAG framework incorporates the concept of an \"Agent\" into the answer generation process, thereby further improving the accuracy and consistency of responses. The Agent serves as a core element in the answer generation process, fulfilling the following roles:\n\n- Answer Evaluation: Assessing the accuracy, fluency, and reliability of the generated responses.\n- Answer Improvement: Enhancing the responses based on the evaluation results.\n\nFigure 3. Agent-based advanced RAG workflow\n\n# 3.3. Application of the LangGraph Module\n\nLangGraph is a module released by LangChain designed to build stateful multi-actor applications using LLMs. It is utilized to create Agent and multi-Agent workflows, allowing for the definition of flows that include essential cycles for most Agent architectures, and providing detailed control over the application's flow and state, which is critical for creating reliable Agents (LangGraph, 2024).\n\nBuilt on top of LangChain, LangGraph facilitates the development of AI agents driven by LLMs by creating essential cyclic graphs. LangGraph treats Agent workflows as cyclic graph structures.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac8e2224-55ac-40d3-9f53-94409eb8c01d": {"__data__": {"id_": "ac8e2224-55ac-40d3-9f53-94409eb8c01d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7108cf2-e11c-4ad3-a4d0-017e967b34c0", "node_type": "4", "metadata": {}, "hash": "63b6b895217dd71a9564a7155a2d66645a049400e5f098800a130fbc8c0ded5e", "class_name": "RelatedNodeInfo"}}, "text": "# Implementation Results of the Advanced RAG System\n\nThis chapter presents the implementation of the RAG model and LangChain framework based on the Advanced RAG concepts introduced in Chapter 3, utilizing data suitable for internal corporate use. The implementation approach and considerations for using LangGraph, which is well-suited for Agent implementation, demonstrated through practical examples.\n\n# 4.1. Development Environment\n\nThe solutions and development platforms applied in this case are based on the framework outlined in Figure 1, and the implementation method utilizing LangGraph and OpenAI LLM is described. The process involves chunking and embedding documents, storing them in ChromaDB, and then transforming them into retrievers for document content search. The results are evaluated, and an Agent RAG Graph is defined and implemented accordingly. The development was carried out using Python, which provides a range of libraries necessary for AI development.\n\nThe development environment for each implementation component is as follows:\n\n- Orchestration Framework: LangChain\n- Agent Graph Workflow: LangGraph\n- Workflow Trace: LangSmith\n- Data Extraction and Chunking: LangChain Modules\n- Embedding: OpenAI\n- Vector Database: Chroma\n- LLM: OpenAI GPT-4-turbo Model\n- Python Development Environment: Google Colab\n\n# 4.2. Results of the step-by-step implementation\n\n# 4.2.1. Installation of Basic Libraries and API Key Setup\n\nBasic libraries, including LangChain for overall orchestration of tasks such as data splitting, OpenAI for API access, ChromaDB for storing RAG knowledge, and a web search library, were installed. To facilitate easy management of knowledge files, Google Drive was integrated with Colab. To ensure security, the keys for various modules were registered in a .env file at a specific location. Figure 4 shows that the .env file containing the keys was successfully read.\n\npip install langchain openai\npip install chromadb pypdf tiktoken\npip install python-dotenv\n# Google Drive Connect\nfrom google.colab import drive\ndrive.mount('/content/drive')\nload_dotenv()\n\n# Figure 4: Installation of Basic Libraries and OpenAI API Key Configuration\n\n# 4.2.2. Retriever Implementation\n\nFor managing internal documents, such as 'Dress Code Standards.pdf', the PyPDFLoader is used to load the document from the specified location. Given the document's characteristics, which include tables, the TextSplitter is adjusted. Instead of using the CharacterTextSplitter with a single delimiter (e.g.,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba2fe01d-c6f7-4a63-911e-845a1bc74cd6": {"__data__": {"id_": "ba2fe01d-c6f7-4a63-911e-845a1bc74cd6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4323689b-3faf-4181-b1c1-fdf3ec3d431d", "node_type": "4", "metadata": {}, "hash": "39d50ecde5fec4981498b48733349a6ab9ce5c690edccc5d90ab72a2efe5f291", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e2a3aa7-b0d5-403f-83fb-12fe8ab67bf3", "node_type": "1", "metadata": {}, "hash": "61c68fb2429e1b2bcaaa6e38fcc367cf2a72ae77b6b82417ff527767f271ccec", "class_name": "RelatedNodeInfo"}}, "text": "# 4.2.3. Evaluation of Search Results\n\nTo assess whether the retrieved documents are relevant to the given question, the implementation is as shown in Figure 6.\n\n|RetrReva|Konacnnin|icoo + ChatProzotinolnto|Mnachiin|pydinticyl|InCor|Ba seNodc|Ficld|Ianacna in olta|\n|---|---|---|---|---|---|---|---|---|\n|Date|outeut|Torra|Cijss Gradelocyeents (Buserode|injry sccrc|Ficldchccting|fetfited cocuncnis|Ba|dccunene|\n|questiom with|crading|ChatOina|#ructuned|arado|tetlate|Jrading|SYS_PROPT|05140|\n|(cicvjrce Fat 0Jeva |uatoi|Cccunttsevaluute:|Jelevanceccntoin|Seaiched Yocu7t|ot nj shcu 0|undicating|OrIiC &|rocv|Resseges|\n|(\"humar|Sarchec nocitonts|Oleseions|{auestion}| | | | | |\n\nThe retrieved documents are linked to user questions. If the document contains keywords related to the user\u2019s query, it is assessed for relevance with the aim of filtering out irrelevant searches. As depicted in Figure 7, relevance is indicated by assigning a binary score of 'yes' or 'no' (e.g., \u201cGRADE: binary_score='yes\u2019\u201d).\n\n|Qtry|considerations|dri-st|Iacticn|nork?|\n|---|---|---|---|---|\n|print(doc)|PF irt(=|inti CPAMF:|questicn\": doc page_content}|docunent\":|\n|Qucry|guicene 5|Putocs|Cenccico| |\n|ConsiJeraliCS|dress seleciio|(1 AI enplovces=|Aeni|cemi|\n|Uress ctandaros|(LJ Haintain|cinoe|busine|<|\n|ale;|eith the opt ion|Jeans|~meakers| |\n\nSubsequently, a question-answer RAG chain is constructed, similar to traditional RAG systems, to integrate with the AI agent as shown in Figure 8.\n\n|Build OA_RAG Chain|Vengche|Core pronets|imcOrC|ChatPronptTenplate|\n|---|---|---|---|---|\n|lenacne|opene|Imoor|Uanachi|6unnablts|\n|Imcor|Runnab|~Passthrouah|hlinnac Hanodi| |\n|Menoche|output carsere|mport Stc@utputParser| | |\n|RAG pronp~Yofot|Jenerat irig ansrer|Pronpt|Assisanc|Rsver|\n|quest on usina|Toloding Fetf ev60|conte}(|Fraament| |\n|(ner e|context|do not|Jnshei|ananei|\n|KMoH|anshe|Constcuct|anseam Vn |-5 5|corresponds|\n|provide|Contet|If the Context VaueRAG does|constfuct nonfornation|anscer|\n|provide|sunnurizcd Jnsher|Questic7|Vuestion\"|lquesticn|\n|Conte[|icontex[|prcnot|ChatpronanTem|atelofonot)|\n|Initialize|corinecti6n|ChatOpena (oca|~njnt|4-turbo'|\n|tcnpcraturc-o)|separate|conte [|0ocuments| |\n|def Torhat docs docs .|returr|Fntn|join(coc PuJe content|docs||\n|Create OA RAG chain| | | | |\n\nUsing the constructed question-answer RAG chain, the relevant answer results are obtained as illustrated in Figure 9.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e2a3aa7-b0d5-403f-83fb-12fe8ab67bf3": {"__data__": {"id_": "9e2a3aa7-b0d5-403f-83fb-12fe8ab67bf3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4323689b-3faf-4181-b1c1-fdf3ec3d431d", "node_type": "4", "metadata": {}, "hash": "39d50ecde5fec4981498b48733349a6ab9ce5c690edccc5d90ab72a2efe5f291", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba2fe01d-c6f7-4a63-911e-845a1bc74cd6", "node_type": "1", "metadata": {}, "hash": "3775ee5756b6d0b44860229420bd4251953676f3eab459a8fa6a5f05da3c88f3", "class_name": "RelatedNodeInfo"}}, "text": "|tcck_docs 'Hhat|Gut|thresnoiderations forreti iever|nycre Gueryselection|result|\n|---|---|---|---|---|\n|context|chain invole question|(opk docs|printlresuit)| |\n|cons dcration 5|drcss|cct Gn|Rchlectino|interachingIit cienidccon uL|\n|acconding|adhacM6|the dress code the businessnit urtThe fespective client| | |\n\nHowever, as shown in Figure 10, when an out-of-context question is attempted, the response \"RAG does not have relevant information\" is received, indicating that the question cannot be answered.", "mimetype": "text/plain", "start_char_idx": 2405, "end_char_idx": 2912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74e498c4-57e1-43de-9e64-3e1fa97cc358": {"__data__": {"id_": "74e498c4-57e1-43de-9e64-3e1fa97cc358", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2d43306-33c1-47b5-99c2-03c72d333ba4", "node_type": "4", "metadata": {}, "hash": "ce95fb33d368c459c60bbb0a735f79bbbcfd23bed5701594a97d6285dbffeadb", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 10: Answer Processing for Unrelated Questions\n\nTo improve this, the question can be rephrased into a more optimized version for web search, as shown in Figure 11. Rewriting the question (question rewriting) helps obtain better contextual information from the web. For example, an incomplete question like \"Tell me what is the capital of the country where BTS is located\" can be improved to \"What is the capital of South Korea, the country where BTS is from?\"\n\n# Figure 11: Question Rewriting and Improved Question\n\nAdditionally, the generated answers are evaluated to determine if they contain hallucinations, as depicted in Figure 12.\n\n# Figure 12: Evaluation of Hallucinations\n\nSubsequently, the answers are assessed for their utility in solving the question, as shown in Figure 13. A binary score of 'yes' or 'no' is assigned to indicate whether the answer is useful for solving the question.\n\n# Figure 13: Evaluation of Answer Relevance\n\n# 4.2.4. Definition of the Agent RAG Graph\n\nTo enhance answer retrieval, the Tavily API is used for web searches, and the connection to this API is established. The Graph State of the Agent is defined, where the state object is passed to each node in the graph. Nodes such as Retrieve, generate_answer, grade_documents, and web_search_add are defined as shown in Figure 14.\n\n# Figure 14: Definition of Web Search Tool and Agent Graph State\n\n# Figure 15: Example of Retrieve Node Graph Implementation\n\nThe Agent RAG Graph can be composed of nodes such as Retrieve, grade_documents, rewrite_query, web_search_add, and generate_answer, as illustrated in Figure 14. The State, consisting of a set of messages, is used to store and represent the state of the agent graph as it passes through various nodes. Figure 15 shows the implementation example of the Retrieve Node Graph, which is used to fetch relevant contextual documents from the vector database. It also defines the node classes for document evaluation (grade_documents), question.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b59250d8-29d8-4934-81f4-dfdd751378ea": {"__data__": {"id_": "b59250d8-29d8-4934-81f4-dfdd751378ea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64ff6217-aa25-4300-af02-ab77711ac109", "node_type": "4", "metadata": {}, "hash": "e4cb3d6a29338b814167bb124b78bb173cc6bbf31c40f0b5bc63f55be3eac60c", "class_name": "RelatedNodeInfo"}}, "text": "# 4.2.5. Implementation of the Agent RAG Graph\n\nIn the implementation phase of the Agent RAG Graph, LangGraph is used to build the Agent into a graph by utilizing the functions developed in the previous section. This involves placing the Agent into relevant nodes and connecting them with defined edges according to the specified workflow. The Agent performs an action that calls the Retrieve function and then adds output information to the state before invoking the Agent. As shown in Figure 16, the StateGraph class is used to define and manage the state-based graph. The provided code sets up the workflow to define the process for retrieving documents or performing other tasks based on the Agent's decisions.\n\n# Figure 17: Graph of the Answer Generation Process\n\n|Define the nodes| | | | | |\n|---|---|---|---|---|---|\n|Forkflon_agent_rag add_node|retrieve|retrieve|retrieve| | |\n|agent_rag add_node|Grans|docunents|grade_docunents|transform|Qupfigrade docunents|\n|Forkflo_agent rag,add_node|reurite_query|search|rerrisearch|Qupryneb search| |\n|Forkflo_agent rag,add_node|generate_answer|generate_answer|generate anshe| | |\n|Bui/d graph| | | | | |\n|Forkflo_agent_rag, set_entry_point|(\"retrieve\")| | | | |\n|Forkflon_agent_rag,add_edge|retrieve|Ocacp|Jocinents| | |\n|agent|[ad add_conditiona|edgese|grade_docunents|decide|Generalp|\n|rpnite|Query|renrite_quety|dpnerat e|ansher|Generale ansher|\n|agent_rag.add_edge|renrite_query|veb_search| | | |\n|agent_rag,add_edge|Sparcn|generate_ansmer| | | |\n|Forkilou aqent|add_conditiona|edgesi| | | |\n|denerate|ansher| | | | |\n|07402|generat ion|docunents_and nmest|cundorted|Oenerate|ansar|\n|useful|END;| | | | |\n|not useful|neb_search| | | | |\n\n# 4.3. Test Results\n\nThe implemented Agent RAG workflow was tested with various questions to improve the accuracy of the answers, and the process of streaming responses to questions can be confirmed through the stream method.\n\n# 4.3.1. Verification of Questions in RAG Knowledge Information\n\nWhen the question \"Tell me the things to consider when choosing a work uniform\" was input for the document \u2018Dress Code Standards\u2019 Figure 18, which is RAG information, the question was rewritten to \"What are the main factors to consider when choosing a work uniform?\" to improve the accuracy of the answer. This resulted in a more accurate response. This process can be confirmed through the streaming of responses to questions using the stream method as shown in Figure 19.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c382a921-4270-441f-a39f-01dc0475a6ef": {"__data__": {"id_": "c382a921-4270-441f-a39f-01dc0475a6ef", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ffee998-698f-47b8-84d1-f94c271167ac", "node_type": "4", "metadata": {}, "hash": "9e003fc4eb548fe6e4aae2e44cfd20c91883ef5cf586584df17a78f3ffd2ed16", "class_name": "RelatedNodeInfo"}}, "text": "# Dress Code Guidelines for Work\n\n# Purpose\n\nIndividuality of each employee including professionalism and decorum.\n\n# Considerations for Dress Selection\n\n# Dress Standards\n\nMaintain neat and simple business casual attire, with the option of jeans.\n\nShorts are permissible only from June to September; they should be lightweight, above the knee length (revealing the upper thigh while standing).\n\nAcceptable clothing includes:\n\n|Category|Clothing|\n|---|---|\n|Jacket|Should be tailored to fit well.|\n|Casual Shirts|Clothes that are not politically charged.|\n|Pants|No ripped pants; neat cotton pants.|\n|Shoes|Formal shoes; no sandals.|\n\n# Attire for Female Employees\n\nBusiness casual clothing that allows you to comfortably combine tops and bottoms (blouses, etc.).\n\nAcceptable clothing includes:\n\n|Category|Clothing|\n|---|---|\n|Blouses|Collarless blouses.|\n|Pants|Primary color cotton pants; no ripped jeans or skirts.|\n\n# Questions\n\nFor inquiries, please contact the HR department.\n\n# References\n\nFigure 18: Dress Code Standards.pdf\n\nFigure 19: Answer Generation Process for Information in RAG Knowledge\n\nFigure 20: Answer Generation Process through Web Search for Information Not in RAG Knowledge\n\nFigure 21: Workflow Trace via LangSmith\n\n# Conclusion and Discussion\n\nThis study has reviewed various methods to enhance the accuracy of RAG and explored the theoretical background of Advanced RAG models aimed at improving knowledge-based QA systems. Through the implementation of a graph-based Agent RAG system, along with specific implementation code and validation results, this research has demonstrated the feasibility of these approaches.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e822eff-880a-4580-bd2d-828068c0a735": {"__data__": {"id_": "1e822eff-880a-4580-bd2d-828068c0a735", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "016912ae-85f0-4be1-898f-0279dd464857", "node_type": "4", "metadata": {}, "hash": "c87104bdb83cc73c44cec2f674c6570933575663fc6e5f48faba3ed166ce5ea2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbce0d03-07a7-405a-a7d4-778ffdb84cc7", "node_type": "1", "metadata": {}, "hash": "4cdedaa7e6c74fe63626a9bb5fbf4e1c2af54307f29460f632c86d7bf8bae88e", "class_name": "RelatedNodeInfo"}}, "text": "# An Enhanced RAG System\n\nThe proposed graph-based Advanced RAG system offers a novel approach that significantly improves RAG performance, addressing the limitations of existing RAG models. The experimental results show that this system markedly enhances the accuracy and relevance of responses to user queries. The utilization of LangGraph's graph technology has effectively assessed the reliability of information, contributing to the improvement of information quality through question rewriting and web search optimization. By enhancing real-time data accessibility and strengthening the system's ability to handle various types of questions, the LangGraph-based method has expanded the potential applications of AI-driven customer support and information retrieval. These findings provide a crucial foundation for the advancement of RAG-based generative AI services.\n\nHowever, several limitations remain. The LangGraph-based system is optimized for specific domains, which may result in performance degradation when applied to other fields. Additionally, the system's complexity may require additional resources for implementation and maintenance. Further validation processes are necessary to ensure the accuracy and reliability of real-time data, which could impact overall system performance. Future research should focus on improving the generalizability of graph-based RAG systems. Expanding the system's applicability through testing and optimization across various domains, as well as developing and validating algorithms to enhance real-time data reliability, will be essential for further performance improvements. Lastly, given the rapid advancements in RAG technology, it is crucial to not only keep pace with technological progress but also to deeply understand and continually improve how information is retrieved and how accurate and reliable answers are generated.\n\n# References\n\n1. Adam, M., Wessel, M., & Benlian, A. (2021). AI-based chatbots in customer service and their effects on user compliance. Electronic Markets, 31(2), 427-445.\n2. Ahn, J., & Park, H. (2023). Development of a Case-Based Nursing Education Program Using Generative Artificial Intelligence. Journal of Korean Academic Society of Nursing Education, 29(3), 234-246. https://doi.org/10.5977/jkasne.2023.29.3.234\n3. Angels B., Vinamra B., Renato L., et al., (2024, January 30). RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. arXiv preprint arXiv:2401.08406.\n4. Asai A., Wu Z., Wang Y., Sil A., Hajishirzi H., (2023, October 17). Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. arXiv preprint arXiv:2310.11511.\n5. Devtorium. (2023, July 26). How Vector Databases Can Enhance Custom AI Solutions. https://devtorium.com/blog/how-vector-databases-can-enhance-custom-ai-solutions/\n6. Jang, D. (2024, February 24). Enhancing Search-Augmented Generation (RAG) Performance Using Korean Reranker. Retrieved from https://aws.amazon.com/ko/blogs/tech/korean-reranker-rag/\n7. Jeon, J., Kim, S., Kim, J., & Yoon, S. (2024, January 31). Solving Knowledge-Based QA Problems Using Search-Augmented Generation (RAG) Technology on Web Application Servers (WAS). Proceedings of the Korean Institute of Communications and Information Sciences Conference, Gangwon.\n8. Jeong, C. S., & Jeong, J. H. (2020). A Study on the Method of Implementing an AI Chatbot to Respond to the POST COVID-19 Untact Era, Journal of Information Technology Services, 19(4), 31\u201347. https://doi.org/10.9716/KITS.2020.19.4.031\n9. Jeong, C. S. (2023a). A Study on the RPA Interface Method for Hybrid AI Chatbot Implementation, KIPS Transactions on Software and Data Engineering, 12(1), 41-50. https://doi.org/10.3745/KTSDE.2023.12.1.41\n10. Jeong, C. S. (2023b). A Case Study in Applying Hyperautomation Platform for E2E Business Process Automation, Information Systems Review, 25(2), 31-56. https://doi.org/10.14329/isr.2023.25.2.031\n11. Jeong, C. S. (2023c).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbce0d03-07a7-405a-a7d4-778ffdb84cc7": {"__data__": {"id_": "fbce0d03-07a7-405a-a7d4-778ffdb84cc7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "016912ae-85f0-4be1-898f-0279dd464857", "node_type": "4", "metadata": {}, "hash": "c87104bdb83cc73c44cec2f674c6570933575663fc6e5f48faba3ed166ce5ea2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e822eff-880a-4580-bd2d-828068c0a735", "node_type": "1", "metadata": {}, "hash": "a610145554cc82b0a53d174ad917aba240efa8c4de766e49ced6beae306d01ad", "class_name": "RelatedNodeInfo"}}, "text": "https://doi.org/10.9716/KITS.2020.19.4.031\n9. Jeong, C. S. (2023a). A Study on the RPA Interface Method for Hybrid AI Chatbot Implementation, KIPS Transactions on Software and Data Engineering, 12(1), 41-50. https://doi.org/10.3745/KTSDE.2023.12.1.41\n10. Jeong, C. S. (2023b). A Case Study in Applying Hyperautomation Platform for E2E Business Process Automation, Information Systems Review, 25(2), 31-56. https://doi.org/10.14329/isr.2023.25.2.031\n11. Jeong, C. S. (2023c). A Study on the Service Integration of Traditional Chatbot and ChatGPT, Journal of Information Technology Applications & Management, 3(4), 11-28. https://doi.org/10.21219/jitam.2023.30.4.001\n12. Jeong, C. S. (2023d). A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture. Advances in Artificial Intelligence and Machine Learning, 3(4). 1588-1618. https://dx.doi.org/10.54364/AAIML.2023.1191\n13. Jeong, C. S. (2023e). Generative AI service implementation using LLM application architecture: based on RAG model and LangChain framework. Journal of Intelligence", "mimetype": "text/plain", "start_char_idx": 3490, "end_char_idx": 4583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33accfda-5170-4b59-9787-b813702a79cc": {"__data__": {"id_": "33accfda-5170-4b59-9787-b813702a79cc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eca9605d-a892-4b3d-8858-7c72e3405936", "node_type": "4", "metadata": {}, "hash": "50e989b819aedfc803a47edd6e573968379aafc8544f756ae796fa0975a7dcc0", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Jeong, C. S. (2023). Domain-specialized LLM: Financial fine-tuning and utilization method using Mistral 7B. Journal of Intelligence and Information Systems, 29(4), 129-164. https://dx.doi.org/10.13088/jiis.2023.29.4.129\n2. Jeong, C. S. (2024). Domain-specialized LLM: Financial fine-tuning and utilization method using Mistral 7B. Journal of Intelligence and Information Systems, 30(1), 93-120. https://dx.doi.org/10.13088/jiis.2024.30.1.093\n3. Jeong, S., Baek, J., Cho, S., Hwang S.J., Park, J.C., Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park. (2024, March 28). Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity. arXiv preprint arXiv:2403.14403.\n4. Kim, J. (2024). A Study on Data Chunking Strategies to Enhance LLM Service Quality Using RAG Techniques. Master's Thesis, Korea University, Seoul.\n5. LangGraph. (2024, July 22). LangGraph Overview. https://langchain-ai.github.io/langgraph/\n6. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., & Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.\n7. Matt, A. (2023, September 19). 10 Ways to Improve the Performance of Retrieval Augmented Generation Systems. https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c\n8. Microsoft. (2023, August 01). Retrieval Augmented Generation using Azure Machine Learning prompt flow. https://learn.microsoft.com/en-us/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2\n9. Park, E. (2024). The Impact of Customers' Regulatory Focus and Familiarity with Generative AI-Based Chatbots on Privacy Disclosure Intent: Focusing on Privacy Calculus Theory. Journal of Knowledge Management Research, 25(2), 49-68. https://doi.org/10.15813/kmr.2024.25.2.003\n10. Przegalinska, A., Ciechanowski, L., Stroz, A., Gloor, P., & Mazurek, G. (2019). In bot we trust: A new methodology of chatbot performance measures. Business Horizons, 62(6), 785-797.\n11. S\u00e1nchez-D\u00edaz, X., Ayala-Bastidas, G., Fonseca-Ortiz, P., Garrido, L. (2018). A Knowledge-Based Methodology for Building a Conversational Chatbot as an Intelligent Tutor, Advances in Computational Intelligence, Vol. 11289. 165-175. https://doi.org/10.1007/978-3-030-04497-8_14\n12. Skelter Labs. (2024, January 5). 2024 Year Of The RAG: Reasons for RAG's Attention and Future Trends. https://www.skelterlabs.com/blog/2024-year-of-the-rag\n13. Yan S.Q., Gu J.C., Zhu Y., Ling Z.H. (2024, March 27). Corrective Retrieval Augmented Generation. arXiv preprint arXiv:2401.15884.\n14. Yunfan G., Yun X., Xinyu G., Kangxiang J., Jinliu P., Yuxi B., Yi D., Jiawei S., Meng W., Haofen W. (2024, March 27). Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv preprint arXiv:2312.10997.\n15. Anderson JC. Current status of chorion villus biopsy. In: Tudenhope D, Chenoweth J, editors. Proceedings of the 4th Congress of the Australian Perinatal Society; 1986. Brisbane, Queensland: Australian Perinatal Society; 1987. p. 190-6.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88a967fc-b8b6-4a22-b679-930c0d669887": {"__data__": {"id_": "88a967fc-b8b6-4a22-b679-930c0d669887", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fdbe99b-a72f-4ea9-9d58-1efbbc0e8af8", "node_type": "4", "metadata": {}, "hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b", "class_name": "RelatedNodeInfo"}}, "text": "# ColPali: Efficient Document Retrieval with Vision Language Models\n\n# Manuel Faysse* 1,3 Hugues Sibille\u22171,4 Tony Wu\u22171 Bilel Omrani1\n\n# Gautier Viaud1 C\u00e9line Hudelot3 Pierre Colombo2,3\n\n# 1Illuin Technology 2Equall.ai\n\n# 3CentraleSup\u00e9lec, Paris-Saclay 4ETH Z\u00fcrich\n\n# manuel.faysse@centralesupelec.fr\n\n# Abstract\n\nDocuments are visually rich structures that convey information through text, as well as tables, figures, page layouts, or fonts. While modern document retrieval systems exhibit strong performance on query-to-text matching, they struggle to exploit visual cues efficiently, hindering their performance on practical document retrieval applications such as Retrieval Augmented Generation. To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieving tasks spanning multiple domains, languages, and settings. The inherent shortcomings of modern systems motivate the introduction of a new retrieval model architecture, ColPali, which leverages the document understanding capabilities of recent Vision Language Models to produce high-quality contextualized embeddings solely from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically faster and end-to-end trainable. We release all project artifacts at https://huggingface.co/vidore.\n\n# 1 Introduction\n\nDocument Retrieval consists in matching a user query to relevant documents in a given corpus. It is central to many industrial applications, either as a standalone ranking system (search engines) or as part of more complex information extraction or Retrieval Augmented Generation (RAG) pipelines. Over recent years, pretrained language models have enabled large improvements in text embedding models. In practical industrial settings, however, the main performance bottleneck for efficient document retrieval is not in embedding model performance but in the prior data ingestion pipeline. To optimize a standard PDF document, many steps are required. First, PDF parsers or Optical Character Recognition (OCR) systems are used to extract words from the pages. Document layout detection models can then be run to segment paragraphs, titles, and other page objects such as tables, figures, and headers. A chunking strategy is then defined to group text passages with some semantic coherence, and modern retrieval setups may even integrate a captioning step to describe visually rich elements in a natural language form, more suitable for embedding models. In our experiments (Table 2), we typically find that optimizing the ingestion pipeline yields much greater performance on visually rich document retrieval than optimizing the text embedding model.\n\n# Contribution 1: ViDoRe\n\nIn this work, we argue that document retrieval systems should not be evaluated solely on the capabilities of text embedding models (Bajaj et al., 2016; Thakur et al., 2021; Muennighoff et al., 2022), but should also", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96c3832a-9271-46fd-ad6f-bef965528748": {"__data__": {"id_": "96c3832a-9271-46fd-ad6f-bef965528748", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7da3d723-b838-417c-bbe8-a24a50b96ea4", "node_type": "4", "metadata": {}, "hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d", "class_name": "RelatedNodeInfo"}}, "text": "# Standard Retrieval\n\n# m 0.66 NDCG@5\n\n|Method|Offline|Online|Similarity Score|\n|---|---|---|---|\n|ColPali (ours)|0.81 NDCG@5|Vision LLM| |\n|MaxSim| |What| |\n|Page|7.22s|Query|22ms|\n|OCR Detection| | | |\n\nFigure 2: ColPali simplifies document retrieval w.r.t. standard retrieval methods while achieving stronger performances with better latencies. Latencies and results are detailed in section 5 and subsection B.5.\n\nConsider the context and visual elements of the documents to be retrieved. To this end, we create and openly release ViDoRe, a comprehensive benchmark to evaluate systems on page-level document retrieval with a wide coverage of domains, visual elements, and languages. ViDoRe targets practical document retrieval settings, in which user queries may require both textual and visual understanding to be correctly matched to relevant documents. We highlight the shortcomings of current text-centric systems in these settings.1\n\n# Contribution 2: ColPali\n\nWe propose a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents purely from their visual features, allowing for subsequent fast query matching with late interaction mechanisms (Khattab and Zaharia, 2020). Our method, ColPali, outperforms all other retrieval systems on ViDoRe while being fast and end-to-end trainable. We release models and code at https://huggingface.co/vidore.\n\n# 2 Problem Formulation & Related Work\n\n# Problem Setting\n\nIn our setting, a retrieval system scores how relevant a document d from corpus D is with respect to a query q. Computing the similarity score s(q, d) \u2208 R+ for each of the |D| documents in the corpus creates a ranking we can use to extract the most relevant documents. In this work, we focus on page-level retrieval: given a query, is the correct document page retrieved by the system? For coherence with existing literature, we further use the term document to refer to individual pages, i.e. the atomic retrieved elements in our setting. As we focus on practical industrial retrieval applications (RAG, search engines) with potentially large corpora sizes, latency constraints are imposed on scoring systems. Most current retrieval systems can be decomposed into (1) an offline indexation phase in which a document index is built and (2) an online querying phase in which a query is matched to documents from the index and where low latency is vital to the user experience.\n\nEfficient document retrieval systems exhibit joint properties of high retrieval performance (R1), low latency during querying (R2), and high throughput during indexation (R3).\n\n# 2.1 Textual Retrieval Methods\n\n# Document Retrieval in Text Space\n\nStatistical methods based on word frequency like TF-IDF (Sparck Jones, 1972) and BM25 (Robertson et al., 1994) are still widely used due to their simplicity.\n\n1 The benchmark leaderboard is hosted publicly at https://huggingface.co/spaces/vidore/vidore-leaderboard to encourage further developments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69557b56-d695-4be0-b06c-a16756271a7a": {"__data__": {"id_": "69557b56-d695-4be0-b06c-a16756271a7a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60e079ef-a199-49c3-ac39-40f24a2eccfa", "node_type": "4", "metadata": {}, "hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f", "class_name": "RelatedNodeInfo"}}, "text": "# 2.2 Integrating Visual features\n\n# Contrastive Vision Language Models\n\nMapping latent representations of textual content to corresponding representations of visual content has been done by aligning disjoint visual and text encoders through contrastive losses (Radford et al., 2021; Zhai et al., 2023). While some OCR capabilities exist in these models, the visual component is often not optimized for text understanding. The Fine-grained Interactive Language-Image Pre-training (Yao et al., 2021) framework extends the late interaction mechanism to cross-modal vision-language models, relying on max similarity operations between text tokens and image patches.\n\n# Visually Rich Document Understanding\n\nTo go beyond text, some document-focused models jointly encode text tokens alongside visual or document layout features (Appalaraju et al., 2021; Kim et al., 2021; Huang et al., 2022; Tang et al., 2022). Large Language transformer Models (LLMs) with strong reasoning capabilities have recently been combined with Vision Transformers (ViTs) (Dosovitskiy et al., 2020) to create VLMs (Alayrac et al., 2022; Liu et al., 2023b; Bai et al., 2023; Lauren\u00e7on et al., 2024) where image patch vectors from contrastively trained ViT models (Zhai et al., 2023) are fed as input embeddings to the language model and concatenated with the text-token embeddings.\n\n# PaliGemma\n\nThe PaliGemma-3B model (Lucas Beyer* et al., 2024) extends concepts from Pali3 (Chen et al., 2023), and projects SigLIP-So400m/14 (Alabdulmohsin et al., 2023) patch embeddings into Gemma-2B\u2019s text vector space (Gemma Team et al., 2024). Along with its reasonable size w.r.t. other performant VLMs, an interesting property of PaliGemma\u2019s text model is that it is fine-tuned with full-block attention on the prefix (instruction text and image tokens). VLMs display enhanced capabilities in Visual Question Answering, captioning, and document understanding (Yue et al., 2023), but are not optimized for retrieval tasks.\n\n# 3 The ViDoRe Benchmark\n\nExisting benchmarks for contrastive vision-language models primarily evaluate retrieval for natural images (Lin et al., 2014; Borchmann et al., 2021; Thapliyal et al., 2022). On the other hand, textual retrieval benchmarks (Muennighoff et al., 2022) are evaluated at the textual passage level and are not tailored for document retrieval tasks. We fill the gap with ViDoRe, a comprehensive benchmark for document retrieval using visual features.\n\n# 3.1 Benchmark Design\n\nViDoRe is designed to comprehensively evaluate retrieval systems on their capacity to match queries to relevant documents at the page level. This benchmark encompasses multiple orthogonal subtasks, with focuses on various modalities - text, figures, infographics, tables; thematic domains - medical,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "352ba65b-fb57-40bf-8ddf-ce1e29548366": {"__data__": {"id_": "352ba65b-fb57-40bf-8ddf-ce1e29548366", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "128bdb06-8cf0-4df1-b56d-fdd8ffd2769d", "node_type": "4", "metadata": {}, "hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe", "class_name": "RelatedNodeInfo"}}, "text": "# Dataset\n\n|Dataset|# Queries|Domain|\n|---|---|---|\n|Academic Tasks| | |\n|DocVQA (eng)|500 (500)|Industrial|\n|InfoVQA (eng)|500 (500)|Infographics|\n|TAT-DQA (eng)|1600 (1600)|Varied Modalities|\n|arXiVQA (eng)|500 (500)|Scientific Figures|\n|TabFQuAD (fra)|210 (210)|Tables|\n|Practical Tasks| | |\n|Energy (eng)|100 (1000)|Scientific|\n|Government (eng)|100 (1000)|Administrative|\n|Healthcare (eng)|100 (1000)|Medical|\n|AI (eng)|100 (1000)|Scientific|\n|Shift Project (fra)|100 (1000)|Environment|\n\nTable 1: ViDoRe comprehensively evaluates multimodal retrieval methods. The size of the document corpus is indicated in parentheses.\n\n# Academic Tasks\n\nWe repurpose widely used visual question-answering benchmarks for retrieval tasks: for each page-question-answer triplet, we use the question as the query, and the associated page as the gold document (Table 1). These academic datasets either focus on single specific modalities (Mathew et al., 2020, 2021; Li et al., 2024) or target more varied visually rich documents (Zhu et al., 2022). Moreover, we consider TabFQuAD, a human-labeled dataset on tables extracted from French industrial PDF documents released with this work. Details can be found in subsection A.1.\n\n# Practical tasks\n\nWe construct topic-specific retrieval benchmarks spanning multiple domains to go beyond repurposed QA datasets and evaluate retrieval in more realistic industrial situations (e.g. RAG). To achieve this, we collect publicly accessible PDF documents and generate queries pertaining to document pages using Claude-3 Sonnet, a high-quality proprietary vision-language model (Anthropic, 2024). In total, we collect 1,000 document pages per topic, which we associate with 100 queries extensively filtered for quality and relevance by human annotators. The corpus topics are intentionally specific to maximize syntactic proximity between documents, creating challenging retrieval tasks and covering an array of orthogonal domains (Table 1). Query-page pair examples are shown in Appendix E.2.\n\n# Evaluation Metrics\n\nWe evaluate performance on our benchmark (Requirement R1) using standard metrics from the retrieval literature (NDCG, Recall@K, MRR). We report NDCG@5 values as the main performance metric in this work and release the complete sets of results along with the models.\n\nTo validate compliance with practical industrial constraints, we also consider query latencies (R2) and indexing throughputs (R3).\n\n# 3.2 Assessing Current Systems\n\n# Unstructured\n\nWe evaluate retrieval systems representative of those found in standard industrial RAG pipelines. As is common practice, we rely on the Unstructured off-the-shelf tool in the highest resolution settings to construct high-quality text chunks from PDF documents. Unstructured orchestrates the document parsing pipeline, relying on deep learning vision models to detect titles and document layouts (Ge et al., 2021), OCR engines (Smith, 2007) to extract text in non-native PDFs, specialized methods or models to detect and reconstruct tables, and implements a chunking strategy (by-title) that leverages the detected document structure to preserve section boundaries when concatenating texts. As is common practice, in our simplest Unstructured configuration (text-only), only textual elements are kept, and figures, images, and tables are considered noisy information and are filtered out.\n\n# Unstructured + X\n\nWhile Unstructured is a strong baseline by itself, we further augment Unstructured\u2019s output by integrating the visual elements. In (+ OCR), tables, charts, and images are run through an OCR engine, processed by Unstructured, and chunked independently. In (+ Captioning), we set up a fully-fledged captioning strategy (Zhao et al., 2023), in which we feed visual elements to a strong proprietary Vision Language Model (Claude-3 Sonnet (Anthropic, 2024)) to obtain highly detailed textual descriptions of the elements. Both strategies aim to integrate visual elements in the retrieval pipeline but incur significant latency and resource costs (subsection 5.2).\n\n# Embedding Model\n\nTo embed textual chunks, we evaluate Okapi BM25, the de facto standard sparse statistical retrieval method, and the dense encoder of BGE-M3 (Chen et al., 2024), a multilingual neural method with SOTA performance in its size category. Chunks are embedded and scored independently, and page-level scores are obtained by.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "deef38ef-2baa-47ec-a91c-8b57e945d812": {"__data__": {"id_": "deef38ef-2baa-47ec-a91c-8b57e945d812", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "689b0e3e-8117-4195-8759-dc5bb0960102", "node_type": "4", "metadata": {}, "hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34b1e964-5f20-4079-b6a6-3bd00159c71d", "node_type": "1", "metadata": {}, "hash": "7e3111e040a674730fa702d7902f35c16b8c433334150ac0ae151ea4fab47ce1", "class_name": "RelatedNodeInfo"}}, "text": "max-pooling over the page\u2019s chunk scores.5\n\n# Contrastive VLMs\n\nWe also evaluate the strongest available vision-language embedding models; Jina CLIP (Koukounas et al., 2024), Nomic Embed Vision (Nomic, 2024), and SigLIP-So400m/14 (Alabdulmohsin et al., 2023).\n\n# Results\n\nFrom a performance perspective, best results are obtained by combining the Unstructured parser with visual information, either from captioning strategies or by running OCR on the visual elements (Table 2). Little difference is seen between BM25 and BGE-M3 embeddings highlighting the visual information bottleneck. Contrastive VLMs lag behind. Beyond retrieval performance (R1), the indexing latencies (R2) reported in Figure 3 illustrate that PDF parsing pipelines can be very lengthy, especially when incorporating OCR or captioning strategies. Querying latencies at runtime (R3) are very good for all evaluated systems (\u2264 22 ms on NVIDIA L4) due to fast query encoding and cosine similarity matching.\n\n|PDF Parser|Latency (s)|\n|---|---|\n|PDF Parser|(7.22s)|\n|Siglip|(0.12s)|\n|ColPali|(0.39s)|\n\n0 1 2 3 4 5 6 7\n\nLatency (s)\n\nLayout Detection OCR Captioning Page Encoding\n\n# Figure 3\n\nOffline indexing with ColPali is much simpler and faster compared to standard retrieval methods. Indexing speeds reported are computed on Nvidia L4 GPUs and detailed in subsection B.5.\n\n# 4 Late interaction based Vision Retrieval\n\n# 4.1 Architecture\n\nVision-Language Models. Encouraged by their strong document understanding capabilities, we propose adapting recent VLMs for retrieval. The key concept is to leverage the alignment between output embeddings of text and image tokens acquired during multi-modal finetuning. To this extent, we introduce ColPali, a Paligemma-3B extension that is capable of generating ColBERT-style multi-vector representations of text and images (Figure 2). PaliGemma-3B is a strong candidate due to its small size, the many released checkpoints fine-tuned for different image resolutions and tasks,\n\n5We empirically validated the max-pooling strategy over sub-page chunks to be more effective than concatenating all page chunks before embedding pagewise.\n\nand the promising performances on various document understanding benchmarks. We add a projection layer to map the output language modeling embeddings to a vector space of reduced dimension D = 128 as used in the ColBERT paper (Khattab and Zaharia, 2020) to keep lightweight bag-of-embedding representations.\n\n# Late Interaction\n\nGiven query q and document d, we denote as Eq \u2208 RNq \u00d7D and Ed \u2208 RNd\u00d7D their respective multi-vector representation in the common embedding space RD. The late interaction operator, LI (q, d), is the sum over all query vectors Ed(j), of its maximum dot product \u27e8\u00b7|\u00b7\u27e9 with each of the Nd document embedding vectors Ed(1:Nd).\n\nLI (q, d) = \u2211i\u2208[|1,Nq|] maxj\u2208[|1,Nd|] \u27e8Eq(i)|Ed(j)\u27e9\n\n# Contrastive Loss\n\nThe Late Interaction operation is fully differentiable, enabling backpropagation. Let a batch {qk, dk}k\u2208[|1,b|] composed of b query-page pairs, where for all k \u2208 [|1, b|], the document page dk is the document corresponding to query qk. Following Khattab and Zaharia (2020), we define our in-batch contrastive loss L as the softmaxed cross-entropy of the positive scores\n\nL+ = LI (dk, qk) w.r.t. to the maximal negative scores L- = maxl,l\u2260k LI (qk, pl).\n\n# 4.2 Model training\n\nDataset. Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages6. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nParameters. All models are trained for 1 epoch on the train set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34b1e964-5f20-4079-b6a6-3bd00159c71d": {"__data__": {"id_": "34b1e964-5f20-4079-b6a6-3bd00159c71d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "689b0e3e-8117-4195-8759-dc5bb0960102", "node_type": "4", "metadata": {}, "hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "deef38ef-2baa-47ec-a91c-8b57e945d812", "node_type": "1", "metadata": {}, "hash": "b5899512ab126ca7b0cc5fe2a825e271dfbea6ea3e49469597376829a500a0db", "class_name": "RelatedNodeInfo"}}, "text": "to the maximal negative scores L- = maxl,l\u2260k LI (qk, pl).\n\n# 4.2 Model training\n\nDataset. Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages6. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nParameters. All models are trained for 1 epoch on the train set. Unless specified otherwise, we train models in bfloat16 format, use low-rank adapters (LoRA, Hu et al. (2021)) with \u03b1 = 32 and r = 32 on the transformer layers from the language model,\n\n6Multilingual data is present in the pretraining corpus of the language model (Gemma-2B) and potentially occurs during PaliGemma-3B\u2019s multimodal training.", "mimetype": "text/plain", "start_char_idx": 3266, "end_char_idx": 4349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bdfb0db-8258-4e14-90b3-cf81cd649223": {"__data__": {"id_": "4bdfb0db-8258-4e14-90b3-cf81cd649223", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2b2a7b7-fcec-4af1-b893-a7a5f1b6fb6d", "node_type": "4", "metadata": {}, "hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4", "class_name": "RelatedNodeInfo"}}, "text": "# Results\n\n# 5.1 Performance (R1)\n\nWe iteratively construct ColPali, starting from an off-the-shelf SigLIP model (Table 2).\n\n# BiSigLIP: Improving a strong model.\n\nSigLIP7 is a strong vision-language bi-encoder model, pre-trained on the English split of WebLI (Chen et al., 2023), a corpus of billions of image-text pairs. We find that SigLIP largely outperforms both Jina CLIP and Nomic-vision on document retrieval tasks. Further fine-tuning the textual component of this model on our document-oriented dataset (BiSigLIP) yields clear improvements across the board, particularly on figure retrieval (ArxivQA) and table retrieval tasks (TabFQuAD).\n\n# BiPali: Pairing with a language model.\n\nIn the PaliGemma model architecture, SigLIP-generated patch embeddings are fed to a text language model to obtain LLM contextualized output patch embeddings.8 We average pool these representations to obtain a single dense vector, effectively creating a PaliGemma bi-encoder model (BiPali). After fine-tuning on the training dataset, we obtain a model that performs slightly worse in English than the tuned BiSigLIP variant. This can be explained by the fact that contrary to SigLIP, the original PaliGemma is not trained on contrastive matching tasks, but rather on next token prediction. Our contrastive fine-tuning phase on 100K images to transform PaliGemma into a bi-encoder is 5 orders of magnitude smaller than SigLIP\u2019s original contrastive training. However, we see notable improvements in French tasks, indicating that BiPali\u2019s LLM (Gemma 2B) helps multilingual text understanding. This is particularly notable as our training dataset does not contain non-English samples.\n\n# ColPali: Adding Late Interaction.\n\nOne benefit of inputting image patch embeddings through a language model is that they are natively mapped to a latent space similar to textual input (query). This enables leveraging the ColBERT strategy to compute interactions between text tokens and image patches, which enables a step-change improvement in performance compared to BiPali. Results in Table 2 show that our ColPali model also largely outperforms the strong baselines based on Unstructured and captioning, as well as all evaluated text-image embedding models. The difference is particularly stark on the more visually complex benchmark tasks, such as InfographicVQA, ArxivQA, and TabFQuAD representing respectively infographics, figures, and tables. However, text-centric documents are also better retrieved by the ColPali models across all evaluated domains and languages, making our approach the overall best-performing document-retrieval model.\n\n# Negative Results.\n\nFor extensiveness, we also train ColSigLIP, a late interaction variant of the BiSigLIP model but obtain abysmal performances. We attribute this to the large gaps w.r.t. SigLIP\u2019s pre-training, in which only a pooled latent representation is used in the contrastive loss, which does not optimize the representations of individual patch and token embeddings. Similarly, we train a BiSigLIPP aliGemma variant, in which we retrieve the image representations from the SigLIP model that has been further updated by PaliGemma fine-tuning, and use the text representations from PaliGemma\u2019s text model. After fine-tuning on our dataset, performance is severely inferior to SigLIP vanilla which simply encodes with SigLIP\u2019s original text and vision components. This indicates a logical misalignment between SigLIP embeddings, and Gemma embeddings after PaliGemma training. We detail these results in Table 5.\n\n# 5.2 Latencies & Memory Footprint\n\n# Online Querying. (R2)\n\nLogically, querying latencies differ between ColPali and a BGE-M3 embedding model. For BGE, encoding takes about 22 ms for 15 tokens, while encoding a query with ColPali\u2019s language model takes about 30 ms9. For smaller corpus sizes, computing the late interaction operation induces marginally small overheads (\u2248 1 ms per 1000 pages in the corpus), and the cosine similarity computation between bi-encoder vectors.\n\n9Computed for a batch size of 1 (online), and averaged over 1000 queries. See subsection B.5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5068953-bb3b-455b-82ef-0de4ed741b8b": {"__data__": {"id_": "f5068953-bb3b-455b-82ef-0de4ed741b8b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "292d5589-3184-4ff3-b433-03daeaee1cba", "node_type": "4", "metadata": {}, "hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4579b779-7f65-4b09-8941-acf64d4fcb5d", "node_type": "1", "metadata": {}, "hash": "231a44d96b44ef7cef97f4d52fc7bf0c58617c4a316ef1d4aacccf09c35c8c27", "class_name": "RelatedNodeInfo"}}, "text": "# Table 2: Comprehensive evaluation of baseline models and our proposed method on ViDoRe.\n\nResults are presented using NDCG@5 metrics, and illustrate the impact of different components. Text-only metrics are not computed for benchmarks with only visual elements.\n\n|Model|Metrics| | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy Gov.| |Health.|Avg.| |\n|- BM25|-|34.1|-|-|44.0|59.6|90.4|78.3|78.8|82.6| | |\n|- BGE-M3|-|28.4\u21935.7|-|-|36.1\u21937.9|68.5\u21918.9|88.4\u21932.0|76.8\u21931.5|77.7\u21931.1|84.6\u21912.0| | |\n|- BM25|31.6|36.8|62.9|46.5|62.7|64.3|92.8|85.9|83.9|87.2|65.5| |\n|- BGE-M3|31.4\u21930.2|25.7\u219311.1|60.1\u21932.8|70.8\u219124.3|50.5\u219312.2|73.2\u21918.9|90.2\u21932.6|83.6\u21932.3|84.9\u21911.0|91.1\u21913.9|66.1\u21910.6| |\n|- BM25|40.1|38.4|70.0|35.4|61.5|60.9|88.0|84.7|82.7|89.2|65.1| |\n|- BGE-M3|35.7\u21934.4|32.9\u21935.4|71.9\u21911.9|69.1\u219133.7|43.8\u219317.7|73.1\u219112.2|88.8\u21910.8|83.3\u21931.4|80.4\u21932.3|91.3\u21912.1|67.0\u21911.9| |\n|Contrastive VLMs|Jina-CLIP|25.4|11.9|35.5|20.2|3.3|3.8|15.2|19.7|21.4|20.8|17.7|\n|Nomic-vision|17.1|10.7|30.1|16.3|2.7|1.1|12.9|10.9|11.4|15.7|12.9| |\n|SigLIP (Vanilla)|43.2|30.3|64.1|58.1|26.2|18.7|62.5|65.7|66.1|79.1|51.4| |\n|Ours|SigLIP (Vanilla)|43.2|30.3|64.1|58.1|26.2|18.7|62.5|65.7|66.1|79.1|51.4|\n|BiSigLIP (+fine-tuning)|58.5\u219115.3|32.9\u21912.6|70.5\u21916.4|62.7\u21914.6|30.5\u21914.3|26.5\u21917.8|74.3\u219111.8|73.7\u21918.0|74.2\u21918.1|82.3\u21913.2|58.6\u21917.2| |\n|BiPali (+LLM)|56.5\u2193-2.0|30.0\u2193-2.9|67.4\u2193-3.1|76.9\u219114.2|33.4\u21912.9|43.7\u219117.2|71.2\u2193-3.1|61.9\u2193-11.7|73.8\u2193-0.4|73.6\u2193-8.8|58.8\u21910.2| |\n|ColPali (+Late Inter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4579b779-7f65-4b09-8941-acf64d4fcb5d": {"__data__": {"id_": "4579b779-7f65-4b09-8941-acf64d4fcb5d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "292d5589-3184-4ff3-b433-03daeaee1cba", "node_type": "4", "metadata": {}, "hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5068953-bb3b-455b-82ef-0de4ed741b8b", "node_type": "1", "metadata": {}, "hash": "e95fa380b755b4c34d72a363da79a8c57174e8d488e9a48e2fd996853cc786e8", "class_name": "RelatedNodeInfo"}}, "text": ")|79.1\u219122.6|54.4\u219124.5|81.8\u219114.4|83.9\u21917.0|65.8\u219132.4|73.2\u219129.5|96.2\u219125.0|91.0\u219129.1|92.7\u219118.9|94.4\u219120.8|81.3\u219122.5| |\n\n# 5.3 Interpretability\n\nBy superimposing the late interaction heatmap on top of the original image, we can visualize the most salient image patches with respect to each term of the query, yielding interpretable insights into model focus zones. As epitomized in Figure 1, we observe ColPali exhibits strong OCR capabilities as both the words \"hourly\" and \"hours\" present a high similarity score with the query token &lt;_hour&gt;. We also note particular focus on other non-trivial image features such as the x-axis representing hours being salient. Other visualization examples with similar trends of the model transcending pure OCR are shown in Appendix C.\n\n# 6 Ablation study\n\nShould we scale models or patch numbers? We train a variant of PaliGemma with half the number of image patches (512). While there is a clear performance degradation w.r.t. to the 1024-patch ColPali model (Figure 4), memory usage is much lower.\n\nWhile another PaliGemma variant exists with 2048 patches, the different training datamix and the large memory requirements make this model impractical for both training.", "mimetype": "text/plain", "start_char_idx": 1511, "end_char_idx": 2719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2912b69-f359-4b7d-a54e-36e1420fd862": {"__data__": {"id_": "d2912b69-f359-4b7d-a54e-36e1420fd862", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0570d2b-7f87-4bb8-a029-0c83c23aea38", "node_type": "4", "metadata": {}, "hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84", "class_name": "RelatedNodeInfo"}}, "text": "# Relative NDCG@5 (%)\n\n| |ColPali|Idefics2|No Mem.|Full IB|Train|TabF|\n|---|---|---|---|---|---|---|\n|(512)|(64)|Tokens|Loss|Vision|Tuning| |\n\nFigure 4: Relative NDCG@5 performance gain w.r.t. the default ColPali (1024 patches). TabFQuAD fine-tuning measures the performance difference on the TabFQuAD task after the introduction of targeted data in the training set. All other results refer to performance deltas averaged on all ViDoRe tasks.\n\nIdefics2-8B (Lauren\u00e7on et al., 2024), a VLM with a similar architecture and based on a Mistral-7B (Jiang et al., 2023) language backbone and a SigLIP vision encoder paired with a perceiver resampler. The most notable differences with PaliGemma lie in the size of the language model (2B and 7B resp.) and the number of image patches (between 512 and 2048 for PaliGemma, and 64 post-resampling for Idefics212). Our results (Figure 4) suggest language model size has a strong impact on performance, and along with the trained resampler enables more efficient representations for smaller numbers of image embeddings - ColIdefics2 with 64 patches edges out ColPali with 512 patches. Scaling the number of patches of the smaller ColPali model from 512 to 1024, enables largely surpassing the 60-patch ColIdefics2 while being about twice as fast in terms of training and inference latency. These results suggest there are tradeoffs between performance (R1), latencies during online querying (R2) and offline indexation phases (R3), and index memory size.\n\n# Should we fine-tune the vision component?\n\nWe run our contrastive finetuning on a ColPali model in which we also train the vision encoder and the projection layer. Results in Figure 4 show this leads to no significant improvements.\n\n# Do \"query augmentation\" tokens help?\n\nIn ColBERT, special tokens are concatenated to the input query to serve as soft query augmentation buffers. Training without these tokens, we observe no significant performance difference (Figure 4) in the English benchmarks. However, performance on the French tasks seems to improve (Table 5) and inference time.\n\nWith the option of adding 4 sub-image crops of 64 tokens each to the sequence, for a total of 320 tokens.\n\n# Is the Pairwise CE loss best?\n\nTraining with an in-batch negative contrastive loss, instead of the pairwise CE loss that only considers the hardest negative sample, leads to a slight performance degradation (\u22122.4%) on the aggregated benchmark.\n\n# Can the model adapt to new tasks?\n\nContrary to more complex multi-step retrieval pipelines, ColPali can be trained end-to-end, directly optimizing the downstream retrieval task which greatly facilitates fine-tuning to boost performance on specialized domains, multilingual retrieval, or specific visual elements the model struggles with. To demonstrate, we add 1552 samples representing French tables and associated queries to the training set. This represents the only French data in the training set, with all other examples being kept unchanged. We see significant NDCG@5 improvements (Figure 4) and even starker Recall@1 gains (+6.63%) on the TabFQuAD benchmark, with no performance degradation on the rest of the benchmark tasks (+0.34%).\n\n# 7 Conclusions\n\nThrough the conception of a new benchmark ViDoRe, we established the limits of both modern industrial document retrieval pipelines and off-the-shelf image-text contrastive models for visually rich document retrieval. We introduced ColPali, a novel retrieval model that leverages the latest generative Vision Language models to create highly performing multi-vector embeddings purely from visual document features. ColPali largely outperforms the best existing document retrieval methods while enabling faster corpus indexing time and maintaining low querying latencies, suggesting a very high potential for industrial document retrieval applications. We hope to encourage future work by publicly releasing the ViDoRe benchmark and all models and baselines from our study.\n\n# Future Work\n\nFurther performance gains could be obtained by exploring sub-image decomposition (Liu et al., 2023a), optimal image patch resampling strategies (Lauren\u00e7on et al., 2024), or hard-negative mining. Subsequently, our vision is to combine visual retrieval and visually grounded query answering to create RAG systems that purely function from visual features. An interesting line of research could be attempting to generate answers leveraging information stored in the indexed multi-vector patch embeddings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49cb3b07-c632-4a00-9571-c1ad136785cb": {"__data__": {"id_": "49cb3b07-c632-4a00-9571-c1ad136785cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69d4d7ff-482f-4aa8-afeb-114aae7cfe25", "node_type": "4", "metadata": {}, "hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0", "class_name": "RelatedNodeInfo"}}, "text": "# Limitations\n\nFocus. In this work, we evaluate models on document retrieval tasks, covering several modalities (figures, text, tables, infographics). We however primarily focus on PDF-type documents, and evaluating systems on image retrieval with documents stemming from web page screenshots or handwritten documents might be an interesting generalization. We also focus on high-resource languages (English and French) and although we have shown the capacity of the ColPali model to generalize to languages outside of its fine-tuning set, it is unclear how the model would perform on languages that are not as represented in the model\u2019s language backbone. Finally, our setup assumes relevant documents exist, but abstention methods for Information Retrieval systems might be interesting to explore in more practical settings in which confidence estimation might be important (Gisserot-Boukhlef et al., 2024).\n\nSupport. This work relies on multi-vector retrieving derived from the ColBERT late interaction mechanism. Although some vector databases support late interaction engines13, many widely used vector retrieval frameworks do not propose native multi-vector support, and some engineering infrastructure efforts may be required to adapt them to work with ColPali (or ColBERT) models.\n\nData. In the creation of ViDoRe, we partially rely on synthetic query generation based on a commercial large language model, which may induce some amount of bias in the generated queries. To compensate for this, we have iterated on the prompting strategy and given real query examples to the models to help ground generation in realistic settings. We have further manually verified all synthetic queries through a lengthy process to validate their relevance and their quality. Our benchmark also includes many benchmark tasks with no synthetic data, and result trends observed between all tasks are correlated, further confirming the coherence of our benchmark design.\n\n# Ethical Considerations\n\nCarbon Footprint. Our work fully leverages prior pretrained models and training is not particularly compute-intensive. Furthermore, we rely on low-rank adapters to further reduce the computational resources needed, both during training and for storage. Overall, a training run represents about 40 hours of Mi250x AMD GPUs. Our experiments, in total, represent 1405 Mi250x GPU hours from highly efficient compute clusters running on low-carbon nuclear energy, representing a total of around 15kg CO2 eq.\n\nImpact. We believe our work could have a strong impact on improving industrial document retrieval systems. Our method is efficient, performs well, and the additional support towards visually rich information from documents could go a long way in unlocking knowledge sources previously difficult to index or query.\n\nResource Release. For transparency, and to foster future work, we release our comprehensive benchmark under open license and host a public leaderboard14. Our models are released under the same usage license as the base model (Gemma Research license for ColPali, Apache2.0 for ColIdefics2) and should be used as intended by the VLM license.\n\n# Acknowledgements\n\nThis work is partially supported by Illuin Technology, and by a grant from ANRT France. This work was performed using HPC resources from the CINES ADASTRA through Grant 2024-AD011015443. We extend our warm thanks to Jonathan Dong, Caio Corro, Victor Pellegrain and Ender Konukoglu for their valuable feedback on the paper.\n\n# References\n\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. 2023. Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. Publisher: arXiv Version Number: 5.\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a Visual Language Model for Few-Shot Learning. Publisher: arXiv Version Number: 2.\n\nAnthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku.\n\n14https://huggingface.co/spaces/vidore/vidore-leaderboard", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "505ae009-11ad-461a-a326-c1f403176281": {"__data__": {"id_": "505ae009-11ad-461a-a326-c1f403176281", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a81662e-6745-490d-ae37-d99a2d958098", "node_type": "1", "metadata": {}, "hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha. 2021. DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint. Version Number: 2.\n\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. Publisher: arXiv Version Number: 3.\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv preprint. Version Number: 3.\n\nBurton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422\u2013426. Place: New York, NY, USA Publisher: Association for Computing Machinery.\n\n\u0141ukasz Borchmann, Micha\u0142 Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina Szyndler, and Filip Grali\u0144ski. 2021. DUE: End-to-End Document Understanding Benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv preprint. Version Number: 3.\n\nXi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. 2023. PaLI-3 Vision Language Models: Smaller, Faster, Stronger. arXiv preprint. Version Number: 2.\n\nCohere. 2024. Introducing Rerank 3: A New Foundation Model for Efficient Enterprise Search & Retrieval.\n\nTimoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. 2023. Vision Transformers Need Registers. Publisher: [object Object] Version Number: 2.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Publisher: arXiv Version Number: 2.\n\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX: Exceeding YOLO Series in 2021. arXiv preprint. Version Number: 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a81662e-6745-490d-ae37-d99a2d958098": {"__data__": {"id_": "3a81662e-6745-490d-ae37-d99a2d958098", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "505ae009-11ad-461a-a326-c1f403176281", "node_type": "1", "metadata": {}, "hash": "73a1d174247c6265657797b59c09c64dc8a0dbbe89ce49cf76e1b363e58e5267", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b105eae-7e29-4e0a-afc9-2d7b65c3c77b", "node_type": "1", "metadata": {}, "hash": "a33129b68cd64ada00b9243cecd3233f935c166bfedda87a4cb2921a33d65367", "class_name": "RelatedNodeInfo"}}, "text": "2023. Vision Transformers Need Registers. Publisher: [object Object] Version Number: 2.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Publisher: arXiv Version Number: 2.\n\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX: Exceeding YOLO Series in 2021. arXiv preprint. Version Number: 2.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv preprint. Version Number: 4.\n\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, C\u00e9line Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: A simple yet effective abstention mechanism. Preprint, arXiv:2402.12997.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Publisher: arXiv Version Number: 2.\n\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Publisher: arXiv Version Number: 3.", "mimetype": "text/plain", "start_char_idx": 2063, "end_char_idx": 4986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b105eae-7e29-4e0a-afc9-2d7b65c3c77b": {"__data__": {"id_": "5b105eae-7e29-4e0a-afc9-2d7b65c3c77b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a81662e-6745-490d-ae37-d99a2d958098", "node_type": "1", "metadata": {}, "hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "class_name": "RelatedNodeInfo"}}, "text": "Version Number: 4.\n\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, C\u00e9line Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: A simple yet effective abstention mechanism. Preprint, arXiv:2402.12997.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Publisher: arXiv Version Number: 2.\n\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Publisher: arXiv Version Number: 3.\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix.", "mimetype": "text/plain", "start_char_idx": 4371, "end_char_idx": 5288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72929673-b437-4081-94a9-57ce6c25073d": {"__data__": {"id_": "72929673-b437-4081-94a9-57ce6c25073d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fffe6a54-4d42-4a14-a05c-9b64cca9e222", "node_type": "1", "metadata": {}, "hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nand William El Sayed. 2023. Mistral 7B. Publisher: arXiv Version Number: 1.\n\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint. Version Number: 3.\n\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.\n\nGeewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2021. OCR-free Document Understanding Transformer. arXiv preprint. Version Number: 5.\n\nAndreas Koukounas, Georgios Mastrapas, Michael G\u00fcnther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Mart\u00ednez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, and Han Xiao. 2024. Jina CLIP: Your CLIP Model Is Also Your Text Retriever. arXiv preprint. Version Number: 1.\n\nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. 2024. What matters when building vision-language models? arXiv preprint ArXiv:2405.02246 [cs].\n\nJinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Y. Zhao. 2023. Rethinking the Role of Token Retrieval in Multi-Vector Retrieval. arXiv preprint. Version Number: 3.\n\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models. Preprint, arXiv:2403.00231.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. 2014. Microsoft COCO: Common Objects in Context. arXiv preprint. Version Number: 3.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv preprint. Version Number: 2.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. Publisher: arXiv Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fffe6a54-4d42-4a14-a05c-9b64cca9e222": {"__data__": {"id_": "fffe6a54-4d42-4a14-a05c-9b64cca9e222", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72929673-b437-4081-94a9-57ce6c25073d", "node_type": "1", "metadata": {}, "hash": "f30400ea5c39a8dda2862c4f5a6ed31adb53965d1dfd780b04bd6e1abb680842", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab1b2ed2-72ac-4b0d-b667-e034487aa3b8", "node_type": "1", "metadata": {}, "hash": "5b6cd1ec36c79c626e603648645c27d2c8bdf7405d134fa5ee8d442654faaccb", "class_name": "RelatedNodeInfo"}}, "text": "Preprint, arXiv:2403.00231.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. 2014. Microsoft COCO: Common Objects in Context. arXiv preprint. Version Number: 3.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv preprint. Version Number: 2.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. Publisher: arXiv Version Number: 1.\n\nLucas Beyer*, Andreas Steiner*, Andr\u00e9 Susano Pinto*, Alexander Kolesnikov*, Xiao Wang*, Xiaohua Zhai*, Daniel Salz, Maxim Neumann, Ibrahim Al-abdulmohsin, Michael Tschannen, Jeremiah Harmsen, Daniel Keysers, Neil Houlsby, Xi Chen, Emanuele Bugliarello, Thomas Unterthiner, Keran Rong, Matthias Minderer, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Julian Eisenschlos, Manoj Kumar, Matko Bo\u0161njak, Matthias Bauer, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Paul Voigtlaender, Pinelopi Papalampidi, Olivier Henaff, Skanda Kopula, Xi Xiong, Radu Soricut, Model release contributors and general support, Tris Warkentin, Kat Black, Luiz Gustavo Martins, Glenn Cameron, Raj Gundluru, Manvinder Singh, Meg Risdal, Nilay Chauhan, Nate Keating, Nesh Devanathan, Elisa Bandy, Joe Fernandez, Antonia Paterson, Jenny Brennan, Tom Eccles, Pankil Botadra, Ben Bariach, Lav Rai, Minwoo Park, Dustin Luong, Daniel Vlasic, Bo Wu, Wenming Ye, Divyashree Sreepathihalli, Kiranbir Sodhia, Alek Andreev, Armand Joulin, Surya Bhupatiraju, Minh Giang, Joelle Barral, and Zoubin Ghahramani. 2024. PaliGemma.\n\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V Jawahar. 2021. InfographicVQA. arXiv preprint. Version Number: 2.\n\nMinesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2020. DocVQA: A Dataset for VQA on Document Images.\n\nNiklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and Nils Reimers. 2022. MTEB: Massive Text Embedding Benchmark. arXiv preprint. Version Number: 3.\n\nNomic. 2024. Nomic Embed Vision: Expanding The Nomic Latent Space.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. Publisher: arXiv Version Number: 1.\n\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint. Version Number: 1.\n\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109\u2013126. National Institute of Standards and Technology (NIST).\n\nKeshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint. Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 1567, "end_char_idx": 4642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab1b2ed2-72ac-4b0d-b667-e034487aa3b8": {"__data__": {"id_": "ab1b2ed2-72ac-4b0d-b667-e034487aa3b8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fffe6a54-4d42-4a14-a05c-9b64cca9e222", "node_type": "1", "metadata": {}, "hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "class_name": "RelatedNodeInfo"}}, "text": "2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint. Version Number: 1.\n\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109\u2013126. National Institute of Standards and Technology (NIST).\n\nKeshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint. Version Number: 1.\n\nR. Smith. 2007. An Overview of the Tesseract OCR Engine. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2, pages 629\u2013633, Curitiba, Parana, Brazil. IEEE. ISSN: 1520-5363.", "mimetype": "text/plain", "start_char_idx": 4019, "end_char_idx": 4855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4adfed4-ccf9-4e72-9c12-86d00ae9a256": {"__data__": {"id_": "a4adfed4-ccf9-4e72-9c12-86d00ae9a256", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9133d5b3-b632-40b6-b387-ac0329d734a4", "node_type": "4", "metadata": {}, "hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2659d62-fb92-4038-b514-3c9317d1abe4", "node_type": "1", "metadata": {}, "hash": "275baea2efd06dc7d04371ac8ef8f549db38d8eac1441e281d806a98e93a9ea8", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nKaren Sparck Jones. 1972. A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL. Journal of Documentation, 28(1):11\u201321.\n\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. 2022. Unifying Vision, Text, and Layout for Universal Document Processing. arXiv preprint. Version Number: 3.\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. arXiv preprint. Version Number: 4.\n\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. arXiv preprint. Version Number: 2.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint. Version Number: 2.\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. arXiv preprint. ArXiv:2002.10957 [cs].\n\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. FILIP: Fine-grained Interactive Language-Image Pre-Training. arXiv preprint. Version Number: 1.\n\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. arXiv preprint. Version Number: 3.\n\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid Loss for Language Image Pre-Training. Publisher: [object Object] Version Number: 4.\n\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty. 2023. Retrieving Multimodal Information for Augmented Generation: A Survey. arXiv preprint. Version Number: 3.\n\nFengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. 2022. Towards Complex Document Understanding By Discrete Reasoning. Publisher: arXiv Version Number: 3.\n\n# A Benchmark Datasets\n\n# A.1 Academic Datasets\n\nDocVQA (Mathew et al., 2020) includes collected images from the UCSF Industry Documents Library. Questions and answers were manually annotated.\n\nInfoVQA (Mathew et al., 2021) includes infographics collected from the Internet using the search query \u201cinfographics\u201d. Questions and answers were manually annotated.\n\nTAT-DQA (Zhu et al., 2022) is a large-scale Document VQA dataset that was constructed from publicly available real-world financial reports. It focuses on rich tabular and textual content requiring numerical reasoning. Questions and answers were manually annotated by human experts in finance.\n\narXivQA (Li et al., 2024) is a VQA dataset based on figures extracted from arXiv publications. The questions were generated synthetically using GPT-4 Vision.\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2659d62-fb92-4038-b514-3c9317d1abe4": {"__data__": {"id_": "e2659d62-fb92-4038-b514-3c9317d1abe4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9133d5b3-b632-40b6-b387-ac0329d734a4", "node_type": "4", "metadata": {}, "hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4adfed4-ccf9-4e72-9c12-86d00ae9a256", "node_type": "1", "metadata": {}, "hash": "f5cd5448c9452cbe6dbac1ac03e442b08b61542ba0c8aa8aaaffc80752815b7e", "class_name": "RelatedNodeInfo"}}, "text": "Questions and answers were manually annotated.\n\nInfoVQA (Mathew et al., 2021) includes infographics collected from the Internet using the search query \u201cinfographics\u201d. Questions and answers were manually annotated.\n\nTAT-DQA (Zhu et al., 2022) is a large-scale Document VQA dataset that was constructed from publicly available real-world financial reports. It focuses on rich tabular and textual content requiring numerical reasoning. Questions and answers were manually annotated by human experts in finance.\n\narXivQA (Li et al., 2024) is a VQA dataset based on figures extracted from arXiv publications. The questions were generated synthetically using GPT-4 Vision.\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings. We create additional queries to augment the existing human-annotated ones using the same method described in subsection A.2.\n\n# A.2 Practical Datasets\n\nMethodology. Creating a relevant retrieval dataset close to real use cases is a major challenge as the dataset needs to be both sufficiently large for effective fine-tuning and sufficiently diverse to cover a broad range of modalities (full text, tables, charts, ...), domains (industry, healthcare, ...), and query-document interactions (extractive questions, open-ended questions, ...). Our approach to building this dataset involves several steps: (1) we use a web crawler to collect publicly available documents on various themes and sources, (2) we convert these PDFs into a series of images, one per page, and (3) we generate queries related to each image using a VLM.\n\nWeb-Crawler. We implemented a web crawler to efficiently collect large volumes of documents related to a given topic. The crawler is seeded with a user-defined query (e.g. \"artificial intelligence\") and then uses GPT-3.5 Turbo to brainstorm related topics and subtopics. This query augmentation strategy aims at both broadening and deepening the search. GPT-3.5 Turbo is further used to generate diverse search queries from each subtopic.", "mimetype": "text/plain", "start_char_idx": 2614, "end_char_idx": 4670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b8fc04a-98f5-47f5-9d1c-7e379229a881": {"__data__": {"id_": "7b8fc04a-98f5-47f5-9d1c-7e379229a881", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "554e003a-18f7-4226-84fe-3021466167a3", "node_type": "4", "metadata": {}, "hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb", "class_name": "RelatedNodeInfo"}}, "text": "# Query Generation and Document Collection\n\nQuery set is then consumed by a pool of parallel workers whose job is to fetch the associated most relevant documents. We use SerpAPI along with a filetype filter (PDF documents only) to programmatically scrape Google Search rankings. Each file is hashed and stored in a Bloom filter (Bloom, 1970) shared among workers to avoid duplicate documents in the final corpus. Unique scraped files are downloaded, and inserted into a SQLite database along with additional metadata.\n\n# Datamix\n\nUsing the web crawler, we collected approximately 1,000 documents for each of the following four seeds: \"energy\", \"government reports\", \"healthcare industry\", and \"artificial intelligence\". These seeds were meticulously hand-picked to align with real-use cases for retrieval models and visually rich pages. We also removed all documents containing any private information. At this stage, we randomly selected 900 files for the training set and 100 files for the test set, ensuring that data leakage into the test set was avoided during subsequent processing steps.\n\n# Query Generation\n\nTo increase the efficiency of our query generation scheme and to limit API calls, we generate at most 3 questions per image. From all the documents collected, we randomly sample 10,000 images per theme and call Claude-3 Sonnet with the following prompt:\n\nRemember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus. Generate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question. Generate at most THREE pairs of questions and answers per page in a dictionary with the following format, answer ONLY this dictionary NOTHING ELSE:\n\n{\n\"questions\": [\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n}\n]\n}\n\nwhere XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n\nNote: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n\n# Human Validation\n\nWe manually validate every single synthetically created query in ViDoRe to ensure quality, query relevance, and consistency with the benchmark objective of evaluating retrieval in practical industrial settings. During this step, we randomly assign document-pair queries to 4 volunteers.\n\n15 https://serpapi.com/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b470ba55-e6cd-4cee-97f2-c562e542ebd7": {"__data__": {"id_": "b470ba55-e6cd-4cee-97f2-c562e542ebd7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "817bba4f-a97f-4a48-90a0-de713b5685c6", "node_type": "4", "metadata": {}, "hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073", "class_name": "RelatedNodeInfo"}}, "text": "# B     Implementation details\n\n# B.1     Codebase\n\nThe codebase is written in PyTorch16 and leverages HuggingFace tooling for model implementations and trainers17.\n\n# B.2     Pairwise CE loss\n\nOur in-batch contrastive loss L is defined as the softmaxed cross-entropy of the positive scores\n\nsk+ = LI (dk, qk) w.r.t. to the maximal negative scores sk\u2212 = maxl,l\u0338 =k LI (qk, pl).\n\nFor numerical stability, we reformulate the loss with the softplus function, leading to:\n\nL = bk=11 Xsoftplus sk \u2212 skb\n\n# B.3     Hyperparameters\n\nHyperparameters are tuned on a validation split composed of 2% of the training dataset. We find bi-encoder methods to be more sensible to learning rate variations than late interaction-based models and achieve the best performance for all models with a learning rate of 5e \u2212 5. We experiment with LoRA rank and \u03b1 values and do not notice particular improvements past r = \u03b1 = 32. Per-device batch sizes are kept small due to long sequence lengths that complicate scaling past b = 4. Simulating larger batch sizes for in-batch negative sampling should enable even better results. We find the best results with global batch size b = 32 for 1 epoch on our training set.\n\n# B.4     Embedding size\n\nMinimizing storage footprint can be essential to industrial retrieval systems if databases contain millions of documents. With this criterion in view, we have compared the embedding sizes of the models in our study. As shown in Table 3, ColPali\u2019s embedding size is an order of magnitude larger than BM25 and two orders of magnitude larger than BGE-M3. However, this study is limited to the naive method of storing ColPali\u2019s multi-vector embeddings. In practical scenarios, using cluster centroids can reduce the size of ColPali multi-vector embeddings by up to an order of magnitude (Santhanam et al., 2022) and make it a competitive retrieval system.\n\n# B.5     Latency computations\n\nAll latency computations are done on a NVIDIA L4 GPU. Queries are encoded independently (batch size of 1) to simulate online querying, and pages are encoded with a batch size of 4 for PaliGemma derived models, and 8 for BGE-M3. Reported times include image and text processing time before the model forward pass, as well as query-to-index matching times. We note an interesting feature of ColPali is that all documents have the same sequence length, leading to prior knowledge of runtime and memory consumptions. Query latency experiments are averaged over 1000 queries, and indexing times are measured for a 100 page document. Per page time is obtained by diving total time by 100, corresponding to inverse page throughput.\n\n# B.6     Captioning\n\nExamples of captions generated for visually rich document chunks with Claude-3 Sonnet are shown in Figure 6 and Figure 5. The prompt used for generating the description is the following:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b32d07e1-df88-4758-9b50-025a29a84a7d": {"__data__": {"id_": "b32d07e1-df88-4758-9b50-025a29a84a7d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "506cce11-d72a-405c-a08a-4ef5f19ad9bc", "node_type": "4", "metadata": {}, "hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 5: Example from the \"Energy\" test set.\n\nCaption: The image depicts the hourly energy generation profile, illustrating the contributions of various energy sources over 24 hours. The data is presented as a stacked bar chart, with the x-axis representing the hours of the day from 1 to 2, and the y-axis showing the average hourly generation in MW. The bars are segmented into different colors, each representing a distinct energy source: nuclear, bio, geothermal, solar, wind, hydro, natural gas, and other imports. The chart provides insights into the temporal variations in energy generation across different sources, highlighting the interplay between baseload and intermittent sources throughout the day.\n\n# Figure 6: Example from the \"Government Reports\" test set.\n\nCaption: The image shows a table titled \"System of Record\" which outlines the different types of documents or records maintained across various systems or departments within an organization related to project management and construction. The rows list documents like project plans, budgets, schedules, contracts, purchase orders, invoices, change requests, bid submissions, drawings, manuals, meeting minutes, and reports. The columns indicate the system or department responsible for maintaining each record, such as County Servers, Project View, OnBase, CGI Advantage Financial System, and Purchasing Department. The table uses \"W\" and \"T\" markers to denote which system or department serves as the primary source (writer) or storage location (trailer) for each type of document.\n\n# More similarity maps\n\nIn Figure 7, ColPali assigns a high similarity to all patches with the word \"Kazakhstan\" when given the token <_Kazakhstan>. Moreover, our model seems to exhibit world knowledge capabilities as the patch around the word \"Kashagan\" - an offshore oil field in Kazakhstan - also shows a high similarity score. On the other hand, in Figure 8, we observe that ColPali is also capable of complex image understanding. Not only are the patches containing the word \"formulations\" highly similar to the query token _formula, but so is the upper-left molecule structure.\n\nIt is also interesting to highlight that both similarity maps showcase a few white patches with high similarity scores. This behavior might first seem surprising as the white patches should not carry a meaningful signal from the original images. We believe the vectors associated with these patches share a similar role with the ViT registers (Darcet et al., 2023), i.e. these patches were repurposed for internal computations and stored the global information from the whole image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6daebd2d-2c2d-4d8c-b144-b95afa7451fd": {"__data__": {"id_": "6daebd2d-2c2d-4d8c-b144-b95afa7451fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0996ba57-66b0-4db1-b8b3-cd9fa2e3b876", "node_type": "4", "metadata": {}, "hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28", "class_name": "RelatedNodeInfo"}}, "text": "# Historique de production\n\n# Production totale des hydrocarbures liquides\n\n# Kazakhstan (1965-2019)\n\nQuery: \"Quelle partie de la production p\u00e9troli\u00e8re du Kazakhstan provient de champs en mer ?\"\n\n# Ferroelectrics\n\n# Lead Zirconium Titanate\n\nPb(Zr,Ti)O3\n\n1952 Shirane; Pb(Zr,Ti)O3 solid solutions\n\n1955 Jalte coor Berlincourt; Gerson: Complete Study PZT formulations\n\nCurie temperature 170-360\n\nQuery: What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "408e5691-099f-4223-b137-d9c86f1dfbfd": {"__data__": {"id_": "408e5691-099f-4223-b137-d9c86f1dfbfd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "297ac6a3-89db-4283-91b6-8ff124b88fd0", "node_type": "4", "metadata": {}, "hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db357411-c164-4101-8211-5070391e40d4", "node_type": "1", "metadata": {}, "hash": "62816e0b531a77b544cbbfd001f5c8c7767e6aa087dc8bc5a6d4f92170b12b5e", "class_name": "RelatedNodeInfo"}}, "text": "# D. Additional results\n\n# D.1 Other Metrics\n\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy|Gov.|Health.|Avg.| |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Unstructured Text only|BM25|-|26.6|-|-|34.6|45.0|86.0|70.0|68.0|74.0|-|\n| |BGE-M3|-|22.8\u21933.8|-|-|26.1\u21938.5|51.0\u21916.0|81.0\u21935.0|72.0\u21912.0|67.0\u21931.0|77.0\u21913.0|-|\n|Unstructured + OCR|BM25|26.7|28.9|54.0|30.4|50.0|52.0|86.0|77.0|74.0|80.0|55.9|\n| |BGE-M3|28.1\u21911.4|22.9\u21936.0|53.8\u21930.2|55.7\u219125.3|38.6\u219311.4|56.0\u21914.0|82.0\u21934.0|79.0\u21912.0|76.0\u21912.0|83.0\u21913.0|57.5\u21911.6|\n|Unstructured + Captioning|BM25|35.5|30.2|61.5|24.3|49.0|47.0|79.0|76.0|75.0|81.0|55.9|\n| |BGE-M3|29.3\u21936.2|26.0\u21934.2|62.1\u21910.6|58.6\u219134.3|30.6\u219318.4|55.0\u21918.0|80.0\u21911.0|78.0\u21912.0|69.0\u21936.0|83.0\u21912.0|57.2\u21911.3|\n|Contrastive VLMs|Jina-CLIP|19.4|7.3|26.7|12.5|1.6|2.0|11.0|13.0|15.0|17.0|12.6|\n| |Nomic-vision|10.4|6.7|22.1|9.6|1.6|0.0|9.0|9.0|7.0|13.0|8.8|\n| |SigLIP (Vanilla)|34.2|21.3|51.8|46.1|17.9|13.0|50.0|51.0|47.0|65.0|39.7|\n|Ours|(Copied) SigLIP (Vanilla)|34.2|21.3|51.8|46.1|17.9|13.0|50.0|51.0|47.0|65.0|39.7|\n| |BiSigLIP (+fine-tuning)|49.2\u219115.0|23.8\u21912.5|59.0\u21917.2|52.1\u21916.0|20.7\u21912.8|16.0\u21913.0|62.0\u219112.0|61.0\u219110.0|55.0\u21918.0|72.0\u21917.0|47.1\u21917.4|\n| |BiPali (+LLM)|46.4\u2193-2.8|20.0\u2193-3.8|54.6\u2193-4.4|63.2\u219111.1|20.4\u2193-0.4|34.0\u219118.0|59.0\u2193-3.0|45.0\u2193-16.0|57.0\u21912.0|56.0\u2193-16.0|45.6\u2193-1.5|\n| |ColPali (+Late Inter.)|72.4\u219126.0|45.6\u219125.6|74.6\u219120.0|75.4\u219112.1|53.1\u219132.7|55.0\u219121.0|93.0\u219134.0|85.0\u219140.0|85.0\u219128.0|88.0\u219132.0|72.7\u219127.1|\n\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db357411-c164-4101-8211-5070391e40d4": {"__data__": {"id_": "db357411-c164-4101-8211-5070391e40d4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "297ac6a3-89db-4283-91b6-8ff124b88fd0", "node_type": "4", "metadata": {}, "hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "408e5691-099f-4223-b137-d9c86f1dfbfd", "node_type": "1", "metadata": {}, "hash": "00119033c6c21ba634c0f31ec079ed765db6f54b4f9569e4c2947c753f6644b0", "class_name": "RelatedNodeInfo"}}, "text": ")|72.4\u219126.0|45.6\u219125.6|74.6\u219120.0|75.4\u219112.1|53.1\u219132.7|55.0\u219121.0|93.0\u219134.0|85.0\u219140.0|85.0\u219128.0|88.0\u219132.0|72.7\u219127.1|\n\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe. Results are presented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\n\n# D.2 Model Variants\n\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy|Gov.|Health.|Avg.|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|ColSigLIP (PaliGemma)|3.1|3.0|5.1|6.2|2.5|1.0|3.4|3.4|2.3|2.2|3.2|\n|BiSigLIP (PaliGemma)|18.5|14.6|33.4|39.5|16.1|5.2|27.6|32.6|36.6|35.7|26.0|\n|ColSigLIP (Original)|2.6|2.2|2.3|5.7|1.8|1.0|2.6|4.1|1.4|1.5|2.5|\n|ColPali (No Mem. Tokens)|80.4|53.2|82.4|77.4|65.7|63.4|97.0|89.9|93.6|92.4|79.6|\n|ColPali (Best)|79.1|54.4|81.8|83.9|65.8|73.2|96.2|91.0|92.7|94.4|81.3|\n\nTable 5: Evaluation of some \"negative results\" and ablations on ViDoRe; ColPali for reference. Results are presented using NDCG@5 metrics. Text-only metrics are not computed for benchmarks with only visual elements.", "mimetype": "text/plain", "start_char_idx": 1308, "end_char_idx": 2354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdd7138c-81c1-4e18-89c3-02b02d81d930": {"__data__": {"id_": "cdd7138c-81c1-4e18-89c3-02b02d81d930", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "765b95d7-7a8e-473e-9cbe-e435fe78268a", "node_type": "4", "metadata": {}, "hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2", "class_name": "RelatedNodeInfo"}}, "text": "# ViDoRe examples\n\n# Energy\n\n|Query|Response|\n|---|---|\n|What types of accounts or products allow investors to defer paying taxes?| |\n|What is the projected peak electricity demand in California for the year 2030?| |\n|What is the estimated total savings for a PV system in Durham under the net metering (flat rate) billing option over the system\u2019s useful life of 25 years?|Projected 2030 electricity capacities|\n\n# Artificial Intelligence\n\n|Query|Response|\n|---|---|\n|What are some common outcome areas targeted by TAII for different age groups?| |\n|What did the robot monitor to determine when to activate or deactivate the blower motor and blinker?| |\n|What is the key approach used in the PDP architecture?| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fd0f356-20b5-4be9-94f4-f45f8e44c155": {"__data__": {"id_": "5fd0f356-20b5-4be9-94f4-f45f8e44c155", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c478a956-8f9e-492d-a6c1-6ae1cd7e6a9a", "node_type": "4", "metadata": {}, "hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451", "class_name": "RelatedNodeInfo"}}, "text": "# Healthcare Industry\n\n# Query: What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?\n\nFerroelectrics\n\nLojd Circon Um\n\nPblzo Tlal\n\n1952 Shlrano Sufuri\n\n# Query: What government entities are involved in public financing for health care in the US?\n\nUCLA Health System Financing\n\n# Government Reports\n\n# Query: What does the AVPU scale stand for in assessing the level of consciousness of a seriously ill child?\n\n# Query: What are some mandates for the EPA under the Pollution Prevention Act?\n\n# Query: What is the strategy of KPMG Hazem Hassan?\n\n# Query: What is the trust signal score for the consumer industry best-in-class archetype?\n\n# Who we are?\n\nWE\n\nM dats\n\nGMOnsecy\n\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ff732f0-d8c2-445f-8291-0881fa48edeb": {"__data__": {"id_": "9ff732f0-d8c2-445f-8291-0881fa48edeb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43782e9d-aa09-436f-a65d-7b7a5b82c66e", "node_type": "4", "metadata": {}, "hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc", "class_name": "RelatedNodeInfo"}}, "text": "# Shift\n\n# Query: Selon le graphique, quelle est la capacit\u00e9 d\u2019import et la consommation r\u00e9elle de carburants SAF (biocarburants durables pour l\u2019aviation) pr\u00e9vues en 2050 ?\n\n# Query: Quelle partie de la production p\u00e9troli\u00e8re du Kazakhstan provient de champs en mer ?\n\n# Query: Quels sont les pays ayant la plus grande part des d\u00e9couvertes cumul\u00e9es de p\u00e9trole brut en 2020 (en milliers de barils, hors d\u00e9couvertes cumul\u00e9es) ?\n\n|Lucmolo|Fetall|\n|---|---|\n|20| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bbd7ac7-afbc-4c94-a72c-a7c0e8d1e597": {"__data__": {"id_": "9bbd7ac7-afbc-4c94-a72c-a7c0e8d1e597", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df00ac50-61b0-4923-b855-15bb7066d27f", "node_type": "4", "metadata": {}, "hash": "43fcbaf6b2647b08c4e3722db9672a4e54bd90085ad50eb701650750434c509a", "class_name": "RelatedNodeInfo"}}, "text": "# arXiv:2103.15294v1 [cs.AI] 29 Mar 2021\n\n# \u201cWeak AI\u201d is Likely to Never Become \u201cStrong AI\u201d, So What is its Greatest Value for us?\n\n# starBin Liu\n\nFirst posted March 30th, 2021\n\n# Abstract\n\nAI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, \u201cStarcraft\u201d and poker), and protein structure prediction. However, at the same time, AI is also bearing serious controversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In this paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research, termed \u201cweak AI\u201d and \u201cstrong AI\u201d (a.k.a. artificial general intelligence); (3) clarifies how to judge which paradigm a research work should be classified into; (4) discusses what is the greatest value of \u201cweak AI\u201d if it has no chance to develop into \u201cstrong AI\u201d.\n\n# Index Terms\n\nArtificial intelligence, artificial general intelligence, deep learning, weak AI, strong AI\n\n# I. INTRODUCTION\n\nThe last decade has seen impressive applications of AI represented mostly by deep neural networks, i.e., deep learning [1]. The striking point lies in that the computing agent has reached and even surpassed humans in many tasks, e.g., image classification [2], speech recognition [3, 4], games [5\u20137], protein structure prediction [8]. Even ten years ago, it was hard to imagine that AI would achieve so many amazing breakthroughs.\n\nOn the other side, AI is also bearing serious controversies during the same period. Among the critics, Judea Pearl, a pioneer for probabilistic reasoning in AI and a winner of the Turing award, argues that \u201c... all the impressive achievements of deep learning amount to just curve fitting,\u201d and a necessary ability to be supplemented for AI is causal reasoning [9, 10]. Gary Marcus, a professor of cognitive science, starB. Liu is with Zhejiang Lab, Hangzhou, China. e-mail: bins@ieee.org or liubin@zhejianglab.com.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a17bcae-60fa-41d6-807e-d6394d06e32e": {"__data__": {"id_": "3a17bcae-60fa-41d6-807e-d6394d06e32e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8850640-e1e5-4d63-a1b6-b0f874a8eac7", "node_type": "4", "metadata": {}, "hash": "6cd12d9c0749d39671f9a9e094212289c34c2ba057b4e0502e77f2ce6ce02fed", "class_name": "RelatedNodeInfo"}}, "text": "# 2\n\nsummarizes ten limitations of deep learning [11], namely, \u201c... it is data-hungry, ... it has limited capacity for transfer, ... it has no natural way to deal with hierarchical structure, ... it struggles with open-ended inference, ... it is not sufficiently transparent, ... it has not been well integrated with prior knowledge, ... it cannot inherently distinguish causation from correlation, ... it presumes a largely stable world, in ways that may be problematic, ... it works well as an approximation, but its answers often cannot be fully trusted, ... it is difficult to engineer with\u201d. In a recent issue of the journal Frontiers in Psychology, another cognitive scientist J. Mark Bishop argues that AI \u201cis stupid and causal reasoning will not fix it\u201d [12].\n\nIn this paper, I attempt to concisely respond to current controversies about AI. Specifically, I emphasize discrimination between two paradigms of AI research, namely \u201cweak AI\u201d and \u201cstrong AI\u201d (Section II); provide a conceptual guide to judge which paradigm a research work should be classified into (Section II-A), explain why controversies about AI last (Section III), present major views on whether \u201cweak AI\u201d will grow into \u201cstrong AI\u201d (Section IV) and discuss what is the greatest value of \u201cweak AI\u201d if it has no chance to become \u201cstrong AI\u201d (Section V).\n\n# II. WHAT DO \u201cWEAK AI\u201d AND \u201cSTRONG AI\u201d MEAN?\n\n\u201cWeak AI\u201d and \u201cStrong AI\u201d are two terms coined by John Searle in the \u201cChinese room argument\u201d (CRA) [13]. CRA is a thought experiment as follows: \u201cSearle imagines himself alone in a room following a computer program for responding to Chinese characters slipped under the door. Searle understands nothing of Chinese, and yet, by following the program for manipulating symbols and numerals just as a computer does, he sends appropriate strings of Chinese characters back out under the door, and this leads those outside to mistakenly suppose there is a Chinese speaker in the room\u201d [14]. The term \u201cstrong AI\u201d entails that, \u201c... the computer is not merely a tool in the study of the mind; rather, the appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.\u201d In contrast, the term \u201cweak AI\u201d implies that \u201c... the principal value of the computer in the study of the mind is that it gives us a very powerful tool.\u201d J. Mark Bishop summarizes that \u201cweak AI focuses on epistemic issues relating to engineering a simulation of human intelligent behavior, whereas strong AI, in seeking to engineer a computational system with all the causal power of a mind, focuses on the ontological\u201d [12].\n\nI borrow the terms \u201cweak AI\u201d and \u201cstrong AI\u201d here without an intent to discuss CRA. See related discussions in e.g., [15\u201318].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9777acee-6cc8-4661-a04b-592b172a6a01": {"__data__": {"id_": "9777acee-6cc8-4661-a04b-592b172a6a01", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1aa02e05-6795-4fb4-992d-ce56d9fc6590", "node_type": "4", "metadata": {}, "hash": "ca9d3583962f6888b09659920290d0b59a977ab0da9c6582d6a3eee21d1ccd89", "class_name": "RelatedNodeInfo"}}, "text": "# Simply put, \u201cweak AI\u201d represents computational systems that exhibit as if they own human intelligence, but they do not. In contrast, \u201cstrong AI\u201d represents computational systems that have human intelligence. Correspondingly, all AI research can be categorized into two paradigms: one is targeted for realizing \u201cstrong AI\u201d; and the other produces advanced \u201cweak AI\u201d systems to meet a variety of practical needs.\n\n# A. How to Judge a Research Work Belongs to Which Paradigm?\n\nThe biggest motivation for realizing \u201cstrong AI\u201d is to answer the question: what are the generation mechanisms of humansquoteright intelligence and how to implement these mechanisms with a machine. Therefore, given a research work, it is easy to judge whether it belongs to the \u201cstrong AI\u201d paradigm. If this work provides any new and useful clue for us to answer the above question, it falls within the \u201cstrong AI\u201d paradigm; otherwise, it belongs to the \u201cweak AI\u201d paradigm.\n\nBased on the above method, part of the (especially early) works on neural networks that deepen our understanding of the working mechanism of biological neural systems, surely belongs to the \u201cstrong AI\u201d paradigm. On the other hand, most research works that involve artificial neural networks and deep learning, even if they are proposed under the inspiration of research on neuroscience, cognitive science, behavior psychology, they belong to the \u201cweak AI\u201d paradigm as long as they do not give us any new insight on the generation mechanisms of humansquoteright intelligence or on how to better implement mechanisms that have already been found.\n\n# III. WHY CONTROVERSIES ABOUT AI LAST?\n\nIn controversies about AI, party A believes that AI has made substantial progress in the past decade; party B doubts or even negates the development of AI.\n\nI argue that controversies arise mainly because these two parties mix two different concepts, \u201cweak AI\u201d and \u201cstrong AI\u201d, together, when they talk about AI. The fact is that \u201cweak AI\u201d has made substantial progress in the past decade, while \u201cstrong AI\u201d has not. Party A thinks that \u201cweak AI\u201d is an important member of the AI family; progress gained from \u201cweak AI\u201d also belongs to this AI family. In contrast, in the mind of Party B, there always exists one ideal form of AI, namely a realized \u201cstrong AI\u201d, and the \u201cdistance\u201d between current AI and this ideal AI is treated as a criterion for evaluating current AI. Compared with decades ago, current AI still lacks basic human-level abilities such as causal reasoning [9], robust decision making [19], commonsense utilization [20], and knowledge transfer, which implies", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e074fbb-0a90-4dae-9a3a-25e6e604a35f": {"__data__": {"id_": "9e074fbb-0a90-4dae-9a3a-25e6e604a35f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8db11412-4ed1-4c39-91dc-1c155d197d36", "node_type": "4", "metadata": {}, "hash": "95ac358bc219b476177ea99f84bc1a2107242b498f089f0379af0bdb6a80075e", "class_name": "RelatedNodeInfo"}}, "text": "# IV. WILL \u201cWEAK AI\u201d GROW INTO \u201cSTRONG AI\u201d?\n\nA metaphor is often used to reply to this question: the relationship between \u201cweak AI\u201d and \u201cstrong AI\u201d is like that between flying machines and birds. Flying machines are not developed by accurately mimicking birds' flying. Birds perform much better in maneuvering than the most advanced flying machine today. Birds can flexibly re-purpose their behaviors while flying machines cannot. But the appearance of flying machines has met demands of speedy transportation and others. People may think that, since it is unlikely and not necessary for flying machines to develop into birds, then similarly, \u201cweak AI\u201d is unlikely and not necessary to grow into \u201cstrong AI\u201d.\n\nTo formally consider whether \u201cweak AI\u201d will grow into \u201cstrong AI\u201d, let recall the Turing test [21] and the CRA (mentioned in Section II). An example statement of the Turing test is as follows [22]: \u201cOriginally known as the Imitation Game, the test evaluates if a machine's behavior can be distinguished from a human. In this test, there is a person known as the \u201cinterrogator\u201d who seeks to identify a difference between computer-generated output and human-generated ones through a series of questions. If the interrogator cannot reliably discern the machines from human subjects, the machine passes the test. However, if the evaluator can identify the human responses correctly, then this eliminates the machine from being categorized as intelligent.\u201d Through the lens of CRA, Searle argues that the Turing test has serious flaws, as passing the test does not indicate that the machine has consciousness or understanding. The absence of an effective evaluation method hampers the development of \u201cstrong AI\u201d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35314791-4a1e-416d-908c-f7668f875876": {"__data__": {"id_": "35314791-4a1e-416d-908c-f7668f875876", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf4f6a01-18f1-4d3a-8c8d-8710bf99147f", "node_type": "4", "metadata": {}, "hash": "40d2e806636370700174ff136ff612f035ba871b5f7658fbfb9283bee1d6c3aa", "class_name": "RelatedNodeInfo"}}, "text": "# Besides, philosophers and cognitive scientists often use Godel's first incompleteness theorem [23] to argue that a machine cannot generate human's consciousness or understanding. See related discussions in e.g., [12].\n\n# V. WHAT IS THE GREATEST VALUE OF \u201cWEAK AI\u201d FOR US?\n\nIn his most recent paper, Geoffrey Hinton states that \u201cThe difference between science and philosophy is that experiments can show that extremely plausible ideas are just wrong and extremely implausible ones, like learning an entire complicated system by end-to-end gradient descent, are just right\u201d [24]. In [25], Judea Pearl argues that \u201cModern connectionism has in fact been viewed as a Triumph of Radical Empiricism over its rationalistic rivals. Indeed, the ability to emulate knowledge acquisition processes on digital machines offer enormously flexible testing grounds in which philosophical theories about the balance between empiricism and innateness can be submitted to experimental evaluation on digital machines.\u201d\n\nCombining their arguments, one can see that they both attribute recent deep learning's success as a success of empiricism which is data-driven, other than driven by philosophical theory or intuition.\n\nA very important lesson that can be learned from the fast-pacing development and applications of AI in the past decade is that deep learning running on big enough data can produce unexpected shortcuts to solve extremely difficult problems. For example, by combining deep learning, reinforcement learning [26], and Monte Carlo tree search [27], a computer program AlphaGO [28] can win the human champion without having to understand any of the Go-playing strategies that have been accumulated by humans for more than four thousand years. The Generative Pre-trained Transformer 3 (GPT-3) [29] can generate human-like texts through deep learning without having to understand any syntax or semantics underlying the texts.\n\nIt is shown that the greatest value of \u201cweak AI\u201d represented by deep learning lies in that it provides scalable, less-labor-involved, accurate, and generalizable tools for distilling, representing and then exploiting patterns hidden from big data. Although such \u201cweak AI\u201d has no real intelligence, to a large extent it meets urgent needs for scalable, efficient, and accurate processing of big data.\n\nIn a foreseeable future, \u201cweak AI\u201d is likely to become more robustly (with e.g., portfolio [19] or dynamic portfolio methods [30\u201335]), while a big challenge is how to model \u201cunknown unknowns\u201d; it will perform more automatically through e.g., auto machine learning [36], but it cannot become completely automatic provided that \u201cstrong AI\u201d is realized [37]; it may perform as if it owns abilities of cognition and understanding, but it does not.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "880ade31-999f-4e82-a6ef-78550859d536": {"__data__": {"id_": "880ade31-999f-4e82-a6ef-78550859d536", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa86fa59-3492-42b9-a42b-f32b374b009b", "node_type": "4", "metadata": {}, "hash": "477b973cce4001a36ccc3d84301b49e39432cc81d849c2d90506a3ceee57d707", "class_name": "RelatedNodeInfo"}}, "text": "# VI. CONCLUSIONS\n\nAI has made great progress in the past decade. It has influenced almost all facets of human society by providing more efficient algorithmic solutions to representation, management, analysis of multi-modal big data. Controversies about AI last mainly because \u201cweak AI\u201d becomes so strong while \u201cstrong AI\u201d is almost as weak as it was decades ago. Almost all breakthroughs of AI that have attracted the public attention in the past decade are within the \u201cweak AI\u201d paradigm. \u201cWeak AI\u201d is developing much faster than expected. Even ten years ago, one could not imagine that a computer program would beat the human champion soon in playing Go. In contrast, the \u201cfruits\u201d people have got from the \u201cstrong AI\u201d paradigm are not so striking as from \u201cweak AI\u201d. I suggest, when talking about AI in the future, one should better make a statement in advance whether this talk is about \u201cweak AI\u201d or \u201cstrong AI\u201d. In this way, more focused and constructive discussions can be expected.\n\nIn a foreseeable future, \u201cweak AI\u201d cannot develop into \u201cstrong AI\u201d (see why in Section IV), but it provides a channel to synthesize advances obtained from related disciplines such as cloud computing, computer storage, high-speed wireless mobile communications. Through this synthesis of technologies, more advanced algorithmic tools will be developed in the \u201cweak AI\u201d paradigm, then \u201cweak AI\u201d will continue to influence human society more profoundly, through big data. The man-computer symbiosis world that Licklider predicted more than sixty years ago [38] is becoming a reality.\n\n# REFERENCES\n\n1. Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.\n2. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d Advances in neural information processing systems, vol. 25, pp. 1097\u20131105, 2012.\n3. W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke, \u201cThe microsoft 2017 conversational speech recognition system,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5934\u20135938.\n4. G. Saon, G. Kurata, T. Sercu et al., \u201cEnglish conversational telephone speech recognition by humans and machines,\u201d arXiv preprint arXiv:1703.02136, 2017.\n5. D. Silver, T. Hubert, J. Schrittwieser et al., \u201cA general reinforcement learning algorithm that masters chess, shogi, and go through self-play,\u201d Science, vol. 362, no. 6419, pp. 1140\u20131144, 2018.\n6. N. Brown and T. Sandholm, \u201cSuperhuman ai for heads-up no-limit poker: Libratus beats top professionals,\u201d Science, vol. 359, no. 6374, pp. 418\u2013424, 2018.\n7. O. Vinyals, I. Babuschkin, W. M. Czarnecki et al., \u201cGrandmaster level in starcraft ii using multi-agent reinforcement learning,\u201d Nature, vol. 575, no. 7782, pp. 350\u2013354, 2019.\n8. A. W. Senior, R. Evans, J. Jumper et al., \u201cImproved protein structure prediction using potentials from deep learning,\u201d Nature, vol. 577, no. 7792, pp. 706\u2013710, 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79f530ba-9140-4a7d-adba-5a41c28b0da4": {"__data__": {"id_": "79f530ba-9140-4a7d-adba-5a41c28b0da4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9bd26e1-2124-42b9-afcc-d7a70506bee3", "node_type": "4", "metadata": {}, "hash": "4e670de985bf94887739d00bd3fa60d72931fd2992f19e3a46b701b2ceaf6c04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de2f1616-35f7-41da-a1a7-40d3d6a9b107", "node_type": "1", "metadata": {}, "hash": "5bfa94367e0221f9505312a5a1e82a071e116a67c28ec1b9a78b8eb479d97d5f", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. J. Pearl, \u201cThe limitations of opaque learning machines,\u201d Possible minds: twenty-five ways of looking at AI, pp. 13\u201319, 2019.\n2. J. Pearl and D. Mackenzie, \u201cAi canquoterightt reason why,\u201d Wall Street Journal, 2018.\n3. G. Marcus, \u201cDeep learning: A critical appraisal,\u201d arXiv preprint arXiv:1801.00631, 2018.\n4. J. M. Bishop, \u201cArtificial intelligence is stupid and causal reasoning will not fix it,\u201d Frontiers in Psychology, vol. 11, pp. 1\u201318, 2021.\n5. S. John, \u201cMinds, brains, and programs,\u201d Behavioral and Brain Sciences, vol. 3, no. 3, pp. 417\u2013457, 1980.\n6. D. Cole, \u201cThe chinese room argument,\u201d https://plato.stanford.edu/entries/chinese-room/.\n7. G. Rey, \u201cWhatquoterights really going on in Searlequoterights \u201cChinese room\u201d,\u201d Philosophical Studies, vol. 50, no. 2, pp. 169\u201385, 1986.\n8. M. J. Shaffer, \u201cA logical hole in the chinese room,\u201d Minds and Machines, vol. 19, no. 2, pp. 229\u2013235, 2009.\n9. A. Sloman and M. Croucher, \u201cHow to turn an information processor into an understander,\u201d Behavioral and Brain Sciences, vol. 3, no. 3, pp. 447\u2013448, 1980.\n10. M. A. Boden, Computer models of mind: Computational approaches in theoretical psychology. Cambridge University Press, 1988.\n11. T. G. Dietterich, \u201cSteps toward robust artificial intelligence,\u201d AI Magazine, vol. 38, no. 3, pp. 3\u201324, 2017.\n12. G. Marcus, \u201cThe next decade in AI: four steps towards robust artificial intelligence,\u201d arXiv preprint arXiv:2002.06177, 2020.\n13. A. M. Turing, \u201cComputing machinery and intelligence,\u201d in Parsing the turing test. Springer, 2009, pp. 23\u201365.\n14. IBM Cloud Education, \u201cStrong AI,\u201d https://www.ibm.com/cloud/learn/strong-ai.\n15. P. Raatikainen, \u201cGdieresisodelquoterights incompleteness theorems,\u201d https://plato.stanford.edu/entries/goedel-incompleteness/.\n16. G. Hinton, \u201cHow to represent part-whole hierarchies in a neural network,\u201d arXiv preprint arXiv:2102.12627, 2021.\n17. J. Pearl, \u201cRadical empiricism and machine learning research,\u201d Causal Analysis in Theory and Practice (Blog), vol. 26, 2020.\n18. R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.\n19. S. Gelly and D. Silver, \u201cMonte-carlo tree search and rapid action value estimation in computer go,\u201d Artificial Intelligence, vol. 175, no. 11, pp. 1856\u20131875, 2011.\n20. D. Silver, A. Huang, C. J. Maddison et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.\n21. T. B. Brown, B. Mann, N. Ryder et al., \u201cLanguage models are few-shot learners,\u201d arXiv preprint arXiv:2005.14165, 2020.\n22. B. Liu, Y. Qi, and K. Chen, \u201cSequential online prediction in the presence of outliers and change points: an instant temporal structure learning approach,\u201d Neurocomputing, vol. 413, pp. 240\u2013258, 2020.\n23. Y. Qi, B. Liu, Y. Wang, and G. Pan, \u201cDynamic ensemble modeling approach to nonstationary neural decoding in brain-computer interfaces,\u201d Advances in neural information processing systems, pp. 6087\u20136096, 2019.\n24. B. Liu, \u201cRobust particle filter by dynamic averaging of multiple noise models,\u201d in IEEE Intquoterightl Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 4034\u20134038.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de2f1616-35f7-41da-a1a7-40d3d6a9b107": {"__data__": {"id_": "de2f1616-35f7-41da-a1a7-40d3d6a9b107", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9bd26e1-2124-42b9-afcc-d7a70506bee3", "node_type": "4", "metadata": {}, "hash": "4e670de985bf94887739d00bd3fa60d72931fd2992f19e3a46b701b2ceaf6c04", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79f530ba-9140-4a7d-adba-5a41c28b0da4", "node_type": "1", "metadata": {}, "hash": "a21f87be1d7c7e0fbc2b066f74f81c60f0919308e1e456d4be2cbd27ca4d411c", "class_name": "RelatedNodeInfo"}}, "text": "22. B. Liu, Y. Qi, and K. Chen, \u201cSequential online prediction in the presence of outliers and change points: an instant temporal structure learning approach,\u201d Neurocomputing, vol. 413, pp. 240\u2013258, 2020.\n23. Y. Qi, B. Liu, Y. Wang, and G. Pan, \u201cDynamic ensemble modeling approach to nonstationary neural decoding in brain-computer interfaces,\u201d Advances in neural information processing systems, pp. 6087\u20136096, 2019.\n24. B. Liu, \u201cRobust particle filter by dynamic averaging of multiple noise models,\u201d in IEEE Intquoterightl Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 4034\u20134038.\n25. Y. Dai and B. Liu, \u201cRobust video object tracking via bayesian model averaging-based feature fusion,\u201d Optical Engineering, vol. 55, no. 8, pp. 1\u201311, 2016.\n26. B. Liu, \u201cData-driven model set design for model averaged particle filter,\u201d in IEEE Intquoterightl Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 5835\u20135839.\n27. \u2014\u2014, \u201cInstantaneous frequency tracking under model uncertainty via dynamic model averaging and particle filtering,\u201d IEEE Trans. on Wireless Communications, vol. 10, no. 6, pp. 1810\u20131819, 2011.\n28. F. Hutter, L. Kotthoff, and J. Vanschoren, Automated machine learning: methods, systems, challenges. Springer Nature, 2019.\n29. B. Liu, \u201cA very brief and critical discussion on automl,\u201d arXiv preprint arXiv:1811.03822, 2018.\n30. J. Licklider, \u201cMan-computer symbiosis,\u201d IRE Transactions on human factors in electronics, no. 1, pp. 4\u201311, 1960.", "mimetype": "text/plain", "start_char_idx": 2559, "end_char_idx": 4055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3fc9fef-176e-4336-8bfe-29cf61ac25b8": {"__data__": {"id_": "d3fc9fef-176e-4336-8bfe-29cf61ac25b8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d441ac7-0382-4003-85c4-e87e55ba43c0", "node_type": "4", "metadata": {}, "hash": "ec56454cc530e91ce85c97b623342e819a871cf7204e355611ecad6ce788cdd7", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Alec Radford, Girish Sastry, Amanda Askell, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Jong Wook Kim, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n\n# Abstract\n\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. Task-agnostic objectives such as autoregressive and masked language modeling have scaled across many orders of magnitude in compute, model capacity, and data, steadily improving capabilities. The development of \u201ctext-to-text\u201d as a standardized input-output interface (McCann et al., 2018; Radford et al., 2019; Raffel et al., 2019) has enabled task-agnostic architectures to zero-shot transfer to downstream datasets removing the need for specialized output heads or dataset specific customization. Flagship systems like GPT-3 (Brown et al., 2020) are now competitive across many tasks with bespoke models while requiring little to no dataset specific training data.\n\nWe demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\n\nWe release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.\n\n# 1. Introduction and Motivating Work\n\nPre-training methods which learn directly from raw text have revolutionized NLP over the last few years (Dai & Le, 2015; Peters et al., 2018; Howard & Ruder, 2018; Radford et al., 2018; Devlin et al., 2018; Raffel et al., 2019). Equal contribution 1OpenAI, San Francisco, CA 94110, USA. Correspondence to: alec@openai.com, jongwook@openai.com.\n\nLi et al. (2017) then extended this approach to predicting phrase n-grams in addition to individual words and demonstrated the ability of their system to zero-shot transfer to other image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3443094a-9a77-4792-b5a2-25acba5e3435": {"__data__": {"id_": "3443094a-9a77-4792-b5a2-25acba5e3435", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc7115c2-4292-4f6a-bd89-2f0a8e055c4b", "node_type": "4", "metadata": {}, "hash": "52964e8956caa5f485404300f4c2657cc4290ed70e4b4e1727bf3d58883674bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53787251-5f15-4db9-bbff-7827715814c3", "node_type": "1", "metadata": {}, "hash": "2c95329d32d4e57f1527e36c9da1ca96f3d4da5c7cecca1c1e6205c88be8d859", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# (1) Contrastive pre-training\n\n# (2) Create dataset classifier from label text\n\n|plane|car|\n|---|---|\n|Text|A photo of|\n|dog|a {object}.|\n|bird|\u2026|\n\n# (3) Use for zero-shot prediction\n\n| |Image|Image| | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | |I1|I\u00b7T11|I\u00b7T21|I\u00b7T31|\u2026|I\u00b7TN1| | | | | |\n| | |I2|I\u00b7T12|I\u00b7T22|I\u00b7T32|\u2026|I\u00b7TN2| | | | | |\n| | |I3|I\u00b7T13|I\u00b7T23|I\u00b7T33|\u2026|I\u00b7TN3| | | | | |\n| | |IN|IN\u00b7T1|IN\u00b7T2|IN\u00b7T3|\u2026|IN\u00b7TN| | | | | |\n\nFigure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset\u2019s classes.\n\nClassification datasets by scoring target classes based on their dictionary of learned visual n-grams and predicting the one with the highest score. Adopting more recent architectures and pre-training approaches, VirTex (Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al., 2020), and ConVIRT (Zhang et al., 2020) have recently demonstrated the potential of transformer-based language modeling, masked language modeling, and contrastive objectives to learn image representations from text.\n\nWhile exciting as proofs of concept, using natural language supervision for image representation learning is still rare. This is likely because demonstrated performance on common benchmarks is much lower than alternative approaches. For example, Li et al. (2017) reach only 11.5% accuracy on ImageNet in a zero-shot setting. This is well below the 88.4% accuracy of the current state of the art (Xie et al., 2020). It is even below the 50% accuracy of classic computer vision approaches (Deng et al., 2012). Instead, more narrowly scoped but well-targeted uses of weak supervision have improved performance. Mahajan et al. (2018) showed that predicting ImageNet-related hashtags on Instagram images is an effective pre-training task. When fine-tuned to ImageNet these pre-trained models increased accuracy by over 5% and improved the overall state of the art at the time. Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have also demonstrated large gains on a broader set of transfer benchmarks by pre-training models to predict the classes of the noisily labeled JFT-300M dataset.\n\nThis line of work represents the current pragmatic middle ground between learning from a limited amount of supervised \u201cgold-labels\u201d and learning from practically unlimited amounts of raw text. However, it is not without compromises. Both works carefully design, and in the process limit, their supervision to 1000 and 18291 classes respectively. Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their \u201czero-shot\u201d capabilities.\n\nA crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale. While Mahajan et al. (2018) and Kolesnikov et al. (2019) trained their models for accelerator years on millions to billions of images, VirTex, ICMLM, and ConVIRT trained for accelerator days on one to two hundred thousand images. In this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53787251-5f15-4db9-bbff-7827715814c3": {"__data__": {"id_": "53787251-5f15-4db9-bbff-7827715814c3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc7115c2-4292-4f6a-bd89-2f0a8e055c4b", "node_type": "4", "metadata": {}, "hash": "52964e8956caa5f485404300f4c2657cc4290ed70e4b4e1727bf3d58883674bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3443094a-9a77-4792-b5a2-25acba5e3435", "node_type": "1", "metadata": {}, "hash": "4c17104a17166eb34db6f9d78783869750b2ea9d9fbd1559950461ece6e7cf85", "class_name": "RelatedNodeInfo"}}, "text": "This severely curtails their flexibility and limits their \u201czero-shot\u201d capabilities.\n\nA crucial difference between these weakly supervised models and recent explorations of learning image representations directly from natural language is scale. While Mahajan et al. (2018) and Kolesnikov et al. (2019) trained their models for accelerator years on millions to billions of images, VirTex, ICMLM, and ConVIRT trained for accelerator days on one to two hundred thousand images. In this work, we close this gap and study the behaviors of image classifiers trained with natural language supervision at large scale. Enabled by the large amounts of publicly available data of this form on the internet, we create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method of learning from natural language supervision. We study the scalability of CLIP by training a series of eight models spanning almost 2 orders of magnitude of compute and observe that transfer performance is a smoothly predictable function of compute (Hestness et al., 2017; Kaplan et al., 2020). We find that CLIP, similar to the GPT family, learns to perform a wide set of tasks during pre-training including OCR, geo-localization, action recognition, and many others. We measure this by benchmarking the zero-shot transfer performance of CLIP on over 30 existing datasets and find.", "mimetype": "text/plain", "start_char_idx": 3137, "end_char_idx": 4637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e64f9311-e1c0-4873-a0d6-64d5389d9d07": {"__data__": {"id_": "e64f9311-e1c0-4873-a0d6-64d5389d9d07", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2841b048-9911-4aa9-9cb8-fe075bc96d24", "node_type": "4", "metadata": {}, "hash": "d34461b1bd3cf6914fe9c7bf3b9255f656c71b0688588b8978638bee9f7119d4", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 3 Zero-Shot ImageNet Accuracy\n\n40 vision. Although early work wrestled with the complexity of natural language when using topic model and n-gram representations, improvements in deep contextual representation learning suggest we now have the tools to effectively leverage this abundant source of supervision (McCann et al., 2017).\n\n4X efficiency 3X efficiency\n\n15 Learning from natural language has several potential strengths over other training methods. It\u2019s much easier to scale natural language supervision compared to standard crowd-sourced labeling for image classification since it does not require annotations to be in a classic \u201cmachine learning compatible format\u201d such as the canonical 1-of-N majority vote \u201cgold label\u201d. Instead, methods which work on natural language can learn passively from the supervision contained in the vast amount of text on the internet. Learning from natural language also has an important advantage over most unsupervised or self-supervised learning approaches in that it doesn\u2019t \u201cjust\u201d learn a representation but also connects that representation to language which enables flexible zero-shot transfer. In the following subsections, we detail the specific approach we settled on.\n\nIt can be competitive with prior task-specific supervised models. We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient. We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model\u2019s capability. These results have significant policy and ethical implications, which we consider in Section 7.\n\n# 2. Approach\n\n# 2.1. Natural Language Supervision\n\nAt the core of our approach is the idea of learning perception from supervision contained in natural language. As discussed in the introduction, this is not at all a new idea, however terminology used to describe work in this space is varied, even seemingly contradictory, and stated motivations are diverse. Zhang et al. (2020), Gomez et al. (2017), Joulin et al. (2016), and Desai & Johnson (2020) all introduce methods which learn visual representations from text paired with images but describe their approaches as unsupervised, self-supervised, weakly supervised, and supervised respectively.\n\nWe emphasize that what is common across this line of work is not any of the details of the particular methods used but the appreciation of natural language as a training signal. All these approaches are learning from natural language supervision.\n\n# 2.2. Creating a Sufficiently Large Dataset\n\nExisting work has mainly used three datasets, MS-COCO (Lin et al., 2014), Visual Genome (Krishna et al., 2017), and YFCC100M (Thomee et al., 2016). While MS-COCO and Visual Genome are high quality crowd-labeled datasets, they are small by modern standards with approximately 100,000 training photos each. By comparison, other computer vision systems are trained on up to 3.5 billion Instagram photos (Mahajan et al., 2018). YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality. Many images use automatically generated filenames like 20160716 113957.JPG as \u201ctitles\u201d or contain \u201cdescriptions\u201d of camera exposure settings. After filtering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos. This is approximately the same size as ImageNet.\n\nA major motivation for natural language supervision is the large quantities of data of this form available publicly on the internet. Since existing datasets do not adequately reflect this possibility, considering results only on them would underestimate the potential of this line of research. To address this, we constructed a new dataset of 400 million (image, text) pairs collected from a variety of publicly available sources on the Internet. To attempt to cover as broad a set of visual concepts as possible, we search for (image, text) pairs as part of the construction process whose text includes one of a set of 500,000 queries.\n\n1 The base query list is all words occurring at least 100 times in the English version of Wikipedia. This is augmented with bi-grams.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aba90739-dd09-4f5c-a892-7283ccf6bc35": {"__data__": {"id_": "aba90739-dd09-4f5c-a892-7283ccf6bc35", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "227f9f2f-a929-45ce-bc7c-f8e455f35aab", "node_type": "4", "metadata": {}, "hash": "6c67ece6756fe50faf79f90605751fca135ef4d3ddc584ec9a7463718bce2b57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b998e5ba-567b-4c7a-b8dc-7895b7d20daf", "node_type": "1", "metadata": {}, "hash": "f3fe476189f27b793bfb6d78a96edcc314af5b9da128933069d6b945ebe5eb4b", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 2.3. Selecting an Efficient Pre-Training Method\n\nState-of-the-art computer vision systems use very large amounts of compute. Mahajan et al. (2018) required 19 GPU years to train their ResNeXt101-32x48d and Xie et al. (2020) required 33 TPUv3 core-years to train their Noisy Student EfficientNet-L2. When considering that both these systems were trained to predict only 1000 ImageNet classes, the task of learning an open set of visual concepts from natural language seems daunting. In the course of our efforts, we found training efficiency was key to successfully scaling natural language supervision and we selected our final pre-training method based on this metric.\n\nOur initial approach, similar to VirTex, jointly trained an image CNN and text transformer from scratch to predict the caption of an image. However, we encountered difficulties efficiently scaling this method. In Figure 2 we show that a 63 million parameter transformer language model, which already uses twice the compute of its ResNet-50 image encoder, learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-of-words encoding of the same text.\n\nBoth these approaches share a key similarity. They try to predict the exact words of the text accompanying each image. This is a difficult task due to the wide variety of descriptions, comments, and related text that co-occur with images. Recent work in contrastive representation learning for images has found that contrastive objectives can learn better representations than their equivalent predictive objective (Tian et al., 2019). Other work has found that although generative models of images can learn high quality image representations, they require over an order of magnitude more compute than contrastive models with the same performance (Chen et al., 2020a). Noting these findings, we explored training a system to solve the potentially easier proxy task of predicting only which text as a whole is paired with which image and not the exact words of that text. Starting with the same bag-of-words encoding baseline, we swapped the predictive objective for a contrastive objective in Figure 2 and observed a further 4x efficiency improvement in the rate of zero-shot transfer to ImageNet.\n\nGiven a batch of N (image, text) pairs, CLIP is trained to predict which of the N \u00d7 N possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N\u00b2 \u2212 N incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores. In Figure 3 we include pseudocode of the core of an implementation of CLIP. To our knowledge this batch construction technique and objective was first introduced in the area of deep metric learning as the multi-class N-pair loss (Sohn, 2016), was popularized for contrastive representation learning by Oord et al. (2018) as the InfoNCE loss, and was recently adapted for contrastive (text, image) representation learning in the domain of medical imaging by Zhang et al. (2020).\n\nDue to the large size of our pre-training dataset, over-fitting is not a major concern and the details of training CLIP are simplified compared to the implementation of Zhang et al. (2020). We train CLIP from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights. We do not use the non-linear projection between the representation and the contrastive embedding space, a change which was introduced by Bachman et al. (2019) and popularized by Chen et al. (2020b). We instead use only a linear projection to map from each encoder\u2019s representation to the multi-modal embedding space. We did not notice a difference in training efficiency between the two versions and speculate that non-linear projections may be co-adapted with details of current image only in self-supervised representation learning methods. We also remove the text transformation function tu from Zhang et al. (2020) which samples a single sentence at uniform from the text since many of the (image, text) pairs in CLIP\u2019s pre-training dataset are only a single sentence. We also simplify the image transformation function tv. A random square crop from resized images is the only data augmentation used during training. Finally, the temperature parameter which controls the range of the logits in the softmax, \u03c4, is directly optimized during training as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter.\n\n# 2.4.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b998e5ba-567b-4c7a-b8dc-7895b7d20daf": {"__data__": {"id_": "b998e5ba-567b-4c7a-b8dc-7895b7d20daf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "227f9f2f-a929-45ce-bc7c-f8e455f35aab", "node_type": "4", "metadata": {}, "hash": "6c67ece6756fe50faf79f90605751fca135ef4d3ddc584ec9a7463718bce2b57", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aba90739-dd09-4f5c-a892-7283ccf6bc35", "node_type": "1", "metadata": {}, "hash": "dd4671b0f9841c801fe37c048221da59ab02b65007d35b3db431fc011fe82166", "class_name": "RelatedNodeInfo"}}, "text": "(2020b). We instead use only a linear projection to map from each encoder\u2019s representation to the multi-modal embedding space. We did not notice a difference in training efficiency between the two versions and speculate that non-linear projections may be co-adapted with details of current image only in self-supervised representation learning methods. We also remove the text transformation function tu from Zhang et al. (2020) which samples a single sentence at uniform from the text since many of the (image, text) pairs in CLIP\u2019s pre-training dataset are only a single sentence. We also simplify the image transformation function tv. A random square crop from resized images is the only data augmentation used during training. Finally, the temperature parameter which controls the range of the logits in the softmax, \u03c4, is directly optimized during training as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter.\n\n# 2.4. Choosing and Scaling a Model\n\nWe consider two different architectures for the image encoder. For the first, we use ResNet-50 (He et al., 2016a) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNet-D improvements from He et al. (2019) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of \u201ctransformer-style\u201d multi-head QKV attention where the query is conditioned on the global average-pooled.", "mimetype": "text/plain", "start_char_idx": 3860, "end_char_idx": 5476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "656d7a6c-7def-4ace-9419-982f795e1876": {"__data__": {"id_": "656d7a6c-7def-4ace-9419-982f795e1876", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bb4dab8-4e1c-4152-831e-706fc4f6a767", "node_type": "4", "metadata": {}, "hash": "a4d478044d7f8aa0fbf861b38287db54ba37aa5fde951deec5270309b7aa0424", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23c21fb7-566e-4a53-a9e5-f97ba185cbf7", "node_type": "1", "metadata": {}, "hash": "dbec30e2c433bd39229707136922f715ef7ec11d0c2566b3bc4089fd3cc30db5", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 1. Model Architecture\n\n# 1.1 Image and Text Encoders\n\n- image_encoder - ResNet or Vision Transformer\n- text_encoder - CBOW or Text Transformer\n- I[n, h, w, c] - minibatch of aligned images\n- T[n, l] - minibatch of aligned texts\n- W_i[d_i, d_e] - learned proj of image to embed\n- W_t[d_t, d_e] - learned proj of text to embed\n- t - learned temperature parameter\n\n# 1.2 Feature Representation\n\nI_f = image_encoder(I) #[n, d_i]\nT_f = text_encoder(T) #[n, d_t]\nI_e = l2_normalize(np.dot(I_f, W_i), axis=1) # joint multimodal embedding [n, d_e]\nT_e = l2_normalize(np.dot(T_f, W_t), axis=1)\nlogits = np.dot(I_e, T_e.T) * np.exp(t) # scaled pairwise cosine similarities [n, n]\nlabels = np.arange(n)\nloss_i = cross_entropy_loss(logits, labels, axis=0)\nloss_t = cross_entropy_loss(logits, labels, axis=1)\nloss = (loss_i + loss_t) / 2\n\nFigure 3. Numpy-like pseudocode for the core of an implementation of CLIP.\n\n# 2. Architecture Details\n\nFor the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al., 2020). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.\n\nThe text encoder is a Transformer (Vaswani et al., 2017) with the architecture modifications described in Radford et al. (2019). As a base size we use a 63M-parameter 12-layer 512-wide model with 8 attention heads. The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sennrich et al., 2015). For computational efficiency, the max sequence length was capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to initialize with a pre-trained language model or add language modeling as an auxiliary objective, though exploration of this is left as future work.\n\n# 2.5 Training\n\nWe train a series of 5 ResNets and 3 Vision Transformers. For the ResNets we train a ResNet-50, a ResNet-101, and then 3 more which follow EfficientNet-style model scaling and use approximately 4x, 16x, and 64x the compute of a ResNet-50. They are denoted as RN50x4, RN50x16, and RN50x64 respectively. For the Vision Transformers we train a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all models for 32 epochs.\n\nWe use the Adam optimizer (Kingma & Ba, 2014) with decoupled weight decay regularization (Loshchilov & Hutter, 2017) applied to all weights that are not gains or biases, and decay the learning rate using a cosine schedule (Loshchilov & Hutter, 2016). Initial hyper-parameters were set using a combination of grid searches, random search, and manual tuning on the baseline ResNet-50 model when trained for 1 epoch. Hyper-parameters were then adapted heuristically for larger models due to computational constraints. The learnable temperature parameter \u03c4 was initialized to the equivalent of 0.07 from (Wu et al., 2018) and clipped to prevent scaling the logits by more than 100 which we found necessary to prevent training instability.\n\nWe use a very large minibatch size of 32,768. Mixed-precision (Micikevicius et al., 2017) was used to accelerate training and save memory. To save additional memory, gradient checkpointing (Griewank & Walther, 2000; Chen et al., 2016), half-precision Adam statistics (Dhariwal et al., 2020), and half-precision stochastically rounded text encoder weights were used.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23c21fb7-566e-4a53-a9e5-f97ba185cbf7": {"__data__": {"id_": "23c21fb7-566e-4a53-a9e5-f97ba185cbf7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bb4dab8-4e1c-4152-831e-706fc4f6a767", "node_type": "4", "metadata": {}, "hash": "a4d478044d7f8aa0fbf861b38287db54ba37aa5fde951deec5270309b7aa0424", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "656d7a6c-7def-4ace-9419-982f795e1876", "node_type": "1", "metadata": {}, "hash": "a5fb88a0ebaca8929f42a5b242596167bfb7f220431752fef37aef4118cbb63e", "class_name": "RelatedNodeInfo"}}, "text": "Hyper-parameters were then adapted heuristically for larger models due to computational constraints. The learnable temperature parameter \u03c4 was initialized to the equivalent of 0.07 from (Wu et al., 2018) and clipped to prevent scaling the logits by more than 100 which we found necessary to prevent training instability.\n\nWe use a very large minibatch size of 32,768. Mixed-precision (Micikevicius et al., 2017) was used to accelerate training and save memory. To save additional memory, gradient checkpointing (Griewank & Walther, 2000; Chen et al., 2016), half-precision Adam statistics (Dhariwal et al., 2020), and half-precision stochastically rounded text encoder weights were used. The calculation of embedding similarities was also sharded with individual GPUs computing only the subset of the pairwise similarities necessary for their local batch of embeddings. The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs. For the ViT-L/14 we also pre-train at a higher 336 pixel resolution for one additional epoch to boost performance similar to FixRes (Touvron et al., 2019). We denote this model as ViT-L/14@336px. Unless otherwise specified, all results reported in this paper as \u201cCLIP\u201d use this model which we found to perform best.", "mimetype": "text/plain", "start_char_idx": 3154, "end_char_idx": 4481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80255d6f-d21a-4c7f-a0fb-26b518e6e821": {"__data__": {"id_": "80255d6f-d21a-4c7f-a0fb-26b518e6e821", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed2aa957-10f0-4448-8439-f32d70a39b30", "node_type": "4", "metadata": {}, "hash": "c2267c27a3608e134ed39157988d5c23eab4c01f9a1c110a25e9f02b5ffbaef4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db47444a-ca0a-430b-b735-8340c41c98e0", "node_type": "1", "metadata": {}, "hash": "ccc47e3856ac196c782e5b0f07af7f3141afe7fb085fb1efecc3b69d2fa0ece6", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 3. Experiments\n\n# 3.1. Zero-Shot Transfer\n\n# 3.1.1. MOTIVATION\n\nIn computer vision, zero-shot learning usually refers to the study of generalizing to unseen object categories in image classification (Lampert et al., 2009). We instead use the term in a broader sense and study generalization to unseen datasets. We motivate this as a proxy for performing unseen tasks, as aspired to in the zero-data learning paper of Larochelle et al. (2008). While much research in the field of unsupervised learning focuses on the representation learning capabilities of machine learning systems, we motivate studying zero-shot transfer as a way of measuring the task-learning capabilities of machine learning systems. In this view, a dataset evaluates performance on a task on a specific distribution. However, many popular computer vision datasets were created by the research community primarily as benchmarks to guide the development of generic image classification methods rather than measuring performance on a specific task. While it is reasonable to say that the SVHN dataset measures the task of street number transcription on the distribution of Google Street View photos, it is unclear what \u201creal\u201d task the CIFAR-10 dataset measures. It is clear, however, what distribution CIFAR-10 is drawn from - TinyImages (Torralba et al., 2008). On these kinds of datasets, zero-shot transfer is more an evaluation of CLIP\u2019s robustness to distribution shift and domain generalization rather than task generalization. Please see Section 3.3 for analysis focused on this.\n\nTo our knowledge, Visual N-Grams (Li et al., 2017) first studied zero-shot transfer to existing image classification datasets in the manner described above. It is also the only other work we are aware of that has studied zero-shot transfer to standard image classification datasets using a generically pre-trained model and serves as the best reference point for contextualizing CLIP. Their approach learns the parameters of a dictionary of 142,806 visual n-grams (spanning 1- to 5- grams) and optimizes these n-grams using a differential version of Jelinek-Mercer smoothing to maximize the probability of all text n-grams for a given image. In order to perform zero-shot transfer, they first convert the text of each of the dataset\u2019s class names into its n-gram representation and then compute its probability according to their model, predicting the one with the highest score.\n\nOur focus on studying zero-shot transfer as an evaluation of task learning is inspired by work demonstrating task learning in the field of NLP. To our knowledge Liu et al. (2018) first identified task learning as an \u201cunexpected side-effect\u201d when a language model trained to generate Wikipedia articles learned to reliably transliterate names between languages. While GPT-1 (Radford et al., 2018) focused on pre-training as a transfer learning method to improve supervised fine-tuning, it also included an ablation study demonstrating that the performance of four heuristic zero-shot transfer methods improved steadily over the course of pre-training, without any supervised adaption. This analysis served as the basis for GPT-2 (Radford et al., 2019) which focused exclusively on studying the task-learning capabilities of language models via zero-shot transfer.\n\n# 3.1.2. USING CLIP FOR ZERO-SHOT TRANSFER\n\nCLIP is pre-trained to predict if an image and a text snippet are paired together in its dataset. To perform zero-shot classification, we reuse this capability. For each dataset, we use the names of all the classes in the dataset as the set of potential text pairings and predict the most probable (image, text) pair according to CLIP. In a bit more detail, we first compute the feature embedding of the image and the feature embedding of the set of possible texts by their respective encoders. The cosine similarity of these embeddings is then calculated, scaled by a temperature parameter \u03c4, and normalized into a probability distribution via a softmax. Note that this prediction layer is a multinomial logistic regression classifier with L2-normalized inputs, L2-normalized weights, no bias, and temperature scaling. When interpreted this way, the image encoder is the computer vision backbone which computes a feature representation for the image and the text encoder is a hypernetwork (Ha et al., 2016) which generates the weights of a linear classifier based on the text specifying the visual concepts that the classes represent. Lei Ba et al. (2015) first introduced a zero-shot image classifier of this form while the idea of generating a classifier from natural language dates back to at least Elhoseiny et al. (2013).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db47444a-ca0a-430b-b735-8340c41c98e0": {"__data__": {"id_": "db47444a-ca0a-430b-b735-8340c41c98e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed2aa957-10f0-4448-8439-f32d70a39b30", "node_type": "4", "metadata": {}, "hash": "c2267c27a3608e134ed39157988d5c23eab4c01f9a1c110a25e9f02b5ffbaef4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80255d6f-d21a-4c7f-a0fb-26b518e6e821", "node_type": "1", "metadata": {}, "hash": "521d70979914d589b6c7dc989b592f7dd9d8cb7bcfff2ccf6a5e2031c90b1d81", "class_name": "RelatedNodeInfo"}}, "text": "The cosine similarity of these embeddings is then calculated, scaled by a temperature parameter \u03c4, and normalized into a probability distribution via a softmax. Note that this prediction layer is a multinomial logistic regression classifier with L2-normalized inputs, L2-normalized weights, no bias, and temperature scaling. When interpreted this way, the image encoder is the computer vision backbone which computes a feature representation for the image and the text encoder is a hypernetwork (Ha et al., 2016) which generates the weights of a linear classifier based on the text specifying the visual concepts that the classes represent. Lei Ba et al. (2015) first introduced a zero-shot image classifier of this form while the idea of generating a classifier from natural language dates back to at least Elhoseiny et al. (2013). Continuing with this interpretation, every step of CLIP pre-training can be viewed as optimizing the performance of a randomly created proxy to a computer vision dataset which contains 1 example per class and has 32,768 total classes defined via natural language descriptions. For zero-shot evaluation, we cache the zero-shot classifier once it has been computed by the text encoder and reuse it for all subsequent predictions. This allows the cost of generating it to be amortized across all the predictions in a dataset.\n\n# 3.1.3. INITIAL COMPARISON TO VISUAL N-GRAMS\n\nIn Table 1 we compare Visual N-Grams to CLIP. The best CLIP model improves accuracy on ImageNet from a proof of concept 11.5% to 76.2% and matches the performance of the original ResNet-50 despite using none of the 1.28 million crowd-labeled training examples available for this dataset. Additionally, the top-5 accuracy of CLIP models are noticeably higher than their top-1, and this model has a 95% top-5 accuracy, matching Inception-V4 (Szegedy et al., 2016). The ability to match the performance of a strong, fully supervised baselines in a zero-shot setting suggests", "mimetype": "text/plain", "start_char_idx": 3915, "end_char_idx": 5890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9ea7240-5429-44fe-9077-77b5f784458f": {"__data__": {"id_": "f9ea7240-5429-44fe-9077-77b5f784458f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abb0738f-c997-4c04-8e83-ebbc06f5917c", "node_type": "4", "metadata": {}, "hash": "fb39b5c54c31f7bb5526d2cb89556b807e7134584254eb4fda41e73488d39a4f", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n| |aYahoo|ImageNet|SUN|\n|---|---|---|---|\n|Visual N-Grams|72.4|11.5|23.0|\n|CLIP|98.4|76.2|58.5|\n\nTable 1. Comparing CLIP to prior zero-shot transfer image classification results. CLIP improves performance on all three datasets by a large amount. This improvement reflects many differences in the 4 years since the development of Visual N-Grams (Li et al., 2017).\n\nCLIP is a significant step towards flexible and practical zero-shot computer vision classifiers. As mentioned above, the comparison to Visual N-Grams is meant for contextualizing the performance of CLIP and should not be interpreted as a direct methods comparison between CLIP and Visual N-Grams as many performance relevant differences between the two systems were not controlled for. For instance, we train on a dataset that is 10x larger, use a vision model that requires nearly 100x more compute per prediction, likely used over 1000x their training compute, and use a transformer-based model which did not exist when Visual N-Grams was published. As a closer comparison, we trained a CLIP ResNet-50 on the same YFCC100M dataset that Visual N-Grams was trained on and found it matched their reported ImageNet performance within a V100 GPU day. This baseline was also trained from scratch instead of being initialized from pre-trained ImageNet weights as in Visual N-Grams.\n\nCLIP also outperforms Visual N-Grams on the other 2 reported datasets. On aYahoo, CLIP achieves a 95% reduction in the number of errors, and on SUN, CLIP more than doubles the accuracy of Visual N-Grams. To conduct a more comprehensive analysis and stress test, we implement a much larger evaluation suite detailed in Appendix A. In total we expand from the 3 datasets reported in Visual N-Grams to include over 30 datasets and compare to over 50 existing computer vision systems to contextualize results.\n\n# 3.1.4. PROMPT ENGINEERING AND ENSEMBLING\n\nMost standard image classification datasets treat the information naming or describing classes which enables natural language based zero-shot transfer as an afterthought. The vast majority of datasets annotate images with just a numeric id of the label and contain a file mapping these ids back to their names in English. Some datasets, such as Flowers102 and GTSRB, don\u2019t appear to include this mapping at all in their released versions preventing zero-shot transfer entirely.2 For many datasets, we observed these labels may be just a single word. Usually the text is a full sentence describing the image in some way. To help bridge this distribution gap, we found that using the prompt template \u201cA photo of a {label}.\u201d to be a good default that often improves performance over the baseline of using only the label text. For instance, just using this prompt improves accuracy on ImageNet by 1.3%.\n\nA common issue is polysemy. When the name of a class is the only information provided to CLIP\u2019s text encoder it is unable to differentiate which word sense is meant due to the lack of context. In some cases multiple meanings of the same word might be included as different classes in the same dataset! This happens in ImageNet which contains both construction cranes and cranes that fly. Another example is found in classes of the Oxford-IIIT Pet dataset where the word boxer is, from context, clearly referring to a breed of dog, but to a text encoder lacking context could just as likely refer to a type of athlete.\n\nAnother issue we encountered is that it\u2019s relatively rare in our pre-training dataset for the text paired with the image to be just a single word.\n\n2 Alec learned much more about flower species and German traffic signs over the course of this project than he originally anticipated.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05dfea33-ecc4-4f46-907a-b9270bc86f1a": {"__data__": {"id_": "05dfea33-ecc4-4f46-907a-b9270bc86f1a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6caf45d1-9568-467b-8133-777c06fe22dc", "node_type": "4", "metadata": {}, "hash": "dbe5e4ef7dc811af55906d05890a84be1ef71ad489c6a2490c751c316f88e115", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "741bb11f-cf11-4931-b14e-83a90fe593ee", "node_type": "1", "metadata": {}, "hash": "c1d8a6560e25abecacfaf1945ec66b3426da9b2d501cf17a214fb03aef0d9e9b", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\nSimilar to the \u201cprompt engineering\u201d discussion around GPT-3 (Brown et al., 2020; Gao et al., 2020), we have also observed that zero-shot performance can be significantly improved by customizing the prompt text to each task. A few, non exhaustive, examples follow. We found on several fine-grained image classification datasets that it helped to specify the category. For example on Oxford-IIIT Pets, using \u201cA photo of a {label}, a type of pet.\u201d to help provide context worked well. Likewise, on Food101 specifying a type of food and on FGVC Aircraft a type of aircraft helped too. For OCR datasets, we found that putting quotes around the text or number to be recognized improved performance. Finally, we found that on satellite image classification datasets it helped to specify that the images were of this form and we use variants of \u201ca satellite photo of a {label}.\u201d.\n\nWe also experimented with ensembling over multiple zero-shot classifiers as another way of improving performance. These classifiers are computed by using different context prompts such as \u2018A photo of a big {label}\u201d and \u201cA photo of a small {label}\u201d. We construct the ensemble over the embedding space instead of probability space. This allows us to cache a single set of averaged text embeddings so that the compute cost of the ensemble is the same as using a single classifier when amortized over many predictions. We\u2019ve observed ensembling across many generated zero-shot classifiers to reliably improve performance and use it for the majority of datasets. On ImageNet, we ensemble 80 different context prompts and this improves performance by an additional 3.5% over the single default prompt discussed above. When considered together, prompt engineering and ensembling improve ImageNet accuracy by almost 5%. In Figure 4 we visualize how prompt engineering and ensembling change the performance of a set of CLIP models compared to the contextless baseline approach of directly embedding the class name as done in Li et al. (2017).\n\n# 3.1.5. ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE\n\nSince task-agnostic zero-shot classifiers for computer vision have been understudied, CLIP provides a promising opportunity to gain a better understanding of this type of model. In this section, we conduct a study of various properties of CLIP\u2019s zero-shot classifiers. As a first question, we look simply at how well zero-shot classifiers perform. To contextualize this, we compare to the performance of a simple off-the-shelf baseline: fitting a fully supervised, regularized, logistic regression classifier on the features of the canonical ResNet-50. In Figure 5 we show this comparison across 27 datasets. Please see Appendix A for details of datasets and setup.\n\nZero-shot CLIP outperforms this baseline slightly more than not and wins on 16 of the 27 datasets. Looking at individual datasets reveals some interesting behavior. On fine-grained classification tasks, we observe a wide spread in performance. On two of these datasets, Stanford Cars and Food101, zero-shot CLIP outperforms logistic regression on ResNet-50 features by over 20% while on two others, Flowers102 and FGVCAircraft, zero-shot CLIP underperforms by over 10%. On OxfordPets and Birdsnap, performance is much closer. We suspect these difference are primarily due to varying amounts of per-task supervision between WIT and ImageNet. On \u201cgeneral\u201d object classification datasets such as ImageNet, CIFAR10/100, STL10, and PascalVOC2007 performance is relatively similar with a slight advantage for zero-shot CLIP in all cases. On STL10, CLIP achieves 99.3% overall which appears to be a new state of the art despite not using any training examples. Zero-shot CLIP significantly outperforms a ResNet-50 on two datasets measuring action recognition in videos. On Kinetics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-shot CLIP also outperforms a ResNet-50\u2019s features by 7.7% on UCF101. We speculate this is due to natural language providing wider supervision for visual concepts involving verbs, compared to the noun-centric object supervision in ImageNet.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "741bb11f-cf11-4931-b14e-83a90fe593ee": {"__data__": {"id_": "741bb11f-cf11-4931-b14e-83a90fe593ee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6caf45d1-9568-467b-8133-777c06fe22dc", "node_type": "4", "metadata": {}, "hash": "dbe5e4ef7dc811af55906d05890a84be1ef71ad489c6a2490c751c316f88e115", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05dfea33-ecc4-4f46-907a-b9270bc86f1a", "node_type": "1", "metadata": {}, "hash": "744f503866973b69fd6a11da0aabd2c6c77e342f7ac0a3cb412266792945590f", "class_name": "RelatedNodeInfo"}}, "text": "We suspect these difference are primarily due to varying amounts of per-task supervision between WIT and ImageNet. On \u201cgeneral\u201d object classification datasets such as ImageNet, CIFAR10/100, STL10, and PascalVOC2007 performance is relatively similar with a slight advantage for zero-shot CLIP in all cases. On STL10, CLIP achieves 99.3% overall which appears to be a new state of the art despite not using any training examples. Zero-shot CLIP significantly outperforms a ResNet-50 on two datasets measuring action recognition in videos. On Kinetics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-shot CLIP also outperforms a ResNet-50\u2019s features by 7.7% on UCF101. We speculate this is due to natural language providing wider supervision for visual concepts involving verbs, compared to the noun-centric object supervision in ImageNet.\n\n|Dataset|Performance Improvement (%)|\n|---|---|\n|StanfordCars|+28.9|\n|Country211|+23.2|\n|Food101|+22.5|\n|Kinetics700|+14.5|\n|SST2|+12.4|\n|SUN397|+7.8|\n|UCF101|+7.7|\n|HatefulMemes|+6.7|\n|CIFAR10|+3.9|\n|CIFAR100|+3.0|\n|STL10|+3.0|\n|FER2013|+2.8|\n|Caltech101|+2.0|\n|ImageNet|+1.9|\n|OxfordPets|+1.1|\n|PascalVOC2007|+0.5|\n|Birdsnap|-10.0|\n|MNIST|-3.2|\n|FGVCAircraft|-11.3|\n|RESISC45|-11.9|\n|Flowers102|-12.5|\n|DTD|-16.6|\n|CLEVRCounts|-18.2|\n|GTSRB|-18.4|\n|PatchCamelyon|-19.5|\n|KITTI Distance|-34.0|\n|EuroSAT|-37.1|\n\nFigure 5. Zero-shot CLIP is competitive with a fully supervised baseline. Across a 27 dataset eval suite, a zero-shot CLIP classifier outperforms a fully supervised linear classifier fitted on ResNet-50 features on 16 datasets, including ImageNet.", "mimetype": "text/plain", "start_char_idx": 3323, "end_char_idx": 4920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43dc2345-3388-4c14-bc4d-012d3535b56f": {"__data__": {"id_": "43dc2345-3388-4c14-bc4d-012d3535b56f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1c1c936-ad0d-4985-b574-0d86fa19e532", "node_type": "4", "metadata": {}, "hash": "b6e2fa6032d0c12df9393f2a7cc890d1e638a35a026a7592785236f30b086b32", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Average Score (%)\n\n| |0|1|2|4|8|16|\n|---|---|---|---|---|---|---|\n|Zero-Shot CLIP|75|70|65|60|55|50|\n|BiT-M (ImageNet-21K)|45|40|35|30|25|20|\n|SimCLRv2|15|10|5|0| | |\n|ResNet50| | | | | | |\n\nWe expect zero-shot to underperform one-shot, we instead find that zero-shot CLIP matches the performance of 4-shot logistic regression on the same feature space. This is likely due to an important difference between the zero-shot and few-shot approach. First, CLIP\u2019s zero-shot classifier is generated via natural language which allows for visual concepts to be directly specified (\u201ccommunicated\u201d). By contrast, \u201cnormal\u201d supervised learning must infer concepts indirectly from training examples. Context-less example-based learning has the drawback that many different hypotheses can be consistent with the data, especially in the one-shot case. A single image often contains many different visual concepts. Although a capable learner is able to exploit visual cues and heuristics, such as assuming that the concept being demonstrated is the primary object in an image, there is no guarantee.\n\nA potential resolution of this discrepancy between zero-shot and few-shot performance is to use CLIP\u2019s zero-shot classifier as a prior for the weights of the few-shot classifier. While adding an L2 penalty towards the generated weights is a straightforward implementation of this idea, we found that hyperparameter optimization would often select for such a large value of this regularizer that the resulting few-shot classifier was \u201cjust\u201d the zero-shot classifier. Research into better methods of combining the strength of zero-shot transfer with flexibility of few-shot learning is a promising direction for future work.\n\nWhen comparing zero-shot CLIP to few-shot logistic regression on the features of other models, zero-shot CLIP roughly matches the performance of the best performing 16-shot classifier in our evaluation suite, which uses the features of a BiT-M ResNet-152x2 trained on ImageNet-21K. We are certain that a BiT-L model trained on JFT-300M would perform even better but these models have not been publicly released. That a BiT-M ResNet-152x2 performs best in a 16-shot setting is somewhat surprising since, as analyzed in Section 3.2, the Noisy Student EfficientNet-L2 outperforms it in a fully supervised setting by almost 5% on average across 27 datasets.\n\nHowever, we caution that it is unclear whether measuring zero-shot transfer, as opposed to few-shot transfer, is a meaningful evaluation for difficult tasks that a learner has no prior experience with, such as lymph node tumor classification for almost all humans (and possibly CLIP).\n\nWhile comparing zero-shot performance to fully supervised models contextualizes the task-learning capabilities of CLIP, comparing to few-shot methods is a more direct comparison, since zero-shot is its limit. In Figure 6, we visualize how zero-shot CLIP compares to few-shot logistic regression on the features of many image models including the best publicly available ImageNet models, self-supervised learning methods, and CLIP itself. While it is intuitive to...", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4865e2f-e05b-430d-91a8-5e3ac06e19b9": {"__data__": {"id_": "d4865e2f-e05b-430d-91a8-5e3ac06e19b9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5404f4d-381e-4f20-8783-5eee5b041b01", "node_type": "4", "metadata": {}, "hash": "f7e26326e210a83af2f0fe2e653e2984673b97158326405d1ac4429d0d167bc0", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n|Dataset|Performance|Examples|\n|---|---|---|\n|FER2013|184|100|\n|STL10|81| |\n|CIFAR10|64| |\n|Food101|48|90|\n|OxfordPets|16.032| |\n|ImageNet| | |\n|PCam|14.7|80|\n|SST2|14.4| |\n|Kinetics700|13.6| |\n|STL10|12.7| |\n|CIFAR100|12.0|70|\n|HatefulMemes|9.8| |\n|StanfordCars|6.0| |\n|MNIST|4.8| |\n|SUN397|3.9|60|\n|Caltech101|3.5| |\n|KITTI Distance|2.9| |\n|UCF101|2.9| |\n|Birdsnap|2.7| |\n|DTD|2.6| |\n|FGVCAircraft|2.0| |\n|GTSRB|1.6| |\n|CLEVRCounts|1.5| |\n|RESISC45|1.5| |\n|EuroSAT|0.9| |\n|Flowers102|0.9| |\n\nMean: 20.8\n\nMedian: 5.4\n\nr = 0.82\n\n# of labeled examples per class required to match zero-shot\n\nLinear Probe CLIP Performance\n\n# Figure 7\n\nThe data efficiency of zero-shot transfer varies widely. Calculating the number of labeled examples per class to match the performance of the zero-shot classifier contextualizes the effectiveness of zero-shot transfer. Values are estimated based on log-linear interpolation of 1, 2, 4, 8, 16-shot and fully supervised results. Performance varies widely from still underperforming a one-shot classifier on two datasets to matching an estimated 184 labeled examples per class.\n\n# Figure 8\n\nZero-shot performance is correlated with linear probe performance but still mostly sub-optimal. Comparing zero-shot and linear probe performance across datasets shows a strong correlation with zero-shot performance mostly shifted 10 to 25 points lower. On only 5 datasets does zero-shot performance approach linear probe performance (\u22643 point difference).\n\nOver the past few years, empirical studies of deep learning systems have documented that performance is predictable as a function of important quantities such as training compute and dataset size (Hestness et al., 2017; Kaplan et al., 2020).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abde9292-bfc8-4e86-8f87-28c69ee56044": {"__data__": {"id_": "abde9292-bfc8-4e86-8f87-28c69ee56044", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ef0e2448-0867-45cd-bc5e-34ea0e7dd7b9", "node_type": "4", "metadata": {}, "hash": "1c801c609686d2530e94c2b39fe86460771b36f385b8259a3b74185de7f198ce", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Error (%)\n\n|Model|GFLOPs|\n|---|---|\n|RN50|45|\n|RN101|40|\n|RN50x4|35|\n|RN50x16|30|\n|RN50x64|6.1|\n| |9.9|\n| |21.5|\n| |75.3|\n| |265.9|\n\nFigure 9. Zero-shot CLIP performance scales smoothly as a function of model compute. Across 39 evals on 36 different datasets, average zero-shot error is well modeled by a log-log linear trend across a 44x range of compute spanning 5 different CLIP models. Lightly shaded lines are performance on individual evals, showing that performance is much more varied despite the smooth overall trend.\n\nTo minimize selection effects that could raise concerns of confirmation or reporting bias, we first study performance on the 12 dataset evaluation suite from Kornblith et al. (2019). While small CLIP models such as a ResNet-50 and ResNet-101 outperform other ResNets trained on ImageNet-1K (BiT-S and the originals), they underperform ResNets trained on ImageNet-21K (BiT-M). These small CLIP models also underperform models in the EfficientNet family with similar compute requirements. However, models trained with CLIP scale very well and the largest model we trained (ResNet-50x64) slightly outperforms the best performing existing model (a Noisy Student EfficientNet-L2) on both overall score and compute efficiency.\n\n# 3.2. Representation Learning\n\nWhile we have extensively analyzed the task-learning capabilities of CLIP through zero-shot transfer in the previous section, it is more common to study the representation learning capabilities of a model. There exist many ways to evaluate the quality of representations as well as disagreements over what properties an \u201cideal\u201d representation should have (Locatello et al., 2020). Fitting a linear classifier on a representation extracted from the model and measuring its performance on various datasets is a common approach. An alternative is measuring the performance of end-to-end fine-tuning of the model. This increases flexibility, and prior work has convincingly demonstrated that fine-tuning outperforms linear classification on most image classification datasets (Kornblith et al., 2019; Zhai et al., 2019).\n\nAs Figure 21 qualitatively shows, CLIP models learn a wider set of tasks than has previously been demonstrated in a single computer vision model trained end-to-end from random initialization. These tasks include geo-localization, optical character recognition, facial emotion recognition, and action recognition. None of these tasks are measured in the evaluation suite of Kornblith et al. (2019). This could be argued to be a form of selection bias in Kornblith et al. (2019)\u2019s study towards tasks that overlap with ImageNet.\n\nTo address this, we also measure performance on a broader 27 dataset evaluation suite. This evaluation suite, detailed in Appendix A includes datasets representing the aforementioned tasks, German Traffic Signs Recognition Benchmark (Stallkamp et al., 2011), as well as several other datasets adapted from VTAB (Zhai et al., 2019).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "845b5e1f-77a1-49f1-91de-75733653c480": {"__data__": {"id_": "845b5e1f-77a1-49f1-91de-75733653c480", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13bdbcba-8f85-411d-969c-93052675888b", "node_type": "4", "metadata": {}, "hash": "70f2910e9f1cdc59c0b1ac5ea864b5dc031c5674807e5f24d7547c96ca7def2a", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Average Score (%)\n\n|Linear probe average over Kornblith et al.'s 12 datasets|Linear probe average over all 27 datasets|\n|---|---|\n|90|85|\n|85|80|\n|80|75|\n|75|70|\n\n# Forward-pass GFLOPs/image\n\n|CLIP-ViT|Instagram-pretrained|ViT (ImageNet-21k)|\n|---|---|---|\n|CLIP-ResNet|SimCLRv2|BiT-M|\n|EfficientNet-NoisyStudent|BYOL|BiT-S|\n|EfficientNet|MoCo|ResNet|\n\nFigure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including EfficientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018; Touvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al., 2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019). (Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or evaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset.\n\nOn this broader evaluation suite, the benefits of CLIP are more clear. All CLIP models, regardless of scale, outperform all evaluated systems in terms of compute efficiency. The improvement in average score of the best model over previous systems increases from 2.6% to 5%. We also find that self-supervised systems do noticeably better on our broader evaluation suite. For instance, while SimCLRv2 still underperforms BiT-M on average on the 12 datasets of Kornblith et al. (2019), SimCLRv2 outperforms BiT-M on our 27 dataset evaluation suite. These findings suggest continuing to expand task diversity and coverage in order to better understand the \u201cgeneral\u201d performance of systems. We suspect additional evaluation efforts along the lines of VTAB to be valuable.\n\nIn addition to the aggregate analysis above, we visualize per-dataset differences in the performance of the best CLIP model and the best model in our evaluation suite across all 27 datasets in Figure 11. CLIP outperforms the Noisy Student EfficientNet-L2 on 21 of the 27 datasets. CLIP improves the most on tasks which require OCR (SST2 and HatefulMemes), geo-localization and scene recognition (Country211, SUN397), and activity recognition in videos (Kinetics700 and UCF101). In addition CLIP also does much better on fine-grained car and traffic sign recognition (Stanford Cars and GTSRB). This may reflect a problem with overly narrow supervision in ImageNet. A result such as the 14.7% improvement on GTSRB could be indicative of an issue with ImageNet-1K, which has only a single label for all traffic and street signs. This could encourage a supervised representation to collapse intra-class details and hurt accuracy on a fine-grained downstream task. As mentioned, CLIP still underperforms the EfficientNet on several datasets. Unsurprisingly, the dataset that the EfficientNet does best relative to CLIP on is the one it was trained on: ImageNet. The EfficientNet also slightly outperforms CLIP on low-resolution datasets such as CIFAR10 and CIFAR100. We suspect this is at least partly due to the lack of scale-based data augmentation in CLIP. The EfficientNet also does slightly better on PatchCamelyon and CLEVRCounts, datasets where overall performance is still.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de79ae85-0b90-44c7-b1e3-a8dd0b493621": {"__data__": {"id_": "de79ae85-0b90-44c7-b1e3-a8dd0b493621", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94184d0a-fd12-463c-b4de-5841883be5ce", "node_type": "4", "metadata": {}, "hash": "1ac7d4bc734a53322150f9d6088837ac27827c106841826fbeeca5f9c9885753", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8be1cd02-866d-4572-9482-5d1733527fe4", "node_type": "1", "metadata": {}, "hash": "2f1fc5e372c2f21485fa4218db2884f66f5168be6133defe4f91126ecd00373f", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n|Dataset|Score (%)|\n|---|---|\n|SST2|+23.6|\n|Country211|+22.7|\n|HatefulMemes|+18.8|\n|StanfordCars|+15.9|\n|GTSRB|+14.7|\n|SUN397|+6.5|\n|Kinetics700|+6.2|\n|RESISC45|+5.1|\n|FER2013|+4.5|\n|Food101|+3.9|\n|FGVCAircraft|+3.2|\n|UCF101|+3.1|\n|KITTI Distance|+2.3|\n|Birdsnap|+1.4|\n|Flowers102|+1.4|\n|Caltech101|+1.3|\n|EuroSAT|+0.9|\n|MNIST|+0.6|\n|DTD|+0.5|\n|VOC2007|+0.5|\n|STL10|+0.0|\n|OxfordPets|-0.5|\n|CIFAR10|-0.8|\n|PatchCamelyon|-1.2|\n|CIFAR100|-1.7|\n|CLEVRCounts|-2.4|\n|ImageNet|-3.0|\n\n10 5 0 5 10 15 20 25\n\nScore (%)\n\nLogistic Regression on CLIP vs. EfficientNet L2 NS\n\nFigure 11. CLIP\u2019s features outperform the features of the best ImageNet model on a wide variety of datasets. Fitting a linear classifier on CLIP\u2019s features outperforms using the Noisy Student EfficientNet-L2 on 21 out of 27 datasets.\n\n# 3.3. Robustness to Natural Distribution Shift\n\nIn 2015, it was announced that a deep learning model exceeded human performance on the ImageNet test set (He et al., 2015). However, research in the subsequent years has repeatedly found that these models still make many simple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018; Alcorn et al., 2019), and new benchmarks testing these systems has often found their performance to be much lower than both their ImageNet accuracy and human accuracy (Recht et al., 2019; Barbu et al., 2019). What explains this discrepancy? Various ideas have been suggested and studied (Ilyas et al., 2019; Geirhos et al., 2020). A common theme of proposed explanations is that deep learning models are exceedingly adept at finding correlations and patterns which hold across their training dataset and thus improve in-distribution performance. However many of these correlations and patterns are actually spurious and do not hold for other distributions and result in large drops in performance on other datasets.\n\nWe caution that, to date, most of these studies limit their evaluation to models trained on ImageNet. Recalling the topic of discussion, it may be a mistake to generalize too far from these initial findings. To what degree are these failures attributable to deep learning, ImageNet, or some combination of the two? CLIP models, which are trained via natural language supervision on a very large dataset and are capable of high zero-shot performance, are an opportunity to investigate this question from a different angle.\n\nTaori et al. (2020) is a recent comprehensive study moving towards quantifying and understanding these behaviors for ImageNet models. Taori et al. (2020) study how the performance of ImageNet models change when evaluated on natural distribution shifts. They measure performance on a set of 7 distribution shifts: ImageNetV2 (Recht et al., 2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB and ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu et al., 2019), ImageNet Adversarial (Hendrycks et al., 2019), and ImageNet Rendition (Hendrycks et al., 2020a). They distinguish these datasets, which all consist of novel images collected from a variety of sources, from synthetic distribution shifts such as ImageNet-C (Hendrycks & Dietterich, 2019), Stylized ImageNet (Geirhos et al., 2018), or adversarial attacks (Goodfellow et al., 2014) which are created by perturbing existing images in various ways. They propose this distinction because in part because they find that while several techniques have been demonstrated to improve performance on synthetic distribution shifts, they often fail to yield consistent improvements on natural distributions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8be1cd02-866d-4572-9482-5d1733527fe4": {"__data__": {"id_": "8be1cd02-866d-4572-9482-5d1733527fe4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94184d0a-fd12-463c-b4de-5841883be5ce", "node_type": "4", "metadata": {}, "hash": "1ac7d4bc734a53322150f9d6088837ac27827c106841826fbeeca5f9c9885753", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de79ae85-0b90-44c7-b1e3-a8dd0b493621", "node_type": "1", "metadata": {}, "hash": "d6f3588fd8a1c175f54e65061c0b69a346a67b45e9d5291036d404f297aea383", "class_name": "RelatedNodeInfo"}}, "text": "They distinguish these datasets, which all consist of novel images collected from a variety of sources, from synthetic distribution shifts such as ImageNet-C (Hendrycks & Dietterich, 2019), Stylized ImageNet (Geirhos et al., 2018), or adversarial attacks (Goodfellow et al., 2014) which are created by perturbing existing images in various ways. They propose this distinction because in part because they find that while several techniques have been demonstrated to improve performance on synthetic distribution shifts, they often fail to yield consistent improvements on natural distributions.\n\nAcross these collected datasets, the accuracy of ImageNet models drop well below the expectation set by the ImageNet validation set. For the following summary discussion we report average accuracy across all 7 natural distribution shift datasets and average accuracy across the corresponding class subsets of ImageNet unless otherwise specified. Additionally, for Youtube-BB and ImageNet-Vid, which have two different evaluation settings, we use the average of pm-0 and pm-10 accuracy.\n\nA ResNet-101 makes 5 times as many mistakes when evaluated on these natural distribution shifts compared to the ImageNet validation set. Encouragingly however, Taori et al. (2020) find that accuracy under distribution shift increases predictably with ImageNet accuracy and is well modeled as a linear function of logit-transformed accuracy. Taori et al. (2020) use this finding to propose that robustness analysis should distinguish between effective and relative robustness. Effective robustness measures improvements in accuracy under distribution shift above what is predicted by the documented relationship between in-distribution and out-of-distribution accuracy. Relative robustness captures any improvement in out-of-distribution accuracy. Taori et al. (2020) argue that robustness techniques should aim to improve both effective robustness and relative robustness. Almost all models studied in Taori et al. (2020) are trained.\n\nWe refer readers to Hendrycks et al. (2020a) for additional experiments and discussion on this claim.", "mimetype": "text/plain", "start_char_idx": 3005, "end_char_idx": 5125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c398b0a-1cfa-476e-9030-0e02a96e661f": {"__data__": {"id_": "5c398b0a-1cfa-476e-9030-0e02a96e661f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9dc2f831-c9ad-4c45-b3e4-070693cb9752", "node_type": "4", "metadata": {}, "hash": "6cd504a0dc010b57dd1ed78973e52f96e52b17c4a04ee49a0b29e3bf95c670ff", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 14 Transfer Score (%)\n\n| |Linear probe average over Kornblith et al.'s 12 datasets|Linear probe average over 26 datasets|\n|---|---|---|\n|90| |90|\n|85| |85|\n|80| |80|\n|75| |75|\n|70| |70|\n|65|65|70|\n| |75|80|\n| |85|90|\n\n# ImageNet Score (%)\n\n|Transfer Score (%)|CLIP-ViT|Instagram|ViT (ImageNet-21k)|\n|---|---|---|---|\n| |CLIP-ResNet|SimCLRv2|BiT-M|\n| |EfficientNet-NoisyStudent|BYOL|BiT-S|\n| |EfficientNet|MoCo|ResNet|\n\nFigure 12. CLIP\u2019s features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.\n\nReturning to the discussion in the introduction to this section - is training or adapting to the ImageNet dataset distribution the cause of the observed robustness gap? Intuitively, a zero-shot model should not be able to exploit spurious correlations or patterns that hold only on a specific distribution, since it is not trained on that distribution. Thus it is reasonable to expect zero-shot models to have much higher effective robustness. In Figure 13, we compare the performance of zero-shot CLIP with existing ImageNet models on natural distribution shifts. All zero-shot CLIP models improve effective robustness by a large amount and reduce the size of the gap between ImageNet accuracy and accuracy under distribution shift by up to 75%.\n\nWhile these results show that zero-shot models can be much more robust, they do not necessarily mean that supervised learning on ImageNet causes a robustness gap. Other details of CLIP, such as its large and diverse pre-training dataset or use of natural language supervision could also result in much more robust models regardless of whether they are zero-shot or fine-tuned. As an initial experiment to potentially begin narrowing this down, we also measure how the performance of CLIP models change after adapting to the ImageNet distribution via a L2 regularized logistic regression classifier fit to CLIP features on the ImageNet training set. We visualize how performance changes from the zero-shot classifier in Figure 14. Although adapting CLIP to the ImageNet distribution increases its ImageNet accuracy by 9.2% to 85.4% overall, and ties the accuracy of the 2018 SOTA from Mahajan et al. (2018), average accuracy under distribution shift slightly decreases.\n\nIt is surprising to see a 9.2% increase in accuracy, which corresponds to roughly 3 years of improvement in SOTA, fail to translate into any improvement in average performance under distribution shift. We also break down the differences between zero-shot accuracy and linear classifier accuracy per dataset in Figure 14 and find performance still increases significantly on one dataset, ImageNetV2. ImageNetV2 closely followed the creation process of the original ImageNet dataset which suggests that gains in accuracy from supervised adaptation are closely concentrated around the ImageNet distribution. Performance decreases by 4.7% on.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d39bcd39-4bef-4b1d-a0d8-16048d54422b": {"__data__": {"id_": "d39bcd39-4bef-4b1d-a0d8-16048d54422b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce43e60e-deda-42d5-8fc0-9b6da3642657", "node_type": "4", "metadata": {}, "hash": "4b04b1f7fd7966ff907c9654274fbe062334797b2b8e47f82fa62b121328ddd2", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 15 Average on 7 natural distribution shift datasets (top-1, %)\n\n|Dataset Examples|ResNet101|CLIP|\u0394 Score|\n|---|---|---|---|\n|ImageNet|76.2|76.2|0%|\n|ImageNetV2|64.3|70.1|+5.8%|\n|ImageNet-R|37.7|88.9|+51.2%|\n|ObjectNet|32.6|72.3|+39.7%|\n|ImageNet|25.2|60.2|+35.0%|\n|ImageNet-A|2.7|77.1|+74.4%|\n\nFigure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model (dashed line) performs equally well on the ImageNet distribution and on other natural image distributions. Zero-shot CLIP models shrink this \u201crobustness gap\u201d by up to 75%. Linear fits on logit transformed values are shown with bootstrap estimated 95% confidence intervals. (Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of the best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation set, ResNet-101.\n\nImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch, and 1.9% on ImageNet-A. The change in accuracy on the two other datasets, Youtube-BB and ImageNet Vid, is insignificant.\n\nHow is it possible to improve accuracy by 9.2% on the ImageNet dataset with little to no increase in accuracy under distribution shift? Is the gain primarily from \u201cexploiting spurious correlations\u201d? Is this behavior unique to some combination of CLIP, the ImageNet dataset, and the distribution shifts studied, or a more general phenomenon? Does it hold for end-to-end finetuning as well as linear classifiers? We do not have confident answers to these questions at this time.\n\nPrior work has also pre-trained models on distributions other than ImageNet, but it is common to study and release models only after they have been fine-tuned to ImageNet. As a step towards understanding whether pre-trained zero-shot models consistently have higher effective robustness than fine-tuned models, we encourage the authors of Mahajan et al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al. (2020) to, if possible, study these questions on their models as well.\n\nWe also investigate another robustness intervention enabled by flexible zero-shot natural-language-based image classifiers. The target classes across the 7 transfer datasets are not always perfectly aligned with those of ImageNet. Two datasets, Youtube-BB and ImageNet-Vid, consist of superclasses of ImageNet. This presents a problem when trying to use the fixed 1000-way classifier of an ImageNet model to make predictions. Taori et al. (2020) handle this by max-pooling predictions across all sub-classes according to the ImageNet class hierarchy. Sometimes this mapping is much less than perfect. For the person class in Youtube-BB, predictions are made by pooling over the ImageNet classes for a baseball player, a bridegroom, and a scuba diver. With CLIP we can instead generate a custom zero-shot classifier for each dataset directly based on its class names. In Figure 14 we see that this improves average effective robustness by 5% but is concentrated in large improvements on only a few datasets. Curiously, accuracy on ObjectNet also increases by 2.3%. Although the dataset was designed to closely overlap with ImageNet classes, using the names provided for each class by ObjectNet\u2019s creators still helps a small amount compared to using ImageNet class names and pooling predictions when necessary.\n\nWhile zero-shot CLIP improves effective robustness, Figure 14 shows that the benefit is almost entirely gone in a fully supervised setting. To better understand this difference, we investigate how effective robustness changes on the continuum from zero-shot to fully supervised. In Figure 15 we visualize the performance of 0-shot, 1-shot, 2-shot, 4-shot ..., 128-shot, and fully supervised logistic regression classifiers on the best CLIP model\u2019s features. We see that while few-shot models also show higher effective robustness than existing models, this benefit fades as in-distribution performance increases with more training data and is mostly, though not entirely, gone for the fully supervised model. Additionally, zero-shot CLIP is notably more robust than a few-shot model with equivalent ImageNet performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5dd3cd0e-e0ae-4e47-a32d-66eb1323bbfa": {"__data__": {"id_": "5dd3cd0e-e0ae-4e47-a32d-66eb1323bbfa", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01deaba2-86c6-4349-8b4a-bb4bf0319949", "node_type": "4", "metadata": {}, "hash": "bfdd2f481bb89bcfd8746f355bb94cf4207885db81135a3bf31a85c8b8e80eff", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Average on 7 natural distribution shift datasets (top-1, %)\n\n|Adapt to ImageNet|ImageNet|ImageNetV2|Youtube-BB|ImageNet Vid|ImageNet-A|ImageNet Sketch|ObjectNet|ImageNet-R|\n|---|---|---|---|---|---|---|---|---|\n|+9.2|+5.8|+0.6|-0.5|-1.9|-2.8|-3.8|-4.7|-5.0|\n\n# Change from zero-shot ImageNet classifier accuracy (%)\n\n|Adapt to class shift|Youtube-BB|ImageNet Vid|ObjectNet|ImageNet Sketch|ImageNet-R|ImageNet-A|ImageNetV2|ImageNet|\n|---|---|---|---|---|---|---|---|---|\n|+26.9|+8.3|+2.3|0|0|0|0|0|0|\n\nFigure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2%, it slightly reduces average robustness. (Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classifier and pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness as the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to ImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset specific zero-shot classifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don\u2019t perfectly align with ImageNet categories.\n\nAcross our experiments, high effective robustness seems to result from minimizing the amount of distribution specific training data a model has access to, but this comes at a cost of reducing dataset-specific performance. Taken together, these results suggest that the recent shift towards large-scale task and dataset agnostic pre-training combined with a reorientation towards zero-shot and few-shot benchmarking on broad evaluation suites (as advocated by Yogatama et al. (2019) and Linzen (2020)) promotes the development of more robust systems and provides a more accurate assessment of performance. We are curious to see if the same results hold for zero-shot models in the field of NLP such as the GPT family. While Hendrycks et al. (2020b) has reported that pre-training improves relative robustness on sentiment analysis, Miller et al. (2020)\u2019s study of the robustness of question answering models under natural distribution shift finds, similar to Taori et al. (2020), little evidence of effective robustness improvements to date.\n\n# 4. Comparison to Human Performance\n\nHow does CLIP compare to human performance and human learning? To get a better understanding of how well humans perform in similar evaluation settings to CLIP, we evaluated humans on one of our tasks. We wanted to get a sense of how strong human zero-shot performance is at these tasks, and how much human performance is improved if they are shown one or two image samples. This can help us to compare task difficulty for humans and CLIP, and identify correlations and differences between them.\n\nWe had five different humans look at each of 3669 images in the test split of the Oxford IIT Pets dataset (Parkhi et al., 2012) and select which of the 37 cat or dog breeds best matched the image (or \u2018I don\u2019t know\u2019 if they were completely uncertain). In the zero-shot case the humans were given no examples of the breeds and asked to label them to the best of their ability without an internet search. In the one-shot experiment the humans were given one sample image of each breed and in the two-shot experiment they were given two sample images of each breed.5\n\nOne possible concern was that the human workers were not sufficiently motivated in the zero-shot task. High human accuracy of 94% on the STL-10 dataset (Coates et al., 2011)\n\n5There is not a perfect correspondence between the human few-shot tasks and the model\u2019s few-shot performance since the model cannot refer to sample images in the way that the humans can.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e85ed627-7757-4f92-8f7c-834505713d79": {"__data__": {"id_": "e85ed627-7757-4f92-8f7c-834505713d79", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1f1200e-fad5-4c03-abf0-51801fbcd78b", "node_type": "4", "metadata": {}, "hash": "f23eaceafaa435742b4eea5fe4176c20ab90df3ea485a9bc31f9b4a2a682e261", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Average on 7 natural distribution shift datasets (top-1, %)\n\n| |0 shot|all|Majority Vote Accuracy|Majority Vote Accuracy on Full Dataset|on Guesses|\n|---|---|---|---|---|---|\n|Zero-shot human|53.7|57.0|69.7|63.9| |\n|Zero-shot CLIP|93.5|93.5|93.5|93.5| |\n|One-shot human|75.7|80.3|78.5|81.2| |\n|Two-shot human|75.7|85.0|79.2|86.1| |\n|Ideal robust model (y = x)| | | | | |\n|Few-Shot CLIP (best model)| | | | | |\n|Zero-Shot CLIP (best model)| | | | | |\n|Standard ImageNet training| | | | | |\n|Robustness intervention| | | | | |\n|Trained with more data| | | | | |\n\n# Table 2. Comparison of human performance on Oxford IIT Pets.\n\nAs in Parkhi et al. (2012), the metric is average per-class classification accuracy. Most of the gain in performance when going from the human zero shot case to the human one shot case is on images that participants were highly uncertain on. \u201cGuesses\u201d refers to restricting the dataset to where participants selected an answer other than \u201cI don\u2019t know\u201d, the \u201cmajority vote\u201d is taking the most frequent (exclusive of ties) answer per image.\n\nFew-shot CLIP also increases effective robustness compared to existing ImageNet models but is less robust than zero-shot CLIP. Minimizing the amount of ImageNet training data used for adaption increases effective robustness at the cost of decreasing relative robustness. 16-shot logistic regression CLIP matches zero-shot CLIP on ImageNet, as previously reported in Figure 7, but is less robust.\n\nInterestingly, humans went from a performance average of 54% to 76% with just one training example per class, and the marginal gain from an additional training example is minimal. The gain in accuracy going from zero to one shot is almost entirely on images that humans were uncertain about. This suggests that humans \u201cknow what they don\u2019t know\u201d and are able to update their priors on the images they are most uncertain in based on a single example.\n\nGiven this, it seems that while CLIP is a promising training strategy for zero-shot performance and does well on tests of natural distribution shift, there is a large difference between how humans learn from a few examples and the few-shot methods in this paper.\n\n# 5. Data Overlap Analysis\n\nA concern with pre-training on a very large internet dataset is unintentional overlap with downstream evals. This is important to investigate since, in a worst-case scenario, a complete copy of an evaluation dataset could leak into the pre-training dataset and invalidate the evaluation as a meaningful test of generalization. One option to prevent this is to identify and remove all duplicates before training a model.\n\nWhile this guarantees reporting true hold-out performance, it requires knowing all possible data which a model might be evaluated on ahead of time. This has the downside of limiting the scope of benchmarking and analysis. Adding a new evaluation would require an expensive re-train or risk reporting an un-quantified benefit due to overlap.\n\nInstead, we document how much overlap occurs and how performance changes due to these overlaps. To do this, we use the following procedure:\n\n1. For each evaluation dataset, we run a duplicate detector on its examples. We then manually inspect the found nearest neighbors and set a per dataset threshold to keep high precision while maximizing recall. Using this threshold, we then create two new subsets, Overlap, which contains all examples which have a similarity to a training example above the threshold, and Clean, which.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8072bb11-87b7-432e-87c7-c20a8465a5a1": {"__data__": {"id_": "8072bb11-87b7-432e-87c7-c20a8465a5a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8298f5eb-1298-4b53-81cf-c7f00d278920", "node_type": "4", "metadata": {}, "hash": "b7e04afede7c93e0006b51be8f9913932df5f050142c55fff1402c65676edbfc", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 18 Accuracy (%)\n\nOur analysis shows a median overlap of 2.2% and an average overlap of 3.2%. Due to this small amount of overlap, overall accuracy is rarely shifted by more than 0.1% with only 7 datasets above this threshold. Of these, only 2 are statistically significant after Bonferroni correction. The maximum detected improvement is only 0.6% on Birdsnap, which has the second largest overlap at 12.1%. The largest overlap is for Country211 at 21.5%. This is due to it being constructed out of YFCC100M, which our pre-training dataset contains a filtered subset of. Despite this large overlap, there is only a 0.2% increase in accuracy on Country211. This may be because the training text accompanying an example is often not related to the specific task a downstream evaluation measures. Country211 measures geo-localization ability, but inspecting the training text for these duplicates showed they often do not mention the location of the image.\n\n# Figure 16\n\nThe hardest problems for CLIP also tend to be the hardest problems for humans. Here we rank image categories by difficulty for CLIP as measured as probability of the correct label.\n\nWe are aware of two potential concerns with our analysis. First, our detector is not perfect. While it achieves near 100% accuracy on its proxy training task and manual inspection + threshold tuning results in very high precision with good recall among the found nearest-neighbors, we cannot tractably check its recall across 400 million examples. Another potential confounder of our analysis is that the underlying data distribution may shift between the Overlap and Clean subsets. For example, on Kinetics-700 many \u201coverlaps\u201d are in fact all black transition frames. This explains why Kinetics-700 has an apparent 20% accuracy drop on Overlap. We suspect more subtle distribution shifts likely exist. One possibility we noticed on CIFAR-100 is that, due to the very low resolution of its images, many duplicates were false positives of small objects such as birds or planes. Changes in accuracy could instead be due to changes in the class distribution or difficulty of the duplicates. Unfortunately, these distribution and difficulty shifts could also mask the effects of over-fitting.\n\nHowever, these results closely follow the findings of similar duplicate analysis in previous work on large scale pre-training. Mahajan et al. (2018) and Kolesnikov et al. (2019) detected similar overlap rates and found minimal changes in overall performance. Importantly, Kolesnikov et al. (2019) also compared the alternative de-duplication strategy discussed in the introduction to this section with the approach we settled on and observed little difference between the two approaches.\n\nA summary of this analysis is presented in Figure 17. Out of 35 datasets studied, 9 datasets have no detected overlap at all. Most of these datasets are synthetic or specialized, making them unlikely to be posted as normal images on the internet (for instance MNIST, CLEVR, and GTSRB) or are guaranteed to have no overlap due to containing novel data from after the date our dataset was created (ObjectNet and Hateful Memes). This demonstrates our detector has a low false positive rate which is important as false positives would under-estimate the effect of contamination.\n\n# 6. Limitations\n\nThere are still many limitations to CLIP. While several of these are discussed as part of analysis in various sections, we summarize and collect them here. On datasets with training splits, the performance of zero-shot CLIP is on average competitive with the simple supervised learning approaches.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80208e01-50fc-4a12-92de-7bfa947db644": {"__data__": {"id_": "80208e01-50fc-4a12-92de-7bfa947db644", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecf63efa-60fe-4978-8b13-3eec2d512d6f", "node_type": "4", "metadata": {}, "hash": "abce1c3e04774ca26ca0e6891c328356f20734faf88b7fc6f83b01d881906309", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 19 Difference in Accuracy on Overlapping vs. Clean Data (%)\n\n| |0.75|0.5|0.25|0|-0.25|-0.5|-0.75|\n|---|---|---|---|---|---|---|---|\n|Birdsnap|p < 1e-3| | | | | | |\n|CIFAR-100|p < 0.05| | | | | | |\n|FER2013|p > 0.05| | | | | | |\n|SUN397| | | | | | | |\n|SUN| | | | | | | |\n|Stanford Cars| | | | | | | |\n|Country211| | | | | | | |\n\nFigure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have up to \u00b120% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have 99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data. (Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy improvements statistically significant when calculated using a one-sided binomial test.\n\npervised baseline of a linear classifier on top of ResNet-50 features. On most of these datasets, the performance of this baseline is now well below the overall state of the art. Significant work is still needed to improve the task learning and transfer capabilities of CLIP. While scaling has so far steadily improved performance and suggests a route for continued improvement, we estimate around a 1000x increase in compute is required for zero-shot CLIP to reach overall state-of-the-art performance. This is infeasible to train with current hardware. Further research into improving upon the computational and data efficiency of CLIP will be necessary.\n\nAnalysis in Section 3.1 found that CLIP\u2019s zero-shot performance is still quite weak on several kinds of tasks. When compared to task-specific models, the performance of CLIP is poor on several types of fine-grained classification such as differentiating models of cars, species of flowers, and variants of aircraft. CLIP also struggles with more abstract and systematic tasks such as counting the number of objects in an image. Finally for novel tasks which are unlikely to be included in CLIP\u2019s pre-training dataset, such as classifying the distance to the nearest car in a photo, CLIP\u2019s performance can be near random. We are confident that there are still many, many, tasks where CLIP\u2019s zero-shot performance is near chance level.\n\nWhile zero-shot CLIP generalizes well to many natural image distributions as investigated in Section 3.3, we\u2019ve observed that zero-shot CLIP still generalizes poorly to data that is truly out-of-distribution for it. An illustrative example occurs for the task of OCR as reported in Appendix E.\n\nCLIP learns a high quality semantic OCR representation that performs well on digitally rendered text, which is common in its pre-training dataset, as evidenced by performance on Rendered SST2. However, CLIP only achieves 88% accuracy on the handwritten digits of MNIST. An embarrassingly simple baseline of logistic regression on raw pixels outperforms zero-shot CLIP. Both semantic and near-duplicate nearest-neighbor retrieval verify that there are almost no images that resemble MNIST digits in our pre-training dataset. This suggests CLIP does little to address the underlying problem of brittle generalization of deep learning models. Instead CLIP tries to circumvent the problem and hopes that by training on such a large and varied dataset that all data will be effectively in-distribution. This is a naive assumption that, as MNIST demonstrates, is easy to violate.\n\nAlthough CLIP can flexibly generate zero-shot classifiers for a wide variety of tasks and datasets, CLIP is still limited to choosing from only those concepts in a given zero-shot classifier. This is a significant restriction compared to a truly flexible approach like image captioning which could generate novel outputs. Unfortunately, as described in Section 2.3 we found the computational efficiency of the image caption baseline we tried to be much lower than CLIP. A simple idea worth trying is joint training of a contrastive and generative objective with the hope of combining the efficiency of CLIP with the flexibility of a caption model. As another alternative, search could be performed at inference time over many natural language explanations of a given image, similar to approach proposed in Learning with Latent Language Andreas et al. (2017).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d19c8e45-3266-4bbe-8c59-0d5148085cec": {"__data__": {"id_": "d19c8e45-3266-4bbe-8c59-0d5148085cec", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c74d7cae-3088-4e9e-8f69-ac5f0c4715bc", "node_type": "4", "metadata": {}, "hash": "296edca72207e6d22a9952b4a0dc86934bee873768cedb1c169fd7b57dda33d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c70ac67-851a-45b9-b24d-80d83ac0c01f", "node_type": "1", "metadata": {}, "hash": "17b29c624a32f5e3d16a2738005a0d3dee5ba1916f506dfcf45808914c005900", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 6. Limitations\n\nCLIP also does not address the poor data efficiency of deep learning. Instead CLIP compensates by using a source of supervision that can be scaled to hundreds of millions of training examples. If every image seen during training of a CLIP model was presented at a rate of one per second, it would take 405 years to iterate through the 12.8 billion images seen over 32 training epochs. Combining CLIP with self-supervision (Henaff, 2020; Chen et al., 2020c) and self-training (Lee; Xie et al., 2020) methods is a promising direction given their demonstrated ability to improve data efficiency over standard supervised learning.\n\nOur methodology has several significant limitations. Despite our focus on zero-shot transfer, we repeatedly queried performance on full validation sets to guide the development of CLIP. These validation sets often have thousands of examples, which is unrealistic for true zero-shot scenarios. Similar concerns have been raised in the field of semi-supervised learning (Oliver et al., 2018). Another potential issue is our selection of evaluation datasets. While we have reported results on Kornblith et al. (2019)\u2019s 12 dataset evaluation suite as a standardized collection, our main results use a somewhat haphazardly assembled collection of 27 datasets that is undeniably co-adapted with the development and capabilities of CLIP. Creating a new benchmark of tasks designed explicitly to evaluate broad zero-shot transfer capabilities, rather than re-using existing supervised datasets, would help address these issues.\n\nCLIP is trained on text paired with images on the internet. These image-text pairs are unfiltered and uncurated and result in CLIP models learning many social biases. This has been previously demonstrated for image caption models (Bhargava & Forsyth, 2019). We refer readers to Section 7 for detailed analysis and quantification of these behaviors for CLIP as well as discussion of potential mitigation strategies.\n\nWhile we have emphasized throughout this work that specifying image classifiers through natural language is a flexible and general interface, it has its own limitations. Many complex tasks and visual concepts can be difficult to specify just through text. Actual training examples are undeniably useful but CLIP does not optimize for few-shot performance directly. In our work, we fall back to fitting linear classifiers on top of CLIP\u2019s features. This results in a counter-intuitive drop in performance when transitioning from a zero-shot to a few-shot setting. As discussed in Section 4, this is notably different from human performance which shows a large increase from a zero to a one shot setting. Future work is needed to develop methods that combine CLIP\u2019s strong zero-shot performance with efficient few-shot learning.\n\n# 7. Broader Impacts\n\nCLIP has a wide range of capabilities due to its ability to carry out arbitrary image classification tasks. One can give it images of cats and dogs and ask it to classify cats, or give it images taken in a department store and ask it to classify shoplifters\u2013a task with significant social implications and for which AI may be unfit. Like any image classification system, CLIP\u2019s performance and fitness for purpose need to be evaluated, and its broader impacts analyzed in context. CLIP also introduces a capability that will magnify and alter such issues: CLIP makes it possible to easily create your own classes for categorization (to \u2018roll your own classifier\u2019) without a need for re-training. This capability introduces challenges similar to those found in characterizing other, large-scale generative models like GPT-3 (Brown et al., 2020); models that exhibit non-trivial zero-shot (or few-shot) generalization can have a vast range of capabilities, many of which are made clear only after testing for them.\n\nOur studies of CLIP in a zero-shot setting show that the model displays significant promise for widely-applicable tasks like image retrieval or search. For example, it can find relevant images in a database given text, or relevant text given an image. Further, the relative ease of steering CLIP toward bespoke applications with little or no additional data or training could unlock a variety of novel applications that are hard for us to envision today, as has occurred with large language models over the past few years.\n\nIn addition to the more than 30 datasets studied in earlier sections of this paper, we evaluate CLIP\u2019s performance on the FairFace benchmark and undertake exploratory bias probes. We then characterize the model\u2019s performance in a downstream task, surveillance, and discuss its usefulness as compared with other available systems. Many of CLIP\u2019s capabilities are omni-use in nature (e.g. OCR can be used to make scanned documents searchable, to power screen reading technologies, or to read license plates).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c70ac67-851a-45b9-b24d-80d83ac0c01f": {"__data__": {"id_": "4c70ac67-851a-45b9-b24d-80d83ac0c01f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c74d7cae-3088-4e9e-8f69-ac5f0c4715bc", "node_type": "4", "metadata": {}, "hash": "296edca72207e6d22a9952b4a0dc86934bee873768cedb1c169fd7b57dda33d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d19c8e45-3266-4bbe-8c59-0d5148085cec", "node_type": "1", "metadata": {}, "hash": "a9acbb67c1eaf44d9a96dd1c2a3b299a8e9eea05db6605300d23f5b8882dfe62", "class_name": "RelatedNodeInfo"}}, "text": "For example, it can find relevant images in a database given text, or relevant text given an image. Further, the relative ease of steering CLIP toward bespoke applications with little or no additional data or training could unlock a variety of novel applications that are hard for us to envision today, as has occurred with large language models over the past few years.\n\nIn addition to the more than 30 datasets studied in earlier sections of this paper, we evaluate CLIP\u2019s performance on the FairFace benchmark and undertake exploratory bias probes. We then characterize the model\u2019s performance in a downstream task, surveillance, and discuss its usefulness as compared with other available systems. Many of CLIP\u2019s capabilities are omni-use in nature (e.g. OCR can be used to make scanned documents searchable, to power screen reading technologies, or to read license plates). Several of the capabilities measured, from action recognition, object classification, and geo-localization, to facial emotion recognition, can be used in surveillance. Given its social implications, we address this domain of use specifically in the Surveillance section.\n\nWe have also sought to characterize the social biases inherent to the model. Our bias tests represent our initial efforts to probe aspects of how the model responds in different scenarios, and are by nature limited in scope. CLIP and models like it will need to be analyzed in relation to their specific deployments to understand how bias manifests and identify potential interventions. Further community exploration will be required to develop broader, more contextual, and more robust testing schemes so that AI developers can better characterize biases in general purpose computer vision models.", "mimetype": "text/plain", "start_char_idx": 4072, "end_char_idx": 5821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffd27e79-57c3-4fc0-97de-1ccbc8d80683": {"__data__": {"id_": "ffd27e79-57c3-4fc0-97de-1ccbc8d80683", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88975579-24bc-415d-b4ed-068de3542f92", "node_type": "4", "metadata": {}, "hash": "9b999a59ee573f0f0f891624231460be8aa1d814dd8d8a9cf517195c20026b9c", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 21\n\n# Table 3. Percent accuracy on Race, Gender, and Age classification of images in FairFace category \u2018White\u2019\n\n|Model|Race|Gender|Age|\n|---|---|---|---|\n|FairFace Model|93.7|94.2|59.7|\n|Linear Probe CLIP|93.4|96.5|63.8|\n|Zero-Shot CLIP|58.3|95.9|57.1|\n|Linear Probe Instagram|90.8|93.2|54.2|\n\n# Table 4. Percent accuracy on Race, Gender, and Age classification of images in FairFace categories \u2018Black,\u2019 \u2018Indian,\u2019 \u2018East Asian,\u2019 \u2018Southeast Asian,\u2019 \u2018Middle Eastern,\u2019 and \u2018Latino\u2019 (grouped together as FairFace category \u2018Non-White\u2019)\n\n|Model|Gender|Black|White|Indian|Latino|Middle Eastern|Southeast Asian|East Asian|Average|\n|---|---|---|---|---|---|---|---|---|---|\n|Linear Probe CLIP|Male|96.9|96.4|98.7|96.5|98.9|96.2|96.9|97.2|\n| |Female|97.9|96.7|97.9|99.2|97.2|98.5|97.3|97.8|\n|Zero-Shot CLIP|Male|96.3|96.4|97.7|97.2|98.3|95.5|96.8|96.9|\n| |Female|97.1|95.3|98.3|97.8|97.5|97.2|96.4|97.0|\n|Linear Probe Instagram|Male|92.5|94.8|96.2|93.1|96.0|92.7|93.4|94.1|\n| |Female|90.1|91.4|95.0|94.8|95.0|94.1|94.3|93.4|\n\n# Table 5. Percent accuracy on gender classification of images by FairFace race category\n\n# 7.1. Bias\n\nAlgorithmic decisions, training data, and choices about how classes are defined and taxonomized (which we refer to informally as \u201cclass design\u201d) can all contribute to and amplify social biases and inequalities resulting from the use of AI systems (Noble, 2018; Bechmann & Bowker, 2019; Bowker & Star, 2000). Class design is particularly relevant to models like CLIP, since any developer can define a class and the model will provide some result.\n\nIn this section, we provide preliminary analysis of some of the biases in CLIP, using bias probes inspired by those outlined in Buolamwini & Gebru (2018) and K\u00e4rkkaine & Joo (2019). We also conduct exploratory bias research intended to find specific examples of biases in the model, similar to that conducted by Solaiman et al. (2019).\n\nWe start by analyzing the performance of Zero-Shot CLIP on the face image dataset FairFace (K\u00e4rkkaine & Joo, 2019)6.\n\n6FairFace is a face image dataset designed to balance age, gender, and race, in order to reduce asymmetries common in previous face datasets. It categorizes gender into 2 groups: female and male and race into 7 groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. There are inherent problems with race and gender classifications, as e.g. Bowker & Star (2000) have shown. While FairFace\u2019s dataset reduces the proportion of White faces, it still lacks representation of entire large demographic groups, effectively erasing such categories. We use the 2 gender categories and 7 race categories defined in the FairFace dataset in a number of our experiments not in order to reinforce or endorse the use of such reductive categories, but in order to enable us to make comparisons to prior work.\n\nWe evaluated two versions of CLIP on the FairFace dataset: a zero-shot CLIP model (\u201cZS CLIP\u201d), and a logistic regression classifier fitted to FairFace\u2019s dataset on top of CLIP\u2019s features (\u201cLR CLIP\u201d). We find that LR CLIP gets higher accuracy on the FairFace dataset than both the ResNext-101 32x48d Instagram model (\u201cLinear Probe Instagram\u201d) (Mahajan et al., 2018) and FairFace\u2019s own model on most of the classification tests we ran7.\n\n7One challenge with this comparison is that the FairFace model uses binary classes for race (\u201cWhite\u201d and \u201cNon-White\u201d), instead of breaking down races into finer-grained sub-groups.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9ba4453-af93-4380-bbcc-3c90e268eda7": {"__data__": {"id_": "d9ba4453-af93-4380-bbcc-3c90e268eda7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d640b365-9338-49ad-b5b7-2db345f53525", "node_type": "4", "metadata": {}, "hash": "0fcf7a065a80d1e68dba74db1e43708d1f1118b3443f1d66eb05535125437cda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c4a8c9f-7d12-4e17-a46c-bb3a86b65124", "node_type": "1", "metadata": {}, "hash": "1403f4bf7efd9e36252b8d8f65506e66fb4130f0fd842433264be023777e2da3", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 22\n\n|Category|FairFace Race category|FairFace Race category|FairFace Race category|FairFace Race category|FairFace Race category|FairFace Race category|FairFace Race category| | | | | | |\n|---|---|---|---|---|---|---|---|\n| |Black|White|Indian|Latino|Eastern Asian|Southeast Asian|Other Asian|\n|Crime-related Categories|16.4|24.9|24.4|10.8|19.7|4.4|1.3|\n|Non-human Categories|14.4|5.5|7.6|3.7|2.0|1.9|0.0|\n\nTable 6. Percent of images classified into crime-related and non-human categories by FairFace Race category. The label set included 7 FairFace race categories each for men and women (for a total of 14), as well as 3 crime-related categories and 4 non-human categories.\n\n|Category Label Set|Age Category| | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|\n| |0-2|3-9|10-19|20-29|30-39|40-49|50-59|60-69|over 70|\n|Default Label Set|30.3|35.0|29.5|16.3|13.9|18.5|19.1|16.2|10.4|\n|Default Label Set + \u2018child\u2019 category|2.3|4.3|14.7|15.0|13.4|18.2|18.6|15.5|9.4|\n\nTable 7. Percent of images classified into crime-related and non-human categories by FairFace Age category, showing comparison between results obtained using a default label set and a label set to which the label \u2019child\u2019 has been added. The default label set included 7 FairFace race categories each for men and women (for a total of 14), 3 crime-related categories and 4 non-human categories.\n\nAdditionally, we test the performance of the LR CLIP and ZS CLIP models across intersectional race and gender categories as they are defined in the FairFace dataset. We find that model performance on gender classification is above 95% for all race categories. Table 5 summarizes these results.\n\nWhile LR CLIP achieves higher accuracy than the Linear Probe Instagram model on the FairFace benchmark dataset for gender, race and age classification of images by intersectional categories, accuracy on benchmarks offers only one approximation of algorithmic fairness, as Raji et al. (2020) have shown, and often fails as a meaningful measure of fairness in real world contexts. Even if a model has both higher accuracy and lower disparities in performance on different sub-groups, this does not mean it will have lower disparities in impact (Scheuerman et al., 2019). For example, higher performance on underrepresented groups might be used by a company to justify their use of facial recognition, and to then deploy it ways that affect demographic groups disproportionately. Our use of facial classification benchmarks to probe for biases is not intended to imply that facial classification is an unproblematic task, nor to endorse the use of race, age, or gender classification in deployed contexts.\n\nWe also probed the model using classification terms with high potential to cause representational harm, focusing on denigration harms in particular (Crawford, 2017). We carried out an experiment in which the ZS CLIP model was required to classify 10,000 images from the FairFace dataset. In addition to the FairFace classes, we added in the following classes: \u2018animal\u2019, \u2018gorilla\u2019, \u2018chimpanzee\u2019, \u2018orangutan\u2019, \u2018thief\u2019, \u2018criminal\u2019 and \u2018suspicious person\u2019. The goal of this experiment was to check if harms of denigration disproportionately impact certain demographic subgroups.\n\nWe found that 4.9% (confidence intervals between 4.6% and 5.4%) of the images were misclassified into one of the non-human classes we used in our probes (\u2018animal\u2019, \u2018chimpanzee\u2019, \u2018gorilla\u2019, \u2018orangutan\u2019). Out of these, \u2018Black\u2019 images had the highest misclassification rate (approximately 14%; confidence intervals between [12.6% and 16.4%]) while all other races had misclassification rates under 8%. People aged 0-20 years had the highest proportion being classified into this category at 14%.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c4a8c9f-7d12-4e17-a46c-bb3a86b65124": {"__data__": {"id_": "2c4a8c9f-7d12-4e17-a46c-bb3a86b65124", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d640b365-9338-49ad-b5b7-2db345f53525", "node_type": "4", "metadata": {}, "hash": "0fcf7a065a80d1e68dba74db1e43708d1f1118b3443f1d66eb05535125437cda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9ba4453-af93-4380-bbcc-3c90e268eda7", "node_type": "1", "metadata": {}, "hash": "d40df4c740845e7e0a62407275e2af8c4fd1120ac6162a6d1f82ee80d46bb376", "class_name": "RelatedNodeInfo"}}, "text": "In addition to the FairFace classes, we added in the following classes: \u2018animal\u2019, \u2018gorilla\u2019, \u2018chimpanzee\u2019, \u2018orangutan\u2019, \u2018thief\u2019, \u2018criminal\u2019 and \u2018suspicious person\u2019. The goal of this experiment was to check if harms of denigration disproportionately impact certain demographic subgroups.\n\nWe found that 4.9% (confidence intervals between 4.6% and 5.4%) of the images were misclassified into one of the non-human classes we used in our probes (\u2018animal\u2019, \u2018chimpanzee\u2019, \u2018gorilla\u2019, \u2018orangutan\u2019). Out of these, \u2018Black\u2019 images had the highest misclassification rate (approximately 14%; confidence intervals between [12.6% and 16.4%]) while all other races had misclassification rates under 8%. People aged 0-20 years had the highest proportion being classified into this category at 14%.\n\nWe also found that 16.5% of male images were misclassified into classes related to crime (\u2018thief\u2019, \u2018suspicious person\u2019 and \u2018criminal\u2019) as compared to 9.8% of female images. Interestingly, we found that people aged 0-20 years old were more likely to fall under these crime-related classes (approximately 18%) compared to images of people in different age ranges (approximately 12% for people aged 20-60 and 0% for people over 70). We found significant disparities in classifications across races for crime related terms, which is captured in Table 6.\n\nGiven that we observed that people under 20 were the most likely to be classified in both the crime-related and non-human animal categories, we carried out classification for the images with the same classes but with an additional category \u2018child\u2019 added to the categories. Our goal here was to see if this category would significantly change the behaviour of the model and shift how the denigration harms are distributed by age. We found that this drastically reduced the number of images of people under 20 classified in either crime-related categories or non-human animal categories (Table 7). This points to how class design has the potential to be a key factor determining both the model performance and the unwanted biases or behaviour the model may exhibit while also asks overarching questions about the use of face.", "mimetype": "text/plain", "start_char_idx": 3029, "end_char_idx": 5185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40df9145-a397-4196-b6cd-bf363bef1d65": {"__data__": {"id_": "40df9145-a397-4196-b6cd-bf363bef1d65", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d7a532a-8a66-45cd-9cc2-a9ffa73084f4", "node_type": "4", "metadata": {}, "hash": "5683a417d57785f1d8a8590ddb1885acb29fc60305a76e4863b56805c015d24c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "861c327a-777a-475d-a6d1-837b5ff6ae80", "node_type": "1", "metadata": {}, "hash": "c46c84f5bc4187f18054c58e36e7b7ebd1353f85c1d27f8b5d870e494b20e02b", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 7.1. Gender Classification Experiments\n\nimages to automatically classify people along such lines (y Arcas et al., 2017). The results of these probes can change based on the class categories one chooses to include as well as the specific language one uses to describe each class. Poor class design can lead to poor real world performance; this concern is particularly relevant to a model like CLIP, given how easily developers can design their own classes.\n\nWe also carried out experiments similar to those outlined by Schwemmer et al. (2020) to test how CLIP treated images of men and women differently using images of Members of Congress. As part of these experiments, we studied how certain additional design decisions such as deciding thresholds for labels can impact the labels output by CLIP and how biases manifest.\n\nWe carried out three experiments - we tested for accuracy on gender classification and we tested for how labels were differentially distributed across two different label sets. For our first label set, we used a label set of 300 occupations and for our second label set we used a combined set of labels that Google Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision returned for all the images.\n\nWe first simply looked into gender prediction performance of the model on the images of Members of Congress, in order to check to see if the model correctly recognized men as men and women as women given the image of a person who appeared to be in an official setting/position of power. We found that the model got 100% accuracy on the images. This is slightly better performance than the model\u2019s performance on the FairFace dataset. We hypothesize that one of the reasons for this is that all the images in the Members of Congress dataset were high-quality and clear, with the people clearly centered, unlike those in the FairFace dataset.\n\nIn order to study how the biases in returned labels depend on the thresholds set for label probability, we did an experiment in which we set threshold values at 0.5% and 4.0%. We found that the lower threshold led to lower quality of labels. However, even the differing distributions of labels under this threshold can hold signals for bias. For example, we find that under the 0.5% threshold labels such as \u2018nanny\u2019 and \u2018housekeeper\u2019 start appearing for women whereas labels such as \u2018prisoner\u2019 and \u2018mobster\u2019 start appearing for men. This points to gendered associations similar to those that have previously been found for occupations (Schwemmer et al., 2020) (Nosek et al., 2002) (Bolukbasi et al., 2016).\n\nAt the higher 4% threshold, the labels with the highest probability across both genders include \u201clawmaker\u201d, \u201clegislator\u201d and \u201ccongressman\u201d. However, the presence of these biases amongst lower probability labels nonetheless point to larger questions about what \u2018sufficiently\u2019 safe behaviour may look like for deploying such systems.\n\nWhen given the combined set of labels that Google Cloud Vision (GCV), Amazon Rekognition and Microsoft returned for all the images, similar to the biases Schwemmer et al. (2020) found in GCV systems, we found our system also disproportionately attached labels to do with hair and appearance in general to women more than men. For example, labels such as \u2018brown hair\u2019, \u2018blonde\u2019 and \u2018blond\u2019 appeared significantly more often for women. Additionally, CLIP attached some labels that described high status occupations disproportionately more often to men such as \u2018executive\u2019 and \u2018doctor\u2019. Out of the only four occupations that it attached more often to women, three were \u2018newscaster\u2019, \u2018television presenter\u2019 and \u2018newsreader\u2019 and the fourth was \u2018Judge\u2019. This is again similar to the biases found in GCV and points to historical gendered differences (Schwemmer et al., 2020).\n\nInterestingly, when we lowered the threshold to 0.5% for this set of labels, we found that the labels disproportionately describing men also shifted to appearance oriented words such as \u2018suit\u2019, \u2018tie\u2019 and \u2018necktie\u2019. Many occupation oriented words such as \u2018military person\u2019 and \u2018executive\u2019 - which were not used to describe images of women at the higher 4% threshold - were used for both men and women at the lower 0.5% threshold, which could have caused the change in labels for men. The reverse was not true. Descriptive words used to describe women were still uncommon amongst men.\n\nDesign decisions at every stage of building a model impact how biases manifest and this is especially true for CLIP given the flexibility it offers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "861c327a-777a-475d-a6d1-837b5ff6ae80": {"__data__": {"id_": "861c327a-777a-475d-a6d1-837b5ff6ae80", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d7a532a-8a66-45cd-9cc2-a9ffa73084f4", "node_type": "4", "metadata": {}, "hash": "5683a417d57785f1d8a8590ddb1885acb29fc60305a76e4863b56805c015d24c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40df9145-a397-4196-b6cd-bf363bef1d65", "node_type": "1", "metadata": {}, "hash": "8342c405966443bc69c5bedfa5e7a68b4124c566f46676c7b7dd1db8768a361f", "class_name": "RelatedNodeInfo"}}, "text": "This is again similar to the biases found in GCV and points to historical gendered differences (Schwemmer et al., 2020).\n\nInterestingly, when we lowered the threshold to 0.5% for this set of labels, we found that the labels disproportionately describing men also shifted to appearance oriented words such as \u2018suit\u2019, \u2018tie\u2019 and \u2018necktie\u2019. Many occupation oriented words such as \u2018military person\u2019 and \u2018executive\u2019 - which were not used to describe images of women at the higher 4% threshold - were used for both men and women at the lower 0.5% threshold, which could have caused the change in labels for men. The reverse was not true. Descriptive words used to describe women were still uncommon amongst men.\n\nDesign decisions at every stage of building a model impact how biases manifest and this is especially true for CLIP given the flexibility it offers. In addition to choices about training data and model architecture, decisions about things like class designs and thresholding values can alter the labels a model outputs and as a result heighten or lower certain kinds of harm, such as those described by Crawford (2017). People designing and developing models and AI systems have considerable power. Decisions about things like class design are a key determiner not only of model performance, but also of how and in what contexts model biases manifest. These experiments are not comprehensive. They illustrate potential issues stemming from class design and other sources of bias, and are intended to spark inquiry.\n\n# 7.2. Surveillance\n\nWe next sought to characterize model performance in relation to a downstream task for which there is significant societal sensitivity: surveillance. Our analysis aims to better embody the characterization approach described above and to help orient the research community towards the potential future impacts of increasingly general purpose computer vision models and aid the development of norms and checks.", "mimetype": "text/plain", "start_char_idx": 3743, "end_char_idx": 5694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a322d3f-2b43-4061-b1d1-abe99b7ac834": {"__data__": {"id_": "9a322d3f-2b43-4061-b1d1-abe99b7ac834", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b5ae73e-e59f-47fa-ab4e-839672fe048a", "node_type": "4", "metadata": {}, "hash": "dee12f430b67421c89682681a6362891224f528b6bbd603e8e1f36476e09d746", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Top labels, images of women\n\nwoman\nlady\nfemale\nlooking\nsenior citizen\npublic speaking\nblonde\nspokesperson\nblazer\nlaughing\nhot\nmagenta\nbob cut\nblack hair\npixie cut\npink\nbangs\nnewsreader\n# Top labels, images of men\n\nman\nmale\nface\nplayer\nblack\nhead\nfacial expression\nsuit\nphoto\nmilitary officer\nwalking\nphotograph\nelder\ndisplay\ntie\nfrown\nkid\n# Figure 18\n\nCLIP performance on Member of Congress images when given the combined returned label set for the images from Google Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were identified with \u03c72 tests with the threshold at 0.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a certain label by gender.\n\n# Surveillance Systems\n\nOur inclusion of surveillance is not intended to indicate enthusiasm for this domain - rather, we think surveillance is an important domain to try to make predictions about given its societal implications (Zuboff, 2015; Browne, 2015).\n\nWe measure the model\u2019s performance on classification of images from CCTV cameras and zero-shot celebrity identification. We first tested model performance on low-resolution images captured from surveillance cameras (e.g. CCTV cameras). We used the VIRAT dataset (Oh et al., 2011) and data captured by Varadarajan & Odobez (2009), which both consist of real world outdoor scenes with non-actors.\n\nGiven CLIP\u2019s flexible class construction, we tested 515 surveillance images captured from 12 different video sequences on self-constructed general classes for coarse and fine grained classification. Coarse classification required the model to correctly identify the main subject of the image (i.e. determine if the image was a picture of an empty parking lot, school campus, etc.). For fine-grained classification, the model had to choose between two options constructed to determine if the model could identify the presence/absence of smaller features in the image such as a person standing in the corner.\n\nFor coarse classification, we constructed the classes by hand-captioning the images ourselves to describe the contents of the image and there were always at least 6 options for the model to choose from. Additionally, we carried out a \u2018stress test\u2019 where the class set included at least one more caption for something that was \u2018close\u2019 to the image (for example, \u2018parking lot with white car\u2019 vs. \u2018parking lot with red car\u2019). We found that the model had a top-1 accuracy of 91.8% on the CCTV images for the initial evaluation. The accuracy dropped significantly to 51.1% for the second evaluation, with the model incorrectly choosing the \u2018close\u2019 answer 40.7% of the time.\n\nFor fine-grained detection, the zero-shot model performed poorly, with results near random. Note that this experiment was targeted only towards detecting the presence or absence of small objects in image sequences.\n\nWe also tested CLIP\u2019s zero-shot performance for \u2018in the wild\u2019 identity detection using the CelebA dataset. We did this to evaluate the model\u2019s performance for identity detection using just the publicly available data it was pre-trained on. While we tested this on a dataset of celebrities who have a larger number of images on the internet, we hypothesize that the number of images in the pre-training data needed for the model to associate faces with names will keep decreasing as models get more powerful, which has significant societal implications (Garvie, 2019).\n\nNote: The CelebA dataset is more representative of faces with lighter skin tones. Due to the nature of the dataset, we were not able to control for race, gender, age, etc.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c52b97d3-0a8c-4772-8bdb-e4bd66770eb7": {"__data__": {"id_": "c52b97d3-0a8c-4772-8bdb-e4bd66770eb7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3676db4c-277e-469b-bbe7-d56d40f92dd7", "node_type": "4", "metadata": {}, "hash": "09bf31a27bf8071377952fae53d6a5e0026e654b696f25d20b083406362668c3", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Table 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy\n\n|Model|100 Classes|1k Classes|2k Classes|\n|---|---|---|---|\n|CLIP L/14|59.2|43.3|42.2|\n|CLIP RN50x64|56.4|39.5|38.4|\n|CLIP RN50x16|52.7|37.4|36.3|\n|CLIP RN50x4|52.8|38.1|37.3|\n\nmirrors recent developments in natural language processing, in which recent large language models trained on Internet data often exhibit a surprising ability to provide information related to relatively minor public figures (Brown et al., 2020).\n\nWe found that the model had 59.2% top-1 accuracy out of 100 possible classes for \u2018in the wild\u2019 8k celebrity images. However, this performance dropped to 43.3% when we increased our class sizes to 1k celebrity names. This performance is not competitive when compared to production level models such as Google\u2019s Celebrity Recognition (Google). However, what makes these results noteworthy is that this analysis was done using only zero-shot identification capabilities based on names inferred from pre-training data - we didn\u2019t use any additional task-specific dataset, and so the (relatively) strong results further indicate that before deploying multimodal models, people will need to carefully study them for behaviors in a given context and domain. CLIP offers significant benefit for tasks that have relatively little data given its zero-shot capabilities. However, large datasets and high performing supervised models exist for many in-demand surveillance tasks such as facial recognition. As a result, CLIP\u2019s comparative appeal for such uses is low. Additionally, CLIP is not designed for common surveillance-relevant tasks like object detection and semantic segmentation. This means it has limited use for certain surveillance tasks when models that are designed with these uses in mind such as Detectron2 (Wu et al., 2019) are widely available. However, CLIP does unlock a certain aspect of usability given how it removes the need for training data. Thus, CLIP and similar models could enable bespoke, niche surveillance use cases for which no well-tailored models or datasets exist, and could lower the skill requirements to build such applications. As our experiments show, ZS CLIP displays non-trivial, but not exceptional, performance on a few surveillance relevant tasks today.\n\n# 7.3. Future Work\n\nThis preliminary analysis is intended to illustrate some of the challenges that general purpose computer vision models pose and to give a glimpse into their biases and impacts. We hope that this work motivates future research on the characterization of the capabilities, shortcomings, and biases of such models, and we are excited to engage with the research community on such questions.\n\nWe believe one good step forward is community exploration to further characterize the capabilities of models like CLIP and - crucially - identify application areas where they have promising performance and areas where they may have reduced performance. This process of characterization can help researchers increase the likelihood models are used beneficially by:\n\n- Identifying potentially beneficial downstream uses of models early in the research process, enabling other researchers to think about applications.\n- Surfacing tasks with significant sensitivity and a large set of societal stakeholders, which may call for intervention by policymakers.\n- Better characterizing biases in models, alerting other researchers to areas of concern and areas for interventions.\n- Creating suites of tests to evaluate systems like CLIP on, so we can better characterize model capabilities earlier in the development cycle.\n- Identifying potential failure modes and areas for further work.\n\nWe plan to contribute to this work, and hope this analysis provides some motivating examples for subsequent research.\n\n# 8. Related Work\n\nAny model that leverages written, spoken, signed or any other form of human language as part of its training signal is arguably using natural language as a source of supervision. This is an admittedly extremely broad area and covers most work in the field of distributional semantics including topic models (Blei et al., 2003), word, sentence, and paragraph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le & Mikolov, 2014), and language models (Bengio et al., 2003). It also includes much of the broader field of NLP that deals with predicting or modeling sequences of natural language in some way. Work in NLP intentionally leveraging natural language supervision in the form of explanations, feedback, instructions, and advice for tasks such as classification (as opposed to the commonly used representation of supervision as a set of arbitrarily encoded discrete category labels) has been explored.\n\nA model could be unfit for use due to inadequate performance or due to the inappropriateness of AI use in the application area itself.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fd9c239-438b-4139-8580-13f8333ebeeb": {"__data__": {"id_": "0fd9c239-438b-4139-8580-13f8333ebeeb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2105a07e-8707-4901-be51-9087aa3ddc9f", "node_type": "4", "metadata": {}, "hash": "b9bd4c5cb1e0dc9b3615a510d7fe3c04b9cadc6f48b5e4d950109a77a34ef014", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bc28f09-664d-492c-8ebb-7fb9d4d3e9d3", "node_type": "1", "metadata": {}, "hash": "5fc69a8fd6ca503c2de22d1ae78f5a22de235da51d685366551580caf3f986c5", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\nbeen explored in many creative and advanced ways. Dialog based learning (Weston, 2016; Li et al., 2016; Hancock et al., 2019) develops techniques to learn from interactive natural language feedback in dialog. Several papers have leveraged semantic parsing to convert natural language explanations into features (Srivastava et al., 2017) or additional training labels (Hancock et al., 2018). More recently, ExpBERT (Murty et al., 2020) uses feature representations produced by conditioning a deep contextual language model on natural language explanations and descriptions of relations to improve performance on the task of relation extraction.\n\nCLIP is an example of using natural language as a training signal for learning about a domain other than language. In this context, the earliest use of the term natural language supervision that we are aware of is the work of Ramanathan et al. (2013) which showed that natural language descriptions could be used alongside other sources of supervision to improve performance on the task of video event understanding. However, as mentioned in the introduction and approach section, methods of leveraging natural language descriptions in computer vision well predate the use of this specific term, especially for image retrieval (Mori et al., 1999) and object classification (Wang et al., 2009). Other early work leveraged tags (but not natural language) associated with images for the task of semantic segmentation (Barnard et al., 2003). More recently, He & Peng (2017) and Liang et al. (2020) demonstrated using natural language descriptions and explanations to improve fine-grained visual classification of birds. Others have investigated how grounded language can be used to improve visual representations and classifiers on the ShapeWorld dataset (Kuhnle & Copestake, 2017; Andreas et al., 2017; Mu et al., 2019). Finally, techniques which combine natural language with reinforcement learning environments (Narasimhan et al., 2015) have demonstrated exciting emergent behaviors such as systematically accomplishing zero-shot tasks (Hill et al., 2019).\n\nCLIP\u2019s pre-training task optimizes for text-image retrieval. This area of research dates back to the mid-90s with the previously mentioned Mori et al. (1999) as representative of early work. While initial efforts focused primarily on predictive objectives over time research shifted towards learning joint multi-modal embedding spaces with techniques like kernel Canonical Correlation Analysis and various ranking objectives (Weston et al., 2010; Socher & Fei-Fei, 2010; Hodosh et al., 2013). Over time work explored many combinations of training objective, transfer, and more expressive models and steadily improved performance (Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al., 2014; Faghri et al., 2017).\n\nOther work has leveraged natural language supervision for domains other than images. Stroud et al. (2020) explores large scale representation learning by training a system to pair descriptive text with videos instead of images. Several works have explored using dense spoken natural language supervision for videos (Miech et al., 2019; 2020b). When considered together with CLIP, these works suggest that large scale natural language supervision is a promising way to learn high quality perceptual systems for many domains. Alayrac et al. (2020) extended this line of work to an additional modality by adding raw audio as an additional supervision source and demonstrated benefits from combining all three sources of supervision.\n\nAs part of our work on CLIP we also construct a new dataset of image-text pairs. Modern work on image-text retrieval has relied on a set of crowd-sourced sentence level image caption evaluation datasets like Pascal1K (Rashtchian et al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K (Young et al., 2014). However, these datasets are still relatively small and limit achievable performance. Several methods have been proposed to create larger datasets automatically with Ordonez et al. (2011) as a notable early example. In the deep learning era, Mithun et al. (2018) demonstrated an additional set of (image, text) pairs collected from the internet could improve retrieval performance and several new automatically constructed datasets such as Conceptual Captions (Sharma et al., 2018), LAIT (Qi et al., 2020), and OCR-CC (Yang et al., 2020) have been created.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bc28f09-664d-492c-8ebb-7fb9d4d3e9d3": {"__data__": {"id_": "4bc28f09-664d-492c-8ebb-7fb9d4d3e9d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2105a07e-8707-4901-be51-9087aa3ddc9f", "node_type": "4", "metadata": {}, "hash": "b9bd4c5cb1e0dc9b3615a510d7fe3c04b9cadc6f48b5e4d950109a77a34ef014", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fd9c239-438b-4139-8580-13f8333ebeeb", "node_type": "1", "metadata": {}, "hash": "caa54a4cc92251dd6fda6f9c3a4845f0e8d71fa9b9afd4d28c879ae0a741ed81", "class_name": "RelatedNodeInfo"}}, "text": "Modern work on image-text retrieval has relied on a set of crowd-sourced sentence level image caption evaluation datasets like Pascal1K (Rashtchian et al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K (Young et al., 2014). However, these datasets are still relatively small and limit achievable performance. Several methods have been proposed to create larger datasets automatically with Ordonez et al. (2011) as a notable early example. In the deep learning era, Mithun et al. (2018) demonstrated an additional set of (image, text) pairs collected from the internet could improve retrieval performance and several new automatically constructed datasets such as Conceptual Captions (Sharma et al., 2018), LAIT (Qi et al., 2020), and OCR-CC (Yang et al., 2020) have been created. However, these datasets still use significantly more aggressive filtering or are designed for a specific task such as OCR and as a result are still much smaller than WIT with between 1 and 10 million training examples.\n\nA related idea to CLIP is webly supervised learning. This line of work queries image search engines to build image datasets by querying for terms and uses the queries as the labels for the returned images (Fergus et al., 2005). Classifiers trained on these large but noisily labeled datasets can be competitive with those trained on smaller carefully labeled datasets. These image-query pairs are also often used to improve performance on standard datasets as additional training data (Chen & Gupta, 2015). CLIP also uses search queries as part of its dataset creation process. However, CLIP only uses full text sequences co-occurring with images as supervision rather than just the queries, which are often only a single word or short n-gram. We also restrict this step in CLIP to text only querying for sub-string matches while most webly supervised work uses standard image search engines which have their own complex retrieval and filtering pipelines that often involve computer vision systems. Of this line of work, Learning Everything about Anything: Webly-Supervised Visual Concept Learning (Divvala et al., 2014) has a notably similar ambition and goal as CLIP.\n\nFinally, CLIP is related to a recent burst of activity on learning joint models of vision and language (Lu et al., 2019; Tan).", "mimetype": "text/plain", "start_char_idx": 3724, "end_char_idx": 6028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aabe4c2c-e3f1-4d40-bfda-019374197927": {"__data__": {"id_": "aabe4c2c-e3f1-4d40-bfda-019374197927", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78600fff-960f-468b-9296-3f47b3e5d22c", "node_type": "4", "metadata": {}, "hash": "9c92fcc11fab3062161024a47631135fb7bba78c7ea6a5a2eac46159fc20023f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab6d0bda-a7ca-4a52-a085-88545b1778cb", "node_type": "1", "metadata": {}, "hash": "cfb4acdcfc298f769db3d1f2f4ad3a2ca087b56519725cf19ce8ff63af195bc1", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al., 2020). This line of work focuses on richly connecting vision and language in order to solve complex downstream tasks such as visual question answering, visual commonsense reasoning, or multimodal entailment. These approaches leverage impressively engineered models which combine 3 (or more) pre-trained subsystems, typically an image feature model, a region proposal / object detection model, and a pre-trained masked language model such as BERT. These systems are then jointly fine-tuned via various training objectives on image-text pairs and applied to the aforementioned tasks and achieve impressive results. CLIP is instead focused on learning visual models from scratch via natural language supervision and does not densely connect the two domains with a joint attention model. The only interaction in a CLIP model between the image and text domain is a single dot product in a learned joint embedding space. We are excited to see CLIP hybridized with this line of work.\n\n# 9. Conclusion\n\nWe have investigated whether it is possible to transfer the success of task-agnostic web-scale pre-training in NLP to another domain. We find that adopting this formula results in similar behaviors emerging in the field of computer vision and discuss the social implications of this line of research. In order to optimize their training objective, CLIP models learn to perform a wide variety of tasks during pre-training. This task learning can then be leveraged via natural language prompting to enable zero-shot transfer to many existing datasets. At sufficient scale, the performance of this approach can be competitive with task-specific supervised models although there is still room for much improvement.\n\n# ACKNOWLEDGMENTS\n\nWe\u2019d like to thank the millions of people involved in creating the data CLIP is trained on. We\u2019d also like to thank Susan Zhang for her work on image conditional language models while at OpenAI, Ishaan Gulrajani for catching an error in the pseudocode, and Irene Solaiman, Miles Brundage, and Gillian Hadfield for their thoughtful feedback on the broader impacts section of the paper. We are also grateful to the Acceleration and Supercomputing teams at OpenAI for their critical work on software and hardware infrastructure this project used. Finally, we\u2019d also like to thank the developers of the many software packages used throughout this project including, but not limited, to Numpy (Harris et al., 2020), SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), TensorFlow (Abadi et al., 2016), PyTorch (Paszke et al., 2019), pandas (pandas development team, 2020), and scikit-learn (Pedregosa et al., 2011).\n\n# References\n\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. Tensorflow: A system for large-scale machine learning. In 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16), pp. 265\u2013283, 2016.\n\nAlayrac, J.-B., Recasens, A., Schneider, R., Arandjelovi \u00b4c, R., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S., and Zisserman, A. Self-supervised multimodal versatile networks. arXiv preprint arXiv:2006.16228, 2020.\n\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.- S., and Nguyen, A. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4845\u20134854, 2019.\n\nAndreas, J., Klein, D., and Levine, S. Learning with latent language. arXiv preprint arXiv:1711.00482, 2017.\n\nAssiri, Y. Stochastic optimization of plain convolutional neural networks with simple methods. arXiv preprint arXiv:2001.08856, 2020.\n\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15535\u201315545, 2019.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab6d0bda-a7ca-4a52-a085-88545b1778cb": {"__data__": {"id_": "ab6d0bda-a7ca-4a52-a085-88545b1778cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78600fff-960f-468b-9296-3f47b3e5d22c", "node_type": "4", "metadata": {}, "hash": "9c92fcc11fab3062161024a47631135fb7bba78c7ea6a5a2eac46159fc20023f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aabe4c2c-e3f1-4d40-bfda-019374197927", "node_type": "1", "metadata": {}, "hash": "61d473ca9ca8eab7e31ea8e8dfb7d39108289ddd5f472ded6b4a983094de2189", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4845\u20134854, 2019.\n\nAndreas, J., Klein, D., and Levine, S. Learning with latent language. arXiv preprint arXiv:1711.00482, 2017.\n\nAssiri, Y. Stochastic optimization of plain convolutional neural networks with simple methods. arXiv preprint arXiv:2001.08856, 2020.\n\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15535\u201315545, 2019.\n\nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In Advances in Neural Information Processing Systems, pp. 9453\u20139463, 2019.\n\nBarnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei, D. M., and Jordan, M. I. Matching words and pictures. Journal of machine learning research, 3(Feb):1107\u20131135, 2003.\n\nBechmann, A. and Bowker, G. C. Unsupervised by any other name: Hidden layers of knowledge production in artificial intelligence on social media. Big Data & Society, 6(1):205395171881956, January 2019. doi: 10.1177/2053951718819569. URL https://doi.org/10.1177/2053951718819569.\n\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137\u20131155, 2003.\n\nBhargava, S. and Forsyth, D. Exposing and correcting the gender bias in image captioning datasets and models. arXiv preprint arXiv:1912.00578, 2019.", "mimetype": "text/plain", "start_char_idx": 3465, "end_char_idx": 5045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a0ba75f-2552-46b7-ab96-05e693a4a6e0": {"__data__": {"id_": "1a0ba75f-2552-46b7-ab96-05e693a4a6e0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "52d90776-0151-4b29-bac9-cb34572422e3", "node_type": "4", "metadata": {}, "hash": "61b6030093b70a7b89801f2763e9ea08dd6d87d6ef8332e555a1ad9263ebbaed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "882bef19-bc76-4b65-94a7-6ef9dbc509d3", "node_type": "1", "metadata": {}, "hash": "b167060d3e02c1aa5c9b028089b2fea3ec495bfffed01b0dc64f55233a428ab2", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nBlei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan): 993\u20131022, 2003.\n\nChen, X., Fan, H., Girshick, R., and He, K. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020d.\n\nBolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29:4349\u20134357, 2016.\n\nBowker, G. C. and Star, S. L. Sorting things out: Classification and its consequences. MIT press, 2000.\n\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n\nBrowne, S. Dark Matters: Surveillance of Blackness. Duke University Press, 2015.\n\nBulent Sariyildiz, M., Perez, J., and Larlus, D. Learning visual representations with caption annotations. arXiv e-prints, pp. arXiv\u20132008, 2020.\n\nBuolamwini, J. and Gebru, T. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pp. 77\u201391, 2018.\n\nCarreira, J., Noland, E., Hillier, C., and Zisserman, A. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019.\n\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. In International Conference on Machine Learning, pp. 1691\u20131703. PMLR, 2020a.\n\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.\n\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020b.\n\nChen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. Big self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020c.\n\nChen, X. and Gupta, A. Webly supervised learning of convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1431\u20131439, 2015.\n\nChen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019.\n\nCheng, G., Han, J., and Lu, X. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):1865\u20131883, 2017.\n\nChoi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J., and Dahl, G. E. On empirical comparisons of optimizers for deep learning. arXiv preprint arXiv:1910.05446, 2019.\n\nCoates, A., Ng, A., and Lee, H. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215\u2013223, 2011.\n\nCrawford, K. The trouble with bias. NIPS 2017 Keynote, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "882bef19-bc76-4b65-94a7-6ef9dbc509d3": {"__data__": {"id_": "882bef19-bc76-4b65-94a7-6ef9dbc509d3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "52d90776-0151-4b29-bac9-cb34572422e3", "node_type": "4", "metadata": {}, "hash": "61b6030093b70a7b89801f2763e9ea08dd6d87d6ef8332e555a1ad9263ebbaed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a0ba75f-2552-46b7-ab96-05e693a4a6e0", "node_type": "1", "metadata": {}, "hash": "e35730fe7f2037f5916589de3e7c5f95c28dc53d73d71fc87c05599846357185", "class_name": "RelatedNodeInfo"}}, "text": "Cheng, G., Han, J., and Lu, X. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):1865\u20131883, 2017.\n\nChoi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J., and Dahl, G. E. On empirical comparisons of optimizers for deep learning. arXiv preprint arXiv:1910.05446, 2019.\n\nCoates, A., Ng, A., and Lee, H. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215\u2013223, 2011.\n\nCrawford, K. The trouble with bias. NIPS 2017 Keynote, 2017. URL https://www.youtube.com/watch?v=fMym_BKWQzk.\n\nDai, A. M. and Le, Q. V. Semi-supervised sequence learning. In Advances in neural information processing systems, pp. 3079\u20133087, 2015.\n\nD\u2019Amour, A., Heller, K., Moldovan, D., Adlam, B., Alipanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein, J., Hoffman, M. D., et al. Underspecification presents challenges for credibility in modern machine learning. arXiv preprint arXiv:2011.03395, 2020.\n\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.\n\nDeng, J., Berg, A. C., Satheesh, S., Su, H., Khosla, A., and Fei-Fei, L. ILSVRC 2012, 2012. URL http://www.image-net.org/challenges/LSVRC/2012/.\n\nDesai, K. and Johnson, J. Virtex: Learning visual representations from textual annotations. arXiv preprint arXiv:2006.06666, 2020.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nDhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.", "mimetype": "text/plain", "start_char_idx": 2494, "end_char_idx": 4304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b81694da-c410-4267-9bb8-c3834314e00d": {"__data__": {"id_": "b81694da-c410-4267-9bb8-c3834314e00d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea7c5271-9630-4231-b068-5ab591a699d2", "node_type": "4", "metadata": {}, "hash": "ff73d3e80a19aad106e252dfdadf57172c8710e67c062a6be94c6e78ed9ff1ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0476b83-bc22-48a5-9142-0b55717d8e1c", "node_type": "1", "metadata": {}, "hash": "cc20109f681c04a30d3b25a023be9469c21d8fee666129cce2361487fc49e00e", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nDivvala, S. K., Farhadi, A., and Guestrin, C. Learning everything about anything: Webly-supervised visual concept learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3270\u20133277, 2014.\n\nDodge, S. and Karam, L. A study and comparison of human and deep learning recognition performance under visual distortions. In 2017 26th international conference on computer communication and networks (ICCCN), pp. 1\u20137. IEEE, 2017.\n\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\nElhoseiny, M., Saleh, B., and Elgammal, A. Write a classifier: Zero-shot learning using purely textual descriptions. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2584\u20132591, 2013.\n\nFaghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Improving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017.\n\nFergus, R., Fei-Fei, L., Perona, P., and Zisserman, A. Learning object categories from google\u2019s image search. In Tenth IEEE International Conference on Computer Vision (ICCV\u201905) Volume 1, volume 2, pp. 1816\u20131823. IEEE, 2005.\n\nFrome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and Mikolov, T. Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pp. 2121\u20132129, 2013.\n\nGan, Z., Chen, Y.-C., Li, L., Zhu, C., Cheng, Y., and Liu, J. Large-scale adversarial training for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020.\n\nGao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020.\n\nGarvie, C., May 2019. URL https://www.flawedfacedata.com/.\n\nGeiger, A., Lenz, P., and Urtasun, R. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel, W. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\n\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut learning in deep neural networks. arXiv preprint arXiv:2004.07780, 2020.\n\nGomez, L., Patel, Y., Rusi\u00f1ol, M., Karatzas, D., and Jawahar, C. Self-supervised learning of visual features through embedding images into text topic spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4230\u20134239, 2017.\n\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n\nGoodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler, D., Lee, D.-H., et al. Challenges in representation learning: A report on three machine learning contests.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0476b83-bc22-48a5-9142-0b55717d8e1c": {"__data__": {"id_": "c0476b83-bc22-48a5-9142-0b55717d8e1c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea7c5271-9630-4231-b068-5ab591a699d2", "node_type": "4", "metadata": {}, "hash": "ff73d3e80a19aad106e252dfdadf57172c8710e67c062a6be94c6e78ed9ff1ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b81694da-c410-4267-9bb8-c3834314e00d", "node_type": "1", "metadata": {}, "hash": "0e26f273e04a09cd7e094b93c4d56e9d57c5dd62b90570ceb334f25b45452c85", "class_name": "RelatedNodeInfo"}}, "text": "Gomez, L., Patel, Y., Rusi\u00f1ol, M., Karatzas, D., and Jawahar, C. Self-supervised learning of visual features through embedding images into text topic spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4230\u20134239, 2017.\n\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n\nGoodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler, D., Lee, D.-H., et al. Challenges in representation learning: A report on three machine learning contests. Neural Networks, 64:59\u201363, 2015.\n\nGoogle. Google cloud api: Celebrity recognition. URL https://cloud.google.com/vision/docs/celebrity-recognition.\n\nGriewank, A. and Walther, A. Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1):19\u201345, 2000.\n\nGrill, J.-B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo, Z. D., Azar, M. G., et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\n\nHa, D., Dai, A., and Le, Q. V. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\n\nHancock, B., Bringmann, M., Varma, P., Liang, P., Wang, S., and R\u00e9, C. Training classifiers with natural language explanations. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2018, pp. 1884. NIH Public Access, 2018.\n\nHancock, B., Bordes, A., Mazare, P.-E., and Weston, J. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019.\n\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., Fern\u00e1ndez del", "mimetype": "text/plain", "start_char_idx": 2596, "end_char_idx": 4584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54649484-f0af-41af-a4e8-a218e850c908": {"__data__": {"id_": "54649484-f0af-41af-a4e8-a218e850c908", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7afd3e10-1ca2-471d-a621-75c1dde510a2", "node_type": "4", "metadata": {}, "hash": "015fb1bef71fdecff1ca6dacf0bfa648b0f8c83a08ea39322fef25b4133895e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccb4e6f5-e09e-46d0-9a7f-aa5810c42bd5", "node_type": "1", "metadata": {}, "hash": "0353d1f1215b0209253960b88a53ba2e1b53297db8e174f848dfa575e92a25e8", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nR\u00edo, J., Wiebe, M., Peterson, P., Gerard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585:357\u2013362, 2020. doi: 10.1038/s41586-020-2649-2.\n\nHays, J. and Efros, A. A. Im2gps: estimating geographic information from a single image. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20138. IEEE, 2008.\n\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1026\u20131034, 2015.\n\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016a.\n\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016b.\n\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729\u20139738, 2020.\n\nHe, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M. Bag of tricks for image classification with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 558\u2013567, 2019.\n\nHe, X. and Peng, Y. Fine-grained image classification via combining vision and language. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5994\u20136002, 2017.\n\nHelber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.\n\nHenaff, O. Data-efficient image recognition with contrastive predictive coding. In International Conference on Machine Learning, pp. 4182\u20134192. PMLR, 2020.\n\nHendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.\n\nHendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\n\nHendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples. arXiv preprint arXiv:1907.07174, 2019.\n\nHendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020a.\n\nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., and Song, D. Pretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100, 2020b.\n\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccb4e6f5-e09e-46d0-9a7f-aa5810c42bd5": {"__data__": {"id_": "ccb4e6f5-e09e-46d0-9a7f-aa5810c42bd5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7afd3e10-1ca2-471d-a621-75c1dde510a2", "node_type": "4", "metadata": {}, "hash": "015fb1bef71fdecff1ca6dacf0bfa648b0f8c83a08ea39322fef25b4133895e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54649484-f0af-41af-a4e8-a218e850c908", "node_type": "1", "metadata": {}, "hash": "117f928a7be65079b0d87ef5f03429628264dbd1cdab3686ddade481443b2705", "class_name": "RelatedNodeInfo"}}, "text": "The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020a.\n\nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., and Song, D. Pretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100, 2020b.\n\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n\nHill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick, M., McClelland, J. L., and Santoro, A. Environmental drivers of systematicity and generalization in a situated agent. In International Conference on Learning Representations, 2019.\n\nHodosh, M., Young, P., and Hockenmaier, J. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47: 853\u2013899, 2013.\n\nHongsuck Seo, P., Weyand, T., Sim, J., and Han, B. Cplanet: Enhancing image geolocalization by combinatorial partitioning of maps. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 536\u2013551, 2018.\n\nHoward, J. and Ruder, S. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018.\n\nIlyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems, pp. 125\u2013136, 2019.\n\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n\nJaderberg, M., Simonyan, K., Vedaldi, A., and Zisserman, A. Deep structured output learning for unconstrained text recognition. arXiv preprint arXiv:1412.5903, 2014.\n\nJaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial transformer networks. Advances in neural information processing systems, 28:2017\u20132025, 2015.", "mimetype": "text/plain", "start_char_idx": 2685, "end_char_idx": 4674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "405eea68-78d0-4531-b0ae-3f57b94509a5": {"__data__": {"id_": "405eea68-78d0-4531-b0ae-3f57b94509a5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f9f3699-81e4-492d-9b2b-1d8d500e14d4", "node_type": "4", "metadata": {}, "hash": "47ecb66266efa17d85893a7604261798595e39c167cec871c93a79b20e6e7bb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d60ec123-3928-49b7-8155-2e418d84de73", "node_type": "1", "metadata": {}, "hash": "d5ee7d3902f4eaa7280aeb3181aef2ef8e7bdff1db5bfa94c1fe464fc04fd51e", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nJohnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2901\u20132910, 2017.\n\nKrishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373, 2017.\n\nJoulin, A., Van Der Maaten, L., Jabri, A., and Vasilache, N. Learning visual features from large weakly supervised data. In European Conference on Computer Vision, pp. 67\u201384. Springer, 2016.\n\nKalfaoglu, M., Kalkan, S., and Alatan, A. A. Late temporal modeling in 3d cnn architectures with bert for action recognition. arXiv preprint arXiv:2008.01232, 2020.\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n\nKarpathy, A., Joulin, A., and Fei-Fei, L. F. Deep fragment embeddings for bidirectional image sentence mapping. In Advances in neural information processing systems, pp. 1889\u20131897, 2014.\n\nKeyes, O. The misgendering machines: Trans/hci implications of automatic gender recognition. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW):1\u201322, 2018.\n\nKiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A., Ringshia, P., and Testuggine, D. The hateful memes challenge: Detecting hate speech in multimodal memes. arXiv preprint arXiv:2005.04790, 2020.\n\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.\n\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. Skip-thought vectors. Advances in neural information processing systems, 28: 3294\u20133302, 2015.\n\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Large scale learning of general visual representations for transfer. arXiv preprint arXiv:1912.11370, 2019.\n\nKornblith, S., Shlens, J., and Le, Q. V. Do better imagenet models transfer better? In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2661\u20132671, 2019.\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\n\nKuhnle, A. and Copestake, A. Shapeworld-a new test methodology for multimodal language understanding. arXiv preprint arXiv:1704.04517, 2017.\n\nK\u00e4rkka\u00efnen, K. and Joo, J. Fairface: Face attribute dataset for balanced race, gender, and age, 2019.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d60ec123-3928-49b7-8155-2e418d84de73": {"__data__": {"id_": "d60ec123-3928-49b7-8155-2e418d84de73", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f9f3699-81e4-492d-9b2b-1d8d500e14d4", "node_type": "4", "metadata": {}, "hash": "47ecb66266efa17d85893a7604261798595e39c167cec871c93a79b20e6e7bb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "405eea68-78d0-4531-b0ae-3f57b94509a5", "node_type": "1", "metadata": {}, "hash": "7e72037e15945596f47d8dddb15e1e469d0e6aa744b102f593eb81c4f56e2378", "class_name": "RelatedNodeInfo"}}, "text": "Kornblith, S., Shlens, J., and Le, Q. V. Do better imagenet models transfer better? In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2661\u20132671, 2019.\n\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\n\nKuhnle, A. and Copestake, A. Shapeworld-a new test methodology for multimodal language understanding. arXiv preprint arXiv:1704.04517, 2017.\n\nK\u00e4rkka\u00efnen, K. and Joo, J. Fairface: Face attribute dataset for balanced race, gender, and age, 2019.\n\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people, 2016.\n\nLampert, C. H., Nickisch, H., and Harmeling, S. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 951\u2013958. IEEE, 2009.\n\nLarochelle, H., Erhan, D., and Bengio, Y. Zero-data learning of new tasks. 2008.\n\nLe, Q. and Mikolov, T. Distributed representations of sentences and documents. In International conference on machine learning, pp. 1188\u20131196, 2014.\n\nLeCun, Y. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/\n\nLee, D.-H. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.\n\nLei Ba, J., Swersky, K., Fidler, S., et al. Predicting deep zero-shot convolutional neural networks using textual descriptions. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4247\u20134255, 2015.\n\nLi, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning visual n-grams from web data. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4183\u20134192, 2017.\n\nLi, G., Duan, N., Fang, Y., Gong, M., and Jiang, D. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. 2020a.\n\nLi, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J. Learning through dialogue interactions by asking questions. arXiv preprint arXiv:1612.04936, 2016.", "mimetype": "text/plain", "start_char_idx": 2441, "end_char_idx": 4553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0936e70-74fd-460a-be27-067e462b545c": {"__data__": {"id_": "b0936e70-74fd-460a-be27-067e462b545c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89859f62-6d25-475a-ac37-d8d46e974607", "node_type": "4", "metadata": {}, "hash": "57d42d05ec8c46f349df3173aecdb7ab60331ca1fbde74220857dd7ceabdfc25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "848fda35-49cd-4b13-be0a-325521247386", "node_type": "1", "metadata": {}, "hash": "38c4b1c9aa1d9cf3eef6b0754cde66b5b7abdf592779586c9eb5a25c870e6f3f", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nLi, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. arXiv preprint arXiv:2004.06165, 2020b.\n\nLiang, W., Zou, J., and Yu, Z. Alice: Active learning with contrastive natural language explanations. arXiv preprint arXiv:2009.10259, 2020.\n\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740\u2013755. Springer, 2014.\n\nLinzen, T. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020.\n\nLippe, P., Holla, N., Chandra, S., Rajamanickam, S., Antoniou, G., Shutova, E., and Yannakoudakis, H. A multimodal framework for the detection of hateful memes. arXiv preprint arXiv:2012.12871, 2020.\n\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.\n\nLocatello, F., Bauer, S., Lucic, M., R\u00e4tsch, G., Gelly, S., Sch\u00f6lkopf, B., and Bachem, O. A sober look at the unsupervised learning of disentangled representations and their evaluation. arXiv preprint arXiv:2010.14766, 2020.\n\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\n\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems, pp. 13\u201323, 2019.\n\nLu, Z., Xiong, X., Li, Y., Stroud, J., and Ross, D. Leveraging weakly supervised data and pose representation for action recognition, 2020. URL https://www.youtube.com/watch?v=KOQFxbPPLOE&t=1390s.\n\nLucic, M., Kurach, K., Michalski, M., Gelly, S., and Bousquet, O. Are gans created equal? a large-scale study. Advances in neural information processing systems, 31: 700\u2013709, 2018.\n\nMahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., and van der Maaten, L. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 181\u2013196, 2018.\n\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned in translation: Contextualized word vectors. In Advances in neural information processing systems, pp. 6294\u20136305, 2017.\n\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\n\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "848fda35-49cd-4b13-be0a-325521247386": {"__data__": {"id_": "848fda35-49cd-4b13-be0a-325521247386", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89859f62-6d25-475a-ac37-d8d46e974607", "node_type": "4", "metadata": {}, "hash": "57d42d05ec8c46f349df3173aecdb7ab60331ca1fbde74220857dd7ceabdfc25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0936e70-74fd-460a-be27-067e462b545c", "node_type": "1", "metadata": {}, "hash": "89313fcf134c073708156bb3d48ca34acc98c5370cec3108a89faddace685642", "class_name": "RelatedNodeInfo"}}, "text": "McCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned in translation: Contextualized word vectors. In Advances in neural information processing systems, pp. 6294\u20136305, 2017.\n\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\n\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.\n\nMiech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev, I., and Sivic, J. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE international conference on computer vision, pp. 2630\u20132640, 2019.\n\nMiech, A., Alayrac, J.-B., Laptev, I., Sivic, J., and Zisserman, A. Rareact: A video dataset of unusual interactions. arXiv preprint arXiv:2008.01018, 2020a.\n\nMiech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J., and Zisserman, A. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9879\u20139889, 2020b.\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26:3111\u20133119, 2013.\n\nMiller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question answering models. arXiv preprint arXiv:2004.14444, 2020.\n\nMishra, A., Alahari, K., and Jawahar, C. Scene text recognition using higher order language priors. 2012.\n\nMithun, N. C., Panda, R., Papalexakis, E. E., and Roy-Chowdhury, A. K. Webly supervised joint embedding for cross-modal image-text retrieval. In Proceedings of the 26th ACM international conference on Multimedia, pp. 1856\u20131864, 2018.\n\nMori, Y., Takahashi, H., and Oka, R. Image-to-word transformation based on dividing and vector quantizing images with words. Citeseer, 1999.\n\nMu, J., Liang, P., and Goodman, N. Shaping visual representations with language for few-shot classification. arXiv preprint arXiv:1911.02683, 2019.", "mimetype": "text/plain", "start_char_idx": 2472, "end_char_idx": 4730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "761e7da4-8852-4cbc-825c-af3c81d9974e": {"__data__": {"id_": "761e7da4-8852-4cbc-825c-af3c81d9974e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0825c0e2-b0fb-4a2b-8fef-ac7f7fbaf7f2", "node_type": "4", "metadata": {}, "hash": "bd5e0574adcfc5bcfd37c4b5aadf9815b94a62ac7609959d9404dbaf56703ff6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e54863c2-008e-46d4-be11-8da57d5c32e2", "node_type": "1", "metadata": {}, "hash": "bc136335635a9ad0598695106c1f37a87fc1b9e0cd6540244518600c0faf03f9", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nMuller-Budack, E., Pustu-Iren, K., and Ewerth, R. Geolocation estimation of photos using a hierarchical model and scene classification. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 563\u2013579, 2018.\n\nMurty, S., Koh, P. W., and Liang, P. Expbert: Representation engineering with natural language explanations. arXiv preprint arXiv:2005.01932, 2020.\n\nNarasimhan, K., Kulkarni, T., and Barzilay, R. Language understanding for text-based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.\n\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. 2011.\n\nNoble, S. U. Algorithms of oppression: How search engines reinforce racism. 2018.\n\nNosek, B. A., Banaji, M. R., and Greenwald, A. G. Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics: Theory, Research, and Practice, 6(1):101, 2002.\n\nOh, S., Hoogs, A., Perera, A., Cuntoor, N., Chen, C.-C., Lee, J. T., Mukherjee, S., Aggarwal, J., Lee, H., Davis, L., et al. A large-scale benchmark dataset for event recognition in surveillance video. In CVPR 2011, pp. 3153\u20133160. IEEE, 2011.\n\nOliver, A., Odena, A., Raffel, C. A., Cubuk, E. D., and Goodfellow, I. Realistic evaluation of deep semi-supervised learning algorithms. Advances in neural information processing systems, 31:3235\u20133246, 2018.\n\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nOrdonez, V., Kulkarni, G., and Berg, T. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24:1143\u20131151, 2011.\n\npandas development team, T. pandas-dev/pandas: Pandas, February 2020. URL https://doi.org/10.5281/zenodo.3509134.\n\nParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.\n\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024\u20138035, 2019.\n\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.\n\nPennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532\u20131543, 2014.\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e54863c2-008e-46d4-be11-8da57d5c32e2": {"__data__": {"id_": "e54863c2-008e-46d4-be11-8da57d5c32e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0825c0e2-b0fb-4a2b-8fef-ac7f7fbaf7f2", "node_type": "4", "metadata": {}, "hash": "bd5e0574adcfc5bcfd37c4b5aadf9815b94a62ac7609959d9404dbaf56703ff6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "761e7da4-8852-4cbc-825c-af3c81d9974e", "node_type": "1", "metadata": {}, "hash": "dae2cc0a9c52e9eb6307e26fe031f5c93b9e5f3b7100e55f839287c440a86e43", "class_name": "RelatedNodeInfo"}}, "text": "Journal of Machine Learning Research, 12:2825\u20132830, 2011.\n\nPennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532\u20131543, 2014.\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n\nQi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti, A. Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966, 2020.\n\nQuattoni, A., Collins, M., and Darrell, T. Learning visual representations using images with captions. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20138. IEEE, 2007.\n\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pre-training, 2018.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\nRaji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., and Denton, E. Saving face: Investigating the ethical concerns of facial recognition auditing, 2020.\n\nRamanathan, V., Liang, P., and Fei-Fei, L. Video event understanding using natural language descriptions. In Proceedings of the IEEE International Conference on Computer Vision, pp. 905\u2013912, 2013.\n\nRashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J. Collecting image annotations using amazon\u2019s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, pp. 139\u2013147, 2010.", "mimetype": "text/plain", "start_char_idx": 2756, "end_char_idx": 4689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7126876-5083-46a7-b415-2f3bf5499a60": {"__data__": {"id_": "f7126876-5083-46a7-b415-2f3bf5499a60", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1095d662-7e2e-4dc3-9f35-abcc461be4b8", "node_type": "4", "metadata": {}, "hash": "7f2289fb69e8637ec72e1ead0de0180585eb89d54e4cc7301f7b9e2ea627b7e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ba60313-48a5-4580-92e2-218ec42bf988", "node_type": "1", "metadata": {}, "hash": "2b01132ef2d6a08ed9aabcf37f9126a9526fb8513be3d5625ac265f40d5eaefd", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do image classifiers generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019.\n\nSalimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in neural information processing systems, pp. 901\u2013909, 2016.\n\nScheuerman, M. K., Paul, J. M., and Brubaker, J. R. How computers see gender: An evaluation of gender classification in commercial facial analysis services. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW): 1\u201333, 2019.\n\nSchwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija, S., Schoonvelde, M., and Lockhart, J. W. Diagnosing gender bias in image recognition systems. Socius, 6: 2378023120967171, 2020.\n\nSennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\n\nShankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classifiers generalize across time? arXiv preprint arXiv:1906.02168, 2019.\n\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556\u20132565, 2018.\n\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8317\u20138326, 2019.\n\nSocher, R. and Fei-Fei, L. Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 966\u2013973. IEEE, 2010.\n\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631\u20131642, 2013.\n\nSocher, R., Karpathy, A., Le, Q. V., Manning, C. D., and Ng, A. Y. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2: 207\u2013218, 2014.\n\nSohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems, pp. 1857\u20131865, 2016.\n\nSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W., Kreps, S., McCain, M., Newhouse, A., Blazakis, J., McGuffie, K., and Wang, J. Release strategies and the social impacts of language models, 2019.\n\nSoomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n\nSpeer, R. ftfy. Zenodo, 2019. URL https://doi.org/10.5281/zenodo.2591652. Version 5.5.\n\nSrivastava, N. and Salakhutdinov, R. Multimodal learning with deep boltzmann machines. In NIPS, 2012.\n\nSrivastava, S., Labutov, I., and Mitchell, T. Joint concept learning and semantic parsing from natural language explanations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ba60313-48a5-4580-92e2-218ec42bf988": {"__data__": {"id_": "0ba60313-48a5-4580-92e2-218ec42bf988", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1095d662-7e2e-4dc3-9f35-abcc461be4b8", "node_type": "4", "metadata": {}, "hash": "7f2289fb69e8637ec72e1ead0de0180585eb89d54e4cc7301f7b9e2ea627b7e0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7126876-5083-46a7-b415-2f3bf5499a60", "node_type": "1", "metadata": {}, "hash": "2687cdbb666191bb1d7803f3d17c254043a54010cd7cdbb0e2fe5a0dfb77276f", "class_name": "RelatedNodeInfo"}}, "text": "Soomro, K., Zamir, A. R., and Shah, M. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n\nSpeer, R. ftfy. Zenodo, 2019. URL https://doi.org/10.5281/zenodo.2591652. Version 5.5.\n\nSrivastava, N. and Salakhutdinov, R. Multimodal learning with deep boltzmann machines. In NIPS, 2012.\n\nSrivastava, S., Labutov, I., and Mitchell, T. Joint concept learning and semantic parsing from natural language explanations. In Proceedings of the 2017 conference on empirical methods in natural language processing, pp. 1527\u20131536, 2017.\n\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In IEEE International Joint Conference on Neural Networks, pp. 1453\u20131460, 2011.\n\nStroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R., and Schmid, C. Learning video representations from textual web supervision. arXiv preprint arXiv:2007.14937, 2020.\n\nSzegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. Inception-v4, inception-resnet and the impact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016.\n\nTan, H. and Bansal, M. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.\n\nTan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n\nTaori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L. Measuring robustness to natural distribution shifts in image classification. arXiv preprint arXiv:2007.00644, 2020.\n\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2): 64\u201373, 2016.", "mimetype": "text/plain", "start_char_idx": 2849, "end_char_idx": 4669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42af3159-369b-4328-8b74-a84abe48f9b9": {"__data__": {"id_": "42af3159-369b-4328-8b74-a84abe48f9b9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0106afbf-b82d-411a-8a1a-848383ba3673", "node_type": "4", "metadata": {}, "hash": "c4684b3e4719b21d39ce085dcb0a9d8e00bad7d243b7d818a1a6a661e3190621", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ade21bc-d790-4fbb-bd03-8b3749b9ed00", "node_type": "1", "metadata": {}, "hash": "7360d14b81463b68efd96902bab3ab1449a6bcbc3381cc8ad19ee5630b23d622", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nTian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n\nTian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and Isola, P. Rethinking few-shot image classification: a good embedding is all you need? arXiv preprint arXiv:2003.11539, 2020.\n\nTorralba, A., Fergus, R., and Freeman, W. T. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence, 30(11):1958\u20131970, 2008.\n\nTouvron, H., Vedaldi, A., Douze, M., and J\u00e9gou, H. Fixing the train-test resolution discrepancy. In Advances in neural information processing systems, pp. 8252\u20138262, 2019.\n\nVaradarajan, J. and Odobez, J.-M. Topic models for scene analysis and abnormality detection. In 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, pp. 1338\u20131345. IEEE, 2009.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998\u20136008, 2017.\n\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and Welling, M. Rotation equivariant CNNs for digital pathology. June 2018.\n\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, I., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020. doi: 10.1038/s41592-019-0686-2.\n\nVo, N., Jacobs, N., and Hays, J. Revisiting im2gps in the deep learning era. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2621\u20132630, 2017.\n\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nWang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506\u201310518, 2019.\n\nWang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He, M., Wang, Y., and Liu, W. All you need is boundary: Toward arbitrary-shaped text spotting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 12160\u201312167, 2020.\n\nWang, J., Markert, K., and Everingham, M. Learning models for object recognition from natural language descriptions. In BMVC, volume 1, pp. 2, 2009.\n\nWeston, J., Bengio, S., and Usunier, N. Large scale image annotation: learning to rank with joint word-image embeddings. Machine learning, 81(1):21\u201335, 2010.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ade21bc-d790-4fbb-bd03-8b3749b9ed00": {"__data__": {"id_": "3ade21bc-d790-4fbb-bd03-8b3749b9ed00", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0106afbf-b82d-411a-8a1a-848383ba3673", "node_type": "4", "metadata": {}, "hash": "c4684b3e4719b21d39ce085dcb0a9d8e00bad7d243b7d818a1a6a661e3190621", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42af3159-369b-4328-8b74-a84abe48f9b9", "node_type": "1", "metadata": {}, "hash": "519c1b89d266a3a9e5cc5e350d58ede0b7de8c2067e80068b20ddef439da6780", "class_name": "RelatedNodeInfo"}}, "text": "In Advances in Neural Information Processing Systems, pp. 10506\u201310518, 2019.\n\nWang, H., Lu, P., Zhang, H., Yang, M., Bai, X., Xu, Y., He, M., Wang, Y., and Liu, W. All you need is boundary: Toward arbitrary-shaped text spotting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 12160\u201312167, 2020.\n\nWang, J., Markert, K., and Everingham, M. Learning models for object recognition from natural language descriptions. In BMVC, volume 1, pp. 2, 2009.\n\nWeston, J., Bengio, S., and Usunier, N. Large scale image annotation: learning to rank with joint word-image embeddings. Machine learning, 81(1):21\u201335, 2010.\n\nWeston, J. E. Dialog-based language learning. In Advances in Neural Information Processing Systems, pp. 829\u2013837, 2016.\n\nWeyand, T., Kostrikov, I., and Philbin, J. Planet-photo geolocation with convolutional neural networks. In European Conference on Computer Vision, pp. 37\u201355. Springer, 2016.\n\nWu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Girshick, R. Detectron2. https://github.com/facebookresearch/detectron2, 2019.\n\nWu, Z., Xiong, Y., Yu, S., and Lin, D. Unsupervised feature learning via non-parametric instance-level discrimination. arXiv preprint arXiv:1805.01978, 2018.\n\nXie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10687\u201310698, 2020.\n\nArcas, B. A., Mitchell, M., and Todorov, A. Physiognomy\u2019s new clothes. 2017. URL https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a.\n\nYang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang, L., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware pre-training for text-vqa and text-caption. arXiv preprint arXiv:2012.04638, 2020.\n\nYogatama, D., d\u2019Autume, C. d. M., Connor, J., Kocisky, T., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L., Dyer, C., et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.\n\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.", "mimetype": "text/plain", "start_char_idx": 2467, "end_char_idx": 4737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3356ba8-be4f-45a1-970d-deeae023650a": {"__data__": {"id_": "a3356ba8-be4f-45a1-970d-deeae023650a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25eafcc0-678b-4c42-8644-ded427c5a99e", "node_type": "4", "metadata": {}, "hash": "5e903f64f5750a010e061b971b3ffbf97505820765bf9ef3a91d879c390f7906", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# References\n\nYu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H., and Wang, H. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. arXiv preprint arXiv:2006.16934, 2020.\n\nZeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. In European conference on computer vision, pp. 818\u2013833. Springer, 2014.\n\nZhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neumann, M., Dosovitskiy, A., et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019.\n\nZhang, R. Making convolutional networks shift-invariant again. arXiv preprint arXiv:1904.11486, 2019.\n\nZhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Langlotz, C. P. Contrastive learning of medical visual representations from paired images and text. arXiv preprint arXiv:2010.00747, 2020.\n\nZuboff, S. Big other: surveillance capitalism and the prospects of an information civilization. Journal of Information Technology, 30(1):75\u201389, 2015.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae308dda-de14-447c-8e8e-cc6ffd4d72bf": {"__data__": {"id_": "ae308dda-de14-447c-8e8e-cc6ffd4d72bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e29c671f-515d-4c2c-9106-348dfebbd811", "node_type": "4", "metadata": {}, "hash": "74e90e79831bd9871165ac6226944059fc8940ac6247c37cff39d931c75a4dc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b56f57da-cfe6-4957-b283-5eceed098ce7", "node_type": "1", "metadata": {}, "hash": "b4902f304e232fb392899fb1905c2fccd48f647c03035c4d00f1c94d41c217a3", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# A. Linear-probe evaluation\n\nWe provide additional details for linear probe experiments presented in this paper, including the list of the datasets and models used for evaluation.\n\n# A.1. Datasets\n\nWe use the 12 datasets from the well-studied evaluation suite introduced by (Kornblith et al., 2019) and add 15 additional datasets in order to assess the performance of models on a wider variety of distributions and tasks. These datasets include:\n\n- MNIST\n- The Facial Expression Recognition 2013 dataset (Goodfellow et al., 2015)\n- STL-10 (Coates et al., 2011)\n- EuroSAT (Helber et al., 2019)\n- The NWPU-RESISC45 dataset (Cheng et al., 2017)\n- The German Traffic Sign Recognition Benchmark (GTSRB) dataset (Stallkamp et al., 2011)\n- The KITTI dataset (Geiger et al., 2012)\n- PatchCamelyon (Veeling et al., 2018)\n- The UCF101 action recognition dataset (Soomro et al., 2012)\n- Kinetics 700 (Carreira et al., 2019)\n- 2,500 random samples of the CLEVR dataset (Johnson et al., 2017)\n- The Hateful Memes dataset (Kiela et al., 2020)\n- The ImageNet-1k dataset (Deng et al., 2012)\n\nFor the two video datasets (UCF101 and Kinetics700), we use the middle frame of each video clip as the input image. STL-10 and UCF101 have multiple predefined train/validation/test splits, 10 and 3 respectively, and we report the average over all splits. Details on each dataset and the corresponding evaluation metrics are provided in Table 9.\n\nAdditionally, we created two datasets that we call Country211 and Rendered SST2. The Country211 dataset is designed to assess the geolocation capability of visual representations. We filtered the YFCC100m dataset (Thomee et al., 2016) to find 211 countries (defined as having an ISO-3166 country code) that have at least 300 photos with GPS coordinates, and we built a balanced dataset with 211 categories, by sampling 200 photos for training and 100 photos for testing, for each country.\n\nThe Rendered SST2 dataset is designed to measure the optical character recognition capability of visual representations. To do so, we used the sentences from the Stanford Sentiment Treebank dataset (Socher et al., 2013) and rendered them into images, with black texts on a white background, in a 448\u00d7448 resolution. Two example images from this dataset are shown in Figure 19.\n\n# A.2. Models\n\nIn combination with the datasets listed above, we evaluate the following series of models using linear probes.\n\n|Model|Description|\n|---|---|\n|CLIP-RN|Five ResNet-based contrastive CLIP models are included. As discussed in the paper, the first two models follow ResNet-50 and ResNet-101, and we use EfficientNet-style (Tan & Le, 2019) scaling for the next three models which simultaneously scale the model width, the number of layers, and the input resolution to obtain models with roughly 4x, 16x, and 64x computation.|\n|CLIP-ViT|We include four CLIP models that use the Vision Transformer (Dosovitskiy et al., 2020) architecture as the image encoder. We include three models trained on 224-by-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and the ViT-L/14 model fine-tuned on 336-by-336 pixel input images.|\n|EfficietNet|We use the nine models (B0-B8) from the original EfficientNet paper (Tan & Le, 2019), as well as the noisy-student variants (B0-B7, L2-475, and L2-800) (Tan & Le, 2019).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b56f57da-cfe6-4957-b283-5eceed098ce7": {"__data__": {"id_": "b56f57da-cfe6-4957-b283-5eceed098ce7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e29c671f-515d-4c2c-9106-348dfebbd811", "node_type": "4", "metadata": {}, "hash": "74e90e79831bd9871165ac6226944059fc8940ac6247c37cff39d931c75a4dc6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae308dda-de14-447c-8e8e-cc6ffd4d72bf", "node_type": "1", "metadata": {}, "hash": "ae0c6cfb7b230678640cd4e82fe21d297e1fba9ecbdd52bf82121a51713afec4", "class_name": "RelatedNodeInfo"}}, "text": "We include three models trained on 224-by-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and the ViT-L/14 model fine-tuned on 336-by-336 pixel input images.|\n|EfficietNet|We use the nine models (B0-B8) from the original EfficientNet paper (Tan & Le, 2019), as well as the noisy-student variants (B0-B7, L2-475, and L2-800) (Tan & Le, 2019). The largest models (L2-475 and L2-800) take the input resolutions of 475x475 and 800x800 pixels, respectively.|\n|Instagram-pretrained ResNeXt|We use the four models (32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan et al., 2018), as well as their two FixRes variants which use higher input resolutions (Touvron et al., 2019).|\n|Big Transfer (BiT)|We use BiT-S and BiT-M models (Kolesnikov et al., 2019), trained on the ImageNet-1k and ImageNet-21k datasets. The model weights for BiT-L is not publicly available.|\n|Vision Transformer (ViT)|We also include four ViT (Dosovitskiy et al., 2020) checkpoints pretrained on the ImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-L/16, and ViT-H/14. We note that their best-performing models, trained on the JFT-300M dataset, are not available publicly.|\n\nSimCLRv2: The SimCLRv2 (Chen et al., 2020c) project released pre-trained and fine-tuned models in various settings. We use the seven pretrain-only checkpoints with selective kernels.\n\nLM RN50: This is a multimodal model that uses an autoregressive loss instead of a contrastive loss, while using BYOL. We use the recently released model weights of BYOL (Grill et al., 2020), specifically their 50x1 and 200x2.", "mimetype": "text/plain", "start_char_idx": 3032, "end_char_idx": 4587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bf96c12-68d2-445a-b887-1bd900e22f0f": {"__data__": {"id_": "8bf96c12-68d2-445a-b887-1bd900e22f0f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "87201f69-a510-4839-9328-3fae3cf3af3d", "node_type": "4", "metadata": {}, "hash": "83d694cb947b48837f4f17e68380b39f082d45e6364ed70c4f841fabd27134bc", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\nnicely nuanced narrative and surrounds Montias pumps a lot of energy into his himself with cast of quirky stereotyped street characters but not clear that they lack the skills to get us to this undetermined destination.\n\nFigure 19. Two example images from the Rendered SST2 dataset\n\ncheckpoints.\n\n# Momentum Contrast (MoCo)\n\nWe include the MoCo-v1 (He et al., 2020) and the MoCo-v2 (Chen et al., 2020d) checkpoints.\n\n# VirTex\n\nWe use the pretrained model of VirTex (Desai & Johnson, 2020). We note that VirTex has a similar model design to CLIP-AR but is trained on a 1000x smaller dataset of high-quality captions from MSCOCO.\n\n# ResNet\n\nWe add the original ResNet checkpoints released by (He et al., 2016b), namely ResNet-50, ResNet-101, and ResNet152.\n\n# A.3. Evaluation\n\nWe use image features taken from the penultimate layer of each model, ignoring any classification layer provided. For CLIP-ViT models, we used the features before the linear projection to the embedding space, which corresponds to I f in Figure 3. We train a logistic regression classifier using scikit-learn\u2019s L-BFGS implementation, with maximum 1,000 iterations, and report the corresponding metric for each dataset. We determine the L2 regularization strength \u03bb using a hyperparameter sweep on the validation sets over the range between 10\u22126 and 106, with 96 logarithmically spaced steps. To save compute required for the sweeps, we perform a parametric binary search that starts with \u03bb = [10\u22126, 10\u22124, 10\u22122, 1, 102, 104, 106] and iteratively halves the interval around the peak until it reaches a resolution of 8 steps per decade. The hyperparameter sweeps are performed on a validation split of each dataset.\n\n# A.4. Results\n\nThe individual linear probe scores are provided in Table 10 and plotted in Figure 20. The best-performing CLIP model, using ViT-L/14 architecture and 336-by-336 pixel images, achieved the state of the art in 21 of the 27 datasets, i.e. included in the Clopper-Pearson 99.5% confidence interval around each dataset\u2019s top score. For many datasets, CLIP performs significantly better than other models, demonstrating the advantage of natural language supervision over traditional pre-training approaches based on image classification. See Section 3.2 for more discussions on the linear probe results.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52baddc7-461a-4be8-bde4-d00f50dba61e": {"__data__": {"id_": "52baddc7-461a-4be8-bde4-d00f50dba61e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cba32d09-f544-49b8-9e82-99d49c199884", "node_type": "4", "metadata": {}, "hash": "70bc055df6e11ad25b28f793e026dfeb95e65bcc3c0e1c647429bb434b3f1622", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Table 9. Datasets examined for linear probes.\n\n|Dataset|Classes|Train size|Test size|Evaluation metric|\n|---|---|---|---|---|\n|Food-101|102|75,750|25,250|accuracy|\n|CIFAR-10|10|50,000|10,000|accuracy|\n|CIFAR-100|100|50,000|10,000|accuracy|\n|Birdsnap|500|42,283|2,149|accuracy|\n|SUN397|397|19,850|19,850|accuracy|\n|Stanford Cars|196|8,144|8,041|accuracy|\n|FGVC Aircraft|100|6,667|3,333|mean per class|\n|Pascal VOC 2007 Classification|20|5,011|4,952|11-point mAP|\n|Describable Textures|47|3,760|1,880|accuracy|\n|Oxford-IIIT Pets|37|3,680|3,669|mean per class|\n|Caltech-101|102|3,060|6,085|mean-per-class|\n|Oxford Flowers 102|102|2,040|6,149|mean per class|\n|MNIST|10|60,000|10,000|accuracy|\n|Facial Emotion Recognition 2013|8|32,140|3,574|accuracy|\n|STL-10|10|1,000|8,000|accuracy|\n|EuroSAT|10|10,000|5,000|accuracy|\n|RESISC45|45|3,150|25,200|accuracy|\n|GTSRB|43|26,640|12,630|accuracy|\n|KITTI|4|6,770|711|accuracy|\n|Country211|211|43,200|21,100|accuracy|\n|PatchCamelyon|2|294,912|32,768|accuracy|\n|UCF101|101|9,537|1,794|accuracy|\n|Kinetics700|700|494,801|31,669|mean(top1, top5)|\n|CLEVR Counts|8|2,000|500|accuracy|\n|Hateful Memes|2|8,500|500|ROC AUC|\n|Rendered SST2|2|7,792|1,821|accuracy|\n|ImageNet|1000|1,281,167|50,000|accuracy|\n\nWe note that, for the Birdsnap and Kinetics700 datasets, we used the resources that are available online at the time of this writing.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a1f7780-dbf9-4a95-89bc-ee6e1acc0428": {"__data__": {"id_": "6a1f7780-dbf9-4a95-89bc-ee6e1acc0428", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d501f35-0963-49d2-8548-97a247a39c4d", "node_type": "4", "metadata": {}, "hash": "1c0dfa10c1831094692f0a3b7f0778ccd003ae11258356a3da94d6a25d442bef", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n|Model|Food101|CIFAR10|CIFAR100|Birdsnap|SUN397|Cars|Aircraft|VOC2007|Pets|Caltech101|Flowers|MNIST|FER2013|STL10|EuroSAT|RESISC45|GTSRB|KITTI|Country211|PCA|UCF101|Kinetics700|HatefulMemes|SSTR|ImageNet|VirTex| | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|LM RN50|81.3|86.4|88.9|91.3|95.2|74.3|75.8|79.7|84.5|78.1| | | | |80.4|85.2|87.6|91.6|92.0|84.8|86.7|88.5|72.5|75.1|74.7|83.3|88.0|81.8|57.9|71.3| | | | | | | | |\n|LM RN50|82.8|88.7|91.1|90.5|98.0|92.5|93.6| | | |94.1|95.0|94.0|95.1|95.6|96.3|98.7|98.7|95.9|96.8|95.7|91.7|93.7|94.2|94.9|95.7|96.7|83.9|91.8| | | | | | | | | |\n|LM RN50|61.7|70.3|73.5|73.0|87.5|76.5|77.9|78.7|80.7|78.6|80.2|81.3|82.4|91.0|89.0|80.9|80.1|82.7|74.8|79.0|79.2|82.2|86.2|84.4|86.3|74.5| | | | | | | | | | | | |\n|LM RN50|44.2|56.4|58.6|65.7|77.0|59.7|64.4|70.1|75.2|63.5|66.6|67.6|75.3|75.8|78.5|63.8|64.8|67.4|57.7|61.1|58.4|70.9|72.5|74.0|52.7| | | | | | | | | | | | | |\n|LM RN50|69.6|73.3| | | | | | | | | | | | | |75.1|77.0|81.8|62.0|64.0|65.4|69.6|65.5|67.6|71.6|71.1|75.7|75.5|70.5|77.5|76.6|61.1|62.9|61.3|70.9|72.5|75.0|60.5|\n\nTable 10. Linear probe performance of various pre-trained models over 27 datasets. Scores within the 99.5% Clopper-Pearson confidence interval of each dataset\u2019s top score are shown in bold. We updated the STL10 scores from the previous version of this paper after fixing a CUDA-related bug.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca538e47-e711-496a-a2e4-5c2563d39594": {"__data__": {"id_": "ca538e47-e711-496a-a2e4-5c2563d39594", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2ba21c9-dfb1-4a39-bdd3-9f935d27a740", "node_type": "4", "metadata": {}, "hash": "b9f1cbdfdc16734c278f25d5a671596886bc36ac629c4c2e19d302c9a9563f6f", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n|Dataset|Accuracy|Accuracy|Accuracy|Accuracy|\n|---|---|---|---|---|\n|Food101|95|90|85|80|\n|CIFAR10|98|96|94|92|\n|CIFAR100|75|70|65|60|\n|Birdsnap|85|80|75|70|\n\n|Dataset|Accuracy|Accuracy|Accuracy|Accuracy|\n|---|---|---|---|---|\n|SUN397|90|80|70|60|\n|StanfordCars|90|80|70|65|\n|FGVCAircraft|90|80|70|65|\n|PascalVOC2007|90|80|70|65|\n\n|Dataset|Mean per Class|Mean (Top1, Top5)|\n|---|---|---|\n|DescribableTextures|96|82|\n|OxfordPets|96|94|\n|Caltech101|100|96|\n|Flowers|91|90|\n\n|Dataset|Accuracy|Accuracy|Accuracy|Accuracy|\n|---|---|---|---|---|\n|MNIST|99.00|98.75|98.50|98.25|\n|FacialEmotionRecognition2013|99.5|70.0|67.5|65.0|\n|STL10|98.0|97.5|96.5|96.0|\n|EuroSAT|96.0|95.5| | |\n\n|Dataset|Accuracy|Accuracy|Accuracy|Accuracy|\n|---|---|---|---|---|\n|RESISC45|94|90|75.0|72.5|\n|GTSRB|92|85|80|75|\n|KITTI|90|80|75|70|\n|PatchCamelyon|88|75|70|65|\n\n|Dataset|Accuracy|Accuracy|Accuracy|Accuracy|\n|---|---|---|---|---|\n|UCF101|90|85|80|75|\n|Kinetics700|70|65|60|55|\n|CLEVRCounts|60|55|50|45|\n|Country211|40|35|30|25|\n\n|Dataset|Accuracy|Accuracy|Accuracy|Accuracy|\n|---|---|---|---|---|\n|HatefulMemes|80|75|70|65|\n|SST2|87.5|85.0|82.5|80.0|\n|ImageNet|75.0|72.5|70.0|67.5|\n|GFLOPs/image|CLIP-ViT|CLIP-ResNet|EfficientNet-NoisyStudent|EfficientNet|\n\nFigure 20. Linear probe performance plotted for each of the 27 datasets, using the data from Table 10.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "222e5948-86c8-45a0-a2bd-0a54387ea385": {"__data__": {"id_": "222e5948-86c8-45a0-a2bd-0a54387ea385", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69e278a4-3f40-4360-92a6-5a4003d9e0a7", "node_type": "4", "metadata": {}, "hash": "4335f7826ec1335b2595975f36c557dc26d187b00b79edea8269dc2e21c6ebb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "281a1bc5-7ff7-4927-a82a-8094c6826990", "node_type": "1", "metadata": {}, "hash": "89f3e4cf3986e8c242336f906f0a1ae3ecd89b358d30f0cae50e8f60b91ee7ff", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Food101\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|guacamole|1/101|90.15%|a photo of guacamole, a type of food.|\n|ceviche| | |a photo of ceviche, a type of food.|\n|edamame| | |a photo of edamame, a type of food.|\n|tuna tartare| | |a photo of tuna tartare, a type of food.|\n|hummus| | |a photo of hummus, a type of food.|\n\n# SUN397\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|television studio|1/397|90.22%|a photo of a television studio.|\n|podium indoor| | |a photo of a podium indoor.|\n|conference room| | |a photo of a conference room.|\n|lecture room| | |a photo of a lecture room.|\n|control room| | |a photo of a control room.|\n\n# Youtube-BB\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|airplane, person|1/23|88.98%|a photo of an airplane.|\n|bird| | |a photo of a bird.|\n|giraffe| | |a photo of a giraffe.|\n|car| | |a photo of a car.|\n\n# EuroSAT\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|annual crop land|4/10|12.90%|a centered satellite photo of permanent crop land.|\n|pasture land| | |a centered satellite photo of pasture land.|\n|highway or road| | |a centered satellite photo of highway or road.|\n|annual crop land| | |a centered satellite photo of annual crop land.|\n|brushland or shrubland| | |a centered satellite photo of brushland or shrubland.|\n\n# PatchCamelyon (PCam)\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|healthy lymph node tissue|2/2|22.81%|this is a photo of lymph node tumor tissue|\n|lymph node tumor tissue| | |this is a photo of healthy lymph node tissue|\n\n# ImageNet-A (Adversarial)\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|lynx|5/200|4.18%|a photo of a fox squirrel.|\n|mongoose| | |a photo of a mongoose.|\n|skunk| | |a photo of a skunk.|\n|deer| | |a photo of a deer.|\n|red fox| | |a photo of a red fox.|\n\n# CIFAR-10\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|bird|1/10|40.86%|a photo of a bird.|\n|cat| | |a photo of a cat.|\n|dog| | |a photo of a dog.|\n|wild cat| | |a photo of a wild cat.|\n\n# CLEVR Count\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|4|2/8|17.11%|a photo of 3 objects.|\n|5| | |a photo of 4 objects.|\n|6| | |a photo of 5 objects.|\n|10| | |a photo of 10 objects.|\n\n# Facial Emotion Recognition 2013 (FER2013)\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|angry|5/7|8.16%|a photo of a happy looking face.|\n|neutral| | |a photo of a neutral looking face.|\n|surprised| | |a photo of a surprised looking face.|\n|fearful| | |a photo of a fearful looking face.|\n|angry| | |a photo of an angry looking face.|\n\n# UCF101\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Volleyball Spiking|1/101|99.30%|a photo of a person volleyball spiking.|\n|jump rope| | |a photo of a person jump rope.|\n|long jump| | |a photo of a person long jump.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "281a1bc5-7ff7-4927-a82a-8094c6826990": {"__data__": {"id_": "281a1bc5-7ff7-4927-a82a-8094c6826990", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69e278a4-3f40-4360-92a6-5a4003d9e0a7", "node_type": "4", "metadata": {}, "hash": "4335f7826ec1335b2595975f36c557dc26d187b00b79edea8269dc2e21c6ebb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "222e5948-86c8-45a0-a2bd-0a54387ea385", "node_type": "1", "metadata": {}, "hash": "fe87bfd37d71fcfcfbdf2f29c4ba54030e25a749c4d139947b690daf9e3b385e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b937449-32e0-40aa-a9a6-b20817156dbc", "node_type": "1", "metadata": {}, "hash": "4a1ee98846c4a45e5e46c9d23d40eefe0f31d8d6e4816c8d6f5fae503b91b803", "class_name": "RelatedNodeInfo"}}, "text": "16%|a photo of a happy looking face.|\n|neutral| | |a photo of a neutral looking face.|\n|surprised| | |a photo of a surprised looking face.|\n|fearful| | |a photo of a fearful looking face.|\n|angry| | |a photo of an angry looking face.|\n\n# UCF101\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Volleyball Spiking|1/101|99.30%|a photo of a person volleyball spiking.|\n|jump rope| | |a photo of a person jump rope.|\n|long jump| | |a photo of a person long jump.|\n|soccer penalty| | |a photo of a person soccer penalty.|\n|table tennis shot| | |a photo of a person table tennis shot.|\n\n# Caltech-101\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|kangaroo|1/102|99.81%|a photo of a kangaroo.|\n|gerenuk| | |a photo of a gerenuk.|\n|emu| | |a photo of a emu.|\n|wild cat| | |a photo of a wild cat.|\n\n# ImageNet-R (Rendition)\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Siberian Husky|1/200|76.02%|a photo of a siberian husky.|\n|german shepherd dog| | |a photo of a german shepherd dog.|\n|collie| | |a photo of a collie.|\n|rottweiler| | |a photo of a rottweiler.|\n\n# Oxford-IIIT Pets\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Maine Coon|1/37|99.99%|a photo of a maine coon, a type of pet.|\n|persian| | |a photo of a persian, a type of pet.|\n|ragdoll| | |a photo of a ragdoll, a type of pet.|\n|birman| | |a photo of a birman, a type of pet.|\n|siamese| | |a photo of a siamese, a type of pet.|\n\n# CIFAR-100\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|snake|1/100|38.02%|a photo of a snake.|\n|sweet pepper| | |a photo of a sweet pepper.|\n|flatfish| | |a photo of a flatfish.|\n|turtle| | |a photo of a turtle.|\n|lizard| | |a photo of a lizard.|\n\n# ImageNetV2 Matched Frequency\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|beer bottle|1/1000|88.27%|a photo of a beer bottle.|\n|pirate ship| | |a photo of a pirate ship.|\n|chocolate syrup| | |a photo of a chocolate syrup.|\n|product packet / packaging| | |a photo of a product packet / packaging.|\n|wine bottle| | |a photo of a wine bottle.|\n\n# FGVC Aircraft\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Boeing 717|2/100|9.91%|a photo of a mcdonnell douglas md-90, a type of aircraft.|\n|boeing 717| | |a photo of a boeing 717, a type of aircraft.|\n|fokker 100| | |a photo of a fokker 100, a type of aircraft.|\n|mcdonnell douglas dc-9-30| | |a photo of a mcdonnell douglas dc-9-30, a type of aircraft.|\n|boeing 727-200| | |a photo of a boeing 727-200, a type of aircraft.|\n\n# Country211\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Belize|5/211|3.92%|a photo i took in french guiana.|\n|Gabon| | |a photo i took in gabon.|\n|Cambodia| | |a photo i took in cambodia.|\n|Guyana| | |a photo i took in guyana.|\n|Belize| | |a photo i took in belize.", "mimetype": "text/plain", "start_char_idx": 2674, "end_char_idx": 5683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b937449-32e0-40aa-a9a6-b20817156dbc": {"__data__": {"id_": "0b937449-32e0-40aa-a9a6-b20817156dbc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69e278a4-3f40-4360-92a6-5a4003d9e0a7", "node_type": "4", "metadata": {}, "hash": "4335f7826ec1335b2595975f36c557dc26d187b00b79edea8269dc2e21c6ebb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "281a1bc5-7ff7-4927-a82a-8094c6826990", "node_type": "1", "metadata": {}, "hash": "89f3e4cf3986e8c242336f906f0a1ae3ecd89b358d30f0cae50e8f60b91ee7ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c90e079-7bb1-4622-85e9-c84cc94f8a57", "node_type": "1", "metadata": {}, "hash": "1de90c881aa437d28e5cfd0a7360f4d32ed0b269b78673e9fb7abf20e900ad3c", "class_name": "RelatedNodeInfo"}}, "text": "|\n|fokker 100| | |a photo of a fokker 100, a type of aircraft.|\n|mcdonnell douglas dc-9-30| | |a photo of a mcdonnell douglas dc-9-30, a type of aircraft.|\n|boeing 727-200| | |a photo of a boeing 727-200, a type of aircraft.|\n\n# Country211\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Belize|5/211|3.92%|a photo i took in french guiana.|\n|Gabon| | |a photo i took in gabon.|\n|Cambodia| | |a photo i took in cambodia.|\n|Guyana| | |a photo i took in guyana.|\n|Belize| | |a photo i took in belize.|\n\n# RESISC45\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|roundabout|1/45|96.39%|satellite imagery of roundabout.|\n|intersection| | |satellite imagery of intersection.|\n|church| | |satellite imagery of church.|\n|medium residential| | |satellite imagery of medium residential.|\n|chaparral| | |satellite imagery of chaparral.|\n\n# Stanford Cars\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|2012 Honda Accord Coupe|1/196|63.30%|a photo of a 2012 honda accord coupe.|\n|2012 Honda Accord Sedan| | |a photo of a 2012 honda accord sedan.|\n|2012 Acura TL Sedan| | |a photo of a 2012 acura tl sedan.|\n|2012 Acura TSX Sedan| | |a photo of a 2012 acura tsx sedan.|\n|2008 Acura TL Type-S| | |a photo of a 2008 acura tl type-s.|\n\n# SUN\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|kennel indoor|1/723|98.63%|a photo of a kennel indoor.|\n|kennel outdoor| | |a photo of a kennel outdoor.|\n|jail cell| | |a photo of a jail cell.|\n|jail indoor| | |a photo of a jail indoor.|\n|veterinarians office| | |a photo of a veterinarians office.|\n\n# Kinetics-700\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|country line dancing|1/700|98.98%|a photo of country line dancing.|\n|square dancing| | |a photo of square dancing.|\n|swing dancing| | |a photo of swing dancing.|\n|dancing charleston| | |a photo of dancing charleston.|\n|salsa dancing| | |a photo of salsa dancing.|\n\n# Flowers-102\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|great masterwort|1/102|74.25%|a photo of a great masterwort, a type of flower.|\n|bishop of llandaff| | |a photo of a bishop of llandaff, a type of flower.|\n|pincushion flower| | |a photo of a pincushion flower, a type of flower.|\n|globe flower| | |a photo of a globe flower, a type of flower.|\n|prince of wales feathers| | |a photo of a prince of wales feathers, a type of flower.|\n\n# ImageNet\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|King Charles Spaniel|1/1000|91.61%|a photo of a king charles spaniel.|\n|brittany dog| | |a photo of a brittany dog.|\n|cocker spaniel| | |a photo of a cocker spaniel.|\n|papillon| | |a photo of a papillon.|\n\n# Birdsnap\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Black chinned Hummingbird|4/500|12.00%|a photo of a broad tailed hummingbird, a type of bird.|\n|calliope hummingbird| | |a photo of a calliope hummingbird, a type of bird.", "mimetype": "text/plain", "start_char_idx": 5147, "end_char_idx": 8254, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c90e079-7bb1-4622-85e9-c84cc94f8a57": {"__data__": {"id_": "3c90e079-7bb1-4622-85e9-c84cc94f8a57", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69e278a4-3f40-4360-92a6-5a4003d9e0a7", "node_type": "4", "metadata": {}, "hash": "4335f7826ec1335b2595975f36c557dc26d187b00b79edea8269dc2e21c6ebb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b937449-32e0-40aa-a9a6-b20817156dbc", "node_type": "1", "metadata": {}, "hash": "4a1ee98846c4a45e5e46c9d23d40eefe0f31d8d6e4816c8d6f5fae503b91b803", "class_name": "RelatedNodeInfo"}}, "text": "a type of flower.|\n\n# ImageNet\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|King Charles Spaniel|1/1000|91.61%|a photo of a king charles spaniel.|\n|brittany dog| | |a photo of a brittany dog.|\n|cocker spaniel| | |a photo of a cocker spaniel.|\n|papillon| | |a photo of a papillon.|\n\n# Birdsnap\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|Black chinned Hummingbird|4/500|12.00%|a photo of a broad tailed hummingbird, a type of bird.|\n|calliope hummingbird| | |a photo of a calliope hummingbird, a type of bird.|\n|costas hummingbird| | |a photo of a costas hummingbird, a type of bird.|\n|black chinned hummingbird| | |a photo of a black chinned hummingbird, a type of bird.|\n|annas hummingbird| | |a photo of a annas hummingbird, a type of bird.|\n\n# ImageNet Sketch\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|barn|1/1000|79.56%|a photo of a barn.|\n|church| | |a photo of a church.|\n|threshing machine| | |a photo of a threshing machine.|\n|sawmill| | |a photo of a sawmill.|\n|prison| | |a photo of a prison.|\n\n# Hateful Memes\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|meme|1/2|99.20%|a meme.|\n|hatespeech meme| | |a hatespeech meme.|\n\n# Stanford Sentiment Treebank\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|positive|1/2|78.21%|a positive review of a movie.|\n|negative| | |a negative review of a movie.|\n\n# German Traffic Sign Recognition Benchmark (GTSRB)\n\n|Correct Label|Correct Rank|Correct Probability|Image Description|\n|---|---|---|---|\n|red and white triangle with exclamation mark warning|1/43|45.75%|a zoomed in photo of a \"red and white triangle with exclamation mark warning\" traffic sign.|\n|red and white triangle with black right curve approaching warning| | |a zoomed in photo of a \"red and white triangle with black right curve approaching warning\" traffic sign.|\n|red and white triangle car skidding / slipping warning| | |a zoomed in photo of a \"red and white triangle car skidding / slipping warning\" traffic sign.|\n|red and white triangle rough / bumpy road warning| | |a zoomed in photo of a \"red and white triangle rough / bumpy road warning\" traffic sign.|\n|red and white triangle with black left curve approaching warning| | |a zoomed in photo of a \"red and white triangle with black left curve approaching warning\" traffic sign.|\n\n# Figure 21.\n\nVisualization of predictions from 36 CLIP zero-shot classifiers. All examples are random with the exception of reselecting Hateful Memes to avoid offensive content. The predicted probability of the top 5 classes is shown along with the text used to represent the class. When more than one template is used, the first template is shown. The ground truth label is colored green while an incorrect prediction is colored orange.", "mimetype": "text/plain", "start_char_idx": 7661, "end_char_idx": 10568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca2493cb-040a-4671-a442-2e97db0df167": {"__data__": {"id_": "ca2493cb-040a-4671-a442-2e97db0df167", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c48e9425-f7f7-44af-ace4-d8a338d7ff09", "node_type": "4", "metadata": {}, "hash": "f4aa90d687a246a15655b453b042e3b03408d6941b686aac7a0ab296540752e7", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Table 11. Zero-shot performance of CLIP models over 27 datasets.\n\n|Dataset|CLIP-ResNet|CLIP-ViT|\n|---|---|---|\n|Food101|81.1|75.6|\n|CIFAR10|41.6|32.6|\n|CIFAR100|83.9|81.0|\n|Birdsnap|90.5|82.2|\n|SUN397|84.4|91.3|\n|Stanford Cars|89.2|91.6|\n|FGVC Aircraft|93.8|95.7|\n|VOC2007|92.9|96.2|\n\n# Accuracy Metrics\n\n|Dataset|Mean per Class Accuracy|Top-1 Accuracy|Top-5 Accuracy|\n|---|---|---|---|\n|Food101|85|90|95|\n|CIFAR10|60|70|80|\n|CIFAR100|70|80|90|\n|Birdsnap|85|90|95|\n|SUN397|80|85|90|\n|Stanford Cars|75|80|85|\n|FGVC Aircraft|90|95|100|\n|VOC2007|85|90|95|\n\n# Performance Comparison\n\n|Model|GFLOPs/image|\n|---|---|\n|CLIP-ResNet|70|\n|CLIP-ViT|65|\n\n# Figure 22. CLIP\u2019s zero-shot performance compared to linear-probe ResNet performance", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25d99cd4-75bc-46f0-9acc-037401c44e90": {"__data__": {"id_": "25d99cd4-75bc-46f0-9acc-037401c44e90", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d1bd8e4-9640-4f75-9005-1084c9a0cb54", "node_type": "4", "metadata": {}, "hash": "ab56ffba6ed5806c0d2e4a5de595dd75b7a92120bceae6aa6066bf4f28714657", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "824efe4c-d185-48c9-80a6-750a96533fd8", "node_type": "1", "metadata": {}, "hash": "dc5cc02e04548069848e9f6e92147af60abc0b7d56d9170dc59ce44538b3051e", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# B. Zero-Shot Prediction\n\nTo provide a qualitative summary / overview of CLIP\u2019s zero-shot performance we visualize a randomly selected prediction for 36 different zero-shot CLIP classifiers in Figure 21. In addition, Table 11 and Figure 22 show the individual zero-shot performance scores for each dataset.\n\n# C. Duplicate Detector\n\nOur early attempts at duplicate detection and analysis used nearest neighbors in the model\u2019s learned embedding space. While it is intuitive to use a model\u2019s own notion of similarity, we encountered issues. We found the model\u2019s feature space is weighted very heavily towards semantic similarity. Many false positives occurred due to distinct objects that would be described similarly (soccer balls, flowers of the same species, etc...) having almost perfect similarity. We also observed the model was quite poor at assigning certain kinds of near-duplicates high similarity scores. We noticed repeatedly that images with high-frequency textures (such as fur or stripe patterns) pre-processed by different resizing algorithms (nearest neighbor vs bi-linear) could have surprisingly low similarity. This resulted in many false negatives.\n\nWe built our own near-duplicate detector to fix this issue. We created a synthetic data augmentation pipeline that combined a variety of common image manipulations. The augmentation pipeline combines random cropping and zooming, aspect ratio distortion, downsizing and upscaling to different resolutions, minor rotations, jpeg compression, and HSV color jitter. The pipeline also randomly selects from different interpolation algorithms for all relevant steps. We then trained a model to maximize the similarity of an image and its transformed variant while minimizing similarity to all other images in a training batch. We used the same n-pair / InfoNCE loss as CLIP but with a fixed temperature of 0.07.\n\nWe selected a ResNet-50 as the model architecture. We modified the base ResNet-50 with the anti-alias improvements from (Zhang, 2019) and used weight norm (Salimans & Kingma, 2016) instead of batch norm (Ioffe & Szegedy, 2015) to avoid leaking information about duplicates via batch statistics - a problem previously noted in (Henaff, 2020). We also found the GELU activation function (Hendrycks & Gimpel, 2016) to perform better for this task. We trained the model with a total batch size of 1,712 for approximately 30 million images sampled from our pre-training dataset. At the end of training it achieves nearly 100% accuracy on its proxy training task.\n\n|Dataset|Linear Classifier|Zero Shot|\u2206|YFCC|WIT|\u2206|\n|---|---|---|---|---|---|---|\n|Birdsnap|47.4|35.3|+12.1|19.9|4.5|+15.4|\n|Country211|23.1|17.3|+5.8|5.2|5.3|+0.1|\n|Flowers102|94.4|89.8|+4.6|48.6|21.7|+26.9|\n|GTSRB|66.8|72.5|\u22125.7|6.9|7.0|\u22120.1|\n|UCF101|69.2|74.9|\u22125.7|22.9|32.0|\u22129.1|\n|Stanford Cars|31.4|50.3|\u221218.9|3.8|10.9|\u22127.1|\n|ImageNet|62.0|60.8|+1.2|31.3|27.6|+3.7|\n|Dataset Average|65.5|66.6|\u22121.1|29.6|30.0|\u22120.4|\n|Dataset \u201cWins\u201d|10|15|\u22125|19|18|+1|\n\nTable 12. CLIP performs similarly when trained on only YFCC100M. Comparing a ResNet-50 trained on only YFCC100M with a same sized subset of WIT shows similar average performance and number of wins on zero shot and linear classifier evals. However, large differences in dataset specific performance occur. We include performance on the 3 datasets where YFCC does best and worst compared to WIT according to a linear probe in order to highlight this as well as aggregate performance across all linear and zero-shot evals and the canonical ImageNet dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "824efe4c-d185-48c9-80a6-750a96533fd8": {"__data__": {"id_": "824efe4c-d185-48c9-80a6-750a96533fd8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d1bd8e4-9640-4f75-9005-1084c9a0cb54", "node_type": "4", "metadata": {}, "hash": "ab56ffba6ed5806c0d2e4a5de595dd75b7a92120bceae6aa6066bf4f28714657", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25d99cd4-75bc-46f0-9acc-037401c44e90", "node_type": "1", "metadata": {}, "hash": "793f8240c64851b07a9041f591263c6055ce9cadfcb829abff1e2666b6eaebd5", "class_name": "RelatedNodeInfo"}}, "text": "CLIP performs similarly when trained on only YFCC100M. Comparing a ResNet-50 trained on only YFCC100M with a same sized subset of WIT shows similar average performance and number of wins on zero shot and linear classifier evals. However, large differences in dataset specific performance occur. We include performance on the 3 datasets where YFCC does best and worst compared to WIT according to a linear probe in order to highlight this as well as aggregate performance across all linear and zero-shot evals and the canonical ImageNet dataset.\n\n# D. Dataset Ablation on YFCC100M\n\nTo study whether our custom dataset is critical to the performance of CLIP, we trained a model on a filtered subset of the YFCC100M dataset (details described in Section 2.2) and compared its performance to the same model trained on an equally sized subset of WIT. We train each model for 32 epochs at which point transfer performance begins to plateau due to overfitting. Results are shown in Table 12. Across our whole eval suite, YFCC and WIT perform similarly on average for both zero-shot and linear probe settings. However, performance on specific fine-grained classification datasets can vary widely - sometimes by over 10%. Our speculation is that these differences in performance reflect the relative density of relevant data in each pre-training dataset. For instance, pre-training on YFCC100M, which might contain many photos of birds and flowers (common subjects for photographers), results in better performance on Birdsnap and Flowers102, while pre-training on WIT results in better car and pet classifiers (which appear common in our dataset).\n\nOverall, these results are encouraging as they suggest our approach can use any reasonably filtered collection of paired (text, image) data. This mirrors recent work which reported positive results using the same contrastive pre-training objective on the relatively different domain of medical imaging (Zhang et al., 2020). It also is similar to the findings of noisy student self-training which reported only slight improvements when using their JFT300M dataset over YFCC100M (Xie et al., 2020). We suspect the major advantage of our dataset over the already existing YFCC100M is its much larger size.", "mimetype": "text/plain", "start_char_idx": 3073, "end_char_idx": 5316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d452405-eeb3-41f8-a264-c4bc2bb2c651": {"__data__": {"id_": "5d452405-eeb3-41f8-a264-c4bc2bb2c651", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2e421cd-b4e3-48c8-bec3-c25643c0e7f2", "node_type": "4", "metadata": {}, "hash": "1224d22bade9a46fde3761142dcce83209b58e4269b15b9ac2e075b8ebc53842", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd3bef54-a320-43fe-857f-e9b70fd8274a", "node_type": "1", "metadata": {}, "hash": "735a28dc122cf88a02a51d1d72c749db101667d12c5d50f878882e271557f38d", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# 45\n\nFinally, we caution that WIT includes this filtered subset of YFCC100M. This could result in our ablation underestimating the size of performance differences between YFCC100M and the rest of WIT. We do not think this is likely as YFCC100M is only 3.7% of the overall WIT data blend and it did not noticeably change the performance of models when it was added to the existing data blend during the creation of WIT.\n\n# E. Selected Task and Dataset Results\n\nDue to the large variety of datasets and experiments considered in this work, the main body focuses on summarizing and analyzing overall results. In the following subsections we report details of performance for specific groups of tasks, datasets, and evaluation settings.\n\n# E.1. Image and Text Retrieval\n\nCLIP pre-trains for the task of image-text retrieval on our noisy web-scale dataset. Although the focus of this paper is on representation learning and task learning for the purpose of transfer to a wide variety of downstream datasets, validating that CLIP is able to achieve high transfer performance transfer on exactly what it is pre-trained for is an important sanity check / proof of concept. In Table 13 we check the zero-shot transfer performance of CLIP for both text and image retrieval on the Flickr30k and MSCOCO datasets. Zero-shot CLIP matches or outperforms all prior zero-shot results on these two datasets. Zero-shot CLIP is also competitive with the current overall SOTA for the task of text retrieval on Flickr30k. On image retrieval, CLIP\u2019s performance relative to the overall state of the art is noticeably lower. However, zero-shot CLIP is still competitive with a fine-tuned Unicoder-VL. On the larger MS-COCO dataset fine-tuning improves performance significantly and zero-shot CLIP is not competitive with the most recent work. For both these datasets we prepend the prompt \u201ca photo of\u201d to the description of each image which we found boosts CLIP\u2019s zero-shot R@1 performance between 1 and 2 points.\n\n# E.2. Optical Character Recognition\n\nAlthough visualizations have shown that ImageNet models contain features that respond to the presence of text in an image (Zeiler & Fergus, 2014), these representations are not sufficiently fine-grained to use for the task of optical character recognition (OCR). To compensate, models are augmented with the outputs of custom OCR engines and features to boost performance on tasks where this capability is required (Singh et al., 2019; Yang et al., 2020). Early during the development of CLIP, we noticed that CLIP began to learn primitive OCR capabilities which appeared to steadily improve over the course of the project. To evaluate this qualitatively noticed behavior, we measured performance on 5 datasets requiring the direct and indirect use of OCR. Three of these datasets MNIST (LeCun), SVHN (Netzer et al., 2011), and IIIT5K (Mishra et al., 2012) directly check the ability of a model to perform low-level character and word recognition, while Hateful Memes (Kiela et al., 2020) and SST-2 (Socher et al., 2013) check the ability of a model to use OCR to perform a semantic task. Results are reported in Table 14.\n\nCLIP\u2019s performance is still highly variable and appears to be sensitive to some combination of the domain (rendered or natural images) and the type of text to be recognized (numbers or words). CLIP\u2019s OCR performance is strongest on Hateful Memes and SST-2 - datasets where the text is digitally rendered and consists mostly of words. On IIIT5K, which is natural images of individually cropped words, zero-shot CLIP performs a bit more respectively and its performance is similar to Jaderberg et al. (2014) early work combining deep learning and structured prediction to perform open-vocabulary OCR. However, performance is noticeably lower on two datasets involving recognition of handwritten and street view numbers. CLIP\u2019s 51% accuracy on full number SVHN is well below any published results. Inspection suggests CLIP struggles with repeated characters as well as the low resolution and blurry images of SVHN. CLIP\u2019s zero-shot MNIST performance is also poor and is outperformed by supervised logistic regression on raw pixels, one of the simplest possible machine learning baselines.\n\nSST-2 is a sentence level NLP dataset which we render into images. We include SST-2 in order to check whether CLIP is able to convert low level OCR capability into a higher level representation. Fitting a linear classifier on CLIP\u2019s representation of rendered sentences achieves 80.5% accuracy.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd3bef54-a320-43fe-857f-e9b70fd8274a": {"__data__": {"id_": "bd3bef54-a320-43fe-857f-e9b70fd8274a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2e421cd-b4e3-48c8-bec3-c25643c0e7f2", "node_type": "4", "metadata": {}, "hash": "1224d22bade9a46fde3761142dcce83209b58e4269b15b9ac2e075b8ebc53842", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d452405-eeb3-41f8-a264-c4bc2bb2c651", "node_type": "1", "metadata": {}, "hash": "e72059b7aea8e874356c035fb890633ad018654b642010795ef6dab15dd5a8e8", "class_name": "RelatedNodeInfo"}}, "text": "(2014) early work combining deep learning and structured prediction to perform open-vocabulary OCR. However, performance is noticeably lower on two datasets involving recognition of handwritten and street view numbers. CLIP\u2019s 51% accuracy on full number SVHN is well below any published results. Inspection suggests CLIP struggles with repeated characters as well as the low resolution and blurry images of SVHN. CLIP\u2019s zero-shot MNIST performance is also poor and is outperformed by supervised logistic regression on raw pixels, one of the simplest possible machine learning baselines.\n\nSST-2 is a sentence level NLP dataset which we render into images. We include SST-2 in order to check whether CLIP is able to convert low level OCR capability into a higher level representation. Fitting a linear classifier on CLIP\u2019s representation of rendered sentences achieves 80.5% accuracy. This is on par with the 80% accuracy of a continuous bag of words baseline using GloVe word vectors pre-trained on 840 billion tokens (Pennington et al., 2014). While this is a simple NLP baseline by today\u2019s standard, and well below the 97.5% of the current SOTA, it is encouraging to see that CLIP is able to turn an image of rendered text into a non-trivial sentence level representation. Fully supervised CLIP is also surprisingly strong on Hateful Meme detection, where CLIP is only 0.7 points behind the current single model SOTA and several points above the best baseline from the original paper. Similar to SST-2, these other results on Hateful Memes use the ground truth text which CLIP does not have access to. Finally, we note that zero-shot CLIP outperforms the best results using fully supervised linear probes across all other 56 models included in our evaluation suite. This suggests CLIP\u2019s OCR capability is at least somewhat unique compared to existing work on self-supervised and supervised representation learning.", "mimetype": "text/plain", "start_char_idx": 3726, "end_char_idx": 5641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1045f041-ceaf-4ea2-8df8-cb1e6e1dcaf7": {"__data__": {"id_": "1045f041-ceaf-4ea2-8df8-cb1e6e1dcaf7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45e2e642-e9b3-4f35-854b-399234699578", "node_type": "4", "metadata": {}, "hash": "93e217469bb9e4ee19974b009bb1b09a95a218e6fcbf2a59e8e22447bee2360c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a2ac2ef-70a2-4456-a2a5-ef3f94b111be", "node_type": "1", "metadata": {}, "hash": "f9713113a3bbed31273bf0fbc3afc7ce52578705e16d93e1050ea0c2d91213af", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Finetune Linear ZS Finetune Zero-Shot\n\n# Text Retrieval\n\n# Image Retrieval\n\n| |Flickr30k|Flickr30k|Flickr30k|Flickr30k|Flickr30k|Flickr30k|MSCOCO|MSCOCO|MSCOCO|MSCOCO|MSCOCO|MSCOCO|\n|---|---|---|\n|Model|R@1|R@5|R@10|R@1|R@5|R@10|R@1|R@5|R@10|R@1|R@5|R@10|\n|Unicoder-VLa|86.2|96.3|99.0|62.3|87.1|92.8|71.5|90.9|94.9|46.7|76.0|85.3|\n|Uniterbc|87.3|98.0|99.2|65.7|88.6|93.8|75.6|94.1|96.8|52.9|79.9|88.0|\n|VILLA|87.9|97.5|98.8|-|-|-|76.3|94.2|96.8|-|-|-|\n|ERNIE-ViLe|-|-|-|73.5|92.2|96.0|-|-|-|57.5|82.8|89.8|\n|Visual N-Gramsf|15.4|35.7|45.1|8.7|23.1|33.3|8.8|21.2|29.9|5.0|14.5|21.9|\n|ImageBERTga|-|-|-|44.0|71.2|80.4|-|-|-|32.3|59.0|70.2|\n|Unicoder-VL|64.3|86.8|92.3|-|-|-|48.4|76.0|85.2|-|-|-|\n|Uniterb|83.6|95.7|97.7|-|-|-|68.7|89.2|93.9|-|-|-|\n|CLIP|88.0|98.7|99.4|58.4|81.5|88.1|68.7|90.6|95.2|37.8|62.4|72.2|\n\nTable 13. CLIP improves zero-shot retrieval and is competitive with the best fine-tuned result on Flickr30k text retrieval. Bold indicates best overall performance while an underline indicates best in category performance (zero-shot or fine-tuned). For all other models, best results from the paper are reported regardless of model size / variant. MSCOCO performance is reported on the 5k test set.\n\na(Li et al., 2020a) b(Chen et al., 2019) c(Gan et al., 2020) d(Li et al., 2020b) e(Yu et al., 2020) f (Li et al., 2017) g (Qi et al., 2020)\n\n# IIIT5K Hateful\n\n# UCF101\n\n# K700\n\n# RareAct Finetune Linear ZS MNIST SVHN1k Memes SST-2 Top-1 AVG mWAP mWSAP\n\n| |SOTAf|JOINTg|CBoW|Raw Pixels|ES Best|CLIP|\n|---|---|---|---|---|---|---|\n| |99.8a|-|-|92.5h|98.9|99.2|\n|R(2+1)D-BERTa|96.4b|-|-|-|-|-|\n|NS ENet-L2bd|78.0d|-|-|-|58.6h|77.3|\n|HT100M S3D|91.3|-|-|-|59.0i|80.5|\n|MMV FACfc|91.8c|68.2c-|-|-|-|-|\n|NS ENet-L2|89.4|-|-|-|-|-|\n|CLIP|92.0|73.0|-|-|-|-|\n|CLIP|88.4|51.0|90.0|63.3|67.9|80.3|\n|CLIP|40.7|44.8|-|-|-|-|\n\nTable 14. OCR performance on 5 datasets. All metrics are accuracy on the test set except for Hateful Memes which reports ROC AUC on the dev set. Single model SOTA reported to best of knowledge.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a2ac2ef-70a2-4456-a2a5-ef3f94b111be": {"__data__": {"id_": "1a2ac2ef-70a2-4456-a2a5-ef3f94b111be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45e2e642-e9b3-4f35-854b-399234699578", "node_type": "4", "metadata": {}, "hash": "93e217469bb9e4ee19974b009bb1b09a95a218e6fcbf2a59e8e22447bee2360c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1045f041-ceaf-4ea2-8df8-cb1e6e1dcaf7", "node_type": "1", "metadata": {}, "hash": "48af8248b72b0de4da2990019cdfb2596b1841c26d21e7d5b4efe2a4207bd1cb", "class_name": "RelatedNodeInfo"}}, "text": "OCR performance on 5 datasets. All metrics are accuracy on the test set except for Hateful Memes which reports ROC AUC on the dev set. Single model SOTA reported to best of knowledge.\n\nES Best reports the best performance across the 56 non-CLIP models in our evaluation suite.\n\na(Assiri, 2020) f (Jaderberg et al., 2015) b(Jaderberg et al., 2015) c(Wang et al., 2020) d(Lippe et al., 2020) i(Mahajan et al., 2014) g (Wang et al., 2018) h(Xie et al., 2020)\n\n# E.3. Action Recognition in Videos\n\nFor the purpose of learning, a potentially important aspect of natural language is its ability to express, and therefore supervise, an extremely wide set of concepts. A CLIP model, since it is trained to pair semi-arbitrary text with images, is likely to receive supervision for a wide range of visual concepts involving both common and proper nouns, verbs, and adjectives. ImageNet-1K, by contrast, only labels common nouns. Does the lack of broader supervision in ImageNet result in weaker transfer of ImageNet models to tasks involving the recognition of visual concepts that are not nouns? To investigate this, we measure and compare the performance of CLIP and ImageNet models on several video action classification datasets which measure the ability of a model to recognize verbs. In Table 15 we report results on UCF-101 (Soomro et al., 2012) and Kinetics-700 (Carreira et al., 2019), two common datasets for the task. Unfortunately, our CPU based linear classifier takes a prohibitively long time to evaluate on a video dataset due to the very large number of training frames. To deal with this, we aggressively sub-sample each video to only a single center frame, effectively turning it into an image classification dataset. As a result, our reported performance in a linear evaluation setting likely underestimates performance by a moderate amount.", "mimetype": "text/plain", "start_char_idx": 1912, "end_char_idx": 3764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6f19c5f-87e6-4514-90d2-2c5d3f41da7e": {"__data__": {"id_": "f6f19c5f-87e6-4514-90d2-2c5d3f41da7e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d508908-1077-4695-8eb8-1fc17ec5111a", "node_type": "4", "metadata": {}, "hash": "c543747271af17967bc9a40186fd562ee716de7801ab4bf7fbd0df9fd38ca06b", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# Table 16. Detailed ImageNet robustness performance.\n\n| |IN|IN-V2|IN-A|IN-R|ObjectNet|IN-Sketch|IN-Vid| |YTBB| |\n|---|---|---|---|---|---|---|---|---|---|---|\n|NS EfficientNet-L2a|88.3|80.2|84.9|74.7|68.5|47.6|88.0|82.1|67.7|63.5|\n|FixResNeXt101-32x48d V2b|86.4|78.0|68.4|80.0|57.8|59.1|85.8|72.2|68.9|57.7|\n|Linear Probe CLIP|85.4|75.9|75.3|84.2|66.2|57.4|89.1|77.2|68.7|63.1|\n|Zero-Shot CLIP|76.2|70.1|77.2|88.9|72.3|60.2|95.3|89.2|95.2|88.5|\n\nDespite this handicap, CLIP features transfer surprisingly well to this task. CLIP matches the best prior result on UCF-101 in a linear probe evaluation setting and also outperforms all other models in our evaluation suite. On Kinetics-700, CLIP also outperforms the fine-tuned I3D baseline from the original paper. Since it does not require a training stage, we report CLIP\u2019s zero-shot performance when averaging predictions across all frames. CLIP also performs well in this setting and on Kinetics-700 its performance is within 1% of the fully supervised I3D baseline which is trained on 545000 labeled videos. Encouraged by these results, we also measure CLIP\u2019s performance on the recently introduced RareAct dataset (Miech et al., 2020a) which was designed to measure zero-shot recognition of unusual actions like \u201chammering a phone\u201d and \u201cdrilling an egg\u201d. CLIP improves over the prior state of the art, a S3D model trained on automatically extracted captions from 100 million instructional videos, by 10 points.\n\n# E.4. Geolocalization\n\nAnother behavior we noticed during the development of CLIP was its ability to recognize many places and locations. To quantify this we created the Country211 dataset as described in Appendix A and report results on it throughout the paper. However it is a new benchmark so to compare with prior work on geolocalization we also report results on the IM2GPS test set from Hays & Efros (2008) in Table 17. Since IM2GPS is a regression benchmark, we guess the GPS coordinates of the nearest image in a set of reference images using CLIP\u2019s embedding space. This is not a zero-shot result since it uses nearest-neighbor regression. Despite querying only 1 million images, which is much less than prior work, CLIP performs similarly to several task specific models. It is not, however, competitive with the current state of the art.\n\n# E.5. Robustness to Distribution Shift\n\nSection 3.3 provides a high level summary and analysis of ImageNet-related robustness results. We briefly provide some additional numerical details in this appendix. Performance results per dataset are provided in Table 16 and compared with the current state of the art results reported in Taori et al. (2020)\u2019s evaluation suite. Zero-shot CLIP improves the state of the art on 5 of the 7 datasets, ImageNet-R, ObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-BB. CLIP\u2019s improvements are largest on ImageNet-Vid and Youtube-BB due to its flexible zero-shot capability and on ImageNet-R, which likely reflects CLIP\u2019s pre-training distribution including significant amounts of creative content. A similar behavior has been documented for the Instagram pre-trained ResNeXt models as discussed in Taori et al. (2020).\n\n# Table 17. Geolocalization performance on the IM2GPS test set.\n\nMetric is percent of images localized within a given radius. Models are ordered by average performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7940baf4-e86f-407e-aa36-785357169a41": {"__data__": {"id_": "7940baf4-e86f-407e-aa36-785357169a41", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa18b795-06f5-43ca-90f2-7d3038fc9dca", "node_type": "4", "metadata": {}, "hash": "4ded2e632fd2658394733b0643061164e16e8e8174f0c83dd736f93fb5da5c1c", "class_name": "RelatedNodeInfo"}}, "text": "# Learning Transferable Visual Models From Natural Language Supervision\n\n# F. Model Hyperparameters\n\n|Hyperparameter|Value|\n|---|---|\n|Batch size|32768|\n|Vocabulary size|49408|\n|Training epochs|32|\n|Maximum temperature|100.0|\n|Weight decay|0.2|\n|Warm-up iterations|2000|\n|Adam \u03b21|0.9|\n|Adam \u03b22|0.999 (ResNet), 0.98 (ViT)|\n|Adam|10\u22128 (ResNet), 10\u22126 (ViT)|\n\n# Table 18. Common CLIP hyperparameters\n\n|Model|Learning rate|Embedding dimension|Input resolution|ResNet blocks|ResNet width|Text Transformer layers|Text Transformer width|Text Transformer heads|\n|---|---|---|---|---|---|---|---|---|\n|RN50|5 \u00d7 10\u22124|1024|224|(3, 4, 6, 3)|2048|12|512|8|\n|RN101|5 \u00d7 10\u22124|512|224|(3, 4, 23, 3)|2048|12|512|8|\n|RN50x4|5 \u00d7 10\u22124|640|288|(4, 6, 10, 6)|2560|12|640|10|\n|RN50x16|34 \u00d7 10\u22124|768|384|(6, 8, 18, 8)|3072|12|768|12|\n|RN50x64|.6 \u00d7 10|1024|448|(3, 15, 36, 10)|4096|12|1024|16|\n\n# Table 19. CLIP-ResNet hyperparameters\n\n|Model|Learning rate|Embedding dimension|Input resolution|Vision Transformer layers|Vision Transformer width|Vision Transformer heads|Text Transformer layers|Text Transformer width|Text Transformer heads|\n|---|---|---|---|---|---|---|---|---|---|\n|ViT-B/32|5 \u00d7 10\u22124|512|224|12|768|12|12|512|8|\n|ViT-B/16|5 \u00d7 10\u22124|512|224|12|768|12|12|512|8|\n|ViT-L/14|4 \u00d7 10\u22125|768|224|24|1024|16|12|768|12|\n|ViT-L/14-336px|2 \u00d7 10|768|336|24|1024|16|12|768|12|\n\n# Table 20. CLIP-ViT hyperparameters", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f149a63-6228-4fdb-b6f1-191c4f2e9f70": {"__data__": {"id_": "8f149a63-6228-4fdb-b6f1-191c4f2e9f70", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdacf8a0-0029-4d74-81e1-b539424177ad", "node_type": "4", "metadata": {}, "hash": "7b10edc052aa86afd503a71b451c09aa16a18a809ed151a6d7fc7476ae240ee1", "class_name": "RelatedNodeInfo"}}, "text": "# Nouvelle th\u00e9orie des taches du Soleil\n\n# Esprit Pezenas (1692-1776), s.j.\n\nArchives d\u00e9partementales de l\u2019H\u00e9rault, Ms. D.128, s.d. (c. 1766-70), fol. 261-267.\n\n\u00c9dition comment\u00e9e et annot\u00e9e par Guy Boistel1\n\nCAHIERS FRANC\u00e7illaOIS VIEgraveTE, S\u00e9rie I, n\u00b08, 2004 (publ. 2011)\n\nS\u2019adresser au CFV : http://www.sciences.univ-nantes.fr/cfv/\n\n# COMMENTAIRE\n\n# Pr\u00e9ambule\n\nLe manuscrit \u00ab Nouvelle th\u00e9orie des taches du Soleil \u00bb a \u00e9t\u00e9 identifi\u00e9 et attribu\u00e9 \u00e0 l\u2019astronome et hydrographe j\u00e9suite marseillais Esprit Pezenas (1692-1776) au cours de l\u2019ann\u00e9e 2002, lors de la recherche syst\u00e9matique des papiers le concernant dans les diff\u00e9rents fonds fran\u00e7ais.\n\nPour des raisons techniques, l\u2019\u00e9dition de ce manuscrit n\u2019est imprim\u00e9e qu\u2019en 2011 alors qu\u2019elle para\u00eet dans un num\u00e9ro des Cahiers Fran\u00e7ois Vi\u00e8te mill\u00e9sim\u00e9 2004. Cette \u00e9dition fait suite \u00e0 la publication en 2003 de l\u2019inventaire des manuscrits et des \u0153uvres imprim\u00e9es du p\u00e8re Pezenas dans la Revue d\u2019histoire des sciences2.\n\n# Les d\u00e9buts de la physique solaire au XVIIe si\u00e8cle et le probl\u00e8me scientifique de la d\u00e9termination de la rotation du Soleil par l\u2019observation de ses taches\n\nC\u2019est entre les ann\u00e9es 1610 et 1620 que, gr\u00e2ce aux d\u00e9veloppements successifs de la lunette astronomique, plusieurs astronomes ont contribu\u00e9 \u00e0 la mise en \u00e9vidence de la rotation du Soleil sur lui-m\u00eame \u00e0 l\u2019aide des observations de ses taches. L\u2019Anglais Thomas Harriot (1560-1621) a observ\u00e9 les premi\u00e8res taches solaires en d\u00e9cembre 1610. Johannes Fabricius (1587-1616) en Hollande, fut le premier \u00e0 comprendre que le mouvement des taches solaires d'un jour \u00e0 l'autre \u00e9tait d\u00fb \u00e0 la rotation du Soleil sur lui-m\u00eame. Enfin, il revient au p\u00e8re j\u00e9suite Christoph Scheiner (1575-1650) et \u00e0 Galil\u00e9e (1564-1642) d\u2019avoir observ\u00e9 et \u00e9tudi\u00e9 syst\u00e9matiquement l\u2019apparition et le mouvement des taches.\n\nAvec une petite lunette astronomique, il est ais\u00e9 d\u2019observer le d\u00e9placement et la rotation apparente des taches solaires (qui se traduit par un d\u00e9calage d\u2019environ 13\u00b0 par jour vers l\u2019Ouest en coordonn\u00e9es h\u00e9liographiques). En supposant que ces taches appartiennent \u00e0 la surface solaire \u2013 ce qui constitue alors un v\u00e9ritable d\u00e9bat en soi3 \u2013 et compte tenu des impr\u00e9cisions des observations.\n\n# Notes\n\n1. Docteur habilit\u00e9 \u00e0 diriger des recherches en histoire des sciences et des techniques, professeur certifi\u00e9 de sciences physiques (LGT E. Livet, Nantes), Centre Fran\u00e7ois Vi\u00e8te, Universit\u00e9 de Nantes.\n2. Guy Boistel, 2003, \u00ab Inventaire chronologique des \u0153uvres imprim\u00e9es et manuscrites du p\u00e8re Esprit Pezenas (1692-1776), j\u00e9suite, astronome et hydrographe marseillais \u00bb, Revue d\u2019histoire des sciences, vol. 56/1, 221-245.\n3. Dans son essai intitul\u00e9 Il Saggiatore (1619), Galil\u00e9e revendiqua la paternit\u00e9 de la d\u00e9couverte des taches solaires sur le P. Scheiner ; s\u2019ensuivirent des discussions se d\u00e9pla\u00e7ant assez vite sur les terrains philosophique et th\u00e9ologique. Sur Harriot, Galil\u00e9e et Scheiner, lire la passionnante saga de Walter M. Mitchell, 1916, \u00ab The history of the discovery of the solar spots \u00bb, Popular Astronomy, 1916, 22-30 ; 82-96 ; 149-162 ; 206-218 ; 290-303 : 341-354 ; 428-441 ; 488-499 ; 562-570 (disponibles sur l\u2019abstract service de la Nasa ADS).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc7959ba-8435-41f2-87d8-fe95879d9025": {"__data__": {"id_": "fc7959ba-8435-41f2-87d8-fe95879d9025", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da8e26c2-cfac-4aff-a81e-18a8275c1324", "node_type": "4", "metadata": {}, "hash": "741f0163070311ca6ee218732b336b2a60ab0f039a3a550d34cb9afe83c1fc54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cb937e8-489f-498a-9b14-261eea11cd5c", "node_type": "1", "metadata": {}, "hash": "8df8e7aa45d99a7e5f58092fe116d5ffecdc3c50c2a3367c32f544141a905d92", "class_name": "RelatedNodeInfo"}}, "text": "# Les taches solaires et leur \u00e9tude\n\nEt d\u2019une th\u00e9orie du mouvement du Soleil encore incompl\u00e8te, les astronomes du d\u00e9but du XVIIe si\u00e8cle ont trouv\u00e9 une p\u00e9riode de rotation du Soleil sur lui-m\u00eame comprise entre 26 et 29 jours et demi (soit un mois lunaire). Il faut attendre le d\u00e9but du XIXe si\u00e8cle pour que l\u2019astronome allemand Heinrich Schwabe (1789-1875) mette en \u00e9vidence un cycle d\u2019activit\u00e9 solaire de onze ans, caract\u00e9ris\u00e9 par une variation cyclique du nombre de taches solaires pr\u00e9sentes sur la surface du Soleil.\n\nLe probl\u00e8me scientifique pos\u00e9 par le mouvement des taches solaires n\u2019est pas si simple. Les taches apparaissent vers les latitudes \u00e9lev\u00e9es du disque solaire (35 \u00e0 45\u00b0), de mani\u00e8re sym\u00e9trique dans les deux h\u00e9misph\u00e8res, et glissent progressivement vers l\u2019\u00e9quateur solaire. Mais le Soleil n\u2019est pas une sph\u00e8re solide. Il pr\u00e9sente une rotation diff\u00e9rentielle : des couches de latitudes diff\u00e9rentes ne tournent pas \u00e0 la m\u00eame vitesse, les taches tournant plus vite \u00e0 l\u2019\u00e9quateur. Sur un diagramme montrant l\u2019\u00e9volution de la latitude des taches en fonction du temps, on voit appara\u00eetre des formes, les fameuses \u00ab ailes de papillon \u00bb. Par ailleurs, les premi\u00e8res d\u00e9terminations de la dur\u00e9e de rotation du Soleil ne tenaient pas compte de l\u2019inclinaison de l\u2019\u00e9quateur du Soleil sur l\u2019\u00e9cliptique. C\u2019est au p\u00e8re Scheiner que l\u2019on doit la d\u00e9couverte de cette inclinaison (\u00e9gale \u00e0 environ 7\u00b0). En 1764, Lalande montre que les mesures ne sont toujours pas suffisamment pr\u00e9cises et appelle de tous ses v\u0153ux de meilleures d\u00e9terminations de cette inclinaison par les astronomes.\n\nLe mouvement progressif des taches vers l\u2019\u00e9quateur solaire est relativement lent et, pendant l\u2019observation du d\u00e9placement d\u2019une tache d\u2019un bord \u00e0 l\u2019autre du Soleil, la Terre s\u2019est aussi d\u00e9plac\u00e9e autour de ce dernier. L\u2019observateur n\u2019observe donc pas les taches sous le m\u00eame angle vu de la Terre entre deux observations rapproch\u00e9es de quelques jours. Ainsi, entre le d\u00e9but des observations des taches solaires dans les ann\u00e9es 1610 et le milieu du XVIII si\u00e8cle, quelques astronomes ont propos\u00e9 des m\u00e9thodes, souvent graphiques (le P. Scheiner, Johannes Hevelius (1611-1687), Jean-Dominique Cassini (1625-1712), Joseph-Nicolas Delisle (1688-1768), Jacques Cassini (1677-1756), le P. Esprit Pezenas (1692-1776), J\u00e9r\u00f4me Lalande (1732-1807) notamment) puis des m\u00e9thodes plus analytiques (Guillaume de Saint-Jacques de Silvabelle (1722-1801), Lalande ou Pierre-Achille Dionis Dus\u00e9jour (1734-1794) en particulier) afin de r\u00e9soudre ces diff\u00e9rentes questions en tenant compte ou non, \u00e0 des degr\u00e9s divers, du d\u00e9placement de la Terre autour du Soleil pendant les observations. J\u00e9r\u00f4me Lalande donne un tr\u00e8s bon aper\u00e7u historique de ces diff\u00e9rentes tentatives dans le tome 2 de la premi\u00e8re \u00e9dition de son Astronomie (Paris, 1764).\n\nNotons que le traitement des observations des taches solaires par les diff\u00e9rents astronomes conduit indirectement au calcul de la p\u00e9riode de rotation ; il consiste tout d\u2019abord, et surtout, \u00e0 d\u00e9terminer l\u2019inclinaison de l\u2019\u00e9quateur solaire sur l\u2019\u00e9cliptique, dont le compl\u00e9mentaire est l\u2019inclinaison de l\u2019axe de rotation du Soleil sur ce m\u00eame \u00e9cliptique. Enfin, la p\u00e9riode de rotation est souvent accessoirement d\u00e9duite des divers calculs et pourrait.\n\n# R\u00e9f\u00e9rences\n\nBernard Dame, 1966, \u00ab Galil\u00e9e et les taches solaires (1610-1613) \u00bb, Revue d\u2019histoire des sciences et de leurs applications, 19/4, 307-370 ; William R. Shea, 1970, \u00ab Galileo, Scheiner, and the interpretation of Sunspots \u00bb, Isis, 61, 498-519.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cb937e8-489f-498a-9b14-261eea11cd5c": {"__data__": {"id_": "3cb937e8-489f-498a-9b14-261eea11cd5c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da8e26c2-cfac-4aff-a81e-18a8275c1324", "node_type": "4", "metadata": {}, "hash": "741f0163070311ca6ee218732b336b2a60ab0f039a3a550d34cb9afe83c1fc54", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc7959ba-8435-41f2-87d8-fe95879d9025", "node_type": "1", "metadata": {}, "hash": "e3ddb11abcf69c2b08d164051e5da81dcb0f532efcbd40ad95b2efe73b2a3019", "class_name": "RelatedNodeInfo"}}, "text": "Enfin, la p\u00e9riode de rotation est souvent accessoirement d\u00e9duite des divers calculs et pourrait.\n\n# R\u00e9f\u00e9rences\n\nBernard Dame, 1966, \u00ab Galil\u00e9e et les taches solaires (1610-1613) \u00bb, Revue d\u2019histoire des sciences et de leurs applications, 19/4, 307-370 ; William R. Shea, 1970, \u00ab Galileo, Scheiner, and the interpretation of Sunspots \u00bb, Isis, 61, 498-519.\n\nLa m\u00e9canique c\u00e9leste pr\u00e9-newtonienne est encore suffisamment impr\u00e9cise pour apporter de grandes incertitudes dans ce genre de calculs.\n\nJudit Brody, 2002, The enigma of sunspots. A story of discovery and scientific revolution, Edinburgh, Floris Books, compl\u00e9ment r\u00e9cent des r\u00e9f\u00e9rences donn\u00e9es en note 3. Sur Schwabe : Franz Flury, 1927, \u00ab L\u2019astronome amateur Schwabe \u00bb, Bulletin de l\u2019Observatoire de Lyon, 1927, 86-91.\n\nChristopher Scheiner, 1630, Rosa Ursina, num\u00e9ris\u00e9 et mis en ligne sur le site du Mus\u00e9e des sciences de Florence : http://fermi.imss.fi.it/rd/bdv?/bdviewer/bid=367767. Andr\u00e9 Danjon, 1994, Astronomie g\u00e9n\u00e9rale. Astronomie sph\u00e9rique et \u00e9l\u00e9ments de m\u00e9canique c\u00e9leste, Paris, A. Blanchard, 349-351 : l\u2019inclinaison de l\u2019axe solaire est environ de 7\u00b0,25. La rotation sid\u00e9rale (conventionnelle) du Soleil est de 25,38 jours ; la rotation synodique correspondante est de 27,275 jours.\n\nJ\u00e9r\u00f4me Lalande, 1764, Astronomie, Paris, tome II, p. 1220.\n\nJ. Lalande, 1764, op. cit., tome II, art. 2502 et suiv., 1204-1222.", "mimetype": "text/plain", "start_char_idx": 3152, "end_char_idx": 4529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea948b2f-6610-4b0a-9a51-e9c9bae2d7f3": {"__data__": {"id_": "ea948b2f-6610-4b0a-9a51-e9c9bae2d7f3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6195b65b-3a36-4576-8446-afd7f548778e", "node_type": "4", "metadata": {}, "hash": "4796a3de98e7c58900e16ae2c6c7f7814c4f1a28bbac1783532bb51c1670b6d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43398376-a4e5-4ca6-9391-1cf5529b8515", "node_type": "1", "metadata": {}, "hash": "daa33e7df5401fcd09e483271c0cde06f678b00833ac78b9330e5fd81651366d", "class_name": "RelatedNodeInfo"}}, "text": "# Observations Astronomiques et Techniques\n\npresque passer pour anecdotique. Tous les astronomes s\u2019entendent sur une dur\u00e9e d\u2019environ 27 jours \u00e0 quelques heures pr\u00e8s, en plus ou en moins. L\u2019esprit g\u00e9om\u00e9trique des savants des Lumi\u00e8res est plus souvent pr\u00e9occup\u00e9 par l\u2019aspect th\u00e9orique d\u2019une m\u00e9thode, son style math\u00e9matique, que par ses applications num\u00e9riques. Seuls les v\u00e9ritables astronomes, observateurs au fait des nouveaut\u00e9s en mati\u00e8re de math\u00e9matiques, s\u2019appliquent \u00e0 fournir des donn\u00e9es num\u00e9riques et les r\u00e9sultats de leurs m\u00e9thodes.\n\nEn outre, avec le d\u00e9veloppement de l\u2019optique instrumentale au XVIIIe si\u00e8cle qui voit la construction de miroirs de bronze poli et de lunettes aux verres d\u2019une qualit\u00e9 optique de plus en plus grande, les astronomes disposent d\u2019instruments leur permettant de discerner les taches dans l\u2019atmosph\u00e8re de la plan\u00e8te Jupiter. Ainsi, les m\u00e9thodes d\u00e9velopp\u00e9es pour le traitement des taches solaires servent-elles aussi \u00e0 la d\u00e9termination de la p\u00e9riode de rotation de Jupiter et de toute autre plan\u00e8te pr\u00e9sentant des taches. Lalande montre bien comment ces m\u00e9thodes peuvent aussi servir au traitement de la libration de la Lune par l\u2019observation des \u00ab taches \u00bb lunaires, telles que ses crat\u00e8res et/ou ses mers.\n\nEnfin, l\u2019observation et le dessin des taches b\u00e9n\u00e9ficient aussi de l\u2019am\u00e9lioration des techniques d\u2019observation \u00e0 la fin du XVIIe si\u00e8cle avec le perfectionnement du microm\u00e8tre par l\u2019astronome Adrien Auzout (1622-1691) \u00e0 l\u2019Observatoire royal de Paris, et son emploi syst\u00e9matique par l\u2019abb\u00e9 Jean Picard (1620-1682). Celui-ci incorpore \u00e0 l\u2019oculaire de la lunette astronomique un microm\u00e8tre \u00e0 fil mobile de Auzout. Picard et son \u00e9l\u00e8ve Philippe de la Hire (1667-1719) observent syst\u00e9matiquement au quart de cercle le diam\u00e8tre angulaire apparent du Soleil, atteignant une pr\u00e9cision proche de la seconde d\u2019arc. La Hire et ses successeurs emploieront les d\u00e9terminations des dur\u00e9es de passages des bords du Soleil et de ses taches au m\u00e9ridien, gr\u00e2ce aux gains sensibles des horloges astronomiques, qui tiennent d\u00e9sormais la seconde.\n\nToutes ces observations et ces techniques nouvelles contribuent, encore maintenant, \u00e0 une meilleure connaissance des \u00e9l\u00e9ments de base de la physique solaire et de sa variabilit\u00e9. Il a \u00e9t\u00e9 ainsi possible de mettre en \u00e9vidence une anomalie dans l\u2019\u00e9volution du nombre de taches entre 1645 et 1705, appel\u00e9e \u00ab minimum de Maunder \u00bb, se traduisant notamment par une dissym\u00e9trie marqu\u00e9e et r\u00e9elle de r\u00e9partition des taches sur les deux h\u00e9misph\u00e8res solaires. Des \u00e9tudes plus fines sur les \u00e9poques encadrant ce minimum de Maunder pour lesquelles nous disposons d\u2019observations de taches solaires, r\u00e9v\u00e8lent deux ph\u00e9nom\u00e8nes. Premi\u00e8rement, ce minimum semble s\u2019accompagner d\u2019une baisse de l\u2019activit\u00e9 solaire et de la temp\u00e9rature moyenne terrestre et para\u00eet corroborer l\u2019existence d\u2019un \u00ab petit \u00e2ge glaciaire \u00bb entre 1550 et 1850 d\u00e9duite de la variation d\u2019abondance de l\u2019isotope 14 du carbone. En second lieu, les observations du diam\u00e8tre angulaire du Soleil semblent montrer une variation de la rotation angulaire du Soleil, celle-ci s\u2019acc\u00e9l\u00e9rant sensiblement lors du minimum de Maunder. Ces deux observations sont \u00e9galement contest\u00e9es dans le cadre des d\u00e9bats sur la climatologie.\n\n# Notes et R\u00e9f\u00e9rences\n\n1. J. Lalande, 1764, op. cit., 1222 et suiv.\n2. Guy Picolet (dir.), 1987, Jean Picard et les d\u00e9buts de l\u2019astronomie de pr\u00e9cision au XVIIe si\u00e8cle, Paris, CNRS.\n3. En hommage \u00e0 l\u2019astronome anglais Edward Maunder (1851-1928) qui a \u00e9tendu les \u00e9tudes historiques entreprises par l\u2019allemand Gustav Sp\u00f6rer (1822-1895).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43398376-a4e5-4ca6-9391-1cf5529b8515": {"__data__": {"id_": "43398376-a4e5-4ca6-9391-1cf5529b8515", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6195b65b-3a36-4576-8446-afd7f548778e", "node_type": "4", "metadata": {}, "hash": "4796a3de98e7c58900e16ae2c6c7f7814c4f1a28bbac1783532bb51c1670b6d0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea948b2f-6610-4b0a-9a51-e9c9bae2d7f3", "node_type": "1", "metadata": {}, "hash": "c35cd0b8038a09a9fd4e30cfdc7670cea824065c90fe0d34d648f16de5ba3271", "class_name": "RelatedNodeInfo"}}, "text": "En second lieu, les observations du diam\u00e8tre angulaire du Soleil semblent montrer une variation de la rotation angulaire du Soleil, celle-ci s\u2019acc\u00e9l\u00e9rant sensiblement lors du minimum de Maunder. Ces deux observations sont \u00e9galement contest\u00e9es dans le cadre des d\u00e9bats sur la climatologie.\n\n# Notes et R\u00e9f\u00e9rences\n\n1. J. Lalande, 1764, op. cit., 1222 et suiv.\n2. Guy Picolet (dir.), 1987, Jean Picard et les d\u00e9buts de l\u2019astronomie de pr\u00e9cision au XVIIe si\u00e8cle, Paris, CNRS.\n3. En hommage \u00e0 l\u2019astronome anglais Edward Maunder (1851-1928) qui a \u00e9tendu les \u00e9tudes historiques entreprises par l\u2019allemand Gustav Sp\u00f6rer (1822-1895). Ce dernier est \u00e0 l\u2019origine du \u00ab minimum de Sp\u00f6rer \u00bb qui semble \u00eatre survenu entre 1420 et 1570, mis en \u00e9vidence gr\u00e2ce aux variations d\u2019abondance de l\u2019isotope 14 du carbone dans les anneaux de croissance des arbres corr\u00e9l\u00e9es \u00e0 l\u2019activit\u00e9 solaire. L\u2019isotope C-14 est produit par r\u00e9action des neutrons du vent solaire et de l\u2019azote-14 ; mais plus l'activit\u00e9 solaire est intense et moins il y a de C-14 produit dans la haute atmosph\u00e8re, car le vent solaire d\u00e9vie les rayons cosmiques qui produisent le C-14.\n4. Scheiner d\u00e9bute ses observations 20 ans avant le minimum de Maunder, Hevelius juste au d\u00e9but et celui-ci totalise pr\u00e8s de 4000 jours d\u2019observations continues. Picard et La Hire observent durant le minimum ; La Hire et ses \u00e9l\u00e8ves lors de la reprise de l\u2019activit\u00e9 solaire en 1710. On dispose ensuite de nombreuses observations, plus ou moins r\u00e9guli\u00e8res, des j\u00e9suites notamment, publi\u00e9es dans les M\u00e9moires de Tr\u00e9voux, au cours du XVIIIe si\u00e8cle.", "mimetype": "text/plain", "start_char_idx": 2942, "end_char_idx": 4515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc9d2460-10a1-4ae4-a1ea-367ad4c03055": {"__data__": {"id_": "fc9d2460-10a1-4ae4-a1ea-367ad4c03055", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d2b7df0-0d7d-4d43-a998-ac9bd1d04b63", "node_type": "4", "metadata": {}, "hash": "7fde5ec5bed40c63e44a785ffaf018cd8e9b5f0d5c208b7f356696d04bd544a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88b3022d-8bd2-4efc-acda-3a9824764c2e", "node_type": "1", "metadata": {}, "hash": "acd18904baf5d40736f7158a630e6826afc3e9a13a6c10abce159ae93fd310e3", "class_name": "RelatedNodeInfo"}}, "text": "# Introduction agrave l\u2019\u00e9dition de la \u00ab Nouvelle th\u00e9orie des taches du Soleil \u00bb du P. Pezenas\n\nNous n\u2019irons pas plus loin sur ces questions13. On imagine alors ais\u00e9ment l\u2019importance de la publication de collections d\u2019observations et de dessins des taches solaires, \u2013 comme par exemple, l\u2019Histoire c\u00e9leste ou recueil de toutes les observations astronomiques faites par ordre du Roy, publi\u00e9e en 1741 par l\u2019astronome Pierre-Charles Le Monnier, dont il sera question plus loin \u2013, ou de toute \u00e9tude quantitative sur la rotation du Soleil.\n\nC\u2019est dans ce cadre scientifique que nous pr\u00e9sentons le manuscrit, en grande partie in\u00e9dit, de la \u00ab Nouvelle th\u00e9orie des taches du Soleil \u00bb \u00e9crit par l\u2019astronome et professeur d\u2019hydrographie j\u00e9suite marseillais, le p\u00e8re Esprit Pezenas (1692-1776), texte compos\u00e9 et revu entre les ann\u00e9es 1766 et 1772. Dans ce manuscrit, dont nous allons \u00e9tudier les conditions de sa composition, le P. Pezenas donne l\u2019une des derni\u00e8res m\u00e9thodes g\u00e9om\u00e9triques, apparent\u00e9es aux m\u00e9thodes graphiques, permettant de d\u00e9duire l\u2019inclinaison de l\u2019\u00e9quateur du Soleil sur l\u2019\u00e9cliptique, \u00e0 l\u2019aide de trois observations d\u2019une tache solaire, \u00e0 des \u00e9poques donn\u00e9es, et en tenant compte du mouvement de la Terre par rapport au Soleil pendant la dur\u00e9e des observations. Ce texte comporte tous les \u00e9l\u00e9ments num\u00e9riques permettant de suivre et de comprendre l\u2019application des m\u00e9thodes de traitement g\u00e9om\u00e9trique des observations et des techniques d\u2019observations cit\u00e9es plus haut dans l\u2019introduction : passages des bords et des taches du Soleil au m\u00e9ridien, emploi des microm\u00e8tres objectifs et des microm\u00e8tres \u00e0 fils. Au d\u00e9tour de ses calculs, le P. Pezenas annonce une dur\u00e9e d\u2019environ 26 jours et 9 heures pour la rotation du Soleil autour de son axe, commente et rectifie quelques donn\u00e9es d\u2019observations publi\u00e9es dans l\u2019Histoire c\u00e9leste de Le Monnier.\n\nL\u2019\u00e9tude du P. Pezenas est donc \u00e9clairante \u00e0 plus d\u2019un titre. Elle illustre parfaitement la pratique d\u2019un astronome des Lumi\u00e8res soucieux d\u2019\u00eatre compris de ses lecteurs et montre comment un astronome, connaissant parfaitement la litt\u00e9rature de sa discipline, tire le meilleur parti des recueils d\u2019observations astronomiques.\n\n# Originaire d\u2019une famille noble d\u2019Avignon\n\nEsprit Pezenas suit la formation traditionnelle des j\u00e9suites au Coll\u00e8ge de cette ville et d\u00e9veloppe de solides comp\u00e9tences en math\u00e9matiques. Il devient l\u2019un des plus efficaces pr\u00e9dicateurs j\u00e9suites aupr\u00e8s des populations proven\u00e7ales. Esprit Pezenas se voit confier en 1728 la direction de l\u2019observatoire des j\u00e9suites de Marseille de la maison Sainte-Croix, situ\u00e9e \u00e0 la mont\u00e9e des Accoules, sur la rive nord du vieux port. L\u2019observatoire \u00e9tait inoccup\u00e9 depuis le d\u00e9c\u00e8s de son directeur le P. Thioly en 1720, provoqu\u00e9 par la terrible peste qui s\u00e9vit en Provence en 1721-22, causant des milliers de morts. Pezenas devient professeur d\u2019hydrographie en 1728 aupr\u00e8s des officiers des Gal\u00e8res royales et le restera jusqu\u2019en 1749 (date de la suppression des gal\u00e8res).\n\nApr\u00e8s un voyage de neuf mois \u00e0 Paris cette m\u00eame ann\u00e9e, au cours duquel il noue de pr\u00e9cieuses relations avec des membres de l\u2019Acad\u00e9mie royale des sciences, ainsi qu\u2019avec le c\u00e9l\u00e8bre libraire-\u00e9diteur Antoine Jombert et quelques membres influents de la Cour, Esprit Pezenas devient le directeur du nouvel \u00ab observatoire royal de la Marine \u00bb \u00e0 Marseille. \u00c0 presque soixante ans, il est \u00e9lu correspondant de l\u2019astronome Joseph-Nicolas Delisle pour l\u2019Acad\u00e9mie des sciences, puis correspondant de la toute nouvelle Acad\u00e9mie de Marine cr\u00e9\u00e9e \u00e0 Brest par le ministre Louis-Antoine Rouill\u00e9, et nomm\u00e9 recteur de la maison des j\u00e9suites de Sainte-Croix.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88b3022d-8bd2-4efc-acda-3a9824764c2e": {"__data__": {"id_": "88b3022d-8bd2-4efc-acda-3a9824764c2e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d2b7df0-0d7d-4d43-a998-ac9bd1d04b63", "node_type": "4", "metadata": {}, "hash": "7fde5ec5bed40c63e44a785ffaf018cd8e9b5f0d5c208b7f356696d04bd544a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc9d2460-10a1-4ae4-a1ea-367ad4c03055", "node_type": "1", "metadata": {}, "hash": "3a06aa8c641c664e327c2348536a1be91b81fbbb0859584d7a8b08b908e6fc42", "class_name": "RelatedNodeInfo"}}, "text": "Apr\u00e8s un voyage de neuf mois \u00e0 Paris cette m\u00eame ann\u00e9e, au cours duquel il noue de pr\u00e9cieuses relations avec des membres de l\u2019Acad\u00e9mie royale des sciences, ainsi qu\u2019avec le c\u00e9l\u00e8bre libraire-\u00e9diteur Antoine Jombert et quelques membres influents de la Cour, Esprit Pezenas devient le directeur du nouvel \u00ab observatoire royal de la Marine \u00bb \u00e0 Marseille. \u00c0 presque soixante ans, il est \u00e9lu correspondant de l\u2019astronome Joseph-Nicolas Delisle pour l\u2019Acad\u00e9mie des sciences, puis correspondant de la toute nouvelle Acad\u00e9mie de Marine cr\u00e9\u00e9e \u00e0 Brest par le ministre Louis-Antoine Rouill\u00e9, et nomm\u00e9 recteur de la maison des j\u00e9suites de Sainte-Croix. Conservant sa pension royale de professeur d\u2019hydrographie, Pezenas obtient des cr\u00e9dits destin\u00e9s \u00e0 la r\u00e9novation de l\u2019observatoire.\n\n13 D.V. Hoyt & K.H. Schatten, 1997, The role of the Sun in climate change, Oxford University Press. Voir aussi Andrew E. Dessler & Edward A. Parson, 2006, The Science and Politics of global climate change. A guide to the Debate, Cambridge University Press.", "mimetype": "text/plain", "start_char_idx": 2979, "end_char_idx": 4006, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2c95240-b83a-476a-a383-0e1e892880f6": {"__data__": {"id_": "d2c95240-b83a-476a-a383-0e1e892880f6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ab13869-9119-45b4-9a5b-02a035680d71", "node_type": "4", "metadata": {}, "hash": "72c00767ef0973f64d963e87988618b07389fb3709fdc3256055bfafcc7c8d5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0459641-e4cf-4317-ab31-e4e5731e1788", "node_type": "1", "metadata": {}, "hash": "44643d1d664f462abc620283789554a6d76be917ea960c007fbbc31c6710fbc6", "class_name": "RelatedNodeInfo"}}, "text": "# Au cours des ann\u00e9es 1750\n\nIl parvient \u00e0 l\u2019\u00e9quiper de nouveaux t\u00e9lescopes \u00e0 miroirs de bronze de James Short, et de microm\u00e8tres objectifs de Dollond notamment, instruments co\u00fbteux et repr\u00e9sentant alors ce qui se fait de mieux en mati\u00e8re d\u2019instrumentation astronomique. Pezenas et son \u00e9quipe (les j\u00e9suites Louis Lagrange et Jean-Baptiste Blanchard entre autres) d\u00e9veloppent un programme de recherche orient\u00e9 tr\u00e8s clairement vers une meilleure ma\u00eetrise de l\u2019optique instrumentale (astronomique et nautique), la recherche de com\u00e8tes, l\u2019observation des taches solaires, la th\u00e9orie des tables de la Lune et la traduction d\u2019ouvrages de math\u00e9matiques de langue anglaise.\n\n\u00c0 la fin des ann\u00e9es 1750, attir\u00e9s par la r\u00e9putation du P. Pezenas, des j\u00e9suites polonais et espagnols s\u00e9journent \u00e0 l\u2019observatoire de Marseille pour se former, soit aux observations astronomiques, soit \u00e0 la traduction d\u2019ouvrages de math\u00e9matiques. L\u2019observatoire pr\u00e9figure ainsi un centre de formation j\u00e9suite de haut niveau scientifique.\n\nDurant l\u2019Hiver 1762-63, met un terme brutal \u00e0 ce d\u00e9veloppement scientifique. Malheureusement, la dispersion de la Compagnie de J\u00e9sus, qui a lieu en Provence rejoint d\u00e9finitivement Avignon en 1766. Avec les appuis dont il b\u00e9n\u00e9ficie au plus haut niveau, et contrairement \u00e0 ses coreligionnaires, il peut poursuivre son activit\u00e9 scientifique et \u00e9ditoriale, ainsi que son apostolat, jusqu\u2019\u00e0 son d\u00e9c\u00e8s en 1776. Notons la publication d\u2019une Astronomie des marins (1766), de la traduction et l\u2019adaptation du Cours complet d\u2019optique de Robert Smith en deux volumes assortie de nombreuses additions originales de Pezenas (1767), de la traduction de La montre [de marine] de John Harrison (1767), et de quelques textes importants sur la d\u00e9termination des longitudes en mer par les m\u00e9thodes lunaires.\n\n# La \u00ab Nouvelle th\u00e9orie des taches solaires \u00bb du P. Esprit Pezenas\n\nExaminons le contexte de la composition et de la publication partielle de ce manuscrit non dat\u00e9.\n\nL\u2019observation des taches solaires est, depuis l\u2019affaire Galil\u00e9e, une activit\u00e9 traditionnelle chez les astronomes j\u00e9suites : les cahiers d\u2019observatoires connus comportent de.\n\n# Notes et r\u00e9f\u00e9rences\n\n1. G. Boistel, 2005, \u00ab L\u2019observatoire des j\u00e9suites de Marseille sous la direction du P. Pezenas (1728-1763) \u00bb, in G. Boistel (dir.), Observatoires et patrimoine astronomique fran\u00e7ais, in Cahiers d\u2019histoire et de philosophie des sciences, n\u00b054, SFHST/ENS-LSH, ENS \u00c9ditions, Lyon, 27-45.\n2. G. Boistel, 2010, \u00ab Esprit Pezenas (1692-1776), j\u00e9suite, astronome et traducteur : un acteur m\u00e9connu de la diffusion de la science anglaise en France au XVIIIe si\u00e8cle \u00bb, in B. Joly & R. Fox (\u00e9ds.), \u00c9changes entre savants fran\u00e7ais et britanniques depuis le XVIIe si\u00e8cle, Cahiers de logique et d\u2019\u00e9pist\u00e9mologie n\u00b07, Oxford, College Publications, 135-157.\n3. Rappelons qu\u2019en 1761 le Parlement de Paris prend le pr\u00e9texte de la banqueroute financi\u00e8re du P\u00e8re j\u00e9suite Lavalette \u00e0 La Martinique pour rendre la Compagnie de J\u00e9sus toute enti\u00e8re solidaire de ses cr\u00e9ances et attaquer s\u00e9v\u00e8rement leurs Constitutions. Les parlements des Provinces suivront avec un peu de retard les d\u00e9cisions du Parlement de Paris qui conduiront \u00e0 la dispersion de l\u2019Ordre puis \u00e0 son interdiction temporaire en 1773 (la Compagnie rena\u00eet peu \u00e0 peu apr\u00e8s 1814).\n4.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0459641-e4cf-4317-ab31-e4e5731e1788": {"__data__": {"id_": "c0459641-e4cf-4317-ab31-e4e5731e1788", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ab13869-9119-45b4-9a5b-02a035680d71", "node_type": "4", "metadata": {}, "hash": "72c00767ef0973f64d963e87988618b07389fb3709fdc3256055bfafcc7c8d5a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2c95240-b83a-476a-a383-0e1e892880f6", "node_type": "1", "metadata": {}, "hash": "418494cf0e108a1a85c75fab24f54950004ca3740a72b5322914909514e0a687", "class_name": "RelatedNodeInfo"}}, "text": "), \u00c9changes entre savants fran\u00e7ais et britanniques depuis le XVIIe si\u00e8cle, Cahiers de logique et d\u2019\u00e9pist\u00e9mologie n\u00b07, Oxford, College Publications, 135-157.\n3. Rappelons qu\u2019en 1761 le Parlement de Paris prend le pr\u00e9texte de la banqueroute financi\u00e8re du P\u00e8re j\u00e9suite Lavalette \u00e0 La Martinique pour rendre la Compagnie de J\u00e9sus toute enti\u00e8re solidaire de ses cr\u00e9ances et attaquer s\u00e9v\u00e8rement leurs Constitutions. Les parlements des Provinces suivront avec un peu de retard les d\u00e9cisions du Parlement de Paris qui conduiront \u00e0 la dispersion de l\u2019Ordre puis \u00e0 son interdiction temporaire en 1773 (la Compagnie rena\u00eet peu \u00e0 peu apr\u00e8s 1814).\n4. Le prix britannique pour la mise au point d\u2019une m\u00e9thode de d\u00e9termination des longitudes en mer est remis en 1765 et partag\u00e9 entre l\u2019horloger John Harrison, constructeur de la c\u00e9l\u00e8bre montre marine H4, et l\u2019astronome allemand Tobias Mayer pour de nouvelles tables de la Lune, autorisant l\u2019emploi de la m\u00e9thode des distances lunaires en mer. Voir G. Boistel, 2001, L\u2019astronomie nautique au XVIIIe si\u00e8cle en France : tables de la Lune et longitudes en mer, th\u00e8se de doctorat, Universit\u00e9 de Nantes (3 vols.) ; \u00e9dit\u00e9e en 2003 par l\u2019A.N.R.T., Lille-3, 2 vols.\n5. G. Boistel, 2003 et 2010, op. cit. Voir aussi G. Boistel, 2001, \u00ab Deux documents in\u00e9dits des PP. j\u00e9suites R.J. Boscovich et Esprit Pezenas sur les longitudes en mer \u00bb, Revue d\u2019histoire des sciences, 54/3, 383-397 ; ainsi que G. Boistel, 2002, \u00ab Les longitudes en mer au XVIIIe si\u00e8cle sous le regard critique du p\u00e8re Pezenas \u00bb, in Vincent Jullien (Dir.), Le calcul des longitudes. Un enjeu pour les math\u00e9matiques, l\u2019astronomie, la mesure du temps et la navigation, Rennes, Presses Universitaires de Rennes, 101-121.", "mimetype": "text/plain", "start_char_idx": 2641, "end_char_idx": 4350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0563ede1-e8ae-4889-88fb-5ef44c749ca3": {"__data__": {"id_": "0563ede1-e8ae-4889-88fb-5ef44c749ca3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64030abb-a341-4bcc-98d2-cba573e009da", "node_type": "4", "metadata": {}, "hash": "a412c9926ce87f9bbf6532cf04b2833f5b2b06b577c2c6ed759e896d996705b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b704b4ed-8e14-482d-9603-9bcc0ae4f5f2", "node_type": "1", "metadata": {}, "hash": "11712e66676abaae612793bd2a99299c669ef4abc0488c292a49bcc80c628590", "class_name": "RelatedNodeInfo"}}, "text": "# Observations et D\u00e9veloppements Astronomiques au XVIIIe Si\u00e8cle\n\ntr\u00e8s nombreuses observations et dessins de taches solaires. Au cours des ann\u00e9es 1750, les j\u00e9suites marseillais et leurs \u00e9l\u00e8ves, dont le jeune Guillaume de Saint-Jacques de Silvabelle, d\u00e9veloppent ce type d\u2019observation \u00e0 l\u2019aide des nouveaux instruments qui \u00e9quipent progressivement l\u2019observatoire \u00e0 partir de l\u2019ann\u00e9e 1752 : un grand t\u00e9lescope de type gr\u00e9gorien permettant un grossissement de 800 fois, ainsi que deux plus petits t\u00e9lescopes (Cassegrain et gr\u00e9gorien), avec leurs h\u00e9liom\u00e8tres-objectifs de Dollond.\n\nLe 19 mars 1763, Pezenas est d\u00e9poss\u00e9d\u00e9 de la direction de l\u2019observatoire suite \u00e0 une v\u00e9ritable descente de police \u00e0 laquelle prend part Silvabelle, \u00e9l\u00e8ve r\u00e9put\u00e9 prodige apr\u00e8s que celui-ci a remarqu\u00e9 quelques erreurs dans le Trait\u00e9 sur la pr\u00e9cession des \u00e9quinoxes de d\u2019Alembert. Issu de la noblesse proven\u00e7ale, Saint-Jacques de Silvabelle est un \u00e9l\u00e8ve r\u00e9gulier de l\u2019observatoire d\u00e8s le d\u00e9but des ann\u00e9es 1750. Pezenas intervient personnellement et fait jouer ses amiti\u00e9s acad\u00e9miques pour promouvoir autant qu\u2019il lui est possible le Trait\u00e9 des variations c\u00e9lestes de Silvabelle (dont il nous reste une copie de la main m\u00eame de Pezenas).\n\nSilvabelle observe com\u00e8tes et taches du Soleil aux c\u00f4t\u00e9s des j\u00e9suites de Sainte-Croix au cours des ann\u00e9es 1750 jusqu\u2019en 1762, et les quelques notes d\u2019observations et correspondances qui nous restent, nous donnent l\u2019impression de relations amicales et intellectuelles sinc\u00e8res entre lui et les j\u00e9suites de Sainte-Croix. \u00c0 la suite de la dispersion des j\u00e9suites, Silvabelle obtient le brevet de directeur de l\u2019observatoire royal de Marseille en 1764, assorti d\u2019une pension de 1200 livres ; il occupe cette charge jusqu\u2019en 1801, bien que l\u2019observatoire passe sous la tutelle de l\u2019Acad\u00e9mie des sciences lettres et arts de Marseille en 1781.\n\nAvec la ru\u00e9e des cr\u00e9anciers des j\u00e9suites en 1763, Silvabelle change brusquement de camp et opte pour une attitude rude et inamicale vis-\u00e0-vis de ses anciens amis. Pezenas doit se d\u00e9battre pour ne pas \u00eatre d\u00e9poss\u00e9d\u00e9 compl\u00e8tement de ses biens et notamment faire reconna\u00eetre certains instruments d\u2019astronomie comme lui appartenant, alors que les adversaires des j\u00e9suites craignent que ceux-ci ne partent avec la biblioth\u00e8que de l\u2019observatoire et d\u00e9m\u00e9nagent tous les instruments qui font la r\u00e9putation croissante de cet observatoire depuis les ann\u00e9es 1750. Entre les deux hommes, ce ne sont que coups bas et r\u00e9criminations aupr\u00e8s des autorit\u00e9s locales ou royales. Silvabelle doit s\u2019affirmer comme nouveau directeur de l\u2019observatoire royal de la Marine et ne pas se montrer faible \u00e0 l\u2019\u00e9gard des nouveaux ennemis d\u00e9sign\u00e9s que sont ses anciens amis les j\u00e9suites. Pezenas fait tout pour ne pas sombrer avec les autres membres de la Compagnie de J\u00e9sus et conserver quelques privil\u00e8ges et quelques biens lui permettant de poursuivre l\u2019\u0153uvre de sa vie, ce \u00e0 quoi il parviendra assez bien, gr\u00e2ce aux amiti\u00e9s et soutiens dont il dispose au plus haut niveau : Acad\u00e9mie royale des sciences (Joseph-Nicolas Delisle, Charles-Marie de La Condamine, et dans une moindre mesure, J\u00e9r\u00f4me Lalande), minist\u00e8re de la Marine (Machault d\u2019Arnouville, de Boynes) et sans doute le comte de Saint-Florentin.\n\n# Notes et R\u00e9f\u00e9rences\n\n1. Jean-Marie Homet, 1983, Astronomes et astronomie en Provence, Aix-en-Provence, \u00c9disud. Jean-Michel Faidit, 1991, Les \u00ab amateurs \u00bb de science d\u2019une province au XVIIIe si\u00e8cle : astronomie et astronomes en Languedoc, Th\u00e8se de doctorat en histoire moderne, Universit\u00e9 Montpellier III, par exemple.\n2. Sur les conditions d\u2019acquisition de ces instruments : G. Boistel, 2005, op.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b704b4ed-8e14-482d-9603-9bcc0ae4f5f2": {"__data__": {"id_": "b704b4ed-8e14-482d-9603-9bcc0ae4f5f2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64030abb-a341-4bcc-98d2-cba573e009da", "node_type": "4", "metadata": {}, "hash": "a412c9926ce87f9bbf6532cf04b2833f5b2b06b577c2c6ed759e896d996705b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0563ede1-e8ae-4889-88fb-5ef44c749ca3", "node_type": "1", "metadata": {}, "hash": "eea87a95bc68ea2b8583e5f40f323ef05156788f2671622f5cb7487644922ae9", "class_name": "RelatedNodeInfo"}}, "text": "# Notes et R\u00e9f\u00e9rences\n\n1. Jean-Marie Homet, 1983, Astronomes et astronomie en Provence, Aix-en-Provence, \u00c9disud. Jean-Michel Faidit, 1991, Les \u00ab amateurs \u00bb de science d\u2019une province au XVIIIe si\u00e8cle : astronomie et astronomes en Languedoc, Th\u00e8se de doctorat en histoire moderne, Universit\u00e9 Montpellier III, par exemple.\n2. Sur les conditions d\u2019acquisition de ces instruments : G. Boistel, 2005, op. cit.\n3. Le m\u00e9moire de Silvabelle est examin\u00e9 \u00e0 l\u2019Acad\u00e9mie des sciences et cause quelques remous dont Joseph-Nicolas Delisle se fait l\u2019\u00e9cho aupr\u00e8s de Pezenas et du P. Lagrange : \u00ab Correspondance de Delisle \u00bb, Arch. Observatoire de Paris, B1.7, lettres 51, 71 et 78 de f\u00e9vrier et mars 1753.\n4. Le \u00ab Trait\u00e9 des variations c\u00e9lestes ou sur les in\u00e9galit\u00e9s du mouvement des plan\u00e8tes \u00bb de Silvabelle est publi\u00e9 dans le tome II des M\u00e9moires de math\u00e9matiques et de physique r\u00e9dig\u00e9s \u00e0 l\u2019Observatoire de Marseille, Avignon, 1756, 201-355.\n5. L\u2019action de Silvabelle \u00e0 la t\u00eate de l\u2019observatoire de Marseille reste \u00e0 \u00e9crire. Ses comp\u00e9tences en astronomie sont souvent mises en cause (par La Condamine lui-m\u00eame). Les relations entre Silvabelle et le secr\u00e9taire perp\u00e9tuel de l\u2019Acad\u00e9mie des sciences de Marseille depuis 1767, et futur maire de cette ville en 1791, Jean Raymond Pierre Mourraille, sont conflictuelles au d\u00e9but des ann\u00e9es 1780. L\u2019observatoire est plac\u00e9 sous la tutelle de l\u2019Acad\u00e9mie des sciences, lettres et arts de Marseille en 1781 (Arch. de l\u2019observatoire de Marseille, Arch. d\u00e9partementales des Bouches-du-Rh\u00f4ne, 132 J 174).", "mimetype": "text/plain", "start_char_idx": 3228, "end_char_idx": 4752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "424f8222-10fa-41f6-9a00-edb25f5345a0": {"__data__": {"id_": "424f8222-10fa-41f6-9a00-edb25f5345a0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2626821-8be7-4945-b2da-61289c2fa823", "node_type": "4", "metadata": {}, "hash": "7ba084963c9c9f5196b2edea9697dfb0b729c4ea9cfc38854f4089082e2cacf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a017080e-4f9f-4fb7-b889-967007504c33", "node_type": "1", "metadata": {}, "hash": "fc752325f8bbee02f56a02f7b5b53b9590852d9ec7ef149f113ff865e4492862", "class_name": "RelatedNodeInfo"}}, "text": "# Ministre des Cultes\n\nDans cette querelle personnelle, Silvabelle parvient \u00e0 pr\u00e9senter en 1764 un m\u00e9moire sur l\u2019utilisation de l\u2019observation des taches du Soleil pour en d\u00e9terminer la rotation, examin\u00e9 par Alexis Clairaut et J\u00e9r\u00f4me Lalande le 15 f\u00e9vrier 1764 \u00e0 l\u2019Acad\u00e9mie des sciences. Le m\u00e9moire (dont on n\u2019a pas conserv\u00e9 l\u2019original) est publi\u00e9 en 1768 dans le tome cinq de la collection dite des \u00ab Savants \u00e9trangers \u00bb. L\u2019accueil de ce m\u00e9moire est tr\u00e8s favorable ; nous donnons la transcription du rapport de Clairaut et Lalande en annexe 3, et le m\u00e9moire de Silvabelle en annexe 4. Remarquons que le titre sous lequel est publi\u00e9 le m\u00e9moire de Silvabelle \u2013 \u00ab Probl\u00e8me \u00bb \u2013 n\u2019est gu\u00e8re engageant et ne laisse rien percevoir de son contenu.\n\nLe probl\u00e8me susdit est le suivant : \u00ab Trois observations d\u2019une tache du Soleil \u00e9tant donn\u00e9es, d\u00e9terminer le parall\u00e8le du Soleil que d\u00e9crit la tache & le temps de sa r\u00e9volution \u00bb. On peut voir que Silvabelle expose sa m\u00e9thode dans le nouveau style math\u00e9matique analytique d\u00e9velopp\u00e9 par Clairaut, Euler et d\u2019Alembert, notamment, et a de quoi s\u00e9duire un astronome comme Lalande, ouvert aux nouveaut\u00e9s math\u00e9matiques. Remarquons aussi que Silvabelle pose le probl\u00e8me math\u00e9matique en le mettant en \u00e9quation, mais ne fait aucune application num\u00e9rique ; il ne donne aucune valeur de la p\u00e9riode de rotation du Soleil sur lui-m\u00eame, confirmant ainsi son statut et sa r\u00e9putation de \u00ab G\u00e9om\u00e8tre \u00bb que lui conf\u00e8re le rapport acad\u00e9mique : Silvabelle, \u00e0 l\u2019instar de d\u2019Alembert et de Clairaut, d\u00e9teste le calcul num\u00e9rique !\n\nPezenas soumet \u00e0 l\u2019Acad\u00e9mie un m\u00e9moire sur les taches solaires qui est examin\u00e9 le 20 ao\u00fbt 1766 par les astronomes Joseph-Nicolas Delisle (correspondant de Pezenas depuis 1750) et le chanoine Alexandre-Guy Pingr\u00e9. Le probl\u00e8me propos\u00e9 est le m\u00eame que celui formul\u00e9 par Silvabelle : d\u00e9terminer la p\u00e9riode de rotation du Soleil \u00e0 l\u2019aide de trois observations d\u2019une m\u00eame tache (position apparente et temps de l\u2019observation pour chacune d\u2019elles). Pezenas l\u2019assortit de plusieurs corollaires et probl\u00e8mes annexes. Le rapport (reproduit en annexe 2) est favorable ; la m\u00e9thode de Pezenas, si elle est reconnue ing\u00e9nieuse, curieuse et utile, ressemble un peu, selon les rapporteurs, \u00e0 une m\u00e9thode donn\u00e9e par Delisle dans les ann\u00e9es 1730. Elle est apparent\u00e9e aux m\u00e9thodes graphiques d\u00e9velopp\u00e9es par Delisle en 1738 dans ses m\u00e9moires publi\u00e9s \u00e0 Saint-Petersbourg et par Jacques Cassini dans ses \u00c9l\u00e9ments d\u2019astronomie, publi\u00e9s en 1740.\n\n\u00c0 l\u2019aide des observations du P. Martin Poczobut \u2013 j\u00e9suite polonais ayant s\u00e9journ\u00e9 \u00e0 Marseille entre 1760 et 1762 \u2013, Pezenas estime la p\u00e9riode de r\u00e9volution du Soleil \u00e0 26 jours 9 heures et l\u2019inclinaison de son axe de rotation sur l\u2019\u00e9cliptique \u00e0 5\u00b014\u2019. Les rapporteurs soulignent que le m\u00e9moire de Pezenas est instructif dans le sens o\u00f9 il souligne et corrige des.\n\n# Notes\n\n1. G. Boistel, 2003, op. cit. Voir par exemple la lettre de Pezenas \u00e0 Charles-Marie de La Condamine, du 26 juillet 1771, d\u2019Avignon, Archives du C.N.A.M., NS5 [PEZENAS]/1.\n2. \u00ab Savants \u00e9trangers \u00bb : appellation conventionnelle chez les historiens des sciences pour la collection de l\u2019Acad\u00e9mie royale des sciences publi\u00e9e sous le titre long de : M\u00e9moires de math\u00e9matique et de physique pr\u00e9sent\u00e9s \u00e0 l\u2019Acad\u00e9mie royale des sciences par divers Savans & lus dans ses assembl\u00e9es (Paris, Imprimerie royale). Cette collection regroupe des m\u00e9moires d\u2019auteurs \u00ab \u00e9trangers \u00bb \u00e0 l\u2019Acad\u00e9mie des sciences, et consid\u00e9r\u00e9s comme dignes d\u2019int\u00e9r\u00eat par les acad\u00e9miciens.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a017080e-4f9f-4fb7-b889-967007504c33": {"__data__": {"id_": "a017080e-4f9f-4fb7-b889-967007504c33", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2626821-8be7-4945-b2da-61289c2fa823", "node_type": "4", "metadata": {}, "hash": "7ba084963c9c9f5196b2edea9697dfb0b729c4ea9cfc38854f4089082e2cacf1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "424f8222-10fa-41f6-9a00-edb25f5345a0", "node_type": "1", "metadata": {}, "hash": "afd79592340971e1d97f3f47e88642945b79ea97f1272fdfa8132cac969923ed", "class_name": "RelatedNodeInfo"}}, "text": "cit. Voir par exemple la lettre de Pezenas \u00e0 Charles-Marie de La Condamine, du 26 juillet 1771, d\u2019Avignon, Archives du C.N.A.M., NS5 [PEZENAS]/1.\n2. \u00ab Savants \u00e9trangers \u00bb : appellation conventionnelle chez les historiens des sciences pour la collection de l\u2019Acad\u00e9mie royale des sciences publi\u00e9e sous le titre long de : M\u00e9moires de math\u00e9matique et de physique pr\u00e9sent\u00e9s \u00e0 l\u2019Acad\u00e9mie royale des sciences par divers Savans & lus dans ses assembl\u00e9es (Paris, Imprimerie royale). Cette collection regroupe des m\u00e9moires d\u2019auteurs \u00ab \u00e9trangers \u00bb \u00e0 l\u2019Acad\u00e9mie des sciences, et consid\u00e9r\u00e9s comme dignes d\u2019int\u00e9r\u00eat par les acad\u00e9miciens. Elle donne un fantastique aper\u00e7u sur l\u2019activit\u00e9 savante au XVIII si\u00e8cle hors la capitale. G. de Saint-Jacques de Silvabelle, 1768, \u00ab Probl\u00e8me \u00bb, M\u00e9moires de math\u00e9matique et de physique pr\u00e9sent\u00e9s \u00e0 l\u2019Acad\u00e9mie royale des sciences par divers Savans & lus dans ses assembl\u00e9es, Paris, Imprimerie royale, 631-634.\n3. Joseph-Nicolas Delisle, 1738, M\u00e9moires pour servir \u00e0 l\u2019histoire & au progr\u00e8s de l\u2019astronomie, de la g\u00e9ographie et de la physique, Saint-Petersbourg : \u00ab Th\u00e9orie du mouvement des taches du Soleil \u00bb, 143-179.\n4. Jacques Cassini, 1740, \u00c9l\u00e9ments d\u2019Astronomie, Paris, Imprimerie royale : liv. II, chap. II : \u00ab De la r\u00e9volution du Soleil autour de son axe \u00bb, 86-105.\n5. La pr\u00e9sence de Martin Odlanicki Poczobut (1726-1810), s.j., astronome et po\u00e8te polonais, \u00e0 l\u2019observatoire de Marseille, est attest\u00e9e par Silvabelle de juillet 1761 \u00e0 d\u00e9cembre 1762 au moins. Mais les notes de Silvabelle laissent entendre que Poczobut avait d\u00e9j\u00e0 s\u00e9journ\u00e9 \u00e0 Marseille auparavant, au d\u00e9but de l\u2019ann\u00e9e 1760 probablement. Il se r\u00e9fugie \u00e0 Avignon en 1763 et poursuit ses observations avant de rentrer en Pologne en 1764. Le P. Poczobut deviendra par la suite premier astronome du roi de Pologne, directeur de l\u2019observatoire et recteur de l\u2019Universit\u00e9 de Vilnius. Il sera l\u2019un des correspondants de J\u00e9r\u00f4me Lalande pour l\u2019Acad\u00e9mie des sciences. Voir G. Boistel, 2010, op. cit.", "mimetype": "text/plain", "start_char_idx": 2871, "end_char_idx": 4852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3116f52-5638-4cd4-b917-5773b1015fd9": {"__data__": {"id_": "e3116f52-5638-4cd4-b917-5773b1015fd9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0cfde65f-f4e3-48dd-88ab-bb761abe7b29", "node_type": "4", "metadata": {}, "hash": "a1de1a690a1224ccf2f5eb543468008d9d892d1000b8780e823f7e631446a036", "class_name": "RelatedNodeInfo"}}, "text": "# Erreurs se trouvant dans les observations anciennes de l\u2019abb\u00e9 Picard\n\net collect\u00e9es dans l\u2019Histoire c\u00e9leste de l\u2019astronome Pierre-Charles Le Monnier.\n\nLe m\u00e9moire examin\u00e9 par l\u2019Acad\u00e9mie en 1766 ne sera imprim\u00e9 qu\u2019en 1774. Il est \u00e0 peu de choses pr\u00e8s identique \u00e0 celui que Pezenas ajoute \u00e0 sa traduction du Cours complet d\u2019optique de Smith publi\u00e9e en 176729. C\u2019est aussi la source \u00e0 laquelle J\u00e9r\u00f4me Lalande se r\u00e9f\u00e8re dans son premier m\u00e9moire sur les taches solaires lu \u00e0 l\u2019Acad\u00e9mie en 177530.\n\nDevant les d\u00e9lais de publication des Savants \u00e9trangers (le tome 5 est publi\u00e9 en 1768 avec le m\u00e9moire de Silvabelle ; le tome 6 en 1774 avec le m\u00e9moire de Pezenas), Pezenas propose certainement tardivement son manuscrit \u00e0 la Soci\u00e9t\u00e9 royale des sciences de Montpellier31 qui le refuse car \u00ab ayant \u00e9t\u00e9 imprim\u00e9 ailleurs \u00bb (Figure 1). Compte tenu des diverses dates des rapports et d\u2019impression des recueils des Savants \u00e9trangers, cette mention de rejet permet d\u2019imaginer que le manuscrit conserv\u00e9 \u00e0 Montpellier est une r\u00e9\u00e9criture tardive du m\u00e9moire soumis \u00e0 l\u2019Acad\u00e9mie royale des sciences en 1766, et a d\u00fb \u00eatre compos\u00e9 apr\u00e8s la parution du m\u00e9moire de Silvabelle en 1768. Il doit donc dater de la p\u00e9riode 1768-72 environ et propos\u00e9 par Pezenas \u00e0 la soci\u00e9t\u00e9 savante de Montpellier au moins \u00e0 la fin de l\u2019ann\u00e9e 1774.\n\n# Figure 1\n\nAnnotations concernant le rejet du manuscrit de Pezenas par la Soci\u00e9t\u00e9 royale des sciences de Montpellier et attribution au P. Pezenas.\n\nCe manuscrit montpelli\u00e9rain comporte une introduction \u00e0 caract\u00e8re historique int\u00e9ressante dans le sens o\u00f9 Pezenas balaye la litt\u00e9rature r\u00e9cente, faisant remarquer qu\u2019il n\u2019est\n\n29 E. Pezenas, 1767, Cours complet d\u2019Optique traduit de l\u2019anglois de Robert Smith, Avignon, Veuve Girard, Seguin, Aubert, tome 2, \u00ab Additions \u00bb, 524-528. Notons que la traduction du Cours complet d\u2019optique de Smith \u00e9tait d\u00e9j\u00e0 pr\u00eate en 1752 comme nous l\u2019avons montr\u00e9 par ailleurs (G. Boistel, 2010, op. cit.) ; nous ne savons pas de quand datent les additions du P. Pezenas \u00e0 sa traduction, mais Pezenas b\u00e9n\u00e9ficie entre 1752 et 1755 de remarques des astronomes Nicolas-Louis de Lacaille et Delisle, ainsi que de l\u2019Acad\u00e9micien montpelli\u00e9rain et m\u00e9decin, Amoreux. Pezenas et le P. Jean-Baptiste Blanchard s\u2019occupent des m\u00e9moires d\u2019optique qu\u2019ils publieront en 1755 dans le tome I des M\u00e9moires de math\u00e9matiques et de physique r\u00e9dig\u00e9s \u00e0 l\u2019observatoire de Marseille.\n\n30 J\u00e9r\u00f4me Lalande, \u00ab M\u00e9moire sur les taches du Soleil et sur sa rotation \u00bb, Histoire de l\u2019Acad\u00e9mie royale des sciences avec les m\u00e9moires qui ont \u00e9t\u00e9 lus pour l\u2019ann\u00e9e 1776, M\u00e9m., 457-514, p. 465 en particulier. Lalande signale aussi un paragraphe sur les taches solaires dans l\u2019Astronomie des Marins de Pezenas, publi\u00e9e en 1766. Mais Pezenas ne traite pas des taches solaires en particulier. Il d\u00e9veloppe par contre des m\u00e9thodes de trigonom\u00e9trie sph\u00e9rique et c\u2019est sans doute \u00e0 ces m\u00e9thodes que se r\u00e9f\u00e8re Lalande.\n\n31 Pezenas est \u00e9lu membre associ\u00e9 de la Soci\u00e9t\u00e9 royale de Montpellier et correspondant de Danizy en 1755. Il est en contact avec cette soci\u00e9t\u00e9 savante officiellement depuis 1746 (Arch. d\u00e9part. H\u00e9rault, \u00ab Registres des d\u00e9lib\u00e9rations de la Soc. Roy. Sci. de Montpellier \u00bb, D.120, mars et avril 1755 ; D.121, 6 mars 1755).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a07e326-d505-4585-b191-d284dcf310fc": {"__data__": {"id_": "5a07e326-d505-4585-b191-d284dcf310fc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce9541f0-9a1a-4bd2-b222-95148eec5c29", "node_type": "4", "metadata": {}, "hash": "67b5cb2535a0c0d464b8e1bc6424f430884146253ee8e1a5dd8841bf244d88b7", "class_name": "RelatedNodeInfo"}}, "text": "pas si facile pour les astronomes de reconna\u00eetre que les taches appartiennent \u00e0 la surface solaire. Remarquons que c\u2019est une \u00e9vidence pour le j\u00e9suite Pezenas, et que la d\u00e9termination de la p\u00e9riode de rotation solaire s\u2019en trouve ainsi grandement facilit\u00e9e, \u00e0 condition de tenir compte du mouvement de la Terre autour du Soleil pendant la dur\u00e9e des observations (entre la premi\u00e8re et la troisi\u00e8me observation). Il nous apprend donc que pour certains astronomes de m\u00e9tier au milieu du XVIIIe si\u00e8cle, l\u2019appartenance des taches \u00e0 la surface solaire ne va pas encore de soi, m\u00eame pour un membre de la dynastie Cassini ou un astronome important tel que Georg Wolfgang Krafft de l\u2019Acad\u00e9mie Imp\u00e9riale de Saint-Petersbourg. Cette remarque confirme l\u2019Encyclop\u00e9die qui annonce, \u00e0 l\u2019article TACHE (du Soleil), que les avis sont tr\u00e8s nettement partag\u00e9s quant \u00e0 l\u2019adh\u00e9rence des taches \u00e0 la surface du Soleil.\n\nLe style math\u00e9matique que Pezenas emploie est g\u00e9om\u00e9trique et graphique en ce sens qu\u2019il pose les relations sous forme de proportions (avec l\u2019usage des signes : ou : :) et que la m\u00e9thode s\u2019appuie sur les figures g\u00e9om\u00e9triques d\u00e9duites de la position des taches sur le disque solaire. Il est curieux de noter que, malgr\u00e9 ses connaissances et ses traductions d\u2019ouvrages math\u00e9matiques comme le Trait\u00e9 des fluxions de Colin McLaurin par exemple, Pezenas ne pose pas ces relations sous forme analytique comme il aurait pu le faire et comme l\u2019a fait son adversaire Silvabelle. Pezenas, \u00e2g\u00e9 d\u2019environ 75 ans, est alors dans une p\u00e9riode d\u00e9licate et boulevers\u00e9e de sa vie. Il doit faire face \u00e0 la dispersion des j\u00e9suites et \u00e0 une menace tr\u00e8s pesante sur ses conditions d\u2019existence.\n\n# Conclusion\n\nIllustrant la pratique d\u2019un astronome d\u2019une long\u00e9vit\u00e9 peu commune \u00e0 cette \u00e9poque, ce texte se trouve \u00e0 l\u2019articulation de deux traditions savantes et d\u2019une p\u00e9riode de transition entre deux styles d\u2019\u00e9criture math\u00e9matique. Il nous permet de comprendre comment les astronomes du XVIIIe si\u00e8cle, abordaient les probl\u00e8mes de la d\u00e9termination des dur\u00e9es de rotation des astres proches de la Terre par diverses m\u00e9thodes.\n\n32 Encyclop\u00e9die ou Dictionnaire raisonn\u00e9 des sciences, des arts et des m\u00e9tiers, nouvelle \u00e9dition, 1779, Vol. 32, Gen\u00e8ve, 483-486, 484 en particulier. L\u2019article est en grande partie de J\u00e9r\u00f4me Lalande.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4613155-f9da-4648-9f09-d785155c3511": {"__data__": {"id_": "b4613155-f9da-4648-9f09-d785155c3511", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4ee5fc6-3fc3-4fdf-ae6a-36b0b1f8fb65", "node_type": "4", "metadata": {}, "hash": "7d2500597185348f3d0c1d897bf0c7b34a5e611db5f2bbbd939b14ac9d038294", "class_name": "RelatedNodeInfo"}}, "text": "# CONVENTIONS TYPOGRAPHIQUES D\u2019\u00c9DITION\n\nEn r\u00e8gle g\u00e9n\u00e9rale, l\u2019orthographe a \u00e9t\u00e9 l\u00e9g\u00e8rement modernis\u00e9e pour faciliter la lecture, sans entra\u00eener de modification profonde du manuscrit. Ainsi, les accents et certaines conjugaisons ont \u00e9t\u00e9 retouch\u00e9s. Le style et la syntaxe sont int\u00e9gralement pr\u00e9serv\u00e9s.\n\nEntre crochets figurent quelques commentaires de l\u2019\u00e9diteur signalant des ratures ou des difficult\u00e9s de lecture, tr\u00e8s peu nombreuses dans ce manuscrit.\n\nLes figures sont les figures d\u2019origine, ins\u00e9r\u00e9es dans le texte afin de faciliter les renvois et la compr\u00e9hension des d\u00e9monstrations du p\u00e8re Pezenas.\n\nLa pagination originale du manuscrit conserv\u00e9 aux Archives d\u00e9partementales de l\u2019H\u00e9rault, est indiqu\u00e9e comment suit : /261r/ signifie \u00ab folio 261 recto \u00bb (v pour verso).\n\nLes symboles sont ceux utilis\u00e9s par le P. Pezenas. En voici la signification :\n\n- Q repr\u00e9sente le Soleil.\n- A repr\u00e9sente le point vernal.\n- G repr\u00e9sente le symbole astronomique/astrologique de la constellation de la Balance.\n\n# REMERCIEMENTS\n\nJ\u2019adresse mes remerciements chaleureux \u00e0 Mme Colette Le Lay pour sa relecture attentive, et au Professeur \u00e9m\u00e9rite Jacques Gapaillard, pour ses pr\u00e9cieuses indications sur la conduite des d\u00e9monstrations du P. Pezenas, dont certaines ont \u00e9t\u00e9 int\u00e9gr\u00e9es \u00e0 l\u2019appareil critique de l\u2019\u00e9dition de ce manuscrit.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44a7d982-5a77-4a88-b135-e89f523cb4ff": {"__data__": {"id_": "44a7d982-5a77-4a88-b135-e89f523cb4ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99a75429-7639-43fe-a607-2df150a76a97", "node_type": "4", "metadata": {}, "hash": "7a79d374c11221f9324caf42a20292464fbeb305e54d911d097a8d4bacf780df", "class_name": "RelatedNodeInfo"}}, "text": "# NONPARAMETRIC ESTIMATION IN RANDOM COEFFICIENTS BINARY CHOICE MODELS\n\n# ERIC GAUTIER AND YUICHI KITAMURA\n\n# Abstract\n\nThis paper considers random coefficients binary choice models. The main goal is to estimate the density of the random coefficients nonparametrically. This is an ill-posed inverse problem characterized by an integral transform. A new density estimator for the random coefficients is developed, utilizing Fourier-Laplace series on spheres. This approach offers a clear insight on the identification problem. More importantly, it leads to a closed form estimator formula that yields a simple plug-in procedure requiring no numerical optimization. The new estimator, therefore, is easy to implement in empirical applications, while being flexible about the treatment of unobserved heterogeneity. Extensions including treatments of non-random coefficients and models with endogeneity are discussed.\n\n# 1. Introduction\n\nConsider a binary choice model\n\n(1.1)\nY = I{X\u03b2 \u2265 0}'\n\nwhere I denotes the indicator function and X is a d-vector of covariates. We assume that the first element of X is 1, therefore the vector X is of the form X = (1, tildeX'). The vector \u03b2 is random. The random element (Y, tildeX, \u03b2) is defined on some probability space (\u03a9, F, P), and (y, tilde i, \u03b2), i = 1, ..., N_i denote its realizations. The econometrician observes (y, tilde i), i = 1, ..., N, but \u03b2, i = 1, ..., N remain unobserved.\n\nDate: This Version: August 31, 2011.\n\nKeywords: Inverse problems, Discrete Choice Models.\n\nWe thank Whitney Newey and two anonymous referees for comments that greatly improved this paper. We also thank seminar participants at Chicago, CREST, Harvard/MIT, the Henri Poincar\u00e9 Institute, Hitotsubashi, LSE, Mannheim, Minnesota, Northwestern, NYU, Paris 6, Princeton, Rochester, Simon Fraser, Tilburg, Toulouse 1 University, UBC, UCL, UCLA, UCSD, the Tinbergen Institute and the University of Tokyo, and participants of the 2008 Cowles summer econometrics conference, EEA/ESEM, FEMES, Journ\u00e9es STAR, and SETA and 2009 CIRM Rencontres de Statistiques Math\u00e9matiques for helpful comments. Yuhan Fang and Xiaoxia Xi provided excellent research assistance. Kitamura acknowledges financial support from the National Science Foundation via grants SES-0241770, SES-0551271 and SES-0851759. Gautier is grateful for support from the Cowles Foundation as this research was initiated during his visit as a postdoctoral associate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baebd827-e5b4-44aa-bca9-6b3c1119b967": {"__data__": {"id_": "baebd827-e5b4-44aa-bca9-6b3c1119b967", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "607c7cef-b614-41f4-85c8-c65623ec5c7c", "node_type": "4", "metadata": {}, "hash": "47264044eb824a8b9b9bb339ddd5e89b8d99e1c92aa07d4ec85a33bb993c29d5", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\nunobserved. The vectors Xtilde and beta correspond to observed and unobserved heterogeneity across agents, respectively. Note that the first element of beta in this formulation absorbs the usual scalar stochastic shock term as well as a constant in a standard binary choice model with non-random coefficients. This formulation is used in Ichimura and Thompson (1998), and is convenient for the subsequent development in this paper. Our basic model maintains exogeneity of the covariates Xtilde:\n\n# Assumption 1.1.\n\nbeta is independent of Xtilde,\n\nSection 5.3 considers ways to relax this assumption. Under (1.1) and Assumption 1.1, the choice probability function is given by\n\n(1.2) r(x) = P(Y = 1|X = x) = Ebeta[I braceleftxprimebeta > 0braceright].\n\nDiscrete choice models with random coefficients are useful in applied research since it is often crucial to incorporate unobserved heterogeneity in modeling the choice behavior of individuals. There is a vast and active literature on this topic. Recent contributions include Briesch, Chintagunta and Matzkin (1996), Brownstone and Train (1999), Chesher and Santos Silva (2002), Hess, Bolduc and Polak (2005), Harding and Hausman (2006), Athey and Imbens (2007), Bajari, Fox and Ryan (2007) and Train (2003). A common approach in estimating random coefficient discrete choice models is to impose parametric distributional assumptions. A leading example is the mixed Logit model, which is discussed in details by Train (2003). If one does not impose a parametric distributional assumption, the distribution of beta itself is the structural parameter of interest. The goal for the econometrician is then to recover it nonparametrically from the information about r(x) obtained from the data.\n\nNonparametric treatments for unobserved heterogeneity distributions have been considered in the literature for other models. Heckman and Singer (1984) study the issue of unobserved heterogeneity distributions in duration models and propose a treatment by a nonparametric maximum likelihood estimator (NPMLE). Elbers and Ridder (1982) also develop some identification results in such models. Beran and Hall (1992) and Hoderlein et al. (2007) discuss nonparametric estimation of random coefficients linear regression models. Despite the tremendous importance of random coefficient discrete choice models, as exemplified in the above references, nonparametrics in these models is relatively underdeveloped. In their important paper, Ichimura and Thompson (1998) propose an NPMLE for the CDF of beta. They present sufficient conditions for identification and prove the consistency of the NPMLE. The NPMLE requires high dimensional numerical maximization and can be computationally", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3466bb3-3f61-496f-8d37-00da4d6c4055": {"__data__": {"id_": "d3466bb3-3f61-496f-8d37-00da4d6c4055", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f01f4512-d31b-4b34-8b1a-1216f77f186b", "node_type": "4", "metadata": {}, "hash": "b94cd3d7e38db0fb6f9974fcebefe03ac4e9e0c3fcc23a19795ef36a64b1a2d0", "class_name": "RelatedNodeInfo"}}, "text": "# 3\n\nintensive even for a moderate sample size. Berry and Haile (2008) explore nonparametric identification problems in a random coefficients multinomial choice model that often arises in empirical IO. This paper considers nonparametric estimation of the random coefficients distribution, using a novel approach that shares some similarities with standard deconvolution techniques. This allows us to reconsider the identifiability of the model and obtain a constructive identification result. Moreover, we develop a simple plug-in estimator for the density of beta that requires no numerical optimization or integration. It is easy to implement in empirical applications, while being flexible about the treatment of unobserved heterogeneity.\n\nSince the scale of beta is not identified in the binary choice model, we normalize it so that beta is a vector of Euclidean norm 1 in Rd. The vector beta then belongs to the d - 1 dimensional sphere Sd-1. This is not a restriction as long as the probability that beta is equal to 0 is 0. Also, since only the angle between X and beta matters in the binary decision I\\{X' beta \\geq 0\\}, we can replace X by X / ||X|| without any loss of information. We therefore assume that X is on the sphere Sd-1 as well in the subsequent analysis. Results from the directional data literature are thus relevant to our analysis. We aim to recover the joint probability density function fbeta of beta with respect to the uniform spherical measure \u03c3 over Sd-1 from the random sample (y1, x1), . . . , (yN, xN) of (Y, X).\n\nThe problem considered here is a linear ill-posed inverse problem. We can write\n\n\u222bSd-1 I\\{b \u2265 0\\} fbeta(b) d\u03c3(b) = \u222bH(x) fbeta(b) d\u03c3(b) := H(fbeta)(x)\n\n(1.3) r(x) =  prime\n\nwhere the set H(x) is the hemisphere {b : x' b \u2265 0}. The mapping H is called the hemispherical transformation. Inversion of this mapping was first studied by Funk (1916) and later by Rubin (1999). Groemer (1996) also discusses some of its properties. H is not injective without further restrictions and conditions need to be imposed to ensure identification of fbeta from r. Even under a set of assumptions that guarantee identification, however, the inverse of H is not a continuous mapping, making the problem ill-posed. To see this, suppose we restrict fbeta to be in L2(Sd-1). Since the kernel of H is square integrable by compactness of the sphere, it is Hilbert-Schmidt and thus compact. Therefore if the inverse of H were continuous, H-1H would map the closed unit ball in L2(Sd-1) to a compact set. But the Riesz theorem states that the unit ball is relatively compact if and only if the vector space has finite dimension. The fact that L2(Sd-1) is an infinite dimensional space contradicts this. Therefore the inverse of H cannot be continuous. In order to overcome this problem, we use a one parameter family of regularized inverses that are continuous and converge to the inverse when", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3478e146-d55f-4b21-b054-3dfc060ed083": {"__data__": {"id_": "3478e146-d55f-4b21-b054-3dfc060ed083", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b55d0c87-2b99-4ea2-8be7-afabaa0adf0e", "node_type": "4", "metadata": {}, "hash": "c16a490174fb8177d21e702081c64031adf26bfb3b14bb0a06eecb293690dd9d", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\nThe parameter goes to infinity. This is a common approach to ill-posed inverse problems in statistics (see, e.g. Carrasco et al., 2007).\n\nDue to the particular form of its kernel that involves the scalar product xprimeb, the operator H is an analogue of convolution in Rd, as illustrated in a simple example in Section A.1.1 of Supplemental Appendix. This analogy provides a clear insight into the identification issue. In particular, our problem is closely related to the so-called boxcar deconvolution (see, e.g. Groeneboom and Jongbloed (2003) and Johnstone and Raimondo (2004)), where identifiability is often a significant problem. The connection with deconvolution is also useful in deriving an estimator based on a series expansion on the Fourier basis on S1 or its extension to higher dimensional spheres called Fourier-Laplace series. These bases are defined via the Laplacian on the sphere, and they diagonalize the operator H on L2(Sd-1). Such techniques are used in Healy and Kim (1996) for nonparametric empirical Bayes estimation in the case of the sphere S2. The kernel of the integral operator H, however, does not satisfy the assumptions made by Healy and Kim. Unlike Healy and Kim (1996), we make use of so-called \"condensed\" harmonic expansions. The approach replaces a full expansion on a Fourier-Laplace basis by an expansion in terms of the projections on the finite dimensional eigenspaces of the Laplacian on the sphere. This is useful since an explicit expression of the kernel of the projector is available. It enables us to work in any dimension and does not require a parametrization by hyperspherical coordinates nor the actual knowledge of an orthonormal basis. This approach, to the best of our knowledge, appears to be new in the econometrics literature.\n\nThe paper is organized as follows. Section 2 provides a practical guide for our procedure, which is easy to implement. Section 3 deals with identification while introducing basic notions used throughout the paper. We derive the convergence rates of the estimators in all the Lq spaces for q \u2208 [1, \u221e] and also prove a pointwise CLT in Section 4. Some extensions, such as estimation of marginals, treatments of models with non-random coefficients, and the case with endogenous regressors are presented in Section 5. Simulation results are reported in Section 6. Section 7 concludes. Supplemental Appendix presents analysis of a toy model, technical tools used in the main text, estimators for choice probabilities that are used to construct our density estimators, and the proofs of the main results.\n\n# 2. A Brief Guide for Practical Implementation\n\nThis section presents our basic estimation procedure when a random sample (y, tilde) generated from the model (1.1) is available. As noted in Section 1, normalize covariates data and define xi = (1, tildeprime)/|| (1, tilde xprime) || \u2208 Sd-1, i = 1, ..., N. To estimate the joint density of the random vector beta, use the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a2dd02d-062b-4add-b346-06a08f64221e": {"__data__": {"id_": "3a2dd02d-062b-4add-b346-06a08f64221e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7c35154-1998-45dd-8613-5af301ba4af1", "node_type": "4", "metadata": {}, "hash": "746879176f02a6a3f928dab901c625031679d8e3758cc72b998bc6f85ea14040", "class_name": "RelatedNodeInfo"}}, "text": "# following formula:\n\nf\u2227(b) = maxt|Sd-1|2 \u2211p=0\u221e \u03c7(2p + 1, 2TN)h(2p + 1, d)\n\u2211i=1N (2yi X\u2227(xi), mN-1)C2p+1(xi).\n\n(2.1) \u03b2 \u03bd(d)\n\nThe factors |Sd-1|, \u03c7, h and \u03bb are constants that do not depend on data and trivial to compute.\n\nThe surface area of Sd-1 is given by |Sd-1| = \u0393(d/2) where \u0393 denotes the Gamma function.\n\nThe constants h, \u03bd and \u03bb are obtained via the numerical formulas:\n\n- h(n, d) = (2n + d - 2)(n + d - 2)!\n- \u03bd(d) = n!(d - 2)!(n + d - 2)(d - 2)/2\n- \u03bb(2p + 1, d) = (d - 1)(d + 1)(d + 2p - 1)(-1)p|Sd-2|/3(2p - 1)\n\nThe function \u03c7 is defined on N \u00d7 N and used for smoothing. This is to be chosen by the user: see Proposition A.3 as well as the numerical example reported in Section 6 for examples of \u03c7. The truncation parameter TN needs to be chosen so that it grows with the sample size with a sufficiently slow rate. The trimming factor mN is also user-defined, and it is chosen so that it goes to zero as the sample size increases.\n\nThe notation Cn\u03bd(\u00b7) signifies the Gegenbauer polynomial; they, for example, correspond to the Chebyshev polynomials of the first kind in the case of one random slope (i.e. the case with d = 2).\n\nThe only remaining factor which needs to be calculated in the above formula is the nonparametric density estimator X\u2227 for fX on Sd-1. For example, the following nonparametric estimator can be used:\n\nf\u2227X(x) = maxt|Sd-1| \u03bd(d)(1) \u2211n=0N \u03c7(n, TN)h(n, d)\u2227 \u2211i=1N Cn(xi).\n\n(2.2)\n\nwhere TN is another truncation parameter, playing a role similar to TN.\u2227\n\nOur estimator \u03b2\u2227 requires neither numerical integration nor optimization. This is a clear advantage over existing estimators for random coefficient binary choice models, including many parametric estimators. This is our main proposal, on which the rest of the paper focuses. In Section 4 we explain how the formula (2.1) is derived, and investigate its asymptotic properties.\n\nThe Gegenbauer polynomials are given by:\n\nCn\u03bd(t) = \u2211l=0[n/2] (-1)l\u03bdn-l(2t)n-2l/(l!(n - 2l)!)\n\nwhere (a)0 = 1 and for n in N \\ {0}, (a)n = a(a + 1)(a + n - 1) = \u0393(a + n)/\u0393(a). See Section A.1.2 for further properties of the Gegenbauer polynomials.\n\nWhen d = 2, the following relations can be used in (2.1) and (2.2):\n\n|Sd-1|h(2p + 1, 2)C20(1) = (-1)p(2p + 1)C0p+1(xi) cos((2p + 1) arccos(xi)), p \u2265 0,\n\n\u03bb(2p + 1, 2)C20(1) = 1 - cos(n arccos(xi))C0n(1)\u03c0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ac3f944-8d52-4b76-bc05-813c301ce072": {"__data__": {"id_": "9ac3f944-8d52-4b76-bc05-813c301ce072", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fccbbd9-0eed-4502-8b00-ee971fadb32a", "node_type": "4", "metadata": {}, "hash": "4eaafebadacfeff653f3cbaea4ded7c678d5b24cf814d9365023bc56578ea39f", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\n# 3. Identification Analysis\n\nIn this section we address the following two questions:\n\n1. (Q1) Under what conditions is fbeta identified?\n2. (Q2) Does the random coefficients model impose restrictions?\n\nTo answer these questions it is useful to introduce the notion of the odd and even part of a function defined on the sphere.\n\n# Definition 3.1.\n\nWe denote the odd part and the even part of a function f by\n\nfminus(b) = (f(b) \u2212 f(\u2212b))/2\n\nand\n\nfplus(b) = (f(b) + f(\u2212b))/2,\n\nrespectively, for every b in Sd\u22121.\n\nLet us start with the question (Q1). As noted in Section A.1.4, operating H reduces the even part of a function to a constant 1 and therefore it is impossible to recover fbeta from the knowledge of r, which is what observations offer. Our identification strategy is therefore as follows:\n\n1. (Step 1) Assume conditions that guarantee the identification of fbetaminus; then\n2. (Step 2) Show that fbeta is uniquely determined from fbetaminus under a reasonable assumption.\n\nWe first consider Step 1. Define Hplus = H(n) = {x \u2208 Sd\u22121 : xn \u2265 0}, where n = (1, 0, ..., 0), that is, the northern hemisphere of Sd\u22121. For later use, also define its southern hemisphere Hminus = H(\u2212n). Since the model we consider has a constant as the first element of the covariate vector before normalization, the same vector after normalization is necessarily an element of Hplus. We make the following assumption, which also appears in Ichimura and Thompson (1998), and show that it achieves Step 1.\n\n# Assumption 3.1.\n\nThe support of X is Hplus.\n\nThis assumption demands that tilde, the vector of non-constant covariates in the original scale, is supported on the whole space Rd\u22121. It rules out discrete or bounded covariates; see Section 5 for a potential approach to deal with regressors with limited support. In what follows we assume that the law of X is absolutely continuous with respect to \u03c3 and denote its density by fX. Step 1 of our identification argument is to show that the knowledge of r(x) on Hplus, which is available under Assumption 3.1, identifies fbetaminus. The problem at hand calls for solving r = Hfbeta = 21 + Hfbetaminus for fbetaminus, and the inversion formula derived in (4.1) is potentially useful for the purpose. A direct application.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e357e088-670c-4842-b540-903fd4293b87": {"__data__": {"id_": "e357e088-670c-4842-b540-903fd4293b87", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de2b33cb-3dae-47db-9394-f5866ace0cca", "node_type": "4", "metadata": {}, "hash": "83cd132bd4cf8ca40037d9b6dbeced312f8ba52ffb5c8ff01a98e44e1ff7236c", "class_name": "RelatedNodeInfo"}}, "text": "# 7\n\nof the formula to r is inappropriate, however, since it requires integration of r on the whole sphere Sd-1, but r is defined only on H+ even when X has full support on Rd-1. An appropriate extension of tilde r(x), x element H+ to the entire Sd-1 is in order. Using the random coefficients model (1.1) and Assumption 1.1, then noting that fbeta is a probability density function, conclude\n\n(3.1) H(fbeta)(\u2212x) = for x in H+. This suggests an extension R of r to Sd-1 as follows:\n\n(3.2) x element H+, R(x) = r(x), and x element H-, R(x) = 1 \u2212 r(\u2212x) = 1 \u2212 R(\u2212x).\n\nintegraldisplay H(\u2212x)fbeta(b)d\u03c3(b) = 1 \u2212 H(fbeta)(x) = 1 \u2212 r(x)\n\nThe function R is well-defined on the whole sphere under Assumption 3.1. Later we derive a formula for fbeta in terms of R(x), x element Sd-1, which shows the identifiability of fbeta\u2212 under Assumption 3.1.\n\nNote that\n\n(3.3) R(x) = R+(x) + R\u2212(x) = 1[R(x) + R(\u2212x)] + R\u2212(x)2 = 1[R(x) + (1 \u2212 R(x))] + R\u2212(x)2 by (3.2) = 12 + R\u2212(x)\n\nthus R is completely determined by its odd part and therefore,\n\nR(x) = 1/2 \u2212 H(fbeta or\n\n(3.4) R\u2212 = Hfbeta\u2212.\n\nWe can invert this equation to obtain fbeta\u2212.\n\nNow we turn to Step 2 in our identification argument. Obviously fbeta does not uniquely determine fbeta without further assumptions. This is a fundamental identification problem in our model. We need to identify fbeta from the choice probability function r, but we can choose an appropriate even function g so that fbeta + g is a legitimate density function (see the proof of Proposition 3.1 for such a construction). Then r = H(fbeta + g), and the knowledge of r identifies fbeta only up to such a function g. Ichimura and Thompson (1998, Theorem 1) give a set of conditions that imply the identification of the model (1.1). One of their assumptions postulates that there exists c on Sd-1 such that P(cbeta > 0) = 1.prime This, in our terminology, means that:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb060969-cb55-4f3c-9cd6-bba0a7ae6583": {"__data__": {"id_": "eb060969-cb55-4f3c-9cd6-bba0a7ae6583", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9daf3b4-5159-409c-a2ce-807ba0233721", "node_type": "4", "metadata": {}, "hash": "b9e9b44ba240a3ebc1d0992db3e4686979ff73d663fd12f81ebd9749b75872e0", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\n# Assumption 3.2\n\nThe support of beta is a subset of some hemisphere.\n\nAs noted by Ichimura and Thompson (1998), Assumption 3.2 does not seem too stringent in many economic applications. It is often reasonable to assume that an element of the random coefficients vector, such as a price coefficient, has a known sign. If the j-th element of beta has a known sign (and positive), then Assumption 3.2 holds with c being a unit vector with its j-th element being 1. This is a case in which the location of the hemisphere in Assumption 3.2 is known a priori, though the knowledge about its location is not necessary for identification. Assumption 3.2 implies the following mapping from f beta to fbeta developed in (A.24): minus\n\n(3.5) fbeta(b) = 2f beta minus(b) IbraceleftBig f beta minus(b) > 0 bracerightBig.\n\nThis is useful because it shows that Assumption 3.2 guarantees identification if f betaminus is identified. Moreover, it will be used in the next section to develop a key formula that leads to a simple and practical estimator for fbeta that is guaranteed to be non-negative.\n\n# Remark 3.1\n\nAssumption 3.2 is testable since it imposes restrictions on f beta minus, which is identified under weak conditions. For example, for values of b with f beta minus(b) > 0, f beta minus(\u2212b) < 0 must hold. Or, it implies that f betaminus integrates to 1/(2|Sdminus1|) on a hemisphere H(x) for some x, and \u22121/(2|Sdminus1|) on the other H(\u2212x).\n\nThe subsequent result, Proposition 3.1, answers question (Q2), and a proof is given in Supplemental Appendix.\n\n# Notation\n\nWe use the notation L2(Sdminus1) for the space of square integrable complex valued functions equipped with the hermitian product (f, g)L2 (Sdminus1) = integraltextSdminus1 f (x)g(x)d\u03c3(x), and more generally use Lp(Sdminus1) for p element [1, infinity] the Banach space of p-integrable functions and bardbl periodcentered bardblp the corresponding norm.\n\nWe also use the notation Ws(Sdminus1) (and Hs(Sdminus1) for p = 2) to signify the corresponding Sobolev spaces with norm bardbl periodcentered bardblp, defined as bardblf bardblp, = bardblf bardblp + vextenddoubleparenleftbig\u2212DeltaS parenrightbig ss/2 fvextenddoublepvextenddouble vextenddouble vextenddouble vextenddouble where DeltaS denotes the Laplacian on the Sphere Sdminus1: See Section A.1.3 for further discussions.\n\n# Proposition 3.1\n\nA [0, 1]-valued function r is compatible with the random coefficients model (1.1) with fbeta in L2(Sdminus1) and Assumption 1.1 if and only if r is homogeneous of degree 0 and its extension R according to (3.2) belongs to Hdslash2(Sdminus1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57284202-cce4-4812-b82f-1a190f444061": {"__data__": {"id_": "57284202-cce4-4812-b82f-1a190f444061", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14d8a03a-2214-4539-9c5b-fd6b58968c9a", "node_type": "4", "metadata": {}, "hash": "86c386183b20baac7f3718fdf3e70e5ed5a83256848fb9442bba62c734311fab", "class_name": "RelatedNodeInfo"}}, "text": "# 9\n\nThe global smoothness assumption that R belongs to *Hdslash2(Sdminus1) imposes substantial restriction on the property of observables, that is, the behavior of the choice probability function r. Note that the smoothness condition in this proposition is stated in terms of R, and even if the choice probability function r is sufficiently smooth on the support of X, which is H+, it is not necessarily consistent with the random coefficients binary choice model (1.1) unless its extension is smooth globally on Sdminus1. In particular, the Sobolev embedding of Hs(Sdminus1) into the space of continuous functions for s > (d \u2212 1)/2 implies that if the extension R is in Hdslash2(Sdminus1), it has to be continuous on Sdminus1*. This, in turn, means that the corresponding r has to satisfy certain matching conditions at a boundary point x of H+ (i.e. xprimen = 0) and its opposite point \u2212x.\n\n# 4. Nonparametric Estimation of fbeta\n\n# 4.1. Derivation of the closed form estimation formula\n\nThis section discusses how the closed form estimation formula (2.1) is derived. Suppose an odd function *f minus defined on Sdminus1 satisfies an integral equation f minus = Hg* with g square integrable with respect to the spherical measure. In Section A.1.4 we show that the solution to this equation is given by:\n\n*Hminus1(f minus)(y) = \u2211p=0infinity 1 / lambda(2p + 1, d) integraldisplay Sdminus1q2p+1(x, y)f minus(x)d\u03c3(x)*\n\nwhere expressions for lambda and q are provided in Proposition A.4 and Theorem A.1, respectively. If an appropriate estimator *circumflexRminus of Rminus is available, an application of the inversion formula (4.1) to (3.4) suggests the following estimator for f beta minus*:\n\n*f betacircumflexminus = Hminus1(circumflexRminus) = \u2211p=0infinity 1 / lambda(2p + 1, d) integraldisplay Sdminus1q2p+1(periodcentered, x) Rminus(x)d\u03c3(x).circumflex*\n\nThen use the mapping (3.5) to define\n\n*fcircumflexbeta(b) = 2 fminus(b)Icircumflex beta {circumflex beta fminus(b) > 0}*\n\nas an estimator for *fbeta*.\n\nWe use the following notation in the rest of the paper:\n\n**Notation.** For two sequences of positive numbers *(an)n\u2208N and (bn)n\u2208N, we write an equivasymptotic bn when there exists a positive M such that M-1bn \u2264 an \u2264 M bn* for every positive n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2254, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0584527-9447-4c4d-84c9-4c2665f997a1": {"__data__": {"id_": "c0584527-9447-4c4d-84c9-4c2665f997a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a156122-9e72-40f0-862b-a4301d2b2da7", "node_type": "4", "metadata": {}, "hash": "1419a3f24b3a30837ff9bb21bf9cd2133e6daeabddff28cc936f6767f743b0f5", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\n# Proposition A.6\n\nProposition A.6 implies that if \\( f_{-} - f_{\\beta} \\in H^s(S^{d-1}) \\) then \\( R_{-} - R_{-} \\in H^{\\sigma}(S^{d-1}) \\), \\( \\sigma = s + 2 \\) and \\( \\hat{\\beta} - \\hat{d} \\) for \\( v \\in [0, s] \\, , \\)\n\n(4.4) \\(\\| \\hat{f}_{\\beta} - f_{\\beta} \\|^2_{L^2} \\sim \\| R_{-} - R_{-} \\|^2_{L^2} + d / 2 \\cdot \\hat{d}\\)\n\nAs discussed earlier, the estimation of \\( f_{\\beta} \\) is related to deconvolution in \\( S^{d-1} \\), and the degree of ill-posedness in our model is \\( d/2 \\), which is indeed the rate at which the absolute values of the eigenvalues of \\( H \\) (c.f. Proposition A.4) \\( \\lambda(n, d) \\), \\( n = 2p + 1 \\), \\( p \\in \\mathbb{N} \\) converges to zero as \\( p \\) grows, as shown in (A.27).\n\nExisting results for deconvolution problems (see, for example, Fan, 1991 and Kim and Koo, 2000) then suggest that we should be able to estimate \\( f_{\\beta} \\) at the rate \\( N^{-2s + 2d - 1} \\) in \\( L^2(S^{d-1}) \\) provided that \\( f_{\\beta} \\in H^s(S^{d-1}) \\). The relationship (4.4), evaluated at \\( v = 0 \\), implies that this can be achieved if \\( \\sigma - 2 - 1 \\) in the \\( \\| \\cdot \\|_{L^2} \\) norm. The latter is the usual nonparametric rate for estimation of densities on \\( d - 1 \\) dimensional smooth submanifolds of \\( \\mathbb{R}^d \\) (see, for example, Hendriks, 1990).\n\nThe estimation formula given in (4.2) is natural and reasonable, though it typically requires numerical evaluation of integrals to implement it. Moreover, in practice one needs to evaluate the infinite sum in (4.2), for example, by truncating the series. This results in a general estimator that can be written in the following two equivalent forms:\n\n(4.5) \\(\\hat{f}_{\\beta} = H^{-1} \\left( \\tilde{P}_{N} R_{-} \\right)\\)\n\n\\( T \\sum_{T_{N}} \\frac{1}{\\lambda(2p + 1, d)} \\int_{S^{d-1}} q^{2p + 1} \\, \\text{(period centered, } x) R_{-}(x) d\\sigma(x) \\)\n\nfor suitably chosen \\( \\tilde{N} \\) that goes to infinity with \\( N \\) and \\( P_{\\tilde{N}} \\) defined in (A.20). The sequence \\( H^{-1} \\bullet T P_{\\tilde{N}}, N = 1, 2, \\ldots \\) can be interpreted as regularized inverses of \\( H \\), with the spectral cut-off method often used in statistical inverse problems.\n\nWe now discuss how to obtain \\( \\hat{R}_{-} \\) in the calculation of (4.5). The following choice is particularly convenient:\n\n(4.6) \\(\\hat{R}_{-}(x) = \\frac{1}{N} \\sum_{i=1}^{N} (2y_{i} - 1) K_{2}^{-T_{N}}(x, x)\\)\n\nwhere \\( m_{N} \\) is a trimming factor going to 0 with the sample size, \\( K(x, \\text{(period centered)}) \\) denotes the odd part (of the second argument) of the kernel function \\( K(x_{i}, \\text{(period centered)}) \\) defined in (A.23) and \\( \\hat{f}_{X} \\) is a nonparametric density estimator for \\( f_{X} \\). See Section A.1.5 of Supplemental Appendix for the derivation of the above formula. Various nonparametric estimators for \\( f_{X} \\) can be used in (4.6), since estimation of densities.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd64f525-e6d3-4968-84e7-fc957da12524": {"__data__": {"id_": "bd64f525-e6d3-4968-84e7-fc957da12524", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd65fa12-aa34-45ce-ab9b-71ece6f70a7d", "node_type": "4", "metadata": {}, "hash": "e8e0b8b9b9a20b3c19ecc85f1ad4737cf0fee70034e740353c83f4c9e884b342", "class_name": "RelatedNodeInfo"}}, "text": "# 11\n\nOn compact manifolds have been studied by several authors, using histogram (Ruymgaart (1989)), projection estimators (see, e.g. Devroye and Gyor\ufb01 (1985) for the circle and Hendriks (1990) for general compact Riemannian manifolds) or kernel estimators (see, e.g. Devroye and Gyor\ufb01 (1985) for the case of the circle, and Hall et al. (1987) and Klemeldieresisa (2000) for higher dimensional spheres). Note also that Baldi et al. (2009) develops an adaptive density estimator on the sphere using needlet thresholding. In the simulation experiment we use\n\n(4.7) fcircumflexX (x) = max\n\n(\u2211i=1N KTN (xi, x), 0)\n\nfor a suitably chosen TNprime that depends on the sample size and the smoothness of fX and KTNprime is a kernel of the form (A.23) satisfying Assumption A.1. Note that its rate of convergence in sup-norm can be obtained in the same manner as the proof of Theorem 4.1. This estimator is in the spirit of the projection estimators of Hendriks (1990), but here we are able to derive a closed form using the condensed harmonic expansions together with the Addition Formula. Note also that KTN is a smoothed projection kernel (note the factor chi in (A.23)), which is used here in order to have good approximation properties in the Lq(Sd-1) norms with arbitrary q element [1, infinity], in particular in the Linfinity(Sd-1) norm.\n\nUsing (4.5) and (4.6) with TtildeN = 2TN, define\n\nfbeta = H-1(circumflex X) = H-1(bTN1 \u2211i=1N (2yi \u2212 1)K2TN (xi, period centered))\n\nmax (fcircumflexX (xi), mN)\n\nComputing fbeta circumflex- is straightforward. First, note that the estimator (4.6) for R- resides in a finite dimensional space circle plus text TNp=0 H2p+1, therefore P2TN circumflexR- = R- holds. Consequently, unlike in (4.5) where a general estimator for R- is considered, we do not need to apply any additional series truncation to R- prior to the inversion of H. Second, the estimator requires no numerical integration. To see this, note the formula\n\nH-1(K2TN (xi, period centered))(b) = TN-1 \u2211p=0TN-1 chi(2p + 1, 2TN)q2p+1\n\n(xi, b),\n\nwhich follows from\n\n\u222bSd-1 q2p+1(x, b)K2-TN (x, xi)d\u03c3(x) = \u222bSd-1 q2p+1(x, b)chi(2pprime + 1, 2TN)q2p+1(x, xi)d\u03c3(x)prime\n\n= chi(2p + 1, 2TN)q2p+1(b, xi).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee471490-debd-45e2-9bf0-3462f2620c33": {"__data__": {"id_": "ee471490-debd-45e2-9bf0-3462f2620c33", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6e5bf31-41eb-43de-a6b6-746fc8d52230", "node_type": "4", "metadata": {}, "hash": "56b14f7c7ff6b021881725526460734d05e17c5849bc1227a4bfbe22e49dd73e", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\nwhich, in turn, can be seen by the definition of KT in (A.23), the fact that the integral operators with q as kernels are projections and (A.16). Thus\n\n1\n\u2211N\n2yi - 1\nTN - 1\n\u2211\nchi(2p + 1, 2TN)q2p+1commad(xi, b).\n\nf\u03b2(b) = N\nmax\n\u239b\u239dX\u2227(xi), mN\u239e\u23a0\np=0\n\u03bb(2p + 1, d)\n\nUsing (4.3) and the Addition formula (Theorem A.1), we arrive at an estimator for f\u03b2 with the following explicit form:\n\n(4.8)\nf\u03b2(b) = 2\u03b2f\u2212(b)I{f\u03b2(b) > 0},\nTN - 1\n\u239b\u239dtp N nu(d)\u239e\u23a0\n\nwhere\nf\u03b2(b) = |Sd-1| \u2211 chi(2p + 1, 2TN)h(2p + 1, d)nu(d)\n\u239b\u239dTN \u2211 (2yi - 1)C2p+1(xi)\u239e\u23a0\n\nmax\u239b\u239dX\u2227(xi), mN'\u239e\u23a0.\n\np=0\n\u03bb(2p + 1, d)C2p+1(1)\ni=1\nf\n\nThis is equivalent to the formula (2.1) previously presented in Section 2. Likewise, using the definition of the smoothing kernel (A.23) and the Addition Theorem in the above definition (4.7) of f\u2227X, we obtain the formula (2.2) as well.\n\n# 4.2. Rates of Convergence in Lq(Sd-1)-norms.\n\nNow we analyze the rate of our estimator \u03b2. The following assumption is weak and reasonable.\n\n# Assumption 4.1.\n\nfX \u2208 L\u221e.\n\nThe proofs of the following theorems and corollaries in the rest of this section are given in Section A.1.6 of Supplemental Appendix.\n\n# Theorem 4.1 (Upper bounds in Lq(Sd-1)).\n\nSuppose Assumptions A.1, 3.1 and 4.1 hold, and choose TN that does not grow more than polynomially fast in N. If f\u03b2 belongs to Wq and s > 0, and\n\n\u2211i=1 N fX(xi) - fX(xi) = Op(mN),\n\n(4.9)\nmax\nvi\n\nthen, for any 1 \u2264 r \u2264 q,\n\n||f\u2227\u03b2 - f\u03b2||q = Op(mN-1N-1/2TN(2d-1)/2(log N)(1/2 - 1/q))\n\n{q \u2265 2}\n\n\u2212 d/2 mN-2 \u2211i=1 N fX(xi) - ||f|| + TN + TN max ||f||\n\n(4.10)\nTNd/2 + (d-1)(1 - 1/r)\u03c3(fX < mN)1/q - 1/r + 1\n\ns(Sd-1) with q in [1, \u221e]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1606, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "546c059c-6c52-4502-859a-4b00394f2b82": {"__data__": {"id_": "546c059c-6c52-4502-859a-4b00394f2b82", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c58e43f-b0c4-4fca-b572-806b3bed2976", "node_type": "4", "metadata": {}, "hash": "2b4dde41a0ea4c8b6f21a53346ffb53a846631d00744a44306556a4bf11e7b12", "class_name": "RelatedNodeInfo"}}, "text": "# 13\n\nWhen there exists m &gt; 0 such that fX &ge; m \u03c3 a.e. on H+, the following holds for the estimator without the trimming factor (i.e. mN = 0) when the estimator &circ;X which is consistent in sup norm:\n\nvextenddoublecircumflex\u03b2 \u2212 f\u03b2vextenddouble vextenddouble vextenddouble q = Op(N-1/2TN(2d-1)/2(log N)(1/2 - 1/q)I{q &ge; 2} + TN-s + TNd/2 max vsinglefX(xi) \u2212 fcircumflexX(xi)vsingle)vsingle i=1,...,N\n\nThe first term in (4.10) is the stochastic error, the second term is the approximation bias, the third the plug-in error and the fourth the trimming bias. Note that Theorem 4.1 imposes the mild assumption (4.9); otherwise, we need to replace TNvsingle(d/2mN-2maxi=1,...,NvsinglefX(xi) \u2212 vsingleTNd/2mN-2maxi=1,...,NvsinglefX(xi) \u2212 vsinglefcircumflexX(xi)vsingle1 + (log N)(1/2 - 1/q)I{q &ge; 2}N-1/2T(d-1)/2N vsinglefX(xi) \u2212 fcircumflexX(xi)vsingle \u2264 vsinglefX \u2212 fcircumflexXvsingle i=1,...,N max vsinglef vsinglefinfinity, this term can be made of order OP(NN-v/(2v+d-1)log when fX \u2208 Winfinity with a suitably chosen parameter TN if we take (4.7) as an estimator.\n\nThe proof of the latter statement is classical and can be obtained simplifying the proof of Theorem 4.1 and Corollary 4.1. Equation (4.10) yields that, for proper choices of mN going to zero and TN to infinity, fcircumflex\u03b2 is consistent given that fX has some smoothness in the Sobolev scales.\n\nThough the additional condition of fX being bounded away from 0 in the last statement of Theorem 4.1 is convenient, it is restrictive. To see this, consider the d = 2 case. In polar coordinates, fX(cos(\u03b8), sin(\u03b8)) = ftildeX(tan(\u03b8))(1 + tan2(\u03b8)), thus, assuming fX &ge; m on H+, which does not require trimming, yields x \u2208 R, ftildeX(x) &ge; 1 + x2m.\n\nIt implies that ftildeX has tails larger than Cauchy tails and all moments are infinite. The introduction of the trimming factor mN allows us to relax the assumption fX &ge; m, though it introduces bias. As is clear from (4.10), the condition for the trimming bias to go to zero with N depends both on TN and mN. The quantity \u03c3(fX < mN) should decay to zero with N sufficiently fast. We can check, for example, that when ftildeX is standard Gaussian then \u03c3(fX < mN) = O(\u2212 log mN)-1/2, when it is Laplace then \u03c3(fX < mN) = O(\u2212 log mN)-1 and when ftildeX is proportional to (1 + x2)-k with k &gt; 1 we obtain that \u03c3(fX < mN) = O(mN1/(2(k-1)). In all these cases, it is possible to adjust adequately TN and mN and to obtain rates of convergence. The upper bound on the rates become slower as the tail of fX becomes thinner.\n\nfcircumflexX(xi) in (4.10) with.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0467414e-4fc9-40ec-9dfd-c00b29530a63": {"__data__": {"id_": "0467414e-4fc9-40ec-9dfd-c00b29530a63", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2edb74fd-43a3-402c-8445-7cc985047c9c", "node_type": "4", "metadata": {}, "hash": "ba0af43d98285bb9d0096db17cfb95d90df98c843f71ba948b0a4657a7a30ed5", "class_name": "RelatedNodeInfo"}}, "text": "# 14 GAUTIER AND KITAMURA\n\nNonparametric estimation of the regression function with random degenerate design, in the sense that the density of regressors can be low on its support, is a difficult issue. It has been studied for the pointwise risk in Hall et al. 1997, Gadieresisdotlessiffas 2005, Gadieresisdotlessiffas 2009 and Guerre 1999. Extension to inverse problems setting is a widely open problem. We tackle this problem for our specific inverse problem. Future research includes the study of lower bounds from the minimax point of view that account for the degeneracy of the design.\n\nLet us now return to the general case of d \u2212 1 regressors. us to obtain rates that differ slightly from the rates that we would obtain in the ideal case where fX \u2265 m \u03c3 a.e. for positive m on H+.\n\n# Assumption 4.2\n\nSuppose for q in [1, infinity], there exist positive tau and rX such that\n\n- (i) \u03c3(fX < h) = O(htau) and fX \u2208 L\u221e, and either\n- (ii) \u02c6X(xi) extendsingle = O((log N)(1 - 2/q)) {q \u2265 2} - rX\n- (iii) for some constant C,\nlim N \u2192 \u221e (log N)(1 - 2/q) {q \u2265 2} i=1, ..., N fX(xi) - fmax \u02c6X(xi) \u2264 Cv a.s.\n\nholds.\n\nAs seen before, Assumption 4.2 (ii) or (iii) are very mild. (i) holds for a reasonable class of distributions for fX. In the above example where f tilde is proportional to (1 + x\u00b2)^{-k} with k > 1, we have the relation tau = \u03c1/(2(k \u2212 1)). This allows for a higher order moment to exist for a large k.\n\n# Corollary 4.1\n\nAssume that f \u03b2 - belongs to Wq s(Sd\u22121) with q in [1, infinity] and s > 0. Let assumptions A.1, 3.1, 4.1 and 4.2 (i) and (ii) hold, and take\n\nmN \u2248 (log N)(1 - 2/q) {q \u2265 2}, TN \u2248 (log N)(1 - 2/q) {q \u2265 2}\n\nwhere \u03c1 yields a maximum \u03b3 of\n\n\u03b3(\u03c1)min(1 - 2\u03c1, 2s + d + 2(d - 1)(1 - 1/q), 2rX - 4\u03c1, 2\u03c1\u03c4)\n\u03b3(\u03c1) = min(2s + 2d - 1, 2s + d, d - 1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11fc4d0b-0562-4d8d-8d39-252ca1c001be": {"__data__": {"id_": "11fc4d0b-0562-4d8d-8d39-252ca1c001be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8451ab4b-8d01-4a34-9d4c-411736de8603", "node_type": "4", "metadata": {}, "hash": "dedfce62f143964dcfb15f3e8e2f0484e23cb0557f6abb99c6149eb204981d66", "class_name": "RelatedNodeInfo"}}, "text": "# 15\n\nWe then have\n\nvextenddouble\nvextenddoublefbeta \u2212 fbetavextenddouble\nparenleftBiggparenleftbigg\nparenrightbiggminusgammasparenrightBigg\n\n(4.11)\n\nvextenddouble circumflex\nvextenddouble\nvextenddoubleq = Op\nN\n\n(log N )(1minus2slashq)Ibraceleftqgreaterequal2braceright\n\nMoreover, if, instead of Assumption 4.2 (ii), Assumption 4.2 (iii) holds with q = infinity, then there exists a constant C such that\n\nparenleftbigg N parenrightbigggammas vextenddoublevextenddoublefbeta \u2212 fbetavextenddouble\n\n(4.12)\n\nlimN arrowrightinfinity vextenddouble circumflex vextenddouble vextenddoubleinfinity lessequal C a.s.\n\nlog N\n\nThe rate gammas in Corollary 4.1 accounts for the dimension d \u2212 1, the degree of smoothing d/2 of the operator and features of the density of the covariates (i.e. its smoothness and tail behavior).\n\nWe now make stronger assumptions on fX and its estimate that yield, up to a logarithmic term, the convergence rate N minus2s+2sdminus1 which is logarithmic in N : mN = (log N )minusrho for some positive rho.\n\nWe need to be able to trim the estimate of fX with a term\n\n# Assumption 4.3.\n\nSuppose for q in [1, infinity], and positive rsigma and rX,\n\n- (i) \u03c3(fX < (log N )minusrho) = OparenleftbiggparenleftBig(log N )2rho+(1 parenrightBigminusrsigmaparenrightbigg Nminus2slashq)Ibraceleftqgreaterequal2braceright,\n- and either\n- (ii) parenleftBigg parenrightbiggminusrXparenrightBigg max vextendsingle vextendsinglefX (xi) \u2212 circumflex vextendsingle fX (xi)vextendsingle = Opvextendsingle vextendsingle (log N )minus2rhoparenleftbigg (log N )2rho+(1minus2slashq)Ibraceleftqgreaterequal2bracerightN i=1commaperiodperiodperiodcommaN\n- or,\n- (iii) for some constant C, parenleftbigg N parenrightbiggrX vextendsingle vextendsingle limN arrowrightinfinity(log N )2rho (log N )2rho+(1minus2slashq)Ibraceleftqgreaterequal2braceright i=1commaperiodperiodperiodcommaNmaxvextendsinglefX (xi) \u2212 fX (xi)vextendsingle lessequal C vextendsingle circumflex vextendsingle a.s.\n\n# Corollary 4.2.\n\nAssume that f betaminus belongs to Wq s(Sdminus1) with q in [1, infinity] and s > 0. Let assumptions A.1, 3.1, 4.1 and 4.3 (i)-(ii), hold, and take\n\nparenleftbigg N parenrightbigggamma\nTN equivasymptotic (log N )2rho+(1minus2slashq)Ibraceleftqgreaterequal2braceright\n\nwhere\n\nparenleftbigg\ngamma = min 2s + 2d \u2212 1 ,1 2s + d + 2(d \u2212 1)(1 \u2212 1/q) ,2rsigma 2rX 2s + d\nparenrightbigg\n\nthen we have\n\nvextenddouble\nvextenddoublefbeta \u2212 fbetavextenddouble\nparenleftBiggparenleftbigg parenrightbiggminusgammasparenrightBigg\n\n(4.13)\n\nvextenddouble circumflex\nvextenddouble\nvextenddoubleq = Op N .\n\n(log N )2rho+(1minus2slashq)Ibraceleftqgreaterequal2braceright", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f20a9eb-ab77-48e2-b324-e350c9fcfff1": {"__data__": {"id_": "2f20a9eb-ab77-48e2-b324-e350c9fcfff1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80285fa3-ae7f-47d6-813d-9d6175f53962", "node_type": "4", "metadata": {}, "hash": "931452753203d5c15e75889009824f964fdc473ed218cacd99034d85af289c16", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\nMoreover, if, instead of Assumption 4.3 (ii), Assumption (iii) holds with q = infinity, then there exists a constant C such that\n\nlimN &rarr; &infin;\n(N &gamma;)\nvextendvextend - f&beta;vextend\n&le; C a.s.\n(log N)2&rho;+1\n\nWhen fX &isin; W&infin;(s + d/2 + &epsilon;1(Sd-1)) for any positive &epsilon;1, then\n\n2s + 2rX &gt; 2s + 2(d - 1) and &gamma; in Corollary 4.2 is simply min(2s + 2(d - 1), 2s + d + 2(d - 1)(1 - 1/q)2r&sigma;).\n\nRecall that the smoothness s + d/2 is related to the smoothness of R. Indeed, we have seen in Section 3 that R &isin; W2s + d/2(Sd-1) if and only if f&beta; &isin; W2s(Sd-1).\n\nConsider now the most restrictive case where fX &ge; m &sigma; a.e., then the estimator without the trimming factor (i.e. mN = 0) satisfies the following:\n\n# Corollary 4.3\n\nAssume that f&beta; - &lt; belongs to Wq(s)(Sd-1) with q in [1, &infin;] and s &gt; 0. Let assumptions A.1, 3.1 and 4.1 hold, and suppose, for positive rX,\n\nmax\n|fX(xi) - &circ;fX(xi)| = Op(log N)(1 - 2/q)I{q &ge; 2}.\n\nTake\n\n&gamma; = min(2s + 2d - 1, 1, 2rX / (2s + d))\n\nthen we have\n\n|&circ;f&beta; - f&beta;| = Op(log N)(1 - 2/q)I{q &ge; 2}.\n\nMoreover, if we replace (4.15) by for some positive C\n\nmax |fX(xi) - fX(xi)| &le; C\n\nthen\n\nlimN &rarr; &infin;\n(N &gamma;)\n|f&beta; - f&beta;| &le; C a.s.\nlog N\n\nWhen fX belongs to Ws - d/2 + &epsilon;1, for arbitrary positive &epsilon;1, &gamma; = &infin; 2s + 2(d - 1) in Corollary 4.3, and we recover the L2 convergence rate of N2s + 2d - 1, the rate mentioned in Section 4.1. It is in accordance with the L2 rate in Healy and Kim (1996) who study deconvolution on S2 for non-degenerate kernels. Kim and Koo (2000) prove that the rate in Healy and Kim (1996) is optimal in the minimax sense. Their", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8960c3e1-cde1-47f0-9a23-dcd6abf5e25b": {"__data__": {"id_": "8960c3e1-cde1-47f0-9a23-dcd6abf5e25b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7440c110-a5b0-4273-afef-b28801e2dc63", "node_type": "4", "metadata": {}, "hash": "542b656533ef5e5f1093cea0f4d01388bfc7c597d26810bd7201e4a5a904b298", "class_name": "RelatedNodeInfo"}}, "text": "# 17\n\nStatistical problem, however, involves neither a plug-in method nor trimming. Also, somewhat less importantly, it does not cover the case when the convolution kernel is given by an indicator function, which appears in our operator H. Hoderlein et al. (2010) study a linear model of the form W = Xprime\u03b2 where \u03b2 is a d-vector of random coefficients. They obtain a nonparametric random coefficients density estimator that has the L2-rate N-2s+2sd-1 when fX \u2265 m\u03c3 a.e. for positive m3 when fX is assumed to be bounded from below and thus no trimming is required. They also consider trimming but the approach is slightly different and rates of convergence are not given. Unlike the previous results, we cover Lq loss for all q \u2208 [1, \u221e].\n\n# 4.3. Pointwise Asymptotic Normality\n\nThis section discusses the asymptotic normality property of our estimator.\n\n# Theorem 4.2 (Asymptotic normality)\n\nSuppose f\u03b2 belongs to W\u221e(Sd-1) with s > 0, and Assumptions A.1, 3.1 and 4.1 hold. If fcircumflexX, fX, mN and TN satisfy\n\nN1/2TN(d-1)/2mN-2 \u2211i=1N fX(xi) \u2212 fcircumflexX(xi) = op(1),\n\nmax N-1/2TN(d-1)/2mN(1+\u03b51) = o(1) for some \u03b51 > 0,\n\nN1/2TN-2s+2d-1 = o(1),\n\nN1/2TN(d-1)/2\u03c3 {fX < mN} = o(1)\n\nthen\n\nN1 2sN-1(b) (circumflex(b) \u2212 f\u03b2(b)) d N (0, 1) f\u03b2 \u2192 0 holds for b such that f\u03b2(b) \u2260 0, where sN(b) := var(ZN(b)), ZN(b) = 2 (2Y \u2212 1)H-1(K2 \u2212 TN(X, centered))(b).\n\nThe standard error sN(b) is the standard deviation of\n\nTN-1\u03c7(2p + 1, 2TN)h(2p + 1, d) \u2211p=0\u221e (2Y \u2212 1)C2p+1(Xprimeb) nu(d)\n\nZN(b) = |Sd-1| nu(d) max(fX(X), mN)\n\n\u03bb(2p + 1, d)C2p+1(1)\n\n(see equation (4.8)), which can be estimated using an estimate fcircumflexX of fX.\n\nThe next theorem is concerned with the restrictive case where the density of the covariates is bounded from below and hence the trimming factor mN is set at zero.\n\n3 Note that the dimension of their estimator is d, whereas that of ours is d - 1. On the other hand, in their problem W is observable, and it is obviously more informative than our binary outcome Y, which causes difficulties both in identification and estimation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f09e7d54-03bc-4ad6-a68a-83c81274ab42": {"__data__": {"id_": "f09e7d54-03bc-4ad6-a68a-83c81274ab42", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8002965c-6e10-4907-a270-6fd9c8097c12", "node_type": "4", "metadata": {}, "hash": "4c59a7472986b7a1d6efe57b62b1df0d569f1eb262ef80abf1db0042d1536789", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\n# Theorem 4.3 (Asymptotic normality when the density of the covariates is bounded from below).\n\nSuppose f\u03b2 belongs to W\u221e(Sd-1) with s > 0, and Assumptions A.1, 3.1 and 4.1 hold. If fX, fX andminus s circumflex TN satisfy\n\nN1/2TN - (d - 1)/2 vextendsingle vextendsingle fX(xi) - fcircumflexX(xi) vextendsingle = op(1),\n(4.25)\n\ni = 1, ..., N max vextendsingle vextendsingle\n(4.26)\n\nN-1/2TN(d - 1)/2 = o(1)\n(4.27)\n\nthen\n\nN2sN1 - 1(b) &lt;= fcircumflex(b) - f\u03b2(b) &gt; df\u03b2 &rarr; N(0, 1)\n(4.28)\n\nholds for b such that f\u03b2(b) \u2260 0, where sN(b) := var(ZN(b)), ZN(b) = 22(2Y - 1)H-1(K2TN(X, centered))(b).fX(X) -\n\nA formula for ZN for this case is obtained by replacing max(fX(X), mN) with fX(X) in (4.24).\n\n# 5. Discussion\n\n# 5.1. Estimation of Marginals.\n\nIn Section 3 we have provided an expression for the estimator of the full joint density of \u03b2, from which an estimator for a marginal density can be obtained. Let \u03c3k denote prime(\u03b2prime, \u03b2prime) the surface measure and \u03c3k = \u03c3k/|Sk| the uniform probability measure on Sk. We write \u03b2 = and wish to obtain the density of the marginal of \u03b2 which is a vector of dimension k. Also define P and P the projectors such that \u03b2 = P\u03b2 and \u03b2 = P\u03b2 and denote by P*\u03c3d-1 and P*\u03c3d-1 the direct image probability measures. One possibility is to define the marginal law of \u03b2 as the measure P*P\u03b2, where dP\u03b2 = f\u03b2d\u03c3. This may not be convenient, however, since the uniform distribution over Sd-1 would have U-shaped marginals. The U-shape becomes more pronounced as the dimension of \u03b2 increases.\n\nIn order to obtain a flat density for the marginals of the uniform joint distribution on the sphere it is enough to consider densities with respect to the dominating measure P*\u03c3d-1. Notice that sampling U uniformly on Sd-1 is equivalent to sampling U according to P*\u03c3d-1 and then given U forming \u03c1(U)V where V is a draw from the uniform distribution \u03c3d-1-k on Sd-1-k and \u03c1(U) = 1 - ||U||2. Indeed \u03c1(U) is uniformly distributed on Sd-1-k. Thus, when g is an element of L1(Sd-1) we can given U, U/\u03c1(U) write for k in {1, ..., d - 1},\n\n\u222bSd-1 g(b)d\u03c3d-1(b) = Bk \u222bSd-1-k g(\u03c1(b, u, b)) d\u03c3d-1-k(u) dP*\u03c3d-1(b)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "452565f7-75e2-4ca7-b623-15a97207f8db": {"__data__": {"id_": "452565f7-75e2-4ca7-b623-15a97207f8db", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76874a1d-d565-4127-b90b-e80cf519a911", "node_type": "4", "metadata": {}, "hash": "7ce0d15098c2f2b17976a32d05e13556754dd51d37cebbc323ffdaadf76bcc7f", "class_name": "RelatedNodeInfo"}}, "text": "# 5.2. Treatment of Non-Random Coefficients\n\nIt may be useful to develop an extension of the method described in the previous sections to models that have non-random coefficients, at least for two reasons.4 First, the convergence rate of our estimator of the joint density of beta slows down as the dimension d of beta grows, which is a manifestation of the curse of dimensionality. Treating some coefficients as fixed parameters alleviates this problem.\n\nSection 3 precludes covariates with discrete or bounded support. This may not be desirable as many random coefficient discrete choice models in economics involve dummy variables as covariates. As we shall see shortly, identification is possible in a model where the coefficients on covariates with limited support are non-random, provided that at least one of the covariates with \"large support\" has a non-random coefficient as well. More precisely, consider the model:\n\n(5.4)\n\nYi = { beta1i + beta2</primeiX2i + \u03b11Z1i + \u03b12</primeZ2i \u2265 0 }\n\nwhere beta1 \u2208 R and beta2 \u2208 RdX - 1 are random coefficients, whereas the coefficients \u03b11 \u2208 R and \u03b12 \u2208 RdZ - 1 are nonrandom. The covariate vector (Z1, Z2)prime is in RdZ, though the (dZ - 1)-subvector Z2 might have limited support: for example, it can be a vector of dummies. The covariate vector (X2prime, Z1)prime is assumed to be, among other things, continuously distributed. Normalizing the coefficients vector and the vector of covariates to be elements of the unit sphere works well for the development of our procedure, as we have seen in the previous sections. The model (5.4), however, is presented \"in the original scale\" to avoid confusion.\n\n4 Hoderlein et al. (2010) suggest a method to deal with non-random coefficients in their treatment of random coefficient linear regression models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1797, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df2c35d2-8225-4634-90d1-b9508e83f8ff": {"__data__": {"id_": "df2c35d2-8225-4634-90d1-b9508e83f8ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad79f79d-97bc-4b79-b12d-2c4725c583d1", "node_type": "4", "metadata": {}, "hash": "c148c965ca26c9e38b91cab14ead02d2599f89b797b18c4a1abb3cec43bad28f", "class_name": "RelatedNodeInfo"}}, "text": "# GAUTIER AND KITAMURA\n\nDefine beta1 asteriskmath(Z2) := beta1 + \u03b12primeZ2. We also use the notation (beta1 asteriskmath(Z2), \u03b11, beta2)prime asteriskmath(Z2), \u03b11, betaprime2)bardbl element SdX +1, W := bardbl(1, Z1, Xprime)primebardbl element SdX +1.\n\ntau (Z2) := bardbl(beta1 2\n\nThen (5.4) is equivalent to:\n\nY = Ibraceleft(beta1 asteriskmath(Z2), \u03b11, beta2)(1, Z1, Xprime)prime greaterequal 0braceright2 = I braceleftbigtau (Z2)primeW greaterequal 0bracerightbig.\n\nThis has the same form as our original model if we condition on Z2 = z2. We can then apply previous results for identification and estimation under the following assumptions. First, suppose (beta1, betaprime)prime and2 W are independent, instead of Assumption 1.1. Second, we impose some conditions on fW barZ2=z2, the conditional density of W given Z2 = z2. More specifically, suppose there exists a set Z2 propersubset RdZ minus1, such that Assumption 3.1 holds if we replace fX and d with fW barZ2=z2 and dX + 1 for all z2 element Z2. If Z2 is a vector of dummies, for example, Z2 would be a discrete set. By (A.30) and (4.1) we obtain\n\n\u2211infinity lambda(2p + 1, dX + 1)Ebracketleftbigg (2Y \u2212 1)q2p+1commadX +1(W, t)vextendsingle Z2 = z2bracketrightbiggvextendsingle vextendsingle\n\n(5.5) fminusZ2)barZ2=z2 (t) = 1 tau ( fW barZ2=z2 (W ) vextendsingle p=0\n\nfor all z2 element Z2, where the right hand side consists of observables. This determines ftau (Z2)barZ2=z2. That is, the conditional density parenleftbigg vextendsingle vextendsingleZ2 = z2parenrightbigg (beta1 asteriskmath(Z2), \u03b11, beta2) f bardbl(beta1 asteriskmath(Z2), \u03b11, beta2)primebardblvextendsingle is identified for all z2 element Z2 (Here and henceforth we use the notation f (periodcentered|periodcentered) to denote conditional densities with appropriate arguments when adding subscripts is too cumbersome). This obviously identifies\n\n(5.6) f parenleftbigg (beta1 asteriskmath(Z2), \u03b11, beta2)vextendsingle Z2 = z2parenrightbiggvextendsingle vextendsingle vextendsingle bardblbeta2bardbl for all z2 element Z2 as well.\n\nIf we are only interested in the joint distribution of beta2 under a suitable normalization, we can stop here. The presence of the term \u03b11Z1 in (5.4) is unimportant so far. Some more work is necessary, however, if one is interested in the joint distribution of the coefficients on all the regressors. Notice that the distribution (5.6) gives\n\nparenleftbigg beta1 asteriskmath(Z2)vextendsinglevextendsingle vextendsingle Z2 = z2 parenrightbigg parenleftbigg beta1 + \u03b12primeZ2vextendsingleZ2 = z2parenrightbigg,vextendsingle vextendsingle f bardblbeta2bardbl vextendsingle = f bardblbeta2bardbl vextendsingle\n\nfrom which we can, for example, get Eparenleftbigg beta1 asteriskmath(Z2)vextendsingle Z2 = z2parenrightbiggvextendsingle vextendsingle vextendsingle = Eparenleftbigg beta1bardblparenrightbigg + E parenleftbigg 1 parenrightbigg\u03b1prime2z2 for all z2 element Z2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32ea1ebd-52ba-469d-ab6d-7eaa44308208": {"__data__": {"id_": "32ea1ebd-52ba-469d-ab6d-7eaa44308208", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f807cbe-5f10-4af6-a953-35220137d510", "node_type": "4", "metadata": {}, "hash": "a1b4c097829f499134980cf61ef4d8930d01611d6e512c7590553acd42297776", "class_name": "RelatedNodeInfo"}}, "text": "# 5.3. Endogenous Regressors\n\nAssumption 1.1 is violated if some of the regressors are endogenous in the sense that the random coefficients and the covariates are not independent. This problem can be solved if an appropriate vector of instruments is available. To be more specific, suppose we observe (Y, X, Z) generated from the following model\n\n(5.8)\nY = Ibraceleftbeta1 + tildebetaprimeX greaterequal 0braceright\n\n(5.9)\nX = GammaZ + V\n\nwhere V is a vector of reduced form residuals and Z is independent of (beta, V). Note that Hoderlein et al. (2010) utilize a linear structure of the form (5.9) in estimating a random coefficient linear model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e01ab6b-b497-433f-a662-349705bb2875": {"__data__": {"id_": "7e01ab6b-b497-433f-a662-349705bb2875", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "295a4f87-907b-4079-bcbf-c9b0c4db1365", "node_type": "4", "metadata": {}, "hash": "74932dcbe4fc8a27798eb33ce54335c6dfb8ce12d69d4d598a750547786ff215", "class_name": "RelatedNodeInfo"}}, "text": "# 22 GAUTIER AND KITAMURA\n\nThe equations (5.8) and (5.9) yield\n\nY = Ibraceleft beta1 + V beta\n\nSuppose the distribution of GammaZ satisfy Assumption 3.1. It is then possible to estimate the density of tau = tau /bardbltau bardbl where tau = beta1 + V beta, by replacing Gamma with a consistent estimator, which is easy to obtain under the maintained assumptions. This yields an estimator for the joint density of tildebeta/bardbltau bardbl, the random coefficients on the covariates under scale normalization.\n\n# 6. Numerical Examples\n\nThe purpose of this section is to illustrate the performance of our new estimator in finite samples using simulated data. We consider the model of the form (1.1) with d = 3. The covariates are specified to be X = (1, X1, X2) where (X2, X3) prime similar N (00, 2I2). The coefficients vector beta = (beta1, beta2, 1) prime is set random except for the last element. Fixing the last component constant fulfills Assumption 3.2 for identification. Two specifications for the random elements (beta1, beta2) are considered. In the first specification (Model 1) we let (beta1, beta2) prime similar N (00, 0.3 I2). In the second (Model 2) we consider a two point mixture of normals\n\n|(beta1)|(mu)|(bt)|(sigma2)|(rho)|(lambda)|\n|---|---|---|---|---|---|\n|beta2 similar lambdaN|-mu|sigma2|mu|sigma2|rho sigma|\n\nwhere mu = 0.7, \u03c32 = 0.3, rho = 0.5 and lambda = 0.5. Random samples of size 500 from each of the two specifications are generated, then the new estimator (4.8) is computed. It is implemented using the Riesz kernel with s = 2 and l = 3 (see Proposition A.3). The truncation parameter TN is set at 3, and the trimming parameter r is 2. It also requires a nonparametric estimator for fX, and we use the projection estimator (4.7) based on the same Riesz kernel (i.e. s = 2, l = 3) and TN = 10.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4d64bef-9d7f-4f36-902b-87689ce47641": {"__data__": {"id_": "c4d64bef-9d7f-4f36-902b-87689ce47641", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c0d045c-6e11-4640-b7f6-8765969923c2", "node_type": "4", "metadata": {}, "hash": "e22de1d6e12fafc4835643b5ec5e2130fd379d54e6de0ab325e1b65688e35297", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 1. Nonparametric estimator of fbeta for Model 1 (left) and Model 2 (right)\n\nThe figure presents the surface plot of the true density (blue mesh) and our estimate (multi-colored surface). Our estimator (4.8) is defined on S2 in this case, and we performed an appropriate transformation to plot it as a density on R2. In the case of model 1, with the reasonable sample size, the location of the peak of the density, as well as its shape, are successfully recovered by our procedure. For model 2, again, our procedure works well: the estimated surface plot nicely captures the locations of the two peaks and their shapes of the true density, thereby exhibiting the underlying mixture structure. While further experimentations are necessary, these results seem to indicate our estimator's good performance in practical settings.\n\n# 7. Conclusion\n\nIn this paper we have considered nonparametric estimation of a random coefficients binary choice model. By exploiting (previously unnoticed) connections between the model and statistical deconvolution problems and applying results of integral transformation on the sphere, we have developed a new estimator that is practical and possesses desirable statistical properties. It requires neither numerical optimization nor numerical integration, and as such its computational cost is trivial and local maxima and other difficulties in optimization need not be of concern. Its rate of convergence in the Lq norm for all q element [1, infinity] is derived. Our numerical example suggests that the new procedure works well in finite samples, consistent with its good theoretical properties. It is of great theoretical and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f21f7ac-39eb-4f70-9d31-ef7d81e659a3": {"__data__": {"id_": "6f21f7ac-39eb-4f70-9d31-ef7d81e659a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0c468d8-81ce-4f7e-9325-113947e69fcf", "node_type": "4", "metadata": {}, "hash": "4592b511e5f743816648f695692d1ea0de646278a07affcf2feab3a3daad08a8", "class_name": "RelatedNodeInfo"}}, "text": "# 24 GAUTIER AND KITAMURA\n\npractical interest to obtain an adaptive procedure for choosing the smoothing parameters of our estimator, though it is a task we defer to subsequent investigations. With appropriate under-smoothing, the estimator is shown to be asymptotically normal, providing a theoretical basis for nonparametric statistical inference for the random coefficients distribution.\n\n5Gautier and Le Pennec (2011) consider a needlet-based procedure and discuss its rate optimality in a minimax sense and adaptation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf1d653b-7b06-438a-a543-83b31d4ff5b8": {"__data__": {"id_": "bf1d653b-7b06-438a-a543-83b31d4ff5b8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf9320ae-56c5-4c79-b577-4ce45a5126d4", "node_type": "4", "metadata": {}, "hash": "943d43122d884fd197fd22aa9f6142324ed717a5d40015ad6119c2c86a6ea887", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Athey, S., and Imbens, G. W. (2007): \u201cDiscrete Choice Models with Multiple Unobserved Choice Characteristics\u201d. Preprint.\n2. Bajari, P., Fox, J. and Ryan, S. (2007): \u201cLinear Regression Estimation of Discrete choice Models with Nonparametric Distribution of Random Coefficients\u201d. American Economic Review, Papers and Proceedings, 97, 459-463.\n3. Baldi, P., Kerkyacharian, G., Marinucci, D. and Picard, D. (2009): \u201cAdaptive Density Estimation for Directional Data Using Needlets\u201d. Annals of Statistics, 37, 3362-3395.\n4. Beran, R. and Hall, P. (1992): \u201cEstimating Coefficient Distributions in Random Coefficient Regression\u201d. Annals of Statistics, 20, 1970-1984.\n5. Berry, S., and Haile, P. (2008): \u201cNonparametric Identification of Multinomial Choice Demand Models with Heterogeneous Consumers\u201d. Preprint.\n6. Billingsley, P. (1995): Probability and Measure - Third Edition. New York: Wiley.\n7. Bonami, A., and Clerc, J. L. (1973): \u201cSommes de Cesgravearo et Multiplicateurs des D\u00e9veloppements en Harmoniques Sph\u00e9riques\u201d, Transactions of the American Mathematical Society, 183, 223-263.\n8. Briech, R., Chintagunta, P. and Matzkin, R. (2007): \u201cNonparametric Discrete Choice Models with Unobserved Heterogeneity\u201d. Journal of Business and Economic Statistics, 28, 291-307.\n9. Brownstone, D., and Train, K. (1999): \u201cForecasting New Product Penetration with Flexible Substitution Patterns\u201d. Journal of Econometrics, 89, 109-129.\n10. Carrasco, M., Florens, J. P., and Renault, E. (2007): \u201cLinear Inverse Problems in Structural Econometrics Estimation Based on Spectral Decomposition and Regularization\u201d, Handbook of Econometrics, J. J. Heckman and E. E. Leamer (eds.), vol. 6B, North Holland, chapter 77, 5633-5751.\n11. Chesher, A., and Silva Santos, J. M. C. (2002): \u201cTaste Variation in Discrete Choice Models\u201d. Review of Economic Studies, 69, 147-168.\n12. Colzani, L., and Traveglini, G. (1991): \u201cEstimates for Riesz Kernels of Eigenfunction Expansions of Elliptic Differential Operators on Compact Manifolds\u201d. Journal of Functional Analysis, 96, 1-30.\n13. Devroye, L., and Gyorfi, L. (1985): Nonparametric Density Estimation. The L1-View. Wiley, New York.\n14. Ditzian, Z. (1998): \u201cFractional Derivatives and Best Approximation\u201d. Acta Mathematica Hungarica, 81, 323-348.\n15. Elbers, C. and Ridder, G. (1982): \u201cTrue and Spurious Duration Dependence: The Identifiability of the Proportional Hazard Models\u201d. Review of Economics Studies, 49, 403-410.\n16. Erd\u00e9lyi, A. et al. ed. (1953): Higher Transcendental Functions, vol. 1,2 of the Bateman Manuscript Project. McGraw-Hill: New-York.\n17. Fan, J. (1991): \u201cOn the Optimal Rates of Convergence for Nonparametric Deconvolution Problems\u201d. Annals of Statistics, 19, 1257-1272.\n18. Feller, W. (1968): An Introduction to Probability Theory and Its Applications - Third edition, vol. 1. Wiley: New York.\n19. Fisher, N. I., Lewis, T., and Embleton, B. J. J. (1987): Statistical Analysis of Spherical Data. Cambridge University Press: Cambridge.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c09b6203-cfe7-4842-a024-e4b41c88ec7f": {"__data__": {"id_": "c09b6203-cfe7-4842-a024-e4b41c88ec7f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31b7946c-683b-435a-b39f-e1d39da093bb", "node_type": "4", "metadata": {}, "hash": "31862ee6a8bfa0de5dd1075cbe4fbb25c27595ba7db302aeab7f4cdbbcea97ed", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Funk, P. (1916): \u00dcber Eine Geometrische Anwendung der Abelschen Integralgleichung. Mathematische Annalen, 77, 129-135.\n2. Gadieresisdotlessiffas, S. (2005): Convergence Rates for Pointwise Curve Estimation with a Degenerate Design. Mathematical Methods of Statistics, 14, 1-27.\n3. Gadieresisdotlessiffas, S. (2009): Uniform Estimation of a Signal Based on Inhomogeneous Data. Statistica Sinica, 19, 427-447.\n4. Gautier, E., and Le Pennec (2011): Adaptive estimation in nonparametric random coefficients binary choice model by needlet thresholding. Preprint arXiv:1106.3503.\n5. Guerre, E. (1999): Efficient Random Rates for Nonparametric Regression Under Arbitrary Designs. Working Paper.\n6. Groemer, H. (1996): Geometric Applications of Fourier Series and Spherical Harmonics. Cambridge University Press: Cambridge, Encyclopedia of Mathematics and its Applications.\n7. Groeneboom, P., and Jongbloed, G. (2003): Density Estimation in the Uniform Deconvolution Model. Statistica Neerlandica, 57, 136-157.\n8. Gronwall, T. H. (1914): On the Degree of Convergence of Laplace's Series. Transactions of the American Mathematical Association, 15, 1-30.\n9. Hall, P., Marron, J. S., Neumann, M. H., and Tetterington, D. M. (1997): Curve estimation when the design density is low. Annals of Statistics, 25, 756-770.\n10. Hall, P., Watson, G. S., and Cabrera, J. (1987): Kernel Density Estimation with Spherical Data. Biometrika, 74, 751-62.\n11. Harding, M. C. and Hausman, J. (2007): Using a Laplace Approximation to Estimate the Random Coefficients Logit Model by Non-linear Least Squares. International Economic Review, 48, 1311-1328.\n12. Heckman, J. J. and Singer, B. (1984): A Method for Minimizing the Impact of Distributional Assumptions in Econometric Models for Duration Data. Econometrica, 52, 271-320.\n13. Hendriks, H. (1990): Nonparametric Estimation of a Probability Density on a Riemannian Manifold Using Fourier Expansions. Annals of Statistics, 18, 832-849.\n14. Hess, S., Bolduc, D. and Polak, J. (2005): Random Covariance Heterogeneity in Discrete Choice Models. Preprint.\n15. Healy, D. M., and Kim, P. T. (1996): An Empirical Bayes Approach to Directional Data and Efficient Computation on the Sphere. Annals of Statistics, 24, 232-254.\n16. Hoderlein, S., Klemeldieresisa, J., and Mammen, E. (2010): Analyzing the Random Coefficient Model Nonparametrically. Econometric Theory, 26, 804-837.\n17. Ichimura, H., and Thompson, T. S. (1998): Maximum Likelihood Estimation of a Binary Choice Model with Random Coefficients of Unknown Distribution. Journal of Econometrics, 86, 269-295.\n18. Johnstone, I. M., and Raimondo, M. (2004): Periodic Boxcar Deconvolution and Diophantine Approximation. Annals of Statistics, 32, 1781-1804.\n19. Kamzolov, A. I. (1983): The Best Approximation of the Class of Functions Wp alpha(Sn) by Polynomials in Spherical Harmonics. Matematical Notes 32 (1982), 285-293.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a249a43-a580-4a66-871f-16dce3eaab98": {"__data__": {"id_": "1a249a43-a580-4a66-871f-16dce3eaab98", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22987d10-3c54-4c7d-ba44-6a334a25d4b0", "node_type": "4", "metadata": {}, "hash": "ba62590c93c8f78ee04df47848d2778fa6463035708b0ceba2a1f82327e402ab", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\n1. Kim, P. T., and Koo, J. Y. (2000): \u201cDirectional Mixture Models and Optimal Estimation of the Mixing Density\u201d. The Canadian Journal of Statistics, 28, 383-398.\n2. Klemeldieresisa, J. (2000): \u201cEstimation of Densities and Derivatives with Directional Data\u201d. Journal of Multivariate Analysis, 73, 18-40.\n3. Mdieresisuller, C. (1966): Spherical Harmonics. Lecture Notes in Mathematics, 17, Springer.\n4. Narcowich, F., Petrushev, P., and Warda, J. (2006): \u201cDecomposition of Besov and Triebel-Lizorkin spaces on the sphere\u201d. Journal of Functional Analysis, 238, 530-564.\n5. Ragozin, D. (1972): \u201cUniform Convergence of Spherical Harmonic Expansions\u201d. Mathematische Annalen, 195, 87-94.\n6. Rubin, B. (1999): \u201cInversion and Characterization of the Hemispherical Transform\u201d. Journal d\u2019Analyse Math\u00e9matique, 77, 105-128.\n7. Ruymgaart, F. H. (1989): \u201cStrong Uniform Convergence of Density Estimators on Spheres\u201d. Journal of Statistical Planning and Inference, 23, 45-52.\n8. Train, K. E. (2003): Discrete Choice Methods with Simulation. Cambridge University Press: Cambridge.\n\n# Contact Information\n\nCREST (ENSAE), 3 avenue Pierre Larousse, 92245 Malakoff Cedex, France.\n\nE-mail address: eric.gautier@ensae-paristech.fr\n\nCowles Foundation for Research in Economics, Yale University, New Haven, CT-06520.\n\nE-mail address: yuichi.kitamura@yale.edu", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2795b75d-d9c8-4843-8749-4c1efd28e370": {"__data__": {"id_": "2795b75d-d9c8-4843-8749-4c1efd28e370", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d71be8a-7b0f-4d96-bf7e-d9bab0db008f", "node_type": "4", "metadata": {}, "hash": "2abc41f1e0bcafb6bdce3017877c0ec115f156487336a41c0d82e102cfaa323c", "class_name": "RelatedNodeInfo"}}, "text": "# SUPPLEMENTAL APPENDIX FOR \u201cNONPARAMETRIC ESTIMATION IN RANDOM COEFFICIENTS BINARY CHOICE MODELS\u201d\n\n# ERIC GAUTIER AND YUICHI KITAMURA\n\n# A.1.1. A Toy Model\n\nAs noted in the main text, the key insight for our estimation procedure lies in the fact the estimation of *fbeta in (1.3) is mathematically equivalent to a statistical deconvolution problem. To see this, it is useful to first consider the case with d = 2. We parameterize the vectors b = (b1, b2) and x = (x1, x2) on S1 by their angles \u03c6 = arccos(b1) and \u03b8 = arccos(x1) in [0, 2\u03c0). As is often the case when Fourier series techniques are used, we consider spaces of complex valued functions. Let Lp(S1) denote the Banach space of Lebesgue p-integrable functions and its norm by ||\u00b7||p. In the case of L2(S1), the norm is derived from the hermitian product \u222b02\u03c0 f(\u03b8)g(\u03b8)d\u03b8. Let R\u03b8 and f\u03c6 denote the extension R of r according to (3.2) and fbeta after the reparameterization. Our task is then to obtain f\u03c6 from the knowledge of R\u03b8. Rewrite (1.3) using these definitions, then divide both sides by \u03c0*, to get:\n\n(A.1) *R\u03b8(\u03b8) = H(fbeta)(\u03b8) = (1/\u03c0) \u222b02\u03c0 1{|\u03b8 \u2212 \u03c6| < \u03c0/2} f\u03c6(\u03c6)d\u03c6*.\n\nIf we further define *f\u03b8 := R\u03b8/\u03c0 and feta(\u03b7) := \u03c0 1{|\u03b7| < \u03c0/2}, then using the standard notation for convolution, (A.1) can be written as f\u03b8 = feta * f\u03c6. It is now obvious that the estimation of f\u03c6 (thus fbeta) is linked to the following statistical deconvolution problem: unobservable random variables \u03c6 and \u03b7 with densities f\u03c6 and feta are related to an observable random variable \u03b8 according to \u03b8 = \u03b7 + \u03c6, and one wishes to recover f\u03c6 from f\u03b8, the density of \u03b8, when feta* is known (and it is Uniform[\u2212\u03c0/2, \u03c0/2] in this case).\n\n1 It is also useful to note that the inversion of *H is closely related to differentiation. Differentiating the right hand-side of expression (A.1) with respect to \u03b8 identifies f\u03c6(\u03b8 + \u03c0/2) - f\u03c6(\u03b8 - \u03c0/2) where f\u03c6 is defined on the line by periodicity. If f\u03c6 is supported on a semicircle, with an assumption that is elaborated further in Section 3, f\u03c6 (which is positive) is identified. Thus if the model is identified the inverse of H* is a differential operator and as such unbounded.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28fe55ae-7656-4cf4-a0fb-a48de7d7244d": {"__data__": {"id_": "28fe55ae-7656-4cf4-a0fb-a48de7d7244d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3dcc8d03-06ab-4588-8d5c-bd0db6c406e9", "node_type": "4", "metadata": {}, "hash": "3232e6f41a70418fa3a68fd6f6573d84e0a05ab84314ff74b41f1cf98fb2d39b", "class_name": "RelatedNodeInfo"}}, "text": "# A-2 GAUTIER AND KITAMURA\n\ncn(ft) = integraltext 2pift(t) exp(\u2212int)dt/(2pi),0 (A.2) ft(\u03b8) = \u2211cn(ft) exp(int)nelementZ holds in the L1(S1) sense. Recall also that for f and g in L1(S1), after the same reparameterization, (A.3) cn(ft asteriskmath gt) = 2picn(ft)cn(g).t\n\nUsing equation (A.3) we obtain the following proposition.\n\n# Proposition A.1.\n\nc0(Rtheta) = pic0 (fphi) and for n element Z backslash braceleft0braceright, cn(Rtheta) = cn (fphi) 2 sin (npi/2) /n.\n\nAs in classical deconvolution problems on the real line, our aim is to obtain ft (thus fbeta) using equation (A.2) and Proposition A.1. Proposition A.1 shows that c2p(Rtheta ) = 0 holds for all non-zero pquoterights, regardless of the values of c2p(fphi), p element Zbackslashbraceleft0braceright. Thus from r(x) = Rtheta(\u03b8) one can only recover the Fourier coefficients cn(fphi) for n = 0 (which is easily seen to be 1/2pi, by integrating both sides of (A.1) and noting that fbeta is a probability density function) and n = 2p + 1, p element Z. The same phenomenon occurs in higher dimensions, as explained in Section A.1.3.\n\n# Remark A.1.\n\nThe vector spaces H2p+1comma2 = span braceleftbigexp(i(2p + 1)t)/radical2pi, exp(\u2212i(2p + 1)t)/radical2pibracerightbig , p element N are eigenspaces of the compact self-adjoint operator H on L2(S1). These eigenspaces are associated with the eigenvalues 2p+1 . Also, circleplustextpelementNbackslashbraceleft0braceright H2pcomma2 is the null space ker H.2(minus1)p\n\n# A.1.2. The Gegenbauer polynomials.\n\nWe summarize some results on the Gegenbauer polynomials, which are used in various parts of the paper. These can be found in Erdacuteelyi et al. (1953) and Groemer (1996). When nu = 0 and d = 2, it is related to the Chebychev polynomials of the first kind, as\n\nn element N backslash braceleft0braceright, Cn0(t) = n 2Tn(t)\n\nand\n\nC0 0(t) = T0(t) = 1\n\nhold for\n\nTn(t) = cos (n arccos(t)) , n element N.\n\nWhen nu = 1 and d = 4, Cn1(t) coincides with the Chebychev polynomial of the second kind Un(t), which is given by\n\nUn(t) = sin[(n + 1) arccos(t)], n element N.sin[arccos(t)]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b5b956a-bcfc-4bfc-a9c2-8516e983db37": {"__data__": {"id_": "5b5b956a-bcfc-4bfc-a9c2-8516e983db37", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2dd3d89-e0f2-49d4-846b-5a4c897db878", "node_type": "4", "metadata": {}, "hash": "e5c0f9c7c068509aa45d4f7c401eab11048567f582068d3e60093a3b0624d69b", "class_name": "RelatedNodeInfo"}}, "text": "# A-3\n\nThe Gegenbauer polynomials are orthogonal with respect to the weight function (1 \u2212 t2)\u03bd\u22121/2dt on [\u22121, 1]. Note that C0\u03bd(t) = 1 and C1\u03bd(t) = 2\u03bdt for \u03bd \u2260 0 while C10(t) = 2t. Moreover, the following recursion relation holds:\n\n(A.4) (n + 2)Cn\u03bd+2(t) = 2(\u03bd + n + 1)tCn\u03bd+1(t) \u2212 (2\u03bd + n)Cn\u03bd(t).\n\nImplementation of our estimator requires evaluation of the Gegenbauer polynomials for a series of successive values of n. The recursion relation (A.4) is therefore a powerful tool. The Gegenbauer polynomials are related to each other through differentiation, that is, they satisfy:\n\n(A.5) dt C\u03bd(t) = 2\u03bdCn\u22121dn\u03bd+1(t) for \u03bd > 0 and\n\n(A.6) dt C0(t) = 2Cn\u22121(t).\n\nFor \u03bd \u2260 0 the Rodrigues formula states that:\n\n(A.7) C\u03bd(t) = (\u22122)n(1 \u2212 t2)\u2212\u03bd+1/2(\u03bd + 1/2)nn!(2\u03bd)n dnt(1 \u2212 t2)n+\u03bd\u22121/2.\n\nThe following results are also used in the paper:\n\n(A.8) t\u2208[\u22121, 1] Cn\u03bd(1) \u2264 1,\n\n(A.9) \u03bd > 0, n \u2208 \u2115, Cn\u03bd(1) = (n + 2\u03bd \u2212 1)n\n\n(A.10) C0(1) = 1 and n \u2208 \u2115 \\ {0}, Cn0(1) = n2,\n\n(A.11) C\u03bd(\u2212t) = (\u22121)nCn\u03bd(t).\n\nThese orthogonal polynomials are normalized such that:\n\n(A.12) ||Cn\u03bd(d)||2 = |Sd\u22122|\u222b1(Cn\u03bd(d)(t))2(1 \u2212 t2)(d\u22123)/2dt = |Sd\u22121|(Cn\u03bd(d)(1))2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b496285-34e9-4dbb-a4b0-b66b02c55dc2": {"__data__": {"id_": "8b496285-34e9-4dbb-a4b0-b66b02c55dc2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "773f0249-f85d-4d43-8a80-9a8555b60715", "node_type": "4", "metadata": {}, "hash": "7af86e8d5b81e27755c02e810635a1a8e2bdf8f34fa304f3b49ba8389ec16951", "class_name": "RelatedNodeInfo"}}, "text": "# A-4 GAUTIER AND KITAMURA\n\n# A.1.3. Tools for Higher Dimensional Spheres\n\nLet us introduce some concepts used for the treatment of the general case d \u2265 2. We consider functions defined on the sphere Sd-1, which is a d \u2212 1 dimensional smooth submanifold in Rd. The canonical measure on Sd-1 (or the spherical measure) is denoted by \u03c3. It is a uniform measure on Sd-1 satisfying \u222bSd-1 d\u03c3 = |Sd-1|, where |Sd-1| signifies the surface area of the unit sphere.\n\nRecall that the basis functions exp(\u00b1i n)/\u221a(2\u03c0) are eigenfunctions of \u2212\u0394 associated with eigenvalue n2. In a similar way, the Laplacian on the sphere Sd-1, d \u2265 2, denoted by \u0394S, can be used to obtain an orthonormal basis for higher dimensional spheres. It can be defined by the formula\n\n(A.13) \u0394S f = (\u0394f\u2227)\n\nwhere \u0394 is the Laplacian in Rd, f\u2227 the radial extension of f, that is f\u2227(x) = f(x/|x|), and f\u2227 the restriction of f to Sd-1. Likewise the gradient on the sphere is given by:\n\n(A.14) \u2207S f = (\u2207f\u2227)\n\nwhere \u2207 is the gradient in Rd.\n\n# Definition A.1\n\nA surface harmonic of degree n is the restriction of a homogeneous harmonic polynomial (a homogeneous polynomial p whose Laplacian \u0394p is zero) of degree n in Rd to Sd-1.\n\nThe reader is referred to M. M\u00fcller (1966) and Groemer (1996) for clear and detailed expositions on these concepts and important results concerning spherical harmonics used in this paper. Erd\u00e9lyi et al. (1953, vol. 2, chapter 9) provide detailed accounts focusing on special functions. Here are some useful results:\n\n# Lemma A.1\n\nThe following properties hold:\n\n- (i) \u2212\u0394S is a positive self-adjoint unbounded operator on L2(Sd-1), thus it has orthogonal eigenspaces and a basis of eigenfunctions;\n- (ii) Surface harmonics of degree n are eigenfunctions of \u2212\u0394S for the eigenvalue \u03b6n := n(n + d \u2212 2);\n- (iii) The dimension of the vector space Hn of surface harmonics of degree n is\n\n(A.15) h(n, d) := (2n + d \u2212 2)(n + d \u2212 2)!/n!(d \u2212 2)!(n + d \u2212 2)\n\n- (iv) A system formed of orthonormal bases (Yn, l)h(n)l=1 of Hn for each degree n = 0, . . . , \u221e is complete in L1(Sd-1), that is, for every f \u2208 L1(Sd-1) the following equality holds in the L1(Sd-1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a02c6a00-5c8c-4119-8868-96e0fb10b81f": {"__data__": {"id_": "a02c6a00-5c8c-4119-8868-96e0fb10b81f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "237efde8-02cf-4d81-8cc6-e1a7a0211568", "node_type": "4", "metadata": {}, "hash": "001d5ea9f19565c0fc7fc14fe1dd6c828ee3a48efabc9c5385a128e8cebf704f", "class_name": "RelatedNodeInfo"}}, "text": "# A-5\n\nsense: \u2211h(ncommad)(f, Yncommal)L2(Sd-1) Yncommal.\n\ninfinity \u2211\n\nf =\n\nn=0 l=1\n\nThus h(n, d) is the multiplicity of the eigenvalue zetancommad, and Hncommad is the corresponding eigenspace.\n\n# Lemma A.1\n\n(i), (ii) and (iv) give the decomposition\n\nL2(Sd-1) = \u2295n \u2208 N Hncommad.\n\nThe space of surface harmonics of degree 0 is the one dimensional space spanned by 1. A series expansion on an orthonormal basis of surface harmonics is called a Fourier series when d = 2, a Laplace series when d = 3 and in the general case a Fourier-Laplace series.\n\nOrthonormal bases of surface harmonics usually involve parametrization by angles, such as the spherical coordinates when d = 3 as used by Healy and Kim (1996) or hyperspherical coordinates for d > 3. Instead, here we work with the decomposition of a function on the spaces Hncommad as presented in the next definition so that we avoid specific expressions of basis functions.\n\n# Definition A.2\n\nThe condensed harmonic expansion of a function f in L1(Sd-1) is the series summation \u221en=0 Qncommadf, where Qncommad is the projector from L2(Sd-1) to Hncommad.\n\nThis leads to a simple method both in terms of theoretical developments and practical implementations. The projector Qncommad can be expressed as an integral operator with kernel\n\nqncommad(x, y) = h(ncommad) \u2211l=1 Yncommal(x)Yncommal(y),\n\nwhere (Yncommal)h(ncommad)l=1 is any orthonormal basis of Hncommad. The kernel has a simple expression given by the addition formula:\n\n# Theorem A.1\n\n(Addition Formula). For every x and y \u2208 Sd-1, we have\n\nqncommad(x, y) = flatqncommad(x prime y), flatqncommad(t) := |Sd-1| Cnnu(d)(t) h(n, d) Cnnu(d)(1)\n\nwhere Cnnu are Gegenbauer and nu(d) = (d \u2212 2)/2.\n\nThe Sobolev spaces are defined in the Fourier-Laplace domain through the fractional Laplacian defined on a certain subset of Lp(Sd-1) as\n\n(\u2212\u0394S)1/2 f := \u2211zetancommad \u221e\n\n(A.18) (1/2) Qncommadf.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46f75e9d-83fc-49b8-85c2-70bd04da440b": {"__data__": {"id_": "46f75e9d-83fc-49b8-85c2-70bd04da440b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20fc4542-1c44-48b6-b10b-dc2ec0b186a3", "node_type": "4", "metadata": {}, "hash": "0d1884bdc35fac4b09b18f0d9b86b612d57df58bf2ad808a693b5ca01dca5036", "class_name": "RelatedNodeInfo"}}, "text": "# A-6\n\n# GAUTIER AND KITAMURA\n\nFor the case where p = 2, instead of the definition of the norm ||f||p given in Section 3 it is also possible to use an equivalent norm, the square of which is equal to\n\n||f||2 = \u2211n=0\u221e(1 + \u03b6n)||Qnf||22.\n\nThe following integration by parts holds for functions f in H1(Sd-1):\n\n\u222bSd-1 f(x) \u0394S f(x) d\u03c3(x) = - \u222bSd-1 \u2207x f \u2207x' f d\u03c3(x) (A.19)\n\nand as a consequence for the second definition of the norm of H1(Sd-1) we have\n\n||f||2,1 = ||f||2 + ||\u2207S f||2.\n\nIn Section A.1.1 we observed the close relationship between the random coefficients binary choice model and convolution for d = 2. This connection remains valid in higher dimensions. Suppose a function f(x, y) defined on Sd-1 \u00d7 Sd-1 depends on x and y only through the spherical distance d(x, y) = arccos(x' y) (that is, f is a zonal function). Consider the following integral:\n\nh(x) = \u222bSd-1 f(x, y) g(y) d\u03c3(y) := f * g(x),\n\nthen the function h is a convolution on the sphere. We now see that the choice probability function\n\nr(x) = H(f\u03b2)(x) = \u222bSd-1 I{ x' b \u2265 0 } f\u03b2(b) d\u03c3(b) is a special case of h and therefore can also be regarded as convolution. Obtaining f\u03b2 from r (or, inverting H) is therefore a deconvolution problem.\n\nIn what follows we often write f(x, *) when a function f on Sd-1 \u00d7 Sd-1 is regarded as a function of *. Also, the notation ||f(x, *)||p is used for the Lp norm of f(x, *), that is, ||f(x, *)||p = \u222bSd-1 |f(x, y)|p d\u03c3(y). Note that if f is a zonal function as in the above definition of spherical convolution, its Lp norm ||f(x, *)||p does not depend on x. The following Young inequalities for convolution on the sphere (see, for example, Kamzolov, 1983) are useful:\n\n# Proposition A.2 (Young inequalities)\n\nSuppose f(x, *) and g belong to Lr(Sd-1) and Lp(Sd-1), respectively. Then h(x) = f * g(x) is well-defined in Lq(Sd-1) and\n\nwhere 1 \u2264 p, q, r \u2264 \u221e and 1/q = 1/p + 1/r - 1.\n\nLet PT denote the projection operator onto T = 0 Hn, i.e.\n\nPT f(x) = ||h||q \u2264 ||f||r ||g||p,\n\n\u2211n=0\u221e Qn f(x) = \u222bSd-1 T DT(x, y) f(y) d\u03c3(y) (A.20)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2026, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fc2a466-1999-48d6-89e2-45845a9c85ee": {"__data__": {"id_": "9fc2a466-1999-48d6-89e2-45845a9c85ee", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34b3522a-8d41-4f6c-b3ca-4d7ba8bac658", "node_type": "4", "metadata": {}, "hash": "6411cea6a13132c5b6a65f8b3da16deb5ba5757b72fd37b96a7a0d0b09d174c0", "class_name": "RelatedNodeInfo"}}, "text": "# A-7\n\nwhere\n\n\u2211qncommad(x, y).T\n\nDT (x, y) =\n\nn=0\n\nThe kernel DT extends the classical Dirichlet kernel on the circle to the sphere Sdminus1. The sum over T in the definition of DT also has the simple closed form in terms of derivatives of Gegenbauer polynomials; see Equation (52) in Mdieresisuller (1966).\n\nThe linear form f arrowright integraltextSdminus1 DT (x, y)f (y)d\u03c3(y) converges to integraltextSdminus1 f (y)ddeltax(y) = f (x) as T goes to infinity, where deltax denotes the Dirac measure. The Dirichlet kernel yields the best approximation PT f of f in L2(Sdminus1) by polynomials that belong to circleplustextT=0 Hncommad, but is known to have flaws. For example, DT does not satisfy\n\nf element L1(Sdminus1), limT arrowrightinfinity bardblDT asteriskmath f \u2212 f bardblL1(Sdminus1) = 0,\n\nthat is, the sequence DT, T = 0, 1, ... is not an approximate identity (see, e.g., Devroye and Gyorfi 1985) in L1(Sdminus1). Indeed, the L1(Sdminus1) norm of the kernel is not uniformly bounded; more precisely, we have\n\n(A.21) bardblDT (periodcentered, x)bardbl1 equivasymptotic T (dminus2)slash2\n\nwhen d greaterequal 3 and\n\n(A.22) bardblDT (periodcentered, x)bardbl1 equivasymptotic log T\n\nwhen d = 2 (as noted above, these norms do not depend on the value of x element Sdminus1). These bounds can be found in Gronwall (1914) for d = 3 and Ragozin (1972) and Colzani and Traveglini (1991) for higher dimensions. Also, DT does not have good approximation properties in Linfinity(Sdminus1); in particular, we do not have\n\nf element Linfinity(Sdminus1), limT arrowrightinfinity bardblDT asteriskmath f \u2212 f bardblLinfinity(Sdminus1) = 0.\n\nNear the points of discontinuity of f, DT asteriskmath f has oscillations which do not decay to zero as T grows to infinity, known as the Gibbs oscillations. This phenomenon deteriorates as the dimension increases.\n\nThese problems can be addressed by using kernels that involve extra smoothing instead of the Dirichlet kernel DT. To this end, define a general class of kernel\n\n\u2211chi(n, T )qncommad(x, y)T\n\n(A.23) KT (x, y) =\n\nn=0\n\nfor some sequence chi(n, T). These are called smoothed projection kernels. Typically the function chi is chosen so that it puts more weight on lower frequencies. In particular we impose the following conditions:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "630ad981-5fc1-40e9-b3c9-f4f20b9a1892": {"__data__": {"id_": "630ad981-5fc1-40e9-b3c9-f4f20b9a1892", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ce2d18d-74a7-4742-9fe8-6c0301d46ace", "node_type": "4", "metadata": {}, "hash": "8b929f0a9f63a0dbca12fcd822c6bda203b4410309ab34f1df13d533ecd48d76", "class_name": "RelatedNodeInfo"}}, "text": "# A-8 GAUTIER AND KITAMURA\n\n# Assumption A.1.\n\n1. (i) \\( \\| K_T (x, \\star) \\|_1 \\) is uniformly bounded in \\( T \\).\n2. (ii) There exists constants \\( C \\) and \\( \\alpha \\) such that for all \\( x, y, z \\in S^{d-1} \\),\n\\[\n|K_T(z, x) - K_T(z, y| \\leq C \\| x - y \\|_T^\\alpha,\n\\]\nwhere \\( \\| \\cdot \\| \\) denotes the Euclidean norm.\n3. (iii) For \\( p \\in [1, \\infty] \\) and \\( s > 0 \\), there exists a constant \\( C \\) such that for every \\( f \\in W_{p,s}(S^{d-1}) \\),\n\\[\n\\| f - \\int_{S^{d-1}} K_T(\\cdot, y) f(y) d\\sigma(y) \\|_p \\leq C - \\| f \\|_p.\n\\]\n4. (iv) \\( \\chi(\\cdot, T) \\) takes values in [0, 1] and is such that there exists \\( c > 0 \\) such that for all \\( 0 \\leq n \\leq \\lfloor T/2 \\rfloor \\),\n\\[\n\\chi(n, T) \\geq c.\n\\]\n\nThe smoothed projection kernel \\( K_T(x, y) \\) depends on \\( x \\) and \\( y \\) only through \\( d(x, y) \\), thus the value of the norm \\( \\| K_T(x, \\star) \\|_1 \\) in Assumption (i) does not depend on \\( x \\in S^{d-1} \\). Assumption (i) could be relaxed, but imposing this on \\( K_T \\) allows us to make relatively weak assumptions on the smoothness of the density of the covariates later in this paper. Assumption (ii) is used to establish the \\( L^\\infty \\)-rates of convergence of our estimators. Assumption (iii) provides bounds for approximation errors. Under this condition, \\( K_T^* f \\) approximates \\( f \\in L^p(S^{d-1}) \\) with an error of the same order as that of the best \\( n \\)-th degree spherical harmonic approximation of a function \\( f \\in L^p(S^{d-1}) \\) in \\( W_{p,s}(S^{d-1}) \\) (see e.g. Kamzolov 1983 and Ditzian 1998). This is useful in our treatment of the bias terms in our estimators. As concrete examples, the following two choices for the weight function \\( \\chi \\) in (A.23) satisfy Assumption A.1, as shown in the appendix. The first and the second choices of \\( \\chi \\) correspond to the Riesz kernel and the delayed means kernel, respectively.\n\n# Proposition A.3.\n\nIn the definition of the smoothed kernel (A.23), let\n\n\\[\n\\chi(n, T) = \\frac{1 - \\zeta_n}{\\zeta_T + 1}\n\\]\n\nwhere \\( l \\) is an integer satisfying \\( l > \\frac{d - 2}{2} \\), or\n\n\\[\n\\chi(n, T) = \\psi(n/T)\n\\]\n\nwhere \\( \\psi : [0, \\infty) \\to [0, \\infty) \\) is infinitely differentiable, nonincreasing, such that \\( \\psi(x) = 1 \\) if \\( x \\in [0, 1] \\), \\( 0 \\leq \\psi(x) \\leq 1 \\) if \\( x \\in [1, 2] \\), \\( \\psi(x) = 0 \\) if \\( x \\geq 2 \\). Then \\( K_T \\) satisfies Assumption A.1.\n\nThe delayed means kernel has the nice property that it does not require prior knowledge of the regularity \\( s \\) in Assumption A.1. The Dirichlet kernel satisfies (ii), (iii) (for \\( p = 2 \\)) and (iv) of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c400e8ce-7b1c-4606-a672-93f7af5fdd04": {"__data__": {"id_": "c400e8ce-7b1c-4606-a672-93f7af5fdd04", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ffb84b8-0c0a-45a5-a533-6a89fba850c5", "node_type": "4", "metadata": {}, "hash": "e7a661bfefb9d4a4f28a3e0be32587943fe4d096e10684ab8f0c8c6bbf9838d5", "class_name": "RelatedNodeInfo"}}, "text": "# A-9\n\n# Assumption A.1\n\nLike the delayed means kernel, it achieves the optimal rate of approximation without the prior knowledge of s.\n\n# Proof of Proposition A.3\n\nFirst consider the Riesz kernel. (i) follows from (2.4) in Ditzian (1998) and by the fact that Cesgravearo kernels Chare uniformly bounded in L1(Sd-1) for l > d - 2l2 (see, e.g. Bonami and Clerc 1973, p. 225). To show (iii) we use Theorem 4.1 in Ditzian (1998), by letting P(D) = \u0394S, \u03bb = \u03b6T + 1 = T(T + d - 2) + 1, \u03b1 = s/2 and m = 1. Then it implies an approximation error upper bound CKs/2(f, \u0394S, (\u03b6T + 1) - s), which, in turn, is bounded by CT - ||(-\u0394S)s/2f||p (see equations (4.2) and (4.1) therein). By the definition of the norm of the Sobolev space Wps(Sd-1) (see Definition ??) the result follows. Concerning the delayed means, (i) follows from Theorem 2.2 and Proposition 2.5 of Narcowich et al. (2006). (ii) corresponds to Lemma 2.6 in Narcowich et al. (2006). To see (iii), use Lemma 2.4 (c) in Narcowich et al. (2006) to obtain an upper bound C infgelementcircleplusT/2Hn ||f - g||p.\n\nLet \u03bb = \u03b6T/2 + 1 = 2T(2 + d - 2) + 1, \u03b1 = s/2, m = 1, P(D) = \u0394S in Ditzian's (1998) Theorem 6.1, which gives an upper bound on the best spherical harmonic approximation in Lp(Sd-1) to functions in Ws(Sd-1) (see also Kamzolov, 1983), then apply equation (4.1) in Ditzian (1998) again to obtain the desired result.\n\nIf the function f is in L2(Sd-1) then Equations (A.17) and (A.11) imply that Q2pf (x) = Q2pf (-x) and Q2p+1f (x) = -Q2p+1f (-x) for p \u2208 N. Consequently, the odd order terms in the condensed harmonic expansions of f, f+ and f- satisfy Q2p+1f- = Q2p+1f and Q2p+1f+ = 0. Likewise, for the even order terms in the condensed harmonic expansions of these functions Q2pf+ = Q2pf and Q2pf- = 0 hold. We conclude that the sum of the odd order terms in the condensed harmonic expansion corresponds to f- and that of the even order terms to f+. As anticipated from the analysis of the d = 2 case, the operator H reduces the even part of f\u03b2 to a constant 21, therefore Fourier-Laplace series expansions for f\u03b2 derived later involve only odd order terms.\n\nWe now provide a formula that is used to obtain our estimator for f\u03b2. If a non-negative function f has its support included in some hemisphere of Sd-1 then\n\n(A.24) f(x) = 2f-(x) I{f-(x) > 0}.\n\nDenote the support of f by supp(f) and let -supp(f) = {x | -x \u2208 supp(f)}, then this formula follows from the fact that f-(x) = f+(x) \u2265 0 on supp(f) while f-(x) = -f+(x) \u2264 0 on -supp(f) and both f- and f+ are 0 on Sd-1 \\ (supp(f) \u222a -supp(f)).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3f4bd4c-d4b5-4225-a2ca-3f007dd6ae4c": {"__data__": {"id_": "f3f4bd4c-d4b5-4225-a2ca-3f007dd6ae4c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "668fe9bf-02e0-4518-8a22-f3368608d1b2", "node_type": "4", "metadata": {}, "hash": "1c0abcc55a377818a8591dac45d8ec7a913b584aec740cf2ce08b9cb94c5805a", "class_name": "RelatedNodeInfo"}}, "text": "# A-10\n\n# GAUTIER AND KITAMURA\n\nRemark A.2. If f is a probability density function, the coefficient of degree 0 in the expansion of f on surface harmonics is 1/|Sdminus1|. Conversely, any harmonic polynomial or series such that its degree 0 coefficient is 1/|Sdminus1| integrates to one.\n\nThe next theorem shows that Fourier-Laplace series on the sphere is a natural tool for the study of the operator H.\n\n# Theorem A.2 (Funk-Hecke Theorem).\n\nIf g belongs to Hncommad for some n, and a function F on (\u22121, 1) satisfies\n\nintegraldisplay 1\nminus1|F (t)|2(1 \u2212 t2)(dminus3)slash2dt &lt; infinity,\n\nthen\n\nintegraldisplay\nSdminus1F (xprimey)g(y)d\u03c3(y) = lambdan(F )g(x)\n\nwhere\n\nnu(d)(1)minus1integraldisplay 1\nlambdan(F ) = |Sdminus2|Cn\nminus1F (t)Cnnu(d)(t)(1 \u2212 t2) dminus32 dt.\n\nIn other words, the kernel operator defined by\n\nparenleftbigg\nintegraldisplay F (xprimey)f (y)d\u03c3(y)\nparenrightbigg\n\nf element L2(Sdminus1) mapstoarrowright x mapstoarrowright Sdminus1 element L2(Sdminus1)\n\nis, in the subspace Hncommad, equivalent to the multiplication by lambdan(F ). Thus a basis of surface harmonics diagonalizes an integral operator if its kernel is a function of the scalar product xprimey.\n\nRemark A.3. Healy and Kim (1996) use Fourier-Laplace expansions to analyze a deconvolution problem on S2. As we shall see below, the Addition Formula along with condensed harmonic expansions provide a general treatment that works for arbitrary dimensions.\n\n# A.1.4. The Hemispherical Transform.\n\nThe hemispherical transform H, defined by Hf (x) = integraltextSdminus1 Ibraceleftxprimey greaterequal 0bracerightf (y)d\u03c3(y), plays a central role in our analysis. It is a special case of the operator considered in the Funk-Hecke theorem above, with F (t) = Ibraceleftt element [0, 1]braceright, therefore the next proposition follows.\n\n# Notation.\n\nWe define lambda(n, d) = lambdan (I braceleftt element [0, 1]braceright) for d greaterequal 3 and lambda(n, 2) = 2 sin(npislash2).n\n\n# Proposition A.4.\n\nWhen d greaterequal 2, the coefficients lambda(n, d) have the following expressions\n\n- (i) lambda(0, d) = barSdminus1bar22bar\n- (ii) lambda(1, d) = barSdminusdminus1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "288fdf20-f0a5-424b-b42a-575dfef5fbd4": {"__data__": {"id_": "288fdf20-f0a5-424b-b42a-575dfef5fbd4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50f74c09-f222-4af3-abec-6b4df3da63f5", "node_type": "4", "metadata": {}, "hash": "ea941f07af0b50513c8daf855b8334b7190ca2d8fe6a27d0f65e23fa6f2824ba", "class_name": "RelatedNodeInfo"}}, "text": "# A-11\n\n(iii) p element N backslash braceleft0braceright, lambda(2p, d) = 0\n\n(iv) p element N, lambda(2p + 1, d) = (dminus1)(d+1)periodcenteredperiodcenteredperiodcentered(d+2pminus1) (minus1)p barSdminus2bar1periodcentered3periodcenteredperiodcenteredperiodcentered(2pminus1).\n\n# Proof of Proposition A.4.\n\nDe\ufb01ne \u03b1(n, d) := Cnnu(d)(1)|Sdminus2|minus1lambdan (I braceleftt element [0, 1]braceright). By the Funk-Hecke theorem\n\n\u03b1(n, d) = Cnnu(d)(t)(1 \u2212 t2)(dminus3)/2dt,\n\nthus using (A.7),\n\n\u03b1(n, d) = n! ((d \u2212 1)/2)n(\u22122)\u2212n(d \u2212 2)n dtn(1 \u2212 t2)n+(dminus3)/2dt.\n\nTherefore for n greaterequal 1 and d greaterequal 3,\n\n\u03b1(n, d) = \u2212 (\u22122)\u2212n(d \u2212 2)n dn\u22121 vextendsingle\n\nn! ((d \u2212 1)/2)n dtn\u22121 (1 \u2212 t2)n\u22121+(dminus3)/2dtvextendsinglet=0vextendsingle\n\nsince the term on the right hand-side is equal to 0 for t = 1. To prove that the coefficients \u03b1(2p, d) are equal to zero for p positive it is enough to prove\n\nd2p+1 vextendsingle\n\ndt2p+1 (1 \u2212 t2)2p+1+mvextendsinglevextendsinglet=0 = 0, m greaterequal 1, p greaterequal 0.\n\nThe Faacutea di Bruno formula gives that this quantity is equal to\n\n\u2211 (\u22121)2p+1minusk2 (2p + 1)!(m + 1) periodcentered periodcentered periodcentered (2p + 1 + m)(1 \u2212 t2)m+k2 (2t)k1vextendsinglevextendsingle\n\nk1+2k2=2p+1 k1!k2!\n\nand the result follows since k1 in the sum cannot be equal to 0.\n\nWhen n = 2p+1 for p element N we obtain, again using the Faacutea di Bruno formula, that the derivative at t = 0 is equal to\n\n(\u22121)p (2p)!p! [(2p + 1 + (d \u2212 3)/2)(2p + (d \u2212 3)/2) periodcentered periodcentered periodcentered (p + 2 + (d \u2212 3)/2)].\n\nTogether with (A.9), the desired result follows. For the case d = 2 we use Proposition A.1.\n\nDe\ufb01ne Lodd(Sdminus1) and Hodd(Sdminus1) as the restrictions of L2(Sdminus1) and Hs(Sdminus1) to odd functions2 s and similarly L2even (Sdminus1) and Heven(Sdminus1) for even functions.\n\nThe following corollary is a direct consequence of the Funk-Hecke Theorem and Proposition A.4, and corresponds to an observation made in Remark A.1 for the d = 2 case.\n\n# Corollary A.1.\n\nThe null space of the hemispherical transform H is given by\n\nker H = circleplusdisplay H2p, d = infinity braceleftbigg integraldisplay bracerightbigg\n\nf element Leven(Sdminus1) :2 Sdminus1f (x)d\u03c3(x) = 0,\n\np=1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2e99a4e-5875-4ea6-af0c-7b8605c3ebfd": {"__data__": {"id_": "f2e99a4e-5875-4ea6-af0c-7b8605c3ebfd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "999e4dc9-2f1a-45fb-9658-0cfb9cf3dd47", "node_type": "4", "metadata": {}, "hash": "9c6dfd6ea55972f4e169f1ff5f31b93cef356a7af945d4f376392d31af58b6bb", "class_name": "RelatedNodeInfo"}}, "text": "# A-12\n\n# GAUTIER AND KITAMURA\n\nwhen H is viewed as an operator on L2(Sd-1).\n\nThe spaces H0 and H2p+1 for p element N are the eigenspaces associated with the non-zero eigenvalues of H.\n\nAs a consequence of Proposition A.4, H is not injective and restrictions have to be imposed in order to ensure identification of fbeta.\n\nSection 3 presents sufficient conditions that allows us to reconstruct fbeta from fbeta minus.\n\nThe following proposition can be found in Rubin (1999).\n\n# Proposition A.5.\n\nH is a bijection from Lodd(Sd-1) to Hodd2d/2(Sd-1).\n\n# Lemma A.2.\n\n(A.26) h(n, d) equiv asymptotic nd-2,\n\n(A.27) |lambda(2p + 1, d)| equiv asymptotic p-d/2.\n\nProof. Estimate (A.26) is clearly satisfied when d = 2 and 3 since h(n, 2) = 2 and h(n, 3) = 2n + 1.\n\nWhen d \u2265 4 we have\n\nh(n, d) = (d \u2212 2)! (n + (d \u2212 2)/2)[(n + 1)(n + 2) (n + d \u2212 3)],\n\nand the results follow.\n\nNext we turn to (A.27). When d is even and p \u2265 d/2\n\n|lambda(2p + 1, d)| = kappad (2p + 1)(2p + 3) (2p + d \u2212 1)\n\nwhere kappad = |Sd-2|1 (d \u2212 1)d \u2212 1\n\nand (A.27) follows. Sterling's double inequality (see Feller (1968) p.50-53), that is,\n\n\u221a(2p n + 1/2) exp(-n + 1) < n! < \u221a(2p n + 1/2) exp(-n + 1/12n + 1/12n)\n\nimplies that (2p!)2 equiv \u221a(p) (2p)!\n\nand therefore\n\n1 (2p \u2212 1) equiv \u221a(p2 4 (2p).\n\nThus for p \u2265 d/2 and d odd we have\n\n|lambda(2p + 1, d)| equiv (2p + 2)(2p + 4) (2p + d \u2212 1)\n\nand (A.27) holds for both even and odd d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68cc4810-eeef-43b8-b599-ad9278c709e5": {"__data__": {"id_": "68cc4810-eeef-43b8-b599-ad9278c709e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82ee5183-d20f-4fc9-88e1-e990b3526918", "node_type": "4", "metadata": {}, "hash": "d00db3aa0a7fd4d29508c37c564a7a148ae817bb849b69a938191026d3e4a755", "class_name": "RelatedNodeInfo"}}, "text": "# A-13\n\nWe can now easily check that\n\n# Proposition A.6.\n\nFor all s > 0, there exists positive constants Cl and Cu such that for all f in Hs(Sd-1)\n\nCl ||f - v||2 \u2264 ||H(f - v)||2 + d/2 \u2264 Cu ||f - v||2.\n\n# Proof of Proposition A.6.\n\nBy definition we have\n\n||H(f)||2, + d/2 = \u2211p=0\u221e (1 + \u03b62p+1, d)s + d/2 ||Q2p+1, d H(f)||2\n\nwhere according to the Funk-Hecke Theorem\n\nQ2p+1, d H(f) = Q2p+1, d (\u2211q=0\u221e Q2q+1, d f)\n\n= Q2p+1, d (\u2211q=0\u221e \u03bb(2q + 1, d) Q2q+1, d f)\n\n= \u03bb(2p + 1, d) Q2p+1, d f.\n\nThe result follows since Lemma A.2 implies that (1 + \u03b62p+1, d)s + d/2 \u03bb2(2p + 1, d) \u2261 (1 + \u03b62p+1, d)s.\n\nThe factor d/2 in Proposition A.6 corresponds to the degree of \"regularization\" due to smoothing by H. Now the inverse of an odd function f- is given by\n\nH-1(f-)(y) = \u2211p=0\u221e \u03bb(2p + 1, d) \u222bSd-1 q2p+1, d (x, y) f-(x) d\u03c3(x).\n\nThis is straightforward given our results at hand: for example, operate H on the RHS to see:\n\nH(\u2211p=0\u221e 1Sd-1 q2p+1, d (x, y) f-(x) d\u03c3(x)) = \u2211p=0\u221e \u03bb(2p + 1, d) H Q2p+1, d f-1\n\n= \u2211p=0\u221e \u03bb(2p + 1, d) Q2p+1, d f- (by the Funk-Hecke Theorem)\n\n= \u03bb(2p + 1, d) = f-.\n\nIf f- belongs to Hd/2(Sd-1), then H-1(f-)(b) is a well-defined L2(Sd-1) function. Otherwise it should be understood as a distribution and is only defined in a Sobolev space with negative exponent.\n\nMoreover, if d is a multiple of 4, it is possible to relate the inverse of the operator H with differentiation as in the case of d = 2:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb6c8d10-77e3-43de-820a-dc448e147f50": {"__data__": {"id_": "fb6c8d10-77e3-43de-820a-dc448e147f50", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f91b837-fd94-4813-b2ab-a58be62c3e4a", "node_type": "4", "metadata": {}, "hash": "047df83f5e4a27b1ec589628df9b758817a542c32572f15c7c8996101548c492", "class_name": "RelatedNodeInfo"}}, "text": "# A-14\n\n# GAUTIER AND KITAMURA\n\n# Proposition A.7\n\nIf d is a multiple of 4,\n\nH-1 = |Sd-2|\n\u220fk=1d/4 [\u2212\u0394S + 2(k \u2212 1)(d \u2212 2k)]\n\n# Proof of Proposition A.7\n\nIf we consider the case where d is even, we know from Proposition A.4, that\n\n\u03bb(2p + 1, d) = (\u22121)p|Sd-2|(2p + 1)(2p + 3) . . . (d + 2p \u2212 1).\n\nThus if d is a multiple of 4,\n\n\u03bb(2p + 1, d) = |Sd-2|\n\u220fk=1d/4 [\u2212\u03b62p+1, d + 2(k \u2212 1)(d \u2212 2k)].\n\nUsing this and (4.1),\n\nH-1 = \u2211p=0\u221e \u03bb(2p + 1, d) Q2p+1, d\n\n= \u2211p=0\u221e |Sd-2|(td/4)\n\u220fk=1d/4 [\u2212\u03b62p+1, d + 2(k \u2212 1)(d \u2212 2k)] Q2p+1, d.\n\nRecall (A.18) and the proposition is proved.\n\nThis connection between the inverse of H and differentiation suggests that a Bernstein-type inequality might hold for H-1. Indeed, even though the above inversion formula is concerned with d that are multiples of 4, the following Bernstein inequality holds for every dimension.\n\n# Theorem A.3 (Bernstein inequality)\n\nFor every d \u2265 2 and every q \u2208 [1, \u221e], there exists a positive constant B(d, q) such that for all P in \u2113+ T=0 H2p+1, d\n\n(A.29)\n||H-1P||q \u2264 B(d, q)Td/2||P||q.\n\n# Proof of Theorem A.3\n\nWe can write\n\nwhere P1(D) and P2(D) are defined for all odd function f- by\n\nP1(D)f- =\n\nP2(D)f- = \u2212\u2211p=0\u221e\n\nH-1 = P1(D) \u2212 P2(D)\n\n\u2211p=0\u221e 1\n\u222bSd-1 q4p+3(x, y)f-(x)d\u03c3(x)\n\n\u03bb(4p + 3)\n\u222bSd-1 q4p+1(x, y)f-(x)d\u03c3(x).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c01b273-4bd8-4cc1-ae1f-02a2759a6e06": {"__data__": {"id_": "3c01b273-4bd8-4cc1-ae1f-02a2759a6e06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "acf6df41-134f-4062-b5f9-bd04fbabffc2", "node_type": "4", "metadata": {}, "hash": "d578232fc7bd33eb7cc18e4ef627fca48c3797652e5eea3e74b5fcbaaf57e69f", "class_name": "RelatedNodeInfo"}}, "text": "# A-15\n\nP1(D) and P2(D) are two unbounded operators on B = Lodd(Sdminus1) with non-positive eigenvalues. We apply Theorem 3.2. of Ditzian (1998) to \u2212P1(D) and \u2212P2(D) choosing \u03b1 = 1. Condition (1.6) of Ditzian (1998) can be verified using Proposition 2.2 with r = 1 and p = q and the fact that for the Cesaro kernels Chl are uniformly bounded in L1(Sdminus1) for l > dminus22 (see, e.g. Bonami and Clerc, 1973).\n\nWe see, using the triangle inequality, that for all P in T=0 H2p+1commad,p\n\n|H-1P|q \u2264 C\u03bb2(2T + 1, d) |P|q \u2264 CT |P|q.\n\nThe last inequality follows from (A.27).\n\nRubin (1999) gives other inversion formulas for the Hemispherical transform in terms of differential operators. The fact that the inversion roughly corresponds to differentiation is another manifestation of the ill-posedness of our problem at hand. The inverse operator H-1 is indeed unbounded. We call the factor d/2 in (A.29) the degree of ill-posedness of the inverse problem. For the case q = 2, there exists a lower bound for |H-1P|q in (A.29) of order Td/2 as well, implying that the upper bound Td/2 in the order of T obtained in Theorem A.3 is tight.\n\n# A.1.5. Estimators for the Choice Probability Function\n\nThis section considers estimation of the choice probability function r and its extension R. We propose an estimator for r, which, in turn, yields a computationally simple estimator for f\u03b2. Also the asymptotic results presented here are useful for the next section where we study the limiting properties of our estimator for the random coefficients density f\u03b2.\n\nSince R is square integrable on Sdminus1, it has a condensed harmonic expansion which enables us to obtain the expressions in the next theorem.\n\n# Theorem A.4\n\nFor x in Sdminus1, we have\n\n(A.30) R(x) = 2 + \u2211p=0\u221e E(2Y \u2212 1)q2p+1commad(X, x) fX(X)\n\nThis suggests an estimator of the form\n\nR\u22271(x) = 21 + circumflex\u2212 with R1 = 1/N \u2211i=1N (2yi \u2212 1) \u2211p=0q2p+1commad(xi, x)\n\nwhere fX is an estimator of fX and TN is a suitably chosen sequence diverging to infinity with N.\n\nNote that the second summation corresponds to the Dirichlet kernel. We can generalize this, by", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9899291-6d8c-4e96-aef1-eea5415e583e": {"__data__": {"id_": "c9899291-6d8c-4e96-aef1-eea5415e583e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ed27b10-e7d9-4664-93ad-9c36d6f4f032", "node_type": "4", "metadata": {}, "hash": "5a92ec89f32253c8d434e6fcf95c195cbfcc11d974e9284cd5e866aa3ee99faa", "class_name": "RelatedNodeInfo"}}, "text": "# A-16\n\n# GAUTIER AND KITAMURA\n\nintroducing a class of estimators of the form\n\n1 &nbsp; \u2211N (2yi (\u2212 1)K2TN (xi, x))\n\n(A.31)\n\nRcircumflex2 minus(x) = N-1 \u2211i=1 fcircumflexX (xi) minus\n\nwhere K2TN is the odd part of a kernel of the form (A.23) satisfying Assumption A.1, such as the two minus kernels in Proposition A.3.\n\nThe estimator (A.31) is convenient, though the plug-in term fcircumflexX has to be treated with care. We avoid restrictive assumptions on the distributions of covariates and allow fX(x) to decay to zero as x approaches the boundary of its support H+. To deal with the latter problem, we modify (A.31) by\n\n\u2211N (2yi \u2212 1)K2TN (xi, x) minus\n\n(A.32)\n\nRcircumflex minus(x) = N-1 max( fcircumflexX (x), mN)\n\nwhere mN is a trimming factor going to 0 with the sample size. Our estimator for R is then\n\n(A.33)\n\nR = 2 + circumflexhatwide Rminus.\n\nRemark A.4. Alternative estimators of Rminus are available. For example, one may use kernel regression on the sphere to estimate r in order to obtain an estimator for Rminus. As noted before, however, we then need to use numerical integration to evaluate (4.5) to calculate fminus.circumflex beta\n\nProof of Theorem A.4. R has the following condensed harmonic expansion\n\nR(x) = 2 + \u2211(Q2p+1 commadR)(x).1\n\ninfinity\n\np=1\n\nWe then write using (3.2), changing variables and using (A.11), integral display\n\n(Q2p+1 commadR)(x) = \u222bSd-1 q2p+1 commad(x, z)R(z)d\u03c3(z)\n\n= \u222bH+ q2p+1 commad(x, z)r(z)d\u03c3(z) + \u222bH- q2p+1 commad(x, z)(1 \u2212 r(\u2212z))d\u03c3(z)\n\n= \u222bH+ q2p+1 commad(x, z)r(z)d\u03c3(z) \u2212 \u222bH+ q2p+1 commad(x, z)(1 \u2212 r(z))d\u03c3(z)\n\n= \u222bH+ q2p+1 commad(x, z)(2r(z) \u2212 1)d\u03c3(z)\n\n= q2p+1 commad(x, z)E[2Y \u2212 1 | X = z]fX(z)d\u03c3(z)\n\n= H+\n\n= fX(z)\n\n= E[ (2Y \u2212 1)q2p+1 commad(x, X)]fX(X)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ae11819-bdd6-4442-9d42-0423ce0d1494": {"__data__": {"id_": "2ae11819-bdd6-4442-9d42-0423ce0d1494", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e309dd5b-52ff-47f0-84f9-0be0a1e3ba07", "node_type": "4", "metadata": {}, "hash": "8ace05bc01e03ea2eb6bdb0b5879d0f3658be44d44cba64d0ebcbcc7d8d0d487", "class_name": "RelatedNodeInfo"}}, "text": "# A.1.6. Proofs of Main Results.\n\n# Proof of Proposition 3.1.\n\nIt is straightforward that the model (1.1) and Assumption 1.1 imply that the choice probability function r given by (1.2) is homogeneous of degree 0.1 + HparenleftBigf betaminusparenrightBig Proposition A.5 along with the fact that R = 2 with f beta element Lodd(Sdminus1) implies that R belongs to Hdslash2(Sdminus1).\n\nWe now turn to the proof of sufficiency. If the extension R given by (3.2) belongs to Hdslash2(Sdminus1) then so does Rminus and Proposition A.5 shows that there exists a unique odd function f minus in L2(Sdminus1) such that R = 2 + H parenleftbigf minusparenrightbig = Hparenleftbigg|Sd1minus1| + f minusparenrightbigg.\n\nMoreover, since 0 lessequal R(x) lessequal 1 holds for every x element Sdminus1, the above relationship implies that 2 greaterequal1 Hf minus(x), x element Sdminus1. But Hf minus(x) greaterequal integraltextbraceleftf minus(b)greaterequal0braceright fminus(b)d\u03c3(b) holds for some x. Therefore we conclude that 21 greaterequal integraltextbraceleftf minus(b)greaterequal0braceright fminus(b)d\u03c3(x) = \u2212 integraltextbraceleftf minus (b)lessequal0braceright fminus(b)d\u03c3(b), thus integraltextSdminus1 |f minus(b)|d\u03c3(b) lessequal 1. Also, following the discussion in Section A.1.3, barSd1minus1bar + f minus integrates to 1.\n\nWe have seen in Corollary A.1 that for even function g that has 0 as the coefficient of degree 0 in its expansion on the surface harmonics (i.e. an even function that integrates to zero over the sphere),\n\nR = H parenleftbigg |Sdminus1| + f minusparenrightbigg1.\n\nNow consider integraldisplay g = |f minus| \u2212 1 |Sdminus1|f minus(b)|d\u03c3(b), then this certainly is even and integrates to zero. Using this, define\n\nf betaasteriskmath := g + minus1| + f minus = 2f minusIbraceleftf minus > 0braceright + minus1| 1 \u2212 greaterequal 0.\n\nObviously f betaminus = f minus. This function f beta is non-negative and integrates to one, and thus it is a properasteriskmath asteriskmath probability density function (pdf). It is indeed bounded from below by barSd1minus1bar parenleftbig1 \u2212 integraltextSdminus1 |f minus(b)|d\u03c3(b) parenrightbig.\n\nAs a consequence, there exists a pdf f betaasteriskmath such that\n\nR = H parenleftbigf betaparenrightbig = 2 + H parenleftbigf beta asteriskmath 1 asteriskmathminusparenrightbig and for all x in H+, r(x) = HparenleftBigf betaasteriskmathparenrightBig(x).\n\n# Proof of Theorem 4.1.\n\nWe use the shorthand notation I(b) := Ibraceleftf beta minus(b) > 0braceright and I(b) := Ibraceleft circumflexcircumflex fminus(b) > beta 0braceright. Then fbeta = 2f beta minusI and fcircumflex = 2 f betabeta circumflex Iminuscircumflex.\n\nWe write (2yi \u2212 1)Hminus1 parenleftBig minus parenrightBig 1 \u2211N K2TN (xi, periodcentered) (b) f minusbetacommaT (b) = N i=1 max (fX (xi), mN).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06baff55-bd19-4cce-87c0-3d09702493cd": {"__data__": {"id_": "06baff55-bd19-4cce-87c0-3d09702493cd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf72bcf6-8b39-4a46-8f66-d38b9b5f67d3", "node_type": "4", "metadata": {}, "hash": "ab71543cdf960b54c1bfe8d1adbd95f231db01e325aeba88514682a7ad515f85", "class_name": "RelatedNodeInfo"}}, "text": "# A-18\n\n# GAUTIER AND KITAMURA\n\n(2yi \u2212 1)H-1\n\nf\u2212(b) = 1\n\n\u2211N K2TN(xi, periodcentered) (b)\n\n\u03b2 N fX(xi)\n\ni=1\n\nand use the decomposition\n\n(A.34) f\u03b2^\u2212 \u2212 f\u2212\u03b2 = (f\u2212 \u2212 f\u2212\u03b2, T) + (f\u2212\u03b2, T \u2212 E(f\u2212\u03b2, T)) + (E(f\u2212\u03b2) \u2212 E(f\u2212))\u03b2 + (E(f\u2212)\u03b2 \u2212 f\u2212\u03b2)\n\nand denote the terms on the right hand side by Sp (stochastic component due to plug-in), Se (stochastic component of the infeasible estimator f\u2212\u03b2, T), Bt (trimming bias) and Ba (approximation bias).\n\nTake q element [1, infinity),\n\n||f\u03b2^ \u2212 f\u03b2||q = \u222b f\u03b2 q (f\u03b2^ (b) \u2212 f\u03b2(b))q d\u03c3(b)\n\n= f\u03b2^(b)=1(f\u03b2^ (b) \u2212 f\u03b2(b))q d\u03c3(b) + f\u03b2^(b)=1(f\u03b2^ (b) \u2212 f\u03b2(b))q d\u03c3(b)\n\nI(b)=1, I(b)=0, I(b)=0, Ic(b)=1, I(b)=1, Ic(b)=0\n\nA1 + A2 + A3 + A4.\n\nObviously\n\nA1 = f\u03b2^(b)=1(2f\u2212(b) \u2212 2f\u03b2^\u2212(b))q d\u03c3(b)\n\nA4 = 0. Also,\n\nA2 = f\u03b2^(b)=1(2f\u2212(b) \u2212 f\u03b2(b))q d\u03c3(b).\n\nBut given I(b) = 0 and Ic(b) = 1, 2f\u03b2^\u2212(b) > 0, f\u03b2(b) = 0 and 2f\u03b2^\u2212(b) \u2264 0, so replacing f\u03b2 with 2f\u03b2^\u2212 in the bracket,\n\nA2 \u2264 I(b)=0, Ic(b)=1(2f\u2212(b) \u2212 2f\u03b2^\u2212(b))q d\u03c3(b).\n\nSimilarly,\n\nA3 = f\u03b2^(b)=0(f\u03b2(b) \u2212 2f\u03b2^\u2212(b))q d\u03c3(b).\n\nand given I(b) = 1 and Ic(b) = 0, 2f\u03b2^\u2212(b) > 0, f\u03b2(b) = 0 and 2f\u2212(b) \u2264 0, so replacing f\u03b2 with 2f\u03b2^\u2212 in the bracket,\n\nA3 \u2264 I(b)=0, Ic(b)=1(2f\u2212(b) \u2212 2f\u03b2^\u2212(b))q d\u03c3(b).\n\nOverall,\n\n||f\u03b2^ \u2212 f\u03b2||q \u2264 2q||f\u2212||q \u2212 f\u03b2^\u2212||q.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5df63d7b-8088-4114-ac3d-452a6e7f0b5d": {"__data__": {"id_": "5df63d7b-8088-4114-ac3d-452a6e7f0b5d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0c6bad1-4df4-4755-b2f4-bb537c8e219f", "node_type": "4", "metadata": {}, "hash": "a458b37911b9749e6e076f6f1fe254569f835eb5d44a69930503d009c58c053c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "196820ff-8c45-4d57-8d3e-0cdc452de927", "node_type": "1", "metadata": {}, "hash": "8354e7f9d2a88f968ca93e1aa4f2272f7b486a1f06d5ff70154191f836c7d0c2", "class_name": "RelatedNodeInfo"}}, "text": "# A-19\n\nA similar proof can be carried out replacing Lq(Sdminus1) by Linfinity(Sdminus1). Thus it is enough to consider the behavior of circumflexminusf beta \u2212 f betaminus instead of circumflexfbeta \u2212 fbeta. As noted above, the former can be decomposed into four terms, Sp, Se, Bt and Ba.\n\n# Analysis of Sp\n\nNote that for q element [1, infinity]\n\nvextenddouble\nvextenddouble\nparenlefttp\nparenlefttp\nparenleftbt max (fX (xi), mN parenrightBig \u2212 1parenrighttpparenrighttpvextenddoublevextenddouble\nvextenddoubleHminus1\n1\n\u2211N\n(2yi \u2212 1)K2TN (xi, periodcentered)minus\nvextenddouble\n\nbardblSpbardblq =\nvextenddouble\nvextenddouble\nparenleftbt N\nparenleftBig circumflex\nparenrightbtparenrightbtvextenddoublevextenddouble\nvextenddouble\nN\nmax(fX (xi), mN)\nmax fX (xi), mN\nvextenddoubleq\n\ni=1\nvextenddouble\nvextenddouble\nvextendsingle\nvextendsingle\nvextendsingle\nvextendsingle\nvextenddouble\n1\n\u2211|K2TN (xi, periodcentered|vextenddouble\nN\nvextenddouble\nmax (fX (xi), mN parenrightBig \u2212 1vextendsingle\nvextendsingle\n\nlessequal B(d, q)T Ndslash2\nvextenddouble\nvextenddouble\nN\nmax(fX (xi), mN)\nmax fX (xi), mN\nvextenddoubleq\n\ni=1\nvextenddouble\nvextenddouble\nvextendsingle\nvextendsingle\nvextendsingle\nvextendsingle\nvextenddouble\n1\n\u2211|K2TN (xi, periodcentered)|vextenddouble\nN\nvextenddouble\nvextendsingle\nmax (fX (xi), mN parenrightBig \u2212 1vextendsingle\nvextendsingle\n\nlessequal B(d, q)T Ndslash2mNminus1\nvextenddouble\nvextenddouble\nN\nvextenddouble\nvextenddoubleq\ni=1commaperiodperiodperiodcommaNmax\nvextendsingle\nmax\nparenleftBig circumflex\ni=1\nvextendsingle\nfX (xi), mN\nvextendsingle\nvextenddouble\nN\nvextenddouble\nvextendsingle\nvextendsingle\n\nlessequal B(d, q)T Ndslash2mNminus2\nvextenddouble\nvextenddouble\n1\n\u2211|K2TN (xi, periodcentered)|vextenddoublevextenddouble\nvextenddoubleq\nmax\nvextendsingle\nfX (xi) \u2212\nvextendsingle\nfX (xi)vextendsinglecircumflex\nN\ni=1\ni=1commaperiodperiodperiodcommaN\n\nholds, where we have used the triangle inequality. The Lq-norm on the right hand side is bounded from above by\n\nvextenddouble\nvextenddouble\nN\n1\n\u2211|K2TN (xi, periodcentered)| \u2212 E |K2TN (X, periodcentered)|vextenddoublevextenddouble\nvextenddoubleq\n+ bardblE |K2TN (X, periodcentered)|bardblq := bardblT1bardblq + bardblT2bardblq.\n\ni=1\n\nFirst consider the term bardblT1bardblq. We begin with the case of q element [1, 2].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "196820ff-8c45-4d57-8d3e-0cdc452de927": {"__data__": {"id_": "196820ff-8c45-4d57-8d3e-0cdc452de927", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0c6bad1-4df4-4755-b2f4-bb537c8e219f", "node_type": "4", "metadata": {}, "hash": "a458b37911b9749e6e076f6f1fe254569f835eb5d44a69930503d009c58c053c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5df63d7b-8088-4114-ac3d-452a6e7f0b5d", "node_type": "1", "metadata": {}, "hash": "1955222934b46e71a632e4dfe7ee5793909be5bc8221764d779d77a751ca085e", "class_name": "RelatedNodeInfo"}}, "text": "The Lq-norm on the right hand side is bounded from above by\n\nvextenddouble\nvextenddouble\nN\n1\n\u2211|K2TN (xi, periodcentered)| \u2212 E |K2TN (X, periodcentered)|vextenddoublevextenddouble\nvextenddoubleq\n+ bardblE |K2TN (X, periodcentered)|bardblq := bardblT1bardblq + bardblT2bardblq.\n\ni=1\n\nFirst consider the term bardblT1bardblq. We begin with the case of q element [1, 2]. By the Hdieresisolder inequality,\n\nE bracketleftbigbardblT1bardblqbracketrightbig =\nintegraldisplay\nq\nintegraldisplaySdminus1\nE [T1(x)q] d\u03c3(x)\nlessequal\nE bracketleftbigT1(x)2bracketrightbigqslash2 d\u03c3(x)\nSdminus1\n\nwhere\n\nE bracketleftbigT1(x)2bracketrightbig lessequal\n1\nbracketleftBig (K2TN (X, x))2bracketrightBig\n\n(A.36)\nN E\nlessequal\nC\nbardblK2TN (star2, x)bardbl22\n(boundedness assumption on fX)\n\nN vextenddouble2TNvextenddouble\u2211\nvextenddouble2\nvextenddouble\nvextenddouble\n=\nC\nvextenddouble\nvextenddouble\nchi(n, 2TN )qncommad(star2, x)vextenddoublevextenddouble\n\nN\nvextenddoublen=0\nvextenddouble2\nC\n2TN\n\u2211\nlessequal\nbardblqncommad(star2, x)bardbl22\n(by Assumption A.1(iv))\n\nN\nn=0", "mimetype": "text/plain", "start_char_idx": 1923, "end_char_idx": 2973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9169bd3b-f98d-4f88-ae05-4e481444a99c": {"__data__": {"id_": "9169bd3b-f98d-4f88-ae05-4e481444a99c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2de5b6e5-d385-4ca6-a646-b74ab2ec695c", "node_type": "4", "metadata": {}, "hash": "5aed5d7cb47605b7105f0d8ead53e8882070c6721ea32344198afb7cc73c9e86", "class_name": "RelatedNodeInfo"}}, "text": "# A-20\n\n# GAUTIER AND KITAMURA\n\nvextenddouble vextenddoubleCnnu(d)(star2x)vextenddouble2vextenddouble2\n\nC 2TN\n\n\u2211 h2(n, d)vextenddouble\n\nlessequal N n=0 |Sdminus1|2(Cnnu(d)(1))2\n\nlessequal NC h(n, d) (by (A.12))\n\nlessequal CTdminus1 N (by Lemma A.2).\n\nBy the Markov inequality,\n\n(A.37) T Ndslash2mN minus2bardblT1bardblq = OpparenleftBigmN minus2N minus1slash2T N(2dminus1)slash2parenrightBig,\n\nproviding a convergence rate for bardblT1bardblq, q element [1, 2]. So if we can establish a similar rate for bardblT1bardblinfinity,\n\nall Lq(Sdminus1) convergence rates of T1 for q element (2, infinity] can be interpolated between the L2(Sdminus1) and Linfinity(Sdminus1) convergence rates using the following inequality:\n\n(A.38) f element Linfinity(Sdminus1), bardblf bardblq lessequal bardblf bardbl22slashqbardblf bardblinfinity1minus2slashq.\n\nTo see this, note\n\nbardblf bardblq = bardblf 2|f |qminus2bardbl11slashq lessequal bracketleftbigbardblf 2bardbl1bardbl|f |qminus2bardblinfinitybracketrightbig1slashq = bardblf bardbl22slashqbardblf bardblinfinity1minus2slashq.\n\nWe can thus focus on bardblT1bardblinfinity. We cover the sphere Sdminus1 by N(N, r, d) geodesic balls (caps) (Bi)N(Ncommarcommad)i=1 of centers (tildei)N(Ncommarcommad)x i=1 and radius R(N, r, d), that is, Bi = braceleftx element Sdminus1 :\n\nnotation suggests, we let the radius of the balls depend on N, r and d, as specified more precisely below. Note that N(N, r, d) equivasymptotic R(N, r, d)minus(dminus1).\n\nWe now prove that for every epsilon1 > 0 positive, there exists a positive M such that\n\n(A.39) P vN T Ndslash2mNminus2 xelementSdminussup1 |T1(x)| greaterequal M\n\n(by Hdieresisolder)\n\nbardblx \u2212 tildeibardbl lessequal R(N, r, d)braceright. As thex\n\nparenrightBigg lessequal epsilon1 holds for an appropriately chosen sequence vN arrowup infinity. Write\n\n(A.40) P vN Tdslash2mN N minus2 sup |T1(x)| greaterequal M\n\nxelementSdminus1 parenlefttp braceleftBigvN Tdslash2mN minus2|T1(tildei)| greaterequal M/2bracerightBigparenrightbt lessequal P parenleftbt N x i=1commaperiodperiodperiodcommaN(Ncommarcommad)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9076c44f-5e82-45dd-a754-e7eff254eb31": {"__data__": {"id_": "9076c44f-5e82-45dd-a754-e7eff254eb31", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26e71e46-ff4b-46bd-8f4d-2f3ef79ecfa3", "node_type": "4", "metadata": {}, "hash": "9065115841e6377c9996153b0b691b854f41ee4e113bf102cd3379d66bb91150", "class_name": "RelatedNodeInfo"}}, "text": "# A-21\n\n+ P existentiali element braceleft1, . . . , N(N, r, d)braceright :\nvN T Ndslash2mNminus2 supi |T1(x) \u2212 T1(tildei)| greaterequal M/2x\n\nxelementB parenleftBig dslash2mN minus2|T1(tildei)| greaterequal M/2parenrightBig\n\nlessequal N(N, r, d)i=1commaperiodperiodperiodcommaNNsup P vN T N x\n\nwhere the last inequality is obtained using Assumption A.1 (ii) on the kernel and letting R(N, r, d) equivasymptotic\nmN vminus1T N2 N minus(dslash2+alpha)M (where \u03b1 is given in Assumption A.1 (ii)). Notice\n\nparenleftBigvN Tdslash2mN minus2|T1(tildei)| greaterequal M/2parenrightBig (A.41) P N x\n\n= Pparenleftbtvextendsingle\u2211parenlefttpvextendsingle N bracketleftBigg (X, tildei)|bracketrightBiggvextendsinglevextendsingle parenrighttp\n\nvextendsingle vextendsingle |K2TN dminus1, tildei)|(xj x |K2TN dminus1 x vextendsingle\n\nvextendsingle minus(dminus1)vN minus1T Nminusdslash2mN N M/2parenrightbt2\n\nvextendsinglej=1 \u2212 E vextendsingle greaterequal T N\n\nvextendsingle T N T N vextendsingle braceleftbigg 1 parenleftbigg t2 parenrightbiggbracerightbigg\n\nlessequal 2 exp \u2212 2 omega + Lt/3 (Bernstein inequality)\n\nwhere t = T Nminus(dminus1)vN minus1T Nminusdslash2mN N M/22 N parenleftBigg parenrightBigg\n\nomega greaterequal \u2211var |K2TN dminus1, tildei)|(Xj x j=1 T N\n\nvextendsingle vextendsingle K2TN (Xj , tildei)vextendsinglevextendsingle j = 1, . . . , N,\nvextendsingle vextendsingle x vextendsingle vextendsingle lessequal L (using (A.17) and (A.8)).\n\nvextendsingle T Ndminus1 vextendsingle vextendsinglesummationtext2TN\n\nvextendsingle n=0 chi(n, 2TN )qncommad(Xj , tildei)vextendsingle lessequalvextendsingle\n\nThe bound L in the last line is obtained by noting that |K2TN (Xj , tildei)| =x vextendsingle x vextendsingle\nC summationtext2TN |h(n, d)| equivasymptotic T Nn=0 dminus1, which follows from (A.17), (A.8) and (A.26).\n\nHere we can take omega = CN E[K2TN (X, tildei)x 2], then by the calculations in (A.36), we can write\nomega = CN T Nminus(dminus1). omega is the leading term in the denominator of the exponent in the last inequality.\n\nIf we take vN = (log N )minus1slash2mN N 1slash2T N2 minus(2dminus1)slash2, then\n\nomega + Lt/3 equivasymptotic (log N )M 2.t2 (A.42)\n\nAlso, use this vN in our choice of R(N, r, d) made above to get:\n\nR(N, r, d) equivasymptotic mN vminus1T N2 N minus(dslash2+alpha)M = (log(N ))1slash2N minus1slash2T dminus1minusalphaMN 2\n\nThus (A.43) N(N, r, d) equivasymptotic R(N, r, d)minus(dminus1) = exp (C1 log N + o(log N ))\n\nfor some constant C1 that might be greater than 2 1(d \u2212 1), depending on the value of \u03b1. Indeed, TN\ndoes not grow more than polynomially fast in N . (A.40), (A.41), (A.42) and (A.43) imply that, for a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e574c12f-81c7-4ad3-8857-5365e8699520": {"__data__": {"id_": "e574c12f-81c7-4ad3-8857-5365e8699520", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a5d6bcf-58d3-4ab8-9cf7-3115c85f7dbe", "node_type": "4", "metadata": {}, "hash": "0898f9c7ef2dd18b4ff958b421d558e9c2c535262255b3a93d1ef0b4184a2675", "class_name": "RelatedNodeInfo"}}, "text": "# A-22\n\n# GAUTIER AND KITAMURA\n\npositive constants C and C2, (A.44) P vN T N-2mN-2 sup |T1(x)| &ge; M &le; C exp{(log N)(C1 - C2M2)}\n\nholds. For a large enough M, C1 - C2M2 < 0 and the right hand side of (A.44) converges to zero, so (A.39) follows. In summary, we have just shown that\n\nd-2mN|T1|&infin; = Op{(log N)1/2mN-2N-1/2TN(2d-1)/2}\n\nand with (A.37) and (A.38) we also conclude that\n\nd-2mN|T1|q = Op{(log N)1-1/q mN-2N-1/2TN(2d-1)/2}\n\nConcerning |T2|q, q &in; [1, &infin;], since fX is bounded by assumption, there exists a positive C such that\n\n|T2|q &le; C |K2TN(star1, starq)|1\n\nwhere integration in |.|1 is with respect to argument star1 and integration in |.|q is with respect to starq. But |K2TN(star1, starq)|1 is a constant and does not depend on starq, as previously noted. Thus\n\n|K2TN(star1, starq)|1|q = |Sd-1|1/q|K2TN(star1, starq)|1\n\nand we conclude that this term is O(1) using Assumption A.1 (i) on the kernel, thus\n\nd-2mN|T2|q = O{(-2TN)d/2}\n\nAnalogously to our treatment of |T1|q, we can prove that when q &in; [1, 2],\n\n|S|q = Op{(-1/N-1/2TN(2d-1)/2)}\n\nwhile for q &in; (2, &infin;]\n\n|S|q = Op{mN-1(log N)1/2-1/qN-1/2TN}\n\nLet us now turn to the bias term induced by trimming (2Y - 1)H-1(K2TN(X, periodcentered))(b) - fX(X)\n\nBt(b) = E[fX(X) max(fX(X), mN) - 1]\n\nintegral{z &in; Sd-1: fX(z) < mN} E[2Y - 1|X = z]H-1(K2TN(z, periodcentered))(b) fX(z)mN = -\n\nThis yields\n\n|Bt(b)| &le; |integral|H-1(K2TN(b, periodcentered))(z)| I {z &in; Sd-1: fX(z) < mN} d\u03c3(z)\n\n= |integral|Sd-1|H-1(K2TN(b, periodcentered))(z)| I {z &in; Sd-1: fX(z) < mN} d\u03c3(z)\n\n= |Sd-1|(2d-1)/2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18e7290b-76af-4900-8221-cc450bf6f102": {"__data__": {"id_": "18e7290b-76af-4900-8221-cc450bf6f102", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40f133c5-088a-4c4e-b570-e1f62fdc75ff", "node_type": "4", "metadata": {}, "hash": "0fe1e827d70005411c956e23e7d9f194d9c7ac545dc9aa0dc40700b8303644bf", "class_name": "RelatedNodeInfo"}}, "text": "# A-23\n\nthus, for every 1 \u2264 r \u2264 q,\n\n|Bt|q \u2264 vextendH-1 (K2TN (b, \u2022)) vextend - vextend - vd \u03c3 (fX < mN)1/q - 1/r + 1 (from Proposition A.2)\n\n\u2264 CB(d, r)TN/2 + (d - 1)(1 - 1/r)\u03c3 (fX < mN)1/q - 1/r + 1\n\nwhere in the last inequality we use Theorem A.3 and calculate an upper bound on the Lr-norm of the kernel by interpolation, using H\u00f6lder's inequality, between the uniformly bounded L1-norm and the upper bound on the sup norm of the order of TNd - 1 seen previously, C is a constant. We finally treat Ba using Assumption A.1 (iii) with the condition that f\u03b2 \u2208 Wq s(Sd - 1):\n\n|Ba|q \u2264 CTN-s.\n\nIn the case where fX \u2265 m \u03c3 a.e., we use the decomposition\n\nf\u03b2 - f\u03b2 = \u02c6f\u03b2 - f\u03b2 + (f\u2212\u03b2 - E[f\u2212\u03b2]) + (E[f\u2212\u03b2] - f\u03b2)\n\n= Stildep + \u02dce + Ba.\n\nNow for example,\n\nvextend vextend fX (xi) - fX (xi)\n\nvextend \u2211 |K2TN (xi, \u2022)|N vextend maxi=1,...,N | \u02c6fX (xi)|\n\nbecause fX is a consistent estimator in sup norm, \u02c6m \u03b51, \u03b51 > 0, \u2203N0 > 0 : n \u2265 N0, Pi=1,...,N | \u02c6min fX (xi)| > 2 \u2264 2\n\nand we can treat the terms Stildep and \u02dce on this event.\n\nProof of the corollaries 4.1, 4.2 and 4.3. The rate \u03b3mas in Corollary 4.1 comes from the fact that it coincides with the maximum of\n\nmin2 (\u03b3mas, -\u03b32 - \u03c1 + 2 - \u03b3d - 1, -\u03b32 + rX - 2\u03c1, -\u03b32 + \u03c1\u03c4 - \u03b3(d - 1)(1 - 1/q))\n\nfor rX/2 \u2264 \u03c1 < 1/2 and 0 < \u03b3 < 1/(d - 1) which is what we get from (4.9) and (4.10). Indeed, it is enough to find \u03b3(\u03c1) as the minimum of\n\nmin\u03b3 (d, -\u03c1 + 2 - \u03b3(d - 1), rX - 2\u03c1, \u03c1\u03c4 - \u03b3(d - 1)(1 - 1/q)).\n\nThe first is an increasing function of \u03b3 while the second and fourth are decreasing. The rest follows by simple computations. The proofs of the convergence in probability on Corollaries 4.2 and 4.3 is similar and simpler because there is only one parameter \u03b3. In order to prove the strong uniform consistency", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cae01cb1-91df-44c6-a6b7-9aabdb757c16": {"__data__": {"id_": "cae01cb1-91df-44c6-a6b7-9aabdb757c16", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5646738-50e3-4fc5-a9ab-3c137642ef66", "node_type": "4", "metadata": {}, "hash": "e1205d5f8720280f80ffb0024404797293cf62860d2e9187ab965b6c8fc7fe48", "class_name": "RelatedNodeInfo"}}, "text": "# A-24\n\n# GAUTIER AND KITAMURA\n\nIn Corollary 4.1, noticing that the bias terms Bt and Ba are not stochastic and bounded after proper scaling, we just have to focus on Sp and Se appearing in the proof of Theorem 4.1. Concerning Sp, proceed as before and note that taking M large enough so that C1 - C2M2 < -1 implies summability of the left hand side in (A.44). We conclude from the first Borel-Cantelli lemma that the probability that the events occur infinitely often is zero thus with probability one\n\nlimN &rarr; &infin; v-1B(d, &infin;)TNNd/2 - mN-2 supx &isin; Sd |T1(x)| < M.\n\nThe term T2 is non-stochastic and its treatment in our previous analysis remains valid, therefore we can use the same non-stochastic upper bound. We then use Assumption 4.2 (iii) instead of Assumption 4.2 (ii) to show almost sure uniform boundedness of Sp after proper rescaling. The treatment of Se is analogous to that of T1. The proof is the same in Corollaries 4.2 and 4.3.\n\n# Proof of Theorem 4.2\n\nWe first prove that the Lyapounov condition holds: there exists &delta; > 0 such that for N going to infinity,\n\n|ZN(b) - E[ZN(b)]|2 + &delta; E\n\n(A.47) N&delta;/2 (var(ZN(b)))1 + &delta;/2 &rarr; 0 (see, e.g. Billingsley, 1995). We start from deriving a lower bound on var(ZN(b)). Since E[ZN(b)] converges to f&beta;(b), it is enough to obtain a lower bound on\n\n\u222b0TN-1 \u2211p=0H+ chi(2p + 1, 2TN) max(fX(z), mN) \u03bb(2p + 1, d) q2p + 1 commad(z, b) fX(z) d\u03c3(z)\n\n= 4 \u03bb(2p + 1, d) \u222b0TN-1 \u2211p=0H+ chi(2p + 1, 2TN) q2p + 1 commad(z, b)2 fX(1 - 2I{fX < mN}) d\u03c3(z)\n\n= 4 \u03bb(2p + 1, d) \u222b0TN-1 \u2211p=0H+ chi(2p + 1, 2TN) q2p + 1 commad(z, b)2 d\u03c3(z)\n\n&ge; 4 |fX|&infin;4 \u222b0TN-1 \u2211p=0H+ \u03bb(2p + 1, d) \u222b0TN-1 \u2211p=0H+ chi(2p + 1, 2TN) q2p + 1 commad(z, b)2 d\u03c3(z)\n\n- 4 |fX|&infin;1 I{fX < mN} \u2211p=0H+ \u03bb(2p + 1, d)\n\nWith similar computations as (A.36), using as well (A.27), we know that there exists a constant C such that\n\nvextend TN-1 \u2211 chi(2p + 1, 2TN) q2p + 1 commad(z, star) vextend2 d-1 &le; CTN vextend2 \u03bb(2p + 1, d) vextend2\n\ntherefore using Proposition A.2 with p = q = r = 1 we obtain\n\nE[ZN2(b)] &ge; |fX|&infin;4 TN-1 \u2211 chi(2p + 1, 2TN)2 \u222bH+ q2p + 1 commad(z, b)2 \u03bb(2p + 1, d)2 d\u03c3(z) - CTN2 d-1 \u03c3(fX < mN)\n\np=0", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edb9d7be-7ea0-49cb-a3d1-eec7e6cb36ad": {"__data__": {"id_": "edb9d7be-7ea0-49cb-a3d1-eec7e6cb36ad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7569abaa-5b91-4407-bc3e-b9c2d0e0be71", "node_type": "4", "metadata": {}, "hash": "f02b1c87706a5b3c3e2e5f98fce4eace4620411b801876ec050c66a2a06f25b4", "class_name": "RelatedNodeInfo"}}, "text": "# A-25\n\nUsing Assumption A.1 (iv), the first term on the right hand side can be bounded from below by\n\nfloorleft(TN minus1)slash2floorrightvextenddouble\u2211 vextenddouble vextenddouble2 vextenddouble C vextenddouble vextenddouble q2p+1commad(z, b)vextenddouble lambda(2p + 1, d)vextenddouble2 p=0\n\ni.e. by CT N2dminus1. Thus as mN decays to zero, \u03c3 (fX < mN ) decays to zero and (A.48)\n\nE[ZN](b) greaterequal CT N2 2dminus1.\n\nbracketleftBig|ZN (b)|2+delta bracketrightBig\n\nWe now derive an upper bound of E using Theorem A.3 and interpolation between Linfinity(Sdminus1) and L1(Sdminus1) norms of the kernels using the Hdieresisolder inequality:\n\nbracketleftBig|ZN |2+delta bracketrightBig lessequal bardblfX bardblinfinitymminus(2+delta) vextenddouble\n\nvextenddoubleHminus1 parenleftBigK2TN (z, periodcentered)parenrightBigvextenddouble2+deltaminus vextenddouble\n\nE N vextenddouble vextenddouble2+delta mminus(2+delta)B(d, 2 + delta)2+delta T Nd(2+delta)slash2 vextenddouble\n\nK2TN (z, periodcentered)vextenddouble2+deltaminus vextenddouble2+delta vextenddouble\n\nlessequal bardblfX bardblinfinity N vextenddouble\n\nlessequal CmNminus(2+delta)T Nd(2+delta)slash2T N(dminus1)(1+delta).\n\nBy this and (A.48) an upper bound for the ratio appearing in (A.47) is given by\n\nminus(2+delta) parenleftBiggT Ndminus1parenrightBiggdeltaslash2 mN N.\n\nTherefore the Lyapounov condition is satisfied if (4.20) holds, and it follows that N 1slash2sN minus1(b)Se arrowrightd N (0, 1).\n\nWe now need to prove that the remaining terms Sp, Bt and Ba, multiplied by N 1slash2sN minus1(b), are op(1). The term Sp is treated in a similar manner as in the proof of Theorem 4.1.\n\nparenlefttp vextendsingle vextendsingleHminus1parenleftBigK2TN (xi, periodcentered)parenrightBigminus vextendsingleparenrighttp vextendsingle\n\nparenleftbt 1 \u2211N vextendsingle (b)vextendsingleparenrightbti=1commaperiodperiodperiodcommaN vextendsingle max (fX (xi), mN parenrightBig \u2212 1vextendsingle .) vextendsingle\n\n|Sp(b)| lessequal 2 max vextendsingle\n\nvextendsinglemaxparenleftBig circumflex vextendsingle N i=1 max(fX (xi), mN ) vextendsingle fN(xi), mN X vextendsingle\n\nUsing the Markov inequality, the empirical average in the parenthesis is of the stochastic order of\n\nminus1 vextenddouble vextenddoubleHminus1 parenleftBigK2TN (star, periodcentered)parenrightBigvextenddoubleminus vextenddouble mN vextenddouble vextenddouble1 .\n\nBut\n\nminus1 vextenddouble vextenddoubleHminus1 parenleftBigK2TN (star, periodcentered)parenrightBigvextenddoubleminus vextenddouble dslash2mNminus1 vextenddouble K2TN (star, periodcentered)vextenddouble1minus vextenddouble\n\nmN vextenddouble vextenddouble1 lessequal B(d, 1)T N vextenddouble\n\nlessequal B(d, 1)T Ndslash2mNminus1 bardblK2TN (star, periodcentered)bardbl1\n\nwhere the first inequality follows from Theorem A.3 and the second is obtained using the definition of the odd part and the triangle inequality. Note that the term bardblK2TN (star, periodcentered)bardbl1 in the last line", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b86b2985-a4d4-4c42-9b68-f42ddd237573": {"__data__": {"id_": "b86b2985-a4d4-4c42-9b68-f42ddd237573", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e18db6f5-dd36-4f85-a1f9-64045a62beb2", "node_type": "4", "metadata": {}, "hash": "36d1f60750eae27eb40dd1c715a79998f19656f36b8361ec43ffea14f3e02ca8", "class_name": "RelatedNodeInfo"}}, "text": "# A-26\n\n# GAUTIER AND KITAMURA\n\ndoes not depend on periodcentered and is uniformly bounded. By the lower bound (A.48) it is enough to show\n\nN1/2B(d, 1)T N\u2212(d\u22121/2)|Sp(b)| = op(1). From the inequality above,\n\n1/2B(d, 1)T N\u2212(d\u22121/2)|Sp(b)| \u2264 (1/2T N\u2212(d\u22121)/2mN\u22121) max (fX(xi), mN) \u2212 1.\n\nN max fX(xi), mN\n\nIts right hand side is of op(1) if\n\n\u2211i=1N|fX(xi) \u2212 fX(xi)| = op(N\u22121/2T N\u2212(d\u22121)/2mN),\n\nwhich is met under (4.19).\n\nLet us now consider the bias term induced by the trimming procedure. In the proof of Theorem 4.1 we have obtained an upper bound for ||Bt||\u221e and we deduce that\n\nN1/2T N\u2212(d\u22121/2)||Bt||\u221e = o(1) when condition (4.22) is satisfied. Finally, N1/2T N\u2212(d\u22121/2)||Ba||\u221e = o(1) if condition (4.21) is satisfied.\n\nWe conclude that the asymptotic normality holds for b such that f\u03b2(b) > 0. The factor 4 in the variance comes from the fact that f\u03b2 = 2f\u2212fmin.\n\nThe proof of Theorem 4.3 is almost the same.\n\nCREST (ENSAE), 3 avenue Pierre Larousse, 92245 Malakoff Cedex, France.\n\nE-mail address: eric.gautier@ensae-paristech.fr\n\nCowles Foundation for Research in Economics, Yale University, New Haven, CT-06520.\n\nE-mail address: yuichi.kitamura@yale.edu", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b32431b4-6f96-402b-800a-c60e849ebfce": {"__data__": {"id_": "b32431b4-6f96-402b-800a-c60e849ebfce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d7d3c32-a711-4c19-bd7f-92f59685556f", "node_type": "4", "metadata": {}, "hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b", "class_name": "RelatedNodeInfo"}}, "text": "# ColPali: Efficient Document Retrieval with Vision Language Models\n\n# Manuel Faysse* 1,3 Hugues Sibille\u22171,4 Tony Wu\u22171 Bilel Omrani1\n\n# Gautier Viaud1 C\u00e9line Hudelot3 Pierre Colombo2,3\n\n# 1Illuin Technology 2Equall.ai\n\n# 3CentraleSup\u00e9lec, Paris-Saclay 4ETH Z\u00fcrich\n\n# manuel.faysse@centralesupelec.fr\n\n# Abstract\n\nDocuments are visually rich structures that convey information through text, as well as tables, figures, page layouts, or fonts. While modern document retrieval systems exhibit strong performance on query-to-text matching, they struggle to exploit visual cues efficiently, hindering their performance on practical document retrieval applications such as Retrieval Augmented Generation. To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieving tasks spanning multiple domains, languages, and settings. The inherent shortcomings of modern systems motivate the introduction of a new retrieval model architecture, ColPali, which leverages the document understanding capabilities of recent Vision Language Models to produce high-quality contextualized embeddings solely from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically faster and end-to-end trainable. We release all project artifacts at https://huggingface.co/vidore.\n\n# 1 Introduction\n\nDocument Retrieval consists in matching a user query to relevant documents in a given corpus. It is central to many industrial applications, either as a standalone ranking system (search engines) or as part of more complex information extraction or Retrieval Augmented Generation (RAG) pipelines. Over recent years, pretrained language models have enabled large improvements in text embedding models. In practical industrial settings, however, the main performance bottleneck for efficient document retrieval is not in embedding model performance but in the prior data ingestion pipeline. To optimize a standard PDF document, many steps are required. First, PDF parsers or Optical Character Recognition (OCR) systems are used to extract words from the pages. Document layout detection models can then be run to segment paragraphs, titles, and other page objects such as tables, figures, and headers. A chunking strategy is then defined to group text passages with some semantic coherence, and modern retrieval setups may even integrate a captioning step to describe visually rich elements in a natural language form, more suitable for embedding models. In our experiments (Table 2), we typically find that optimizing the ingestion pipeline yields much greater performance on visually rich document retrieval than optimizing the text embedding model.\n\n# Contribution 1: ViDoRe\n\nIn this work, we argue that document retrieval systems should not be evaluated solely on the capabilities of text embedding models (Bajaj et al., 2016; Thakur et al., 2021; Muennighoff et al., 2022), but should also", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8734d82-4d9a-4e3a-9ae1-dca6f65891ab": {"__data__": {"id_": "d8734d82-4d9a-4e3a-9ae1-dca6f65891ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0dd7bd6-67ea-48bb-9973-7b6ae430a892", "node_type": "4", "metadata": {}, "hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d", "class_name": "RelatedNodeInfo"}}, "text": "# Standard Retrieval\n\n# m 0.66 NDCG@5\n\n|Method|Offline|Online|Similarity Score|\n|---|---|---|---|\n|ColPali (ours)|0.81 NDCG@5|Vision LLM| |\n|MaxSim| |What| |\n|Page|7.22s|Query|22ms|\n|OCR Detection| | | |\n\nFigure 2: ColPali simplifies document retrieval w.r.t. standard retrieval methods while achieving stronger performances with better latencies. Latencies and results are detailed in section 5 and subsection B.5.\n\nConsider the context and visual elements of the documents to be retrieved. To this end, we create and openly release ViDoRe, a comprehensive benchmark to evaluate systems on page-level document retrieval with a wide coverage of domains, visual elements, and languages. ViDoRe targets practical document retrieval settings, in which user queries may require both textual and visual understanding to be correctly matched to relevant documents. We highlight the shortcomings of current text-centric systems in these settings.1\n\n# Contribution 2: ColPali\n\nWe propose a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents purely from their visual features, allowing for subsequent fast query matching with late interaction mechanisms (Khattab and Zaharia, 2020). Our method, ColPali, outperforms all other retrieval systems on ViDoRe while being fast and end-to-end trainable. We release models and code at https://huggingface.co/vidore.\n\n# 2 Problem Formulation & Related Work\n\n# Problem Setting\n\nIn our setting, a retrieval system scores how relevant a document d from corpus D is with respect to a query q. Computing the similarity score s(q, d) \u2208 R+ for each of the |D| documents in the corpus creates a ranking we can use to extract the most relevant documents. In this work, we focus on page-level retrieval: given a query, is the correct document page retrieved by the system? For coherence with existing literature, we further use the term document to refer to individual pages, i.e. the atomic retrieved elements in our setting. As we focus on practical industrial retrieval applications (RAG, search engines) with potentially large corpora sizes, latency constraints are imposed on scoring systems. Most current retrieval systems can be decomposed into (1) an offline indexation phase in which a document index is built and (2) an online querying phase in which a query is matched to documents from the index and where low latency is vital to the user experience.\n\nEfficient document retrieval systems exhibit joint properties of high retrieval performance (R1), low latency during querying (R2), and high throughput during indexation (R3).\n\n# 2.1 Textual Retrieval Methods\n\n# Document Retrieval in Text Space\n\nStatistical methods based on word frequency like TF-IDF (Sparck Jones, 1972) and BM25 (Robertson et al., 1994) are still widely used due to their simplicity.\n\n1 The benchmark leaderboard is hosted publicly at https://huggingface.co/spaces/vidore/vidore-leaderboard to encourage further developments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01172341-d76b-4ffd-8ebb-8e59a49f4b06": {"__data__": {"id_": "01172341-d76b-4ffd-8ebb-8e59a49f4b06", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3a48c5f-496a-4b14-af08-fb39f03370a4", "node_type": "4", "metadata": {}, "hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f", "class_name": "RelatedNodeInfo"}}, "text": "# 2.2 Integrating Visual features\n\n# Contrastive Vision Language Models\n\nMapping latent representations of textual content to corresponding representations of visual content has been done by aligning disjoint visual and text encoders through contrastive losses (Radford et al., 2021; Zhai et al., 2023). While some OCR capabilities exist in these models, the visual component is often not optimized for text understanding. The Fine-grained Interactive Language-Image Pre-training (Yao et al., 2021) framework extends the late interaction mechanism to cross-modal vision-language models, relying on max similarity operations between text tokens and image patches.\n\n# Visually Rich Document Understanding\n\nTo go beyond text, some document-focused models jointly encode text tokens alongside visual or document layout features (Appalaraju et al., 2021; Kim et al., 2021; Huang et al., 2022; Tang et al., 2022). Large Language transformer Models (LLMs) with strong reasoning capabilities have recently been combined with Vision Transformers (ViTs) (Dosovitskiy et al., 2020) to create VLMs (Alayrac et al., 2022; Liu et al., 2023b; Bai et al., 2023; Lauren\u00e7on et al., 2024) where image patch vectors from contrastively trained ViT models (Zhai et al., 2023) are fed as input embeddings to the language model and concatenated with the text-token embeddings.\n\n# PaliGemma\n\nThe PaliGemma-3B model (Lucas Beyer* et al., 2024) extends concepts from Pali3 (Chen et al., 2023), and projects SigLIP-So400m/14 (Alabdulmohsin et al., 2023) patch embeddings into Gemma-2B\u2019s text vector space (Gemma Team et al., 2024). Along with its reasonable size w.r.t. other performant VLMs, an interesting property of PaliGemma\u2019s text model is that it is fine-tuned with full-block attention on the prefix (instruction text and image tokens). VLMs display enhanced capabilities in Visual Question Answering, captioning, and document understanding (Yue et al., 2023), but are not optimized for retrieval tasks.\n\n# 3 The ViDoRe Benchmark\n\nExisting benchmarks for contrastive vision-language models primarily evaluate retrieval for natural images (Lin et al., 2014; Borchmann et al., 2021; Thapliyal et al., 2022). On the other hand, textual retrieval benchmarks (Muennighoff et al., 2022) are evaluated at the textual passage level and are not tailored for document retrieval tasks. We fill the gap with ViDoRe, a comprehensive benchmark for document retrieval using visual features.\n\n# 3.1 Benchmark Design\n\nViDoRe is designed to comprehensively evaluate retrieval systems on their capacity to match queries to relevant documents at the page level. This benchmark encompasses multiple orthogonal subtasks, with focuses on various modalities - text, figures, infographics, tables; thematic domains - medical,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b5a02cc-35fc-4fa9-a9d2-b1f36a697e14": {"__data__": {"id_": "6b5a02cc-35fc-4fa9-a9d2-b1f36a697e14", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "21795d51-479b-40ad-a150-aac48fae5962", "node_type": "4", "metadata": {}, "hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe", "class_name": "RelatedNodeInfo"}}, "text": "# Dataset\n\n|Dataset|# Queries|Domain|\n|---|---|---|\n|Academic Tasks| | |\n|DocVQA (eng)|500 (500)|Industrial|\n|InfoVQA (eng)|500 (500)|Infographics|\n|TAT-DQA (eng)|1600 (1600)|Varied Modalities|\n|arXiVQA (eng)|500 (500)|Scientific Figures|\n|TabFQuAD (fra)|210 (210)|Tables|\n|Practical Tasks| | |\n|Energy (eng)|100 (1000)|Scientific|\n|Government (eng)|100 (1000)|Administrative|\n|Healthcare (eng)|100 (1000)|Medical|\n|AI (eng)|100 (1000)|Scientific|\n|Shift Project (fra)|100 (1000)|Environment|\n\nTable 1: ViDoRe comprehensively evaluates multimodal retrieval methods. The size of the document corpus is indicated in parentheses.\n\n# Academic Tasks\n\nWe repurpose widely used visual question-answering benchmarks for retrieval tasks: for each page-question-answer triplet, we use the question as the query, and the associated page as the gold document (Table 1). These academic datasets either focus on single specific modalities (Mathew et al., 2020, 2021; Li et al., 2024) or target more varied visually rich documents (Zhu et al., 2022). Moreover, we consider TabFQuAD, a human-labeled dataset on tables extracted from French industrial PDF documents released with this work. Details can be found in subsection A.1.\n\n# Practical tasks\n\nWe construct topic-specific retrieval benchmarks spanning multiple domains to go beyond repurposed QA datasets and evaluate retrieval in more realistic industrial situations (e.g. RAG). To achieve this, we collect publicly accessible PDF documents and generate queries pertaining to document pages using Claude-3 Sonnet, a high-quality proprietary vision-language model (Anthropic, 2024). In total, we collect 1,000 document pages per topic, which we associate with 100 queries extensively filtered for quality and relevance by human annotators. The corpus topics are intentionally specific to maximize syntactic proximity between documents, creating challenging retrieval tasks and covering an array of orthogonal domains (Table 1). Query-page pair examples are shown in Appendix E.2.\n\n# Evaluation Metrics\n\nWe evaluate performance on our benchmark (Requirement R1) using standard metrics from the retrieval literature (NDCG, Recall@K, MRR). We report NDCG@5 values as the main performance metric in this work and release the complete sets of results along with the models.\n\nTo validate compliance with practical industrial constraints, we also consider query latencies (R2) and indexing throughputs (R3).\n\n# 3.2 Assessing Current Systems\n\n# Unstructured\n\nWe evaluate retrieval systems representative of those found in standard industrial RAG pipelines. As is common practice, we rely on the Unstructured off-the-shelf tool in the highest resolution settings to construct high-quality text chunks from PDF documents. Unstructured orchestrates the document parsing pipeline, relying on deep learning vision models to detect titles and document layouts (Ge et al., 2021), OCR engines (Smith, 2007) to extract text in non-native PDFs, specialized methods or models to detect and reconstruct tables, and implements a chunking strategy (by-title) that leverages the detected document structure to preserve section boundaries when concatenating texts. As is common practice, in our simplest Unstructured configuration (text-only), only textual elements are kept, and figures, images, and tables are considered noisy information and are filtered out.\n\n# Unstructured + X\n\nWhile Unstructured is a strong baseline by itself, we further augment Unstructured\u2019s output by integrating the visual elements. In (+ OCR), tables, charts, and images are run through an OCR engine, processed by Unstructured, and chunked independently. In (+ Captioning), we set up a fully-fledged captioning strategy (Zhao et al., 2023), in which we feed visual elements to a strong proprietary Vision Language Model (Claude-3 Sonnet (Anthropic, 2024)) to obtain highly detailed textual descriptions of the elements. Both strategies aim to integrate visual elements in the retrieval pipeline but incur significant latency and resource costs (subsection 5.2).\n\n# Embedding Model\n\nTo embed textual chunks, we evaluate Okapi BM25, the de facto standard sparse statistical retrieval method, and the dense encoder of BGE-M3 (Chen et al., 2024), a multilingual neural method with SOTA performance in its size category. Chunks are embedded and scored independently, and page-level scores are obtained by.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c0bdc56-1d2d-441b-8d21-46a0697db649": {"__data__": {"id_": "7c0bdc56-1d2d-441b-8d21-46a0697db649", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "609fd3b8-4f1b-4e43-887d-8b5e5cd945d4", "node_type": "4", "metadata": {}, "hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dabcc144-efb7-4e05-8d81-939d52f156a7", "node_type": "1", "metadata": {}, "hash": "7e3111e040a674730fa702d7902f35c16b8c433334150ac0ae151ea4fab47ce1", "class_name": "RelatedNodeInfo"}}, "text": "max-pooling over the page\u2019s chunk scores.5\n\n# Contrastive VLMs\n\nWe also evaluate the strongest available vision-language embedding models; Jina CLIP (Koukounas et al., 2024), Nomic Embed Vision (Nomic, 2024), and SigLIP-So400m/14 (Alabdulmohsin et al., 2023).\n\n# Results\n\nFrom a performance perspective, best results are obtained by combining the Unstructured parser with visual information, either from captioning strategies or by running OCR on the visual elements (Table 2). Little difference is seen between BM25 and BGE-M3 embeddings highlighting the visual information bottleneck. Contrastive VLMs lag behind. Beyond retrieval performance (R1), the indexing latencies (R2) reported in Figure 3 illustrate that PDF parsing pipelines can be very lengthy, especially when incorporating OCR or captioning strategies. Querying latencies at runtime (R3) are very good for all evaluated systems (\u2264 22 ms on NVIDIA L4) due to fast query encoding and cosine similarity matching.\n\n|PDF Parser|Latency (s)|\n|---|---|\n|PDF Parser|(7.22s)|\n|Siglip|(0.12s)|\n|ColPali|(0.39s)|\n\n0 1 2 3 4 5 6 7\n\nLatency (s)\n\nLayout Detection OCR Captioning Page Encoding\n\n# Figure 3\n\nOffline indexing with ColPali is much simpler and faster compared to standard retrieval methods. Indexing speeds reported are computed on Nvidia L4 GPUs and detailed in subsection B.5.\n\n# 4 Late interaction based Vision Retrieval\n\n# 4.1 Architecture\n\nVision-Language Models. Encouraged by their strong document understanding capabilities, we propose adapting recent VLMs for retrieval. The key concept is to leverage the alignment between output embeddings of text and image tokens acquired during multi-modal finetuning. To this extent, we introduce ColPali, a Paligemma-3B extension that is capable of generating ColBERT-style multi-vector representations of text and images (Figure 2). PaliGemma-3B is a strong candidate due to its small size, the many released checkpoints fine-tuned for different image resolutions and tasks,\n\n5We empirically validated the max-pooling strategy over sub-page chunks to be more effective than concatenating all page chunks before embedding pagewise.\n\nand the promising performances on various document understanding benchmarks. We add a projection layer to map the output language modeling embeddings to a vector space of reduced dimension D = 128 as used in the ColBERT paper (Khattab and Zaharia, 2020) to keep lightweight bag-of-embedding representations.\n\n# Late Interaction\n\nGiven query q and document d, we denote as Eq \u2208 RNq \u00d7D and Ed \u2208 RNd\u00d7D their respective multi-vector representation in the common embedding space RD. The late interaction operator, LI (q, d), is the sum over all query vectors Ed(j), of its maximum dot product \u27e8\u00b7|\u00b7\u27e9 with each of the Nd document embedding vectors Ed(1:Nd).\n\nLI (q, d) = \u2211i\u2208[|1,Nq|] maxj\u2208[|1,Nd|] \u27e8Eq(i)|Ed(j)\u27e9\n\n# Contrastive Loss\n\nThe Late Interaction operation is fully differentiable, enabling backpropagation. Let a batch {qk, dk}k\u2208[|1,b|] composed of b query-page pairs, where for all k \u2208 [|1, b|], the document page dk is the document corresponding to query qk. Following Khattab and Zaharia (2020), we define our in-batch contrastive loss L as the softmaxed cross-entropy of the positive scores\n\nL+ = LI (dk, qk) w.r.t. to the maximal negative scores L- = maxl,l\u2260k LI (qk, pl).\n\n# 4.2 Model training\n\nDataset. Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages6. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nParameters. All models are trained for 1 epoch on the train set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dabcc144-efb7-4e05-8d81-939d52f156a7": {"__data__": {"id_": "dabcc144-efb7-4e05-8d81-939d52f156a7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "609fd3b8-4f1b-4e43-887d-8b5e5cd945d4", "node_type": "4", "metadata": {}, "hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c0bdc56-1d2d-441b-8d21-46a0697db649", "node_type": "1", "metadata": {}, "hash": "b5899512ab126ca7b0cc5fe2a825e271dfbea6ea3e49469597376829a500a0db", "class_name": "RelatedNodeInfo"}}, "text": "to the maximal negative scores L- = maxl,l\u2260k LI (qk, pl).\n\n# 4.2 Model training\n\nDataset. Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages6. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nParameters. All models are trained for 1 epoch on the train set. Unless specified otherwise, we train models in bfloat16 format, use low-rank adapters (LoRA, Hu et al. (2021)) with \u03b1 = 32 and r = 32 on the transformer layers from the language model,\n\n6Multilingual data is present in the pretraining corpus of the language model (Gemma-2B) and potentially occurs during PaliGemma-3B\u2019s multimodal training.", "mimetype": "text/plain", "start_char_idx": 3266, "end_char_idx": 4349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c731ebae-4c4c-4083-baef-014f44f1c49e": {"__data__": {"id_": "c731ebae-4c4c-4083-baef-014f44f1c49e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1af302f3-b0d3-431b-b95d-9472fd47cd27", "node_type": "4", "metadata": {}, "hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4", "class_name": "RelatedNodeInfo"}}, "text": "# Results\n\n# 5.1 Performance (R1)\n\nWe iteratively construct ColPali, starting from an off-the-shelf SigLIP model (Table 2).\n\n# BiSigLIP: Improving a strong model.\n\nSigLIP7 is a strong vision-language bi-encoder model, pre-trained on the English split of WebLI (Chen et al., 2023), a corpus of billions of image-text pairs. We find that SigLIP largely outperforms both Jina CLIP and Nomic-vision on document retrieval tasks. Further fine-tuning the textual component of this model on our document-oriented dataset (BiSigLIP) yields clear improvements across the board, particularly on figure retrieval (ArxivQA) and table retrieval tasks (TabFQuAD).\n\n# BiPali: Pairing with a language model.\n\nIn the PaliGemma model architecture, SigLIP-generated patch embeddings are fed to a text language model to obtain LLM contextualized output patch embeddings.8 We average pool these representations to obtain a single dense vector, effectively creating a PaliGemma bi-encoder model (BiPali). After fine-tuning on the training dataset, we obtain a model that performs slightly worse in English than the tuned BiSigLIP variant. This can be explained by the fact that contrary to SigLIP, the original PaliGemma is not trained on contrastive matching tasks, but rather on next token prediction. Our contrastive fine-tuning phase on 100K images to transform PaliGemma into a bi-encoder is 5 orders of magnitude smaller than SigLIP\u2019s original contrastive training. However, we see notable improvements in French tasks, indicating that BiPali\u2019s LLM (Gemma 2B) helps multilingual text understanding. This is particularly notable as our training dataset does not contain non-English samples.\n\n# ColPali: Adding Late Interaction.\n\nOne benefit of inputting image patch embeddings through a language model is that they are natively mapped to a latent space similar to textual input (query). This enables leveraging the ColBERT strategy to compute interactions between text tokens and image patches, which enables a step-change improvement in performance compared to BiPali. Results in Table 2 show that our ColPali model also largely outperforms the strong baselines based on Unstructured and captioning, as well as all evaluated text-image embedding models. The difference is particularly stark on the more visually complex benchmark tasks, such as InfographicVQA, ArxivQA, and TabFQuAD representing respectively infographics, figures, and tables. However, text-centric documents are also better retrieved by the ColPali models across all evaluated domains and languages, making our approach the overall best-performing document-retrieval model.\n\n# Negative Results.\n\nFor extensiveness, we also train ColSigLIP, a late interaction variant of the BiSigLIP model but obtain abysmal performances. We attribute this to the large gaps w.r.t. SigLIP\u2019s pre-training, in which only a pooled latent representation is used in the contrastive loss, which does not optimize the representations of individual patch and token embeddings. Similarly, we train a BiSigLIPP aliGemma variant, in which we retrieve the image representations from the SigLIP model that has been further updated by PaliGemma fine-tuning, and use the text representations from PaliGemma\u2019s text model. After fine-tuning on our dataset, performance is severely inferior to SigLIP vanilla which simply encodes with SigLIP\u2019s original text and vision components. This indicates a logical misalignment between SigLIP embeddings, and Gemma embeddings after PaliGemma training. We detail these results in Table 5.\n\n# 5.2 Latencies & Memory Footprint\n\n# Online Querying. (R2)\n\nLogically, querying latencies differ between ColPali and a BGE-M3 embedding model. For BGE, encoding takes about 22 ms for 15 tokens, while encoding a query with ColPali\u2019s language model takes about 30 ms9. For smaller corpus sizes, computing the late interaction operation induces marginally small overheads (\u2248 1 ms per 1000 pages in the corpus), and the cosine similarity computation between bi-encoder vectors.\n\n9Computed for a batch size of 1 (online), and averaged over 1000 queries. See subsection B.5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6979f13a-4f38-4cc6-a076-68ecd79ac039": {"__data__": {"id_": "6979f13a-4f38-4cc6-a076-68ecd79ac039", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9bfaabf7-5c60-4c85-aca5-381b31f75774", "node_type": "4", "metadata": {}, "hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e08bd7a-7bfd-48b9-8a95-5b88f47bbf53", "node_type": "1", "metadata": {}, "hash": "231a44d96b44ef7cef97f4d52fc7bf0c58617c4a316ef1d4aacccf09c35c8c27", "class_name": "RelatedNodeInfo"}}, "text": "# Table 2: Comprehensive evaluation of baseline models and our proposed method on ViDoRe.\n\nResults are presented using NDCG@5 metrics, and illustrate the impact of different components. Text-only metrics are not computed for benchmarks with only visual elements.\n\n|Model|Metrics| | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy Gov.| |Health.|Avg.| |\n|- BM25|-|34.1|-|-|44.0|59.6|90.4|78.3|78.8|82.6| | |\n|- BGE-M3|-|28.4\u21935.7|-|-|36.1\u21937.9|68.5\u21918.9|88.4\u21932.0|76.8\u21931.5|77.7\u21931.1|84.6\u21912.0| | |\n|- BM25|31.6|36.8|62.9|46.5|62.7|64.3|92.8|85.9|83.9|87.2|65.5| |\n|- BGE-M3|31.4\u21930.2|25.7\u219311.1|60.1\u21932.8|70.8\u219124.3|50.5\u219312.2|73.2\u21918.9|90.2\u21932.6|83.6\u21932.3|84.9\u21911.0|91.1\u21913.9|66.1\u21910.6| |\n|- BM25|40.1|38.4|70.0|35.4|61.5|60.9|88.0|84.7|82.7|89.2|65.1| |\n|- BGE-M3|35.7\u21934.4|32.9\u21935.4|71.9\u21911.9|69.1\u219133.7|43.8\u219317.7|73.1\u219112.2|88.8\u21910.8|83.3\u21931.4|80.4\u21932.3|91.3\u21912.1|67.0\u21911.9| |\n|Contrastive VLMs|Jina-CLIP|25.4|11.9|35.5|20.2|3.3|3.8|15.2|19.7|21.4|20.8|17.7|\n|Nomic-vision|17.1|10.7|30.1|16.3|2.7|1.1|12.9|10.9|11.4|15.7|12.9| |\n|SigLIP (Vanilla)|43.2|30.3|64.1|58.1|26.2|18.7|62.5|65.7|66.1|79.1|51.4| |\n|Ours|SigLIP (Vanilla)|43.2|30.3|64.1|58.1|26.2|18.7|62.5|65.7|66.1|79.1|51.4|\n|BiSigLIP (+fine-tuning)|58.5\u219115.3|32.9\u21912.6|70.5\u21916.4|62.7\u21914.6|30.5\u21914.3|26.5\u21917.8|74.3\u219111.8|73.7\u21918.0|74.2\u21918.1|82.3\u21913.2|58.6\u21917.2| |\n|BiPali (+LLM)|56.5\u2193-2.0|30.0\u2193-2.9|67.4\u2193-3.1|76.9\u219114.2|33.4\u21912.9|43.7\u219117.2|71.2\u2193-3.1|61.9\u2193-11.7|73.8\u2193-0.4|73.6\u2193-8.8|58.8\u21910.2| |\n|ColPali (+Late Inter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e08bd7a-7bfd-48b9-8a95-5b88f47bbf53": {"__data__": {"id_": "3e08bd7a-7bfd-48b9-8a95-5b88f47bbf53", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9bfaabf7-5c60-4c85-aca5-381b31f75774", "node_type": "4", "metadata": {}, "hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6979f13a-4f38-4cc6-a076-68ecd79ac039", "node_type": "1", "metadata": {}, "hash": "e95fa380b755b4c34d72a363da79a8c57174e8d488e9a48e2fd996853cc786e8", "class_name": "RelatedNodeInfo"}}, "text": ")|79.1\u219122.6|54.4\u219124.5|81.8\u219114.4|83.9\u21917.0|65.8\u219132.4|73.2\u219129.5|96.2\u219125.0|91.0\u219129.1|92.7\u219118.9|94.4\u219120.8|81.3\u219122.5| |\n\n# 5.3 Interpretability\n\nBy superimposing the late interaction heatmap on top of the original image, we can visualize the most salient image patches with respect to each term of the query, yielding interpretable insights into model focus zones. As epitomized in Figure 1, we observe ColPali exhibits strong OCR capabilities as both the words \"hourly\" and \"hours\" present a high similarity score with the query token &lt;_hour&gt;. We also note particular focus on other non-trivial image features such as the x-axis representing hours being salient. Other visualization examples with similar trends of the model transcending pure OCR are shown in Appendix C.\n\n# 6 Ablation study\n\nShould we scale models or patch numbers? We train a variant of PaliGemma with half the number of image patches (512). While there is a clear performance degradation w.r.t. to the 1024-patch ColPali model (Figure 4), memory usage is much lower.\n\nWhile another PaliGemma variant exists with 2048 patches, the different training datamix and the large memory requirements make this model impractical for both training.", "mimetype": "text/plain", "start_char_idx": 1511, "end_char_idx": 2719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f449792-62dd-4874-8473-0cf1c56d6f99": {"__data__": {"id_": "0f449792-62dd-4874-8473-0cf1c56d6f99", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0748b6f2-0135-451c-ad7b-13b12f1db444", "node_type": "4", "metadata": {}, "hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84", "class_name": "RelatedNodeInfo"}}, "text": "# Relative NDCG@5 (%)\n\n| |ColPali|Idefics2|No Mem.|Full IB|Train|TabF|\n|---|---|---|---|---|---|---|\n|(512)|(64)|Tokens|Loss|Vision|Tuning| |\n\nFigure 4: Relative NDCG@5 performance gain w.r.t. the default ColPali (1024 patches). TabFQuAD fine-tuning measures the performance difference on the TabFQuAD task after the introduction of targeted data in the training set. All other results refer to performance deltas averaged on all ViDoRe tasks.\n\nIdefics2-8B (Lauren\u00e7on et al., 2024), a VLM with a similar architecture and based on a Mistral-7B (Jiang et al., 2023) language backbone and a SigLIP vision encoder paired with a perceiver resampler. The most notable differences with PaliGemma lie in the size of the language model (2B and 7B resp.) and the number of image patches (between 512 and 2048 for PaliGemma, and 64 post-resampling for Idefics212). Our results (Figure 4) suggest language model size has a strong impact on performance, and along with the trained resampler enables more efficient representations for smaller numbers of image embeddings - ColIdefics2 with 64 patches edges out ColPali with 512 patches. Scaling the number of patches of the smaller ColPali model from 512 to 1024, enables largely surpassing the 60-patch ColIdefics2 while being about twice as fast in terms of training and inference latency. These results suggest there are tradeoffs between performance (R1), latencies during online querying (R2) and offline indexation phases (R3), and index memory size.\n\n# Should we fine-tune the vision component?\n\nWe run our contrastive finetuning on a ColPali model in which we also train the vision encoder and the projection layer. Results in Figure 4 show this leads to no significant improvements.\n\n# Do \"query augmentation\" tokens help?\n\nIn ColBERT, special tokens are concatenated to the input query to serve as soft query augmentation buffers. Training without these tokens, we observe no significant performance difference (Figure 4) in the English benchmarks. However, performance on the French tasks seems to improve (Table 5) and inference time.\n\nWith the option of adding 4 sub-image crops of 64 tokens each to the sequence, for a total of 320 tokens.\n\n# Is the Pairwise CE loss best?\n\nTraining with an in-batch negative contrastive loss, instead of the pairwise CE loss that only considers the hardest negative sample, leads to a slight performance degradation (\u22122.4%) on the aggregated benchmark.\n\n# Can the model adapt to new tasks?\n\nContrary to more complex multi-step retrieval pipelines, ColPali can be trained end-to-end, directly optimizing the downstream retrieval task which greatly facilitates fine-tuning to boost performance on specialized domains, multilingual retrieval, or specific visual elements the model struggles with. To demonstrate, we add 1552 samples representing French tables and associated queries to the training set. This represents the only French data in the training set, with all other examples being kept unchanged. We see significant NDCG@5 improvements (Figure 4) and even starker Recall@1 gains (+6.63%) on the TabFQuAD benchmark, with no performance degradation on the rest of the benchmark tasks (+0.34%).\n\n# 7 Conclusions\n\nThrough the conception of a new benchmark ViDoRe, we established the limits of both modern industrial document retrieval pipelines and off-the-shelf image-text contrastive models for visually rich document retrieval. We introduced ColPali, a novel retrieval model that leverages the latest generative Vision Language models to create highly performing multi-vector embeddings purely from visual document features. ColPali largely outperforms the best existing document retrieval methods while enabling faster corpus indexing time and maintaining low querying latencies, suggesting a very high potential for industrial document retrieval applications. We hope to encourage future work by publicly releasing the ViDoRe benchmark and all models and baselines from our study.\n\n# Future Work\n\nFurther performance gains could be obtained by exploring sub-image decomposition (Liu et al., 2023a), optimal image patch resampling strategies (Lauren\u00e7on et al., 2024), or hard-negative mining. Subsequently, our vision is to combine visual retrieval and visually grounded query answering to create RAG systems that purely function from visual features. An interesting line of research could be attempting to generate answers leveraging information stored in the indexed multi-vector patch embeddings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44c71317-9345-41d4-a7e1-c383ceee95c9": {"__data__": {"id_": "44c71317-9345-41d4-a7e1-c383ceee95c9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79ea9571-1a91-438d-9c62-5ecd2adebaac", "node_type": "4", "metadata": {}, "hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0", "class_name": "RelatedNodeInfo"}}, "text": "# Limitations\n\nFocus. In this work, we evaluate models on document retrieval tasks, covering several modalities (figures, text, tables, infographics). We however primarily focus on PDF-type documents, and evaluating systems on image retrieval with documents stemming from web page screenshots or handwritten documents might be an interesting generalization. We also focus on high-resource languages (English and French) and although we have shown the capacity of the ColPali model to generalize to languages outside of its fine-tuning set, it is unclear how the model would perform on languages that are not as represented in the model\u2019s language backbone. Finally, our setup assumes relevant documents exist, but abstention methods for Information Retrieval systems might be interesting to explore in more practical settings in which confidence estimation might be important (Gisserot-Boukhlef et al., 2024).\n\nSupport. This work relies on multi-vector retrieving derived from the ColBERT late interaction mechanism. Although some vector databases support late interaction engines13, many widely used vector retrieval frameworks do not propose native multi-vector support, and some engineering infrastructure efforts may be required to adapt them to work with ColPali (or ColBERT) models.\n\nData. In the creation of ViDoRe, we partially rely on synthetic query generation based on a commercial large language model, which may induce some amount of bias in the generated queries. To compensate for this, we have iterated on the prompting strategy and given real query examples to the models to help ground generation in realistic settings. We have further manually verified all synthetic queries through a lengthy process to validate their relevance and their quality. Our benchmark also includes many benchmark tasks with no synthetic data, and result trends observed between all tasks are correlated, further confirming the coherence of our benchmark design.\n\n# Ethical Considerations\n\nCarbon Footprint. Our work fully leverages prior pretrained models and training is not particularly compute-intensive. Furthermore, we rely on low-rank adapters to further reduce the computational resources needed, both during training and for storage. Overall, a training run represents about 40 hours of Mi250x AMD GPUs. Our experiments, in total, represent 1405 Mi250x GPU hours from highly efficient compute clusters running on low-carbon nuclear energy, representing a total of around 15kg CO2 eq.\n\nImpact. We believe our work could have a strong impact on improving industrial document retrieval systems. Our method is efficient, performs well, and the additional support towards visually rich information from documents could go a long way in unlocking knowledge sources previously difficult to index or query.\n\nResource Release. For transparency, and to foster future work, we release our comprehensive benchmark under open license and host a public leaderboard14. Our models are released under the same usage license as the base model (Gemma Research license for ColPali, Apache2.0 for ColIdefics2) and should be used as intended by the VLM license.\n\n# Acknowledgements\n\nThis work is partially supported by Illuin Technology, and by a grant from ANRT France. This work was performed using HPC resources from the CINES ADASTRA through Grant 2024-AD011015443. We extend our warm thanks to Jonathan Dong, Caio Corro, Victor Pellegrain and Ender Konukoglu for their valuable feedback on the paper.\n\n# References\n\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. 2023. Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. Publisher: arXiv Version Number: 5.\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a Visual Language Model for Few-Shot Learning. Publisher: arXiv Version Number: 2.\n\nAnthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku.\n\n14https://huggingface.co/spaces/vidore/vidore-leaderboard", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10f2c3de-efe1-47f3-b441-91df604ad5f4": {"__data__": {"id_": "10f2c3de-efe1-47f3-b441-91df604ad5f4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d34ef88-55ca-4bae-b6ad-0e4318895305", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11dd0dcd-7efc-4312-82a7-7dd8fbe7988c", "node_type": "1", "metadata": {}, "hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha. 2021. DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint. Version Number: 2.\n\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. Publisher: arXiv Version Number: 3.\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv preprint. Version Number: 3.\n\nBurton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422\u2013426. Place: New York, NY, USA Publisher: Association for Computing Machinery.\n\n\u0141ukasz Borchmann, Micha\u0142 Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina Szyndler, and Filip Grali\u0144ski. 2021. DUE: End-to-End Document Understanding Benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv preprint. Version Number: 3.\n\nXi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. 2023. PaLI-3 Vision Language Models: Smaller, Faster, Stronger. arXiv preprint. Version Number: 2.\n\nCohere. 2024. Introducing Rerank 3: A New Foundation Model for Efficient Enterprise Search & Retrieval.\n\nTimoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. 2023. Vision Transformers Need Registers. Publisher: [object Object] Version Number: 2.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Publisher: arXiv Version Number: 2.\n\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX: Exceeding YOLO Series in 2021. arXiv preprint. Version Number: 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11dd0dcd-7efc-4312-82a7-7dd8fbe7988c": {"__data__": {"id_": "11dd0dcd-7efc-4312-82a7-7dd8fbe7988c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d34ef88-55ca-4bae-b6ad-0e4318895305", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10f2c3de-efe1-47f3-b441-91df604ad5f4", "node_type": "1", "metadata": {}, "hash": "73a1d174247c6265657797b59c09c64dc8a0dbbe89ce49cf76e1b363e58e5267", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aef2f663-7aca-4877-96ac-748136df92bf", "node_type": "1", "metadata": {}, "hash": "a33129b68cd64ada00b9243cecd3233f935c166bfedda87a4cb2921a33d65367", "class_name": "RelatedNodeInfo"}}, "text": "2023. Vision Transformers Need Registers. Publisher: [object Object] Version Number: 2.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Publisher: arXiv Version Number: 2.\n\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX: Exceeding YOLO Series in 2021. arXiv preprint. Version Number: 2.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv preprint. Version Number: 4.\n\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, C\u00e9line Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: A simple yet effective abstention mechanism. Preprint, arXiv:2402.12997.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Publisher: arXiv Version Number: 2.\n\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Publisher: arXiv Version Number: 3.", "mimetype": "text/plain", "start_char_idx": 2063, "end_char_idx": 4986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aef2f663-7aca-4877-96ac-748136df92bf": {"__data__": {"id_": "aef2f663-7aca-4877-96ac-748136df92bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d34ef88-55ca-4bae-b6ad-0e4318895305", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11dd0dcd-7efc-4312-82a7-7dd8fbe7988c", "node_type": "1", "metadata": {}, "hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "class_name": "RelatedNodeInfo"}}, "text": "Version Number: 4.\n\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, C\u00e9line Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: A simple yet effective abstention mechanism. Preprint, arXiv:2402.12997.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Publisher: arXiv Version Number: 2.\n\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Publisher: arXiv Version Number: 3.\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix.", "mimetype": "text/plain", "start_char_idx": 4371, "end_char_idx": 5288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e128f73c-e338-4ee5-ae2a-9f4d71047e8c": {"__data__": {"id_": "e128f73c-e338-4ee5-ae2a-9f4d71047e8c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7ac12c6-e24f-4cf0-a601-e16be87f36da", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14c2450b-f530-43dc-bda3-a3318ce76eac", "node_type": "1", "metadata": {}, "hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nand William El Sayed. 2023. Mistral 7B. Publisher: arXiv Version Number: 1.\n\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint. Version Number: 3.\n\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.\n\nGeewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2021. OCR-free Document Understanding Transformer. arXiv preprint. Version Number: 5.\n\nAndreas Koukounas, Georgios Mastrapas, Michael G\u00fcnther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Mart\u00ednez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, and Han Xiao. 2024. Jina CLIP: Your CLIP Model Is Also Your Text Retriever. arXiv preprint. Version Number: 1.\n\nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. 2024. What matters when building vision-language models? arXiv preprint ArXiv:2405.02246 [cs].\n\nJinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Y. Zhao. 2023. Rethinking the Role of Token Retrieval in Multi-Vector Retrieval. arXiv preprint. Version Number: 3.\n\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models. Preprint, arXiv:2403.00231.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. 2014. Microsoft COCO: Common Objects in Context. arXiv preprint. Version Number: 3.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv preprint. Version Number: 2.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. Publisher: arXiv Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14c2450b-f530-43dc-bda3-a3318ce76eac": {"__data__": {"id_": "14c2450b-f530-43dc-bda3-a3318ce76eac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7ac12c6-e24f-4cf0-a601-e16be87f36da", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e128f73c-e338-4ee5-ae2a-9f4d71047e8c", "node_type": "1", "metadata": {}, "hash": "f30400ea5c39a8dda2862c4f5a6ed31adb53965d1dfd780b04bd6e1abb680842", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7827d715-6f00-4efe-ba2f-59e8565404ad", "node_type": "1", "metadata": {}, "hash": "5b6cd1ec36c79c626e603648645c27d2c8bdf7405d134fa5ee8d442654faaccb", "class_name": "RelatedNodeInfo"}}, "text": "Preprint, arXiv:2403.00231.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. 2014. Microsoft COCO: Common Objects in Context. arXiv preprint. Version Number: 3.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv preprint. Version Number: 2.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. Publisher: arXiv Version Number: 1.\n\nLucas Beyer*, Andreas Steiner*, Andr\u00e9 Susano Pinto*, Alexander Kolesnikov*, Xiao Wang*, Xiaohua Zhai*, Daniel Salz, Maxim Neumann, Ibrahim Al-abdulmohsin, Michael Tschannen, Jeremiah Harmsen, Daniel Keysers, Neil Houlsby, Xi Chen, Emanuele Bugliarello, Thomas Unterthiner, Keran Rong, Matthias Minderer, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Julian Eisenschlos, Manoj Kumar, Matko Bo\u0161njak, Matthias Bauer, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Paul Voigtlaender, Pinelopi Papalampidi, Olivier Henaff, Skanda Kopula, Xi Xiong, Radu Soricut, Model release contributors and general support, Tris Warkentin, Kat Black, Luiz Gustavo Martins, Glenn Cameron, Raj Gundluru, Manvinder Singh, Meg Risdal, Nilay Chauhan, Nate Keating, Nesh Devanathan, Elisa Bandy, Joe Fernandez, Antonia Paterson, Jenny Brennan, Tom Eccles, Pankil Botadra, Ben Bariach, Lav Rai, Minwoo Park, Dustin Luong, Daniel Vlasic, Bo Wu, Wenming Ye, Divyashree Sreepathihalli, Kiranbir Sodhia, Alek Andreev, Armand Joulin, Surya Bhupatiraju, Minh Giang, Joelle Barral, and Zoubin Ghahramani. 2024. PaliGemma.\n\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V Jawahar. 2021. InfographicVQA. arXiv preprint. Version Number: 2.\n\nMinesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2020. DocVQA: A Dataset for VQA on Document Images.\n\nNiklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and Nils Reimers. 2022. MTEB: Massive Text Embedding Benchmark. arXiv preprint. Version Number: 3.\n\nNomic. 2024. Nomic Embed Vision: Expanding The Nomic Latent Space.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. Publisher: arXiv Version Number: 1.\n\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint. Version Number: 1.\n\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109\u2013126. National Institute of Standards and Technology (NIST).\n\nKeshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint. Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 1567, "end_char_idx": 4642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7827d715-6f00-4efe-ba2f-59e8565404ad": {"__data__": {"id_": "7827d715-6f00-4efe-ba2f-59e8565404ad", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7ac12c6-e24f-4cf0-a601-e16be87f36da", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14c2450b-f530-43dc-bda3-a3318ce76eac", "node_type": "1", "metadata": {}, "hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "class_name": "RelatedNodeInfo"}}, "text": "2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint. Version Number: 1.\n\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109\u2013126. National Institute of Standards and Technology (NIST).\n\nKeshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint. Version Number: 1.\n\nR. Smith. 2007. An Overview of the Tesseract OCR Engine. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2, pages 629\u2013633, Curitiba, Parana, Brazil. IEEE. ISSN: 1520-5363.", "mimetype": "text/plain", "start_char_idx": 4019, "end_char_idx": 4855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01a93ebb-101f-4a55-a36d-908541ad9f71": {"__data__": {"id_": "01a93ebb-101f-4a55-a36d-908541ad9f71", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d40b8c03-0d86-458a-97ba-d1986dc3e9ef", "node_type": "4", "metadata": {}, "hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e27675e7-a242-4582-8ef7-f5781e55f524", "node_type": "1", "metadata": {}, "hash": "275baea2efd06dc7d04371ac8ef8f549db38d8eac1441e281d806a98e93a9ea8", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nKaren Sparck Jones. 1972. A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL. Journal of Documentation, 28(1):11\u201321.\n\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. 2022. Unifying Vision, Text, and Layout for Universal Document Processing. arXiv preprint. Version Number: 3.\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. arXiv preprint. Version Number: 4.\n\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. arXiv preprint. Version Number: 2.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint. Version Number: 2.\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. arXiv preprint. ArXiv:2002.10957 [cs].\n\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. FILIP: Fine-grained Interactive Language-Image Pre-Training. arXiv preprint. Version Number: 1.\n\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. arXiv preprint. Version Number: 3.\n\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid Loss for Language Image Pre-Training. Publisher: [object Object] Version Number: 4.\n\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty. 2023. Retrieving Multimodal Information for Augmented Generation: A Survey. arXiv preprint. Version Number: 3.\n\nFengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. 2022. Towards Complex Document Understanding By Discrete Reasoning. Publisher: arXiv Version Number: 3.\n\n# A Benchmark Datasets\n\n# A.1 Academic Datasets\n\nDocVQA (Mathew et al., 2020) includes collected images from the UCSF Industry Documents Library. Questions and answers were manually annotated.\n\nInfoVQA (Mathew et al., 2021) includes infographics collected from the Internet using the search query \u201cinfographics\u201d. Questions and answers were manually annotated.\n\nTAT-DQA (Zhu et al., 2022) is a large-scale Document VQA dataset that was constructed from publicly available real-world financial reports. It focuses on rich tabular and textual content requiring numerical reasoning. Questions and answers were manually annotated by human experts in finance.\n\narXivQA (Li et al., 2024) is a VQA dataset based on figures extracted from arXiv publications. The questions were generated synthetically using GPT-4 Vision.\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e27675e7-a242-4582-8ef7-f5781e55f524": {"__data__": {"id_": "e27675e7-a242-4582-8ef7-f5781e55f524", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d40b8c03-0d86-458a-97ba-d1986dc3e9ef", "node_type": "4", "metadata": {}, "hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01a93ebb-101f-4a55-a36d-908541ad9f71", "node_type": "1", "metadata": {}, "hash": "f5cd5448c9452cbe6dbac1ac03e442b08b61542ba0c8aa8aaaffc80752815b7e", "class_name": "RelatedNodeInfo"}}, "text": "Questions and answers were manually annotated.\n\nInfoVQA (Mathew et al., 2021) includes infographics collected from the Internet using the search query \u201cinfographics\u201d. Questions and answers were manually annotated.\n\nTAT-DQA (Zhu et al., 2022) is a large-scale Document VQA dataset that was constructed from publicly available real-world financial reports. It focuses on rich tabular and textual content requiring numerical reasoning. Questions and answers were manually annotated by human experts in finance.\n\narXivQA (Li et al., 2024) is a VQA dataset based on figures extracted from arXiv publications. The questions were generated synthetically using GPT-4 Vision.\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings. We create additional queries to augment the existing human-annotated ones using the same method described in subsection A.2.\n\n# A.2 Practical Datasets\n\nMethodology. Creating a relevant retrieval dataset close to real use cases is a major challenge as the dataset needs to be both sufficiently large for effective fine-tuning and sufficiently diverse to cover a broad range of modalities (full text, tables, charts, ...), domains (industry, healthcare, ...), and query-document interactions (extractive questions, open-ended questions, ...). Our approach to building this dataset involves several steps: (1) we use a web crawler to collect publicly available documents on various themes and sources, (2) we convert these PDFs into a series of images, one per page, and (3) we generate queries related to each image using a VLM.\n\nWeb-Crawler. We implemented a web crawler to efficiently collect large volumes of documents related to a given topic. The crawler is seeded with a user-defined query (e.g. \"artificial intelligence\") and then uses GPT-3.5 Turbo to brainstorm related topics and subtopics. This query augmentation strategy aims at both broadening and deepening the search. GPT-3.5 Turbo is further used to generate diverse search queries from each subtopic.", "mimetype": "text/plain", "start_char_idx": 2614, "end_char_idx": 4670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94e1d8eb-9958-4ed4-bfeb-0a065c1542bf": {"__data__": {"id_": "94e1d8eb-9958-4ed4-bfeb-0a065c1542bf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "357ee6c0-4806-49c3-94ea-903a94afb69f", "node_type": "4", "metadata": {}, "hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb", "class_name": "RelatedNodeInfo"}}, "text": "# Query Generation and Document Collection\n\nQuery set is then consumed by a pool of parallel workers whose job is to fetch the associated most relevant documents. We use SerpAPI along with a filetype filter (PDF documents only) to programmatically scrape Google Search rankings. Each file is hashed and stored in a Bloom filter (Bloom, 1970) shared among workers to avoid duplicate documents in the final corpus. Unique scraped files are downloaded, and inserted into a SQLite database along with additional metadata.\n\n# Datamix\n\nUsing the web crawler, we collected approximately 1,000 documents for each of the following four seeds: \"energy\", \"government reports\", \"healthcare industry\", and \"artificial intelligence\". These seeds were meticulously hand-picked to align with real-use cases for retrieval models and visually rich pages. We also removed all documents containing any private information. At this stage, we randomly selected 900 files for the training set and 100 files for the test set, ensuring that data leakage into the test set was avoided during subsequent processing steps.\n\n# Query Generation\n\nTo increase the efficiency of our query generation scheme and to limit API calls, we generate at most 3 questions per image. From all the documents collected, we randomly sample 10,000 images per theme and call Claude-3 Sonnet with the following prompt:\n\nRemember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus. Generate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question. Generate at most THREE pairs of questions and answers per page in a dictionary with the following format, answer ONLY this dictionary NOTHING ELSE:\n\n{\n\"questions\": [\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n}\n]\n}\n\nwhere XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n\nNote: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n\n# Human Validation\n\nWe manually validate every single synthetically created query in ViDoRe to ensure quality, query relevance, and consistency with the benchmark objective of evaluating retrieval in practical industrial settings. During this step, we randomly assign document-pair queries to 4 volunteers.\n\n15 https://serpapi.com/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4338db1-5a15-457d-9869-403c5356bc9a": {"__data__": {"id_": "a4338db1-5a15-457d-9869-403c5356bc9a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc317cf1-b705-4d8d-9dc0-2958ee98be27", "node_type": "4", "metadata": {}, "hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073", "class_name": "RelatedNodeInfo"}}, "text": "# B     Implementation details\n\n# B.1     Codebase\n\nThe codebase is written in PyTorch16 and leverages HuggingFace tooling for model implementations and trainers17.\n\n# B.2     Pairwise CE loss\n\nOur in-batch contrastive loss L is defined as the softmaxed cross-entropy of the positive scores\n\nsk+ = LI (dk, qk) w.r.t. to the maximal negative scores sk\u2212 = maxl,l\u0338 =k LI (qk, pl).\n\nFor numerical stability, we reformulate the loss with the softplus function, leading to:\n\nL = bk=11 Xsoftplus sk \u2212 skb\n\n# B.3     Hyperparameters\n\nHyperparameters are tuned on a validation split composed of 2% of the training dataset. We find bi-encoder methods to be more sensible to learning rate variations than late interaction-based models and achieve the best performance for all models with a learning rate of 5e \u2212 5. We experiment with LoRA rank and \u03b1 values and do not notice particular improvements past r = \u03b1 = 32. Per-device batch sizes are kept small due to long sequence lengths that complicate scaling past b = 4. Simulating larger batch sizes for in-batch negative sampling should enable even better results. We find the best results with global batch size b = 32 for 1 epoch on our training set.\n\n# B.4     Embedding size\n\nMinimizing storage footprint can be essential to industrial retrieval systems if databases contain millions of documents. With this criterion in view, we have compared the embedding sizes of the models in our study. As shown in Table 3, ColPali\u2019s embedding size is an order of magnitude larger than BM25 and two orders of magnitude larger than BGE-M3. However, this study is limited to the naive method of storing ColPali\u2019s multi-vector embeddings. In practical scenarios, using cluster centroids can reduce the size of ColPali multi-vector embeddings by up to an order of magnitude (Santhanam et al., 2022) and make it a competitive retrieval system.\n\n# B.5     Latency computations\n\nAll latency computations are done on a NVIDIA L4 GPU. Queries are encoded independently (batch size of 1) to simulate online querying, and pages are encoded with a batch size of 4 for PaliGemma derived models, and 8 for BGE-M3. Reported times include image and text processing time before the model forward pass, as well as query-to-index matching times. We note an interesting feature of ColPali is that all documents have the same sequence length, leading to prior knowledge of runtime and memory consumptions. Query latency experiments are averaged over 1000 queries, and indexing times are measured for a 100 page document. Per page time is obtained by diving total time by 100, corresponding to inverse page throughput.\n\n# B.6     Captioning\n\nExamples of captions generated for visually rich document chunks with Claude-3 Sonnet are shown in Figure 6 and Figure 5. The prompt used for generating the description is the following:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05cf5c28-7526-4c1d-a1c3-276dd9605b86": {"__data__": {"id_": "05cf5c28-7526-4c1d-a1c3-276dd9605b86", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4044e15c-8f00-4041-aa79-2d4991feac5b", "node_type": "4", "metadata": {}, "hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 5: Example from the \"Energy\" test set.\n\nCaption: The image depicts the hourly energy generation profile, illustrating the contributions of various energy sources over 24 hours. The data is presented as a stacked bar chart, with the x-axis representing the hours of the day from 1 to 2, and the y-axis showing the average hourly generation in MW. The bars are segmented into different colors, each representing a distinct energy source: nuclear, bio, geothermal, solar, wind, hydro, natural gas, and other imports. The chart provides insights into the temporal variations in energy generation across different sources, highlighting the interplay between baseload and intermittent sources throughout the day.\n\n# Figure 6: Example from the \"Government Reports\" test set.\n\nCaption: The image shows a table titled \"System of Record\" which outlines the different types of documents or records maintained across various systems or departments within an organization related to project management and construction. The rows list documents like project plans, budgets, schedules, contracts, purchase orders, invoices, change requests, bid submissions, drawings, manuals, meeting minutes, and reports. The columns indicate the system or department responsible for maintaining each record, such as County Servers, Project View, OnBase, CGI Advantage Financial System, and Purchasing Department. The table uses \"W\" and \"T\" markers to denote which system or department serves as the primary source (writer) or storage location (trailer) for each type of document.\n\n# More similarity maps\n\nIn Figure 7, ColPali assigns a high similarity to all patches with the word \"Kazakhstan\" when given the token <_Kazakhstan>. Moreover, our model seems to exhibit world knowledge capabilities as the patch around the word \"Kashagan\" - an offshore oil field in Kazakhstan - also shows a high similarity score. On the other hand, in Figure 8, we observe that ColPali is also capable of complex image understanding. Not only are the patches containing the word \"formulations\" highly similar to the query token _formula, but so is the upper-left molecule structure.\n\nIt is also interesting to highlight that both similarity maps showcase a few white patches with high similarity scores. This behavior might first seem surprising as the white patches should not carry a meaningful signal from the original images. We believe the vectors associated with these patches share a similar role with the ViT registers (Darcet et al., 2023), i.e. these patches were repurposed for internal computations and stored the global information from the whole image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d31348e-70a6-4212-8eb2-d2909e182ac0": {"__data__": {"id_": "8d31348e-70a6-4212-8eb2-d2909e182ac0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07e617ae-fa18-4210-9b51-b394b5a0fffb", "node_type": "4", "metadata": {}, "hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28", "class_name": "RelatedNodeInfo"}}, "text": "# Historique de production\n\n# Production totale des hydrocarbures liquides\n\n# Kazakhstan (1965-2019)\n\nQuery: \"Quelle partie de la production p\u00e9troli\u00e8re du Kazakhstan provient de champs en mer ?\"\n\n# Ferroelectrics\n\n# Lead Zirconium Titanate\n\nPb(Zr,Ti)O3\n\n1952 Shirane; Pb(Zr,Ti)O3 solid solutions\n\n1955 Jalte coor Berlincourt; Gerson: Complete Study PZT formulations\n\nCurie temperature 170-360\n\nQuery: What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a10cf413-1be5-4783-b0cd-339c5afbbe86": {"__data__": {"id_": "a10cf413-1be5-4783-b0cd-339c5afbbe86", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3dd0d7-3a7e-4d6f-a94c-d3bbfa985eff", "node_type": "4", "metadata": {}, "hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdd5d77a-f6d9-44ad-aa42-13edc4d90806", "node_type": "1", "metadata": {}, "hash": "62816e0b531a77b544cbbfd001f5c8c7767e6aa087dc8bc5a6d4f92170b12b5e", "class_name": "RelatedNodeInfo"}}, "text": "# D. Additional results\n\n# D.1 Other Metrics\n\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy|Gov.|Health.|Avg.| |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Unstructured Text only|BM25|-|26.6|-|-|34.6|45.0|86.0|70.0|68.0|74.0|-|\n| |BGE-M3|-|22.8\u21933.8|-|-|26.1\u21938.5|51.0\u21916.0|81.0\u21935.0|72.0\u21912.0|67.0\u21931.0|77.0\u21913.0|-|\n|Unstructured + OCR|BM25|26.7|28.9|54.0|30.4|50.0|52.0|86.0|77.0|74.0|80.0|55.9|\n| |BGE-M3|28.1\u21911.4|22.9\u21936.0|53.8\u21930.2|55.7\u219125.3|38.6\u219311.4|56.0\u21914.0|82.0\u21934.0|79.0\u21912.0|76.0\u21912.0|83.0\u21913.0|57.5\u21911.6|\n|Unstructured + Captioning|BM25|35.5|30.2|61.5|24.3|49.0|47.0|79.0|76.0|75.0|81.0|55.9|\n| |BGE-M3|29.3\u21936.2|26.0\u21934.2|62.1\u21910.6|58.6\u219134.3|30.6\u219318.4|55.0\u21918.0|80.0\u21911.0|78.0\u21912.0|69.0\u21936.0|83.0\u21912.0|57.2\u21911.3|\n|Contrastive VLMs|Jina-CLIP|19.4|7.3|26.7|12.5|1.6|2.0|11.0|13.0|15.0|17.0|12.6|\n| |Nomic-vision|10.4|6.7|22.1|9.6|1.6|0.0|9.0|9.0|7.0|13.0|8.8|\n| |SigLIP (Vanilla)|34.2|21.3|51.8|46.1|17.9|13.0|50.0|51.0|47.0|65.0|39.7|\n|Ours|(Copied) SigLIP (Vanilla)|34.2|21.3|51.8|46.1|17.9|13.0|50.0|51.0|47.0|65.0|39.7|\n| |BiSigLIP (+fine-tuning)|49.2\u219115.0|23.8\u21912.5|59.0\u21917.2|52.1\u21916.0|20.7\u21912.8|16.0\u21913.0|62.0\u219112.0|61.0\u219110.0|55.0\u21918.0|72.0\u21917.0|47.1\u21917.4|\n| |BiPali (+LLM)|46.4\u2193-2.8|20.0\u2193-3.8|54.6\u2193-4.4|63.2\u219111.1|20.4\u2193-0.4|34.0\u219118.0|59.0\u2193-3.0|45.0\u2193-16.0|57.0\u21912.0|56.0\u2193-16.0|45.6\u2193-1.5|\n| |ColPali (+Late Inter.)|72.4\u219126.0|45.6\u219125.6|74.6\u219120.0|75.4\u219112.1|53.1\u219132.7|55.0\u219121.0|93.0\u219134.0|85.0\u219140.0|85.0\u219128.0|88.0\u219132.0|72.7\u219127.1|\n\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdd5d77a-f6d9-44ad-aa42-13edc4d90806": {"__data__": {"id_": "cdd5d77a-f6d9-44ad-aa42-13edc4d90806", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a3dd0d7-3a7e-4d6f-a94c-d3bbfa985eff", "node_type": "4", "metadata": {}, "hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a10cf413-1be5-4783-b0cd-339c5afbbe86", "node_type": "1", "metadata": {}, "hash": "00119033c6c21ba634c0f31ec079ed765db6f54b4f9569e4c2947c753f6644b0", "class_name": "RelatedNodeInfo"}}, "text": ")|72.4\u219126.0|45.6\u219125.6|74.6\u219120.0|75.4\u219112.1|53.1\u219132.7|55.0\u219121.0|93.0\u219134.0|85.0\u219140.0|85.0\u219128.0|88.0\u219132.0|72.7\u219127.1|\n\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe. Results are presented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\n\n# D.2 Model Variants\n\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy|Gov.|Health.|Avg.|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|ColSigLIP (PaliGemma)|3.1|3.0|5.1|6.2|2.5|1.0|3.4|3.4|2.3|2.2|3.2|\n|BiSigLIP (PaliGemma)|18.5|14.6|33.4|39.5|16.1|5.2|27.6|32.6|36.6|35.7|26.0|\n|ColSigLIP (Original)|2.6|2.2|2.3|5.7|1.8|1.0|2.6|4.1|1.4|1.5|2.5|\n|ColPali (No Mem. Tokens)|80.4|53.2|82.4|77.4|65.7|63.4|97.0|89.9|93.6|92.4|79.6|\n|ColPali (Best)|79.1|54.4|81.8|83.9|65.8|73.2|96.2|91.0|92.7|94.4|81.3|\n\nTable 5: Evaluation of some \"negative results\" and ablations on ViDoRe; ColPali for reference. Results are presented using NDCG@5 metrics. Text-only metrics are not computed for benchmarks with only visual elements.", "mimetype": "text/plain", "start_char_idx": 1308, "end_char_idx": 2354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f55c64c3-7c35-4ce8-ab63-ecee2d0f70ae": {"__data__": {"id_": "f55c64c3-7c35-4ce8-ab63-ecee2d0f70ae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c54c1f00-fa98-437b-99f9-7fd9426472ee", "node_type": "4", "metadata": {}, "hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2", "class_name": "RelatedNodeInfo"}}, "text": "# ViDoRe examples\n\n# Energy\n\n|Query|Response|\n|---|---|\n|What types of accounts or products allow investors to defer paying taxes?| |\n|What is the projected peak electricity demand in California for the year 2030?| |\n|What is the estimated total savings for a PV system in Durham under the net metering (flat rate) billing option over the system\u2019s useful life of 25 years?|Projected 2030 electricity capacities|\n\n# Artificial Intelligence\n\n|Query|Response|\n|---|---|\n|What are some common outcome areas targeted by TAII for different age groups?| |\n|What did the robot monitor to determine when to activate or deactivate the blower motor and blinker?| |\n|What is the key approach used in the PDP architecture?| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c526329e-1e8d-4f5c-b10f-bdd2e028b2fd": {"__data__": {"id_": "c526329e-1e8d-4f5c-b10f-bdd2e028b2fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4dd0687-9f00-4b8a-9709-9c222256cabf", "node_type": "4", "metadata": {}, "hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451", "class_name": "RelatedNodeInfo"}}, "text": "# Healthcare Industry\n\n# Query: What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?\n\nFerroelectrics\n\nLojd Circon Um\n\nPblzo Tlal\n\n1952 Shlrano Sufuri\n\n# Query: What government entities are involved in public financing for health care in the US?\n\nUCLA Health System Financing\n\n# Government Reports\n\n# Query: What does the AVPU scale stand for in assessing the level of consciousness of a seriously ill child?\n\n# Query: What are some mandates for the EPA under the Pollution Prevention Act?\n\n# Query: What is the strategy of KPMG Hazem Hassan?\n\n# Query: What is the trust signal score for the consumer industry best-in-class archetype?\n\n# Who we are?\n\nWE\n\nM dats\n\nGMOnsecy\n\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5709460d-cf04-4f42-9d20-11f703381225": {"__data__": {"id_": "5709460d-cf04-4f42-9d20-11f703381225", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c4413cd-d62a-4201-aa1d-49a4583a0bb3", "node_type": "4", "metadata": {}, "hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc", "class_name": "RelatedNodeInfo"}}, "text": "# Shift\n\n# Query: Selon le graphique, quelle est la capacit\u00e9 d\u2019import et la consommation r\u00e9elle de carburants SAF (biocarburants durables pour l\u2019aviation) pr\u00e9vues en 2050 ?\n\n# Query: Quelle partie de la production p\u00e9troli\u00e8re du Kazakhstan provient de champs en mer ?\n\n# Query: Quels sont les pays ayant la plus grande part des d\u00e9couvertes cumul\u00e9es de p\u00e9trole brut en 2020 (en milliers de barils, hors d\u00e9couvertes cumul\u00e9es) ?\n\n|Lucmolo|Fetall|\n|---|---|\n|20| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4603d6d1-873c-4316-a400-cbdc1334f250": {"__data__": {"id_": "4603d6d1-873c-4316-a400-cbdc1334f250", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "391aa0f7-6325-4517-ad24-15e3efcf61f6", "node_type": "4", "metadata": {}, "hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b", "class_name": "RelatedNodeInfo"}}, "text": "# ColPali: Efficient Document Retrieval with Vision Language Models\n\n# Manuel Faysse* 1,3 Hugues Sibille\u22171,4 Tony Wu\u22171 Bilel Omrani1\n\n# Gautier Viaud1 C\u00e9line Hudelot3 Pierre Colombo2,3\n\n# 1Illuin Technology 2Equall.ai\n\n# 3CentraleSup\u00e9lec, Paris-Saclay 4ETH Z\u00fcrich\n\n# manuel.faysse@centralesupelec.fr\n\n# Abstract\n\nDocuments are visually rich structures that convey information through text, as well as tables, figures, page layouts, or fonts. While modern document retrieval systems exhibit strong performance on query-to-text matching, they struggle to exploit visual cues efficiently, hindering their performance on practical document retrieval applications such as Retrieval Augmented Generation. To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieving tasks spanning multiple domains, languages, and settings. The inherent shortcomings of modern systems motivate the introduction of a new retrieval model architecture, ColPali, which leverages the document understanding capabilities of recent Vision Language Models to produce high-quality contextualized embeddings solely from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically faster and end-to-end trainable. We release all project artifacts at https://huggingface.co/vidore.\n\n# 1 Introduction\n\nDocument Retrieval consists in matching a user query to relevant documents in a given corpus. It is central to many industrial applications, either as a standalone ranking system (search engines) or as part of more complex information extraction or Retrieval Augmented Generation (RAG) pipelines. Over recent years, pretrained language models have enabled large improvements in text embedding models. In practical industrial settings, however, the main performance bottleneck for efficient document retrieval is not in embedding model performance but in the prior data ingestion pipeline. To optimize a standard PDF document, many steps are required. First, PDF parsers or Optical Character Recognition (OCR) systems are used to extract words from the pages. Document layout detection models can then be run to segment paragraphs, titles, and other page objects such as tables, figures, and headers. A chunking strategy is then defined to group text passages with some semantic coherence, and modern retrieval setups may even integrate a captioning step to describe visually rich elements in a natural language form, more suitable for embedding models. In our experiments (Table 2), we typically find that optimizing the ingestion pipeline yields much greater performance on visually rich document retrieval than optimizing the text embedding model.\n\n# Contribution 1: ViDoRe\n\nIn this work, we argue that document retrieval systems should not be evaluated solely on the capabilities of text embedding models (Bajaj et al., 2016; Thakur et al., 2021; Muennighoff et al., 2022), but should also", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80e11702-b6cf-4b3b-a0fb-54f60ef6dbae": {"__data__": {"id_": "80e11702-b6cf-4b3b-a0fb-54f60ef6dbae", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d21e51f0-3ec7-4a3d-8868-1c39fb86ca72", "node_type": "4", "metadata": {}, "hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d", "class_name": "RelatedNodeInfo"}}, "text": "# Standard Retrieval\n\n# m 0.66 NDCG@5\n\n|Method|Offline|Online|Similarity Score|\n|---|---|---|---|\n|ColPali (ours)|0.81 NDCG@5|Vision LLM| |\n|MaxSim| |What| |\n|Page|7.22s|Query|22ms|\n|OCR Detection| | | |\n\nFigure 2: ColPali simplifies document retrieval w.r.t. standard retrieval methods while achieving stronger performances with better latencies. Latencies and results are detailed in section 5 and subsection B.5.\n\nConsider the context and visual elements of the documents to be retrieved. To this end, we create and openly release ViDoRe, a comprehensive benchmark to evaluate systems on page-level document retrieval with a wide coverage of domains, visual elements, and languages. ViDoRe targets practical document retrieval settings, in which user queries may require both textual and visual understanding to be correctly matched to relevant documents. We highlight the shortcomings of current text-centric systems in these settings.1\n\n# Contribution 2: ColPali\n\nWe propose a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents purely from their visual features, allowing for subsequent fast query matching with late interaction mechanisms (Khattab and Zaharia, 2020). Our method, ColPali, outperforms all other retrieval systems on ViDoRe while being fast and end-to-end trainable. We release models and code at https://huggingface.co/vidore.\n\n# 2 Problem Formulation & Related Work\n\n# Problem Setting\n\nIn our setting, a retrieval system scores how relevant a document d from corpus D is with respect to a query q. Computing the similarity score s(q, d) \u2208 R+ for each of the |D| documents in the corpus creates a ranking we can use to extract the most relevant documents. In this work, we focus on page-level retrieval: given a query, is the correct document page retrieved by the system? For coherence with existing literature, we further use the term document to refer to individual pages, i.e. the atomic retrieved elements in our setting. As we focus on practical industrial retrieval applications (RAG, search engines) with potentially large corpora sizes, latency constraints are imposed on scoring systems. Most current retrieval systems can be decomposed into (1) an offline indexation phase in which a document index is built and (2) an online querying phase in which a query is matched to documents from the index and where low latency is vital to the user experience.\n\nEfficient document retrieval systems exhibit joint properties of high retrieval performance (R1), low latency during querying (R2), and high throughput during indexation (R3).\n\n# 2.1 Textual Retrieval Methods\n\n# Document Retrieval in Text Space\n\nStatistical methods based on word frequency like TF-IDF (Sparck Jones, 1972) and BM25 (Robertson et al., 1994) are still widely used due to their simplicity.\n\n1 The benchmark leaderboard is hosted publicly at https://huggingface.co/spaces/vidore/vidore-leaderboard to encourage further developments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a4d9f8a-bee4-4635-b18c-fb55e04dab08": {"__data__": {"id_": "0a4d9f8a-bee4-4635-b18c-fb55e04dab08", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71321f4c-a11b-43b1-9919-2350469415a0", "node_type": "4", "metadata": {}, "hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f", "class_name": "RelatedNodeInfo"}}, "text": "# 2.2 Integrating Visual features\n\n# Contrastive Vision Language Models\n\nMapping latent representations of textual content to corresponding representations of visual content has been done by aligning disjoint visual and text encoders through contrastive losses (Radford et al., 2021; Zhai et al., 2023). While some OCR capabilities exist in these models, the visual component is often not optimized for text understanding. The Fine-grained Interactive Language-Image Pre-training (Yao et al., 2021) framework extends the late interaction mechanism to cross-modal vision-language models, relying on max similarity operations between text tokens and image patches.\n\n# Visually Rich Document Understanding\n\nTo go beyond text, some document-focused models jointly encode text tokens alongside visual or document layout features (Appalaraju et al., 2021; Kim et al., 2021; Huang et al., 2022; Tang et al., 2022). Large Language transformer Models (LLMs) with strong reasoning capabilities have recently been combined with Vision Transformers (ViTs) (Dosovitskiy et al., 2020) to create VLMs (Alayrac et al., 2022; Liu et al., 2023b; Bai et al., 2023; Lauren\u00e7on et al., 2024) where image patch vectors from contrastively trained ViT models (Zhai et al., 2023) are fed as input embeddings to the language model and concatenated with the text-token embeddings.\n\n# PaliGemma\n\nThe PaliGemma-3B model (Lucas Beyer* et al., 2024) extends concepts from Pali3 (Chen et al., 2023), and projects SigLIP-So400m/14 (Alabdulmohsin et al., 2023) patch embeddings into Gemma-2B\u2019s text vector space (Gemma Team et al., 2024). Along with its reasonable size w.r.t. other performant VLMs, an interesting property of PaliGemma\u2019s text model is that it is fine-tuned with full-block attention on the prefix (instruction text and image tokens). VLMs display enhanced capabilities in Visual Question Answering, captioning, and document understanding (Yue et al., 2023), but are not optimized for retrieval tasks.\n\n# 3 The ViDoRe Benchmark\n\nExisting benchmarks for contrastive vision-language models primarily evaluate retrieval for natural images (Lin et al., 2014; Borchmann et al., 2021; Thapliyal et al., 2022). On the other hand, textual retrieval benchmarks (Muennighoff et al., 2022) are evaluated at the textual passage level and are not tailored for document retrieval tasks. We fill the gap with ViDoRe, a comprehensive benchmark for document retrieval using visual features.\n\n# 3.1 Benchmark Design\n\nViDoRe is designed to comprehensively evaluate retrieval systems on their capacity to match queries to relevant documents at the page level. This benchmark encompasses multiple orthogonal subtasks, with focuses on various modalities - text, figures, infographics, tables; thematic domains - medical,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d9388fb-2824-4705-a8db-703f8c20b973": {"__data__": {"id_": "1d9388fb-2824-4705-a8db-703f8c20b973", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57f2020b-d272-48b7-a5aa-76ce658bc649", "node_type": "4", "metadata": {}, "hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe", "class_name": "RelatedNodeInfo"}}, "text": "# Dataset\n\n|Dataset|# Queries|Domain|\n|---|---|---|\n|Academic Tasks| | |\n|DocVQA (eng)|500 (500)|Industrial|\n|InfoVQA (eng)|500 (500)|Infographics|\n|TAT-DQA (eng)|1600 (1600)|Varied Modalities|\n|arXiVQA (eng)|500 (500)|Scientific Figures|\n|TabFQuAD (fra)|210 (210)|Tables|\n|Practical Tasks| | |\n|Energy (eng)|100 (1000)|Scientific|\n|Government (eng)|100 (1000)|Administrative|\n|Healthcare (eng)|100 (1000)|Medical|\n|AI (eng)|100 (1000)|Scientific|\n|Shift Project (fra)|100 (1000)|Environment|\n\nTable 1: ViDoRe comprehensively evaluates multimodal retrieval methods. The size of the document corpus is indicated in parentheses.\n\n# Academic Tasks\n\nWe repurpose widely used visual question-answering benchmarks for retrieval tasks: for each page-question-answer triplet, we use the question as the query, and the associated page as the gold document (Table 1). These academic datasets either focus on single specific modalities (Mathew et al., 2020, 2021; Li et al., 2024) or target more varied visually rich documents (Zhu et al., 2022). Moreover, we consider TabFQuAD, a human-labeled dataset on tables extracted from French industrial PDF documents released with this work. Details can be found in subsection A.1.\n\n# Practical tasks\n\nWe construct topic-specific retrieval benchmarks spanning multiple domains to go beyond repurposed QA datasets and evaluate retrieval in more realistic industrial situations (e.g. RAG). To achieve this, we collect publicly accessible PDF documents and generate queries pertaining to document pages using Claude-3 Sonnet, a high-quality proprietary vision-language model (Anthropic, 2024). In total, we collect 1,000 document pages per topic, which we associate with 100 queries extensively filtered for quality and relevance by human annotators. The corpus topics are intentionally specific to maximize syntactic proximity between documents, creating challenging retrieval tasks and covering an array of orthogonal domains (Table 1). Query-page pair examples are shown in Appendix E.2.\n\n# Evaluation Metrics\n\nWe evaluate performance on our benchmark (Requirement R1) using standard metrics from the retrieval literature (NDCG, Recall@K, MRR). We report NDCG@5 values as the main performance metric in this work and release the complete sets of results along with the models.\n\nTo validate compliance with practical industrial constraints, we also consider query latencies (R2) and indexing throughputs (R3).\n\n# 3.2 Assessing Current Systems\n\n# Unstructured\n\nWe evaluate retrieval systems representative of those found in standard industrial RAG pipelines. As is common practice, we rely on the Unstructured off-the-shelf tool in the highest resolution settings to construct high-quality text chunks from PDF documents. Unstructured orchestrates the document parsing pipeline, relying on deep learning vision models to detect titles and document layouts (Ge et al., 2021), OCR engines (Smith, 2007) to extract text in non-native PDFs, specialized methods or models to detect and reconstruct tables, and implements a chunking strategy (by-title) that leverages the detected document structure to preserve section boundaries when concatenating texts. As is common practice, in our simplest Unstructured configuration (text-only), only textual elements are kept, and figures, images, and tables are considered noisy information and are filtered out.\n\n# Unstructured + X\n\nWhile Unstructured is a strong baseline by itself, we further augment Unstructured\u2019s output by integrating the visual elements. In (+ OCR), tables, charts, and images are run through an OCR engine, processed by Unstructured, and chunked independently. In (+ Captioning), we set up a fully-fledged captioning strategy (Zhao et al., 2023), in which we feed visual elements to a strong proprietary Vision Language Model (Claude-3 Sonnet (Anthropic, 2024)) to obtain highly detailed textual descriptions of the elements. Both strategies aim to integrate visual elements in the retrieval pipeline but incur significant latency and resource costs (subsection 5.2).\n\n# Embedding Model\n\nTo embed textual chunks, we evaluate Okapi BM25, the de facto standard sparse statistical retrieval method, and the dense encoder of BGE-M3 (Chen et al., 2024), a multilingual neural method with SOTA performance in its size category. Chunks are embedded and scored independently, and page-level scores are obtained by.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db28afe2-4eab-4dda-8289-89aaea5bcd85": {"__data__": {"id_": "db28afe2-4eab-4dda-8289-89aaea5bcd85", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91b7c270-194e-4fb5-a8a0-e82033bced17", "node_type": "4", "metadata": {}, "hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a392518-ba7b-4ee5-a096-cf3f2e2444e6", "node_type": "1", "metadata": {}, "hash": "7e3111e040a674730fa702d7902f35c16b8c433334150ac0ae151ea4fab47ce1", "class_name": "RelatedNodeInfo"}}, "text": "max-pooling over the page\u2019s chunk scores.5\n\n# Contrastive VLMs\n\nWe also evaluate the strongest available vision-language embedding models; Jina CLIP (Koukounas et al., 2024), Nomic Embed Vision (Nomic, 2024), and SigLIP-So400m/14 (Alabdulmohsin et al., 2023).\n\n# Results\n\nFrom a performance perspective, best results are obtained by combining the Unstructured parser with visual information, either from captioning strategies or by running OCR on the visual elements (Table 2). Little difference is seen between BM25 and BGE-M3 embeddings highlighting the visual information bottleneck. Contrastive VLMs lag behind. Beyond retrieval performance (R1), the indexing latencies (R2) reported in Figure 3 illustrate that PDF parsing pipelines can be very lengthy, especially when incorporating OCR or captioning strategies. Querying latencies at runtime (R3) are very good for all evaluated systems (\u2264 22 ms on NVIDIA L4) due to fast query encoding and cosine similarity matching.\n\n|PDF Parser|Latency (s)|\n|---|---|\n|PDF Parser|(7.22s)|\n|Siglip|(0.12s)|\n|ColPali|(0.39s)|\n\n0 1 2 3 4 5 6 7\n\nLatency (s)\n\nLayout Detection OCR Captioning Page Encoding\n\n# Figure 3\n\nOffline indexing with ColPali is much simpler and faster compared to standard retrieval methods. Indexing speeds reported are computed on Nvidia L4 GPUs and detailed in subsection B.5.\n\n# 4 Late interaction based Vision Retrieval\n\n# 4.1 Architecture\n\nVision-Language Models. Encouraged by their strong document understanding capabilities, we propose adapting recent VLMs for retrieval. The key concept is to leverage the alignment between output embeddings of text and image tokens acquired during multi-modal finetuning. To this extent, we introduce ColPali, a Paligemma-3B extension that is capable of generating ColBERT-style multi-vector representations of text and images (Figure 2). PaliGemma-3B is a strong candidate due to its small size, the many released checkpoints fine-tuned for different image resolutions and tasks,\n\n5We empirically validated the max-pooling strategy over sub-page chunks to be more effective than concatenating all page chunks before embedding pagewise.\n\nand the promising performances on various document understanding benchmarks. We add a projection layer to map the output language modeling embeddings to a vector space of reduced dimension D = 128 as used in the ColBERT paper (Khattab and Zaharia, 2020) to keep lightweight bag-of-embedding representations.\n\n# Late Interaction\n\nGiven query q and document d, we denote as Eq \u2208 RNq \u00d7D and Ed \u2208 RNd\u00d7D their respective multi-vector representation in the common embedding space RD. The late interaction operator, LI (q, d), is the sum over all query vectors Ed(j), of its maximum dot product \u27e8\u00b7|\u00b7\u27e9 with each of the Nd document embedding vectors Ed(1:Nd).\n\nLI (q, d) = \u2211i\u2208[|1,Nq|] maxj\u2208[|1,Nd|] \u27e8Eq(i)|Ed(j)\u27e9\n\n# Contrastive Loss\n\nThe Late Interaction operation is fully differentiable, enabling backpropagation. Let a batch {qk, dk}k\u2208[|1,b|] composed of b query-page pairs, where for all k \u2208 [|1, b|], the document page dk is the document corresponding to query qk. Following Khattab and Zaharia (2020), we define our in-batch contrastive loss L as the softmaxed cross-entropy of the positive scores\n\nL+ = LI (dk, qk) w.r.t. to the maximal negative scores L- = maxl,l\u2260k LI (qk, pl).\n\n# 4.2 Model training\n\nDataset. Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages6. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nParameters. All models are trained for 1 epoch on the train set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a392518-ba7b-4ee5-a096-cf3f2e2444e6": {"__data__": {"id_": "9a392518-ba7b-4ee5-a096-cf3f2e2444e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91b7c270-194e-4fb5-a8a0-e82033bced17", "node_type": "4", "metadata": {}, "hash": "49941ff576718eeea86d5de8ab9bb8f18dd7d4b4e5ac51dcd22eb549c72398dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db28afe2-4eab-4dda-8289-89aaea5bcd85", "node_type": "1", "metadata": {}, "hash": "b5899512ab126ca7b0cc5fe2a825e271dfbea6ea3e49469597376829a500a0db", "class_name": "RelatedNodeInfo"}}, "text": "to the maximal negative scores L- = maxl,l\u2260k LI (qk, pl).\n\n# 4.2 Model training\n\nDataset. Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages6. We explicitly verify no multi-page PDF document is used both ViDoRe and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters.\n\nParameters. All models are trained for 1 epoch on the train set. Unless specified otherwise, we train models in bfloat16 format, use low-rank adapters (LoRA, Hu et al. (2021)) with \u03b1 = 32 and r = 32 on the transformer layers from the language model,\n\n6Multilingual data is present in the pretraining corpus of the language model (Gemma-2B) and potentially occurs during PaliGemma-3B\u2019s multimodal training.", "mimetype": "text/plain", "start_char_idx": 3266, "end_char_idx": 4349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8091b6c-4a36-4312-9cd1-218af5842e12": {"__data__": {"id_": "a8091b6c-4a36-4312-9cd1-218af5842e12", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39bb65a1-f964-42cd-a973-29a48f3bb2ae", "node_type": "4", "metadata": {}, "hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4", "class_name": "RelatedNodeInfo"}}, "text": "# Results\n\n# 5.1 Performance (R1)\n\nWe iteratively construct ColPali, starting from an off-the-shelf SigLIP model (Table 2).\n\n# BiSigLIP: Improving a strong model.\n\nSigLIP7 is a strong vision-language bi-encoder model, pre-trained on the English split of WebLI (Chen et al., 2023), a corpus of billions of image-text pairs. We find that SigLIP largely outperforms both Jina CLIP and Nomic-vision on document retrieval tasks. Further fine-tuning the textual component of this model on our document-oriented dataset (BiSigLIP) yields clear improvements across the board, particularly on figure retrieval (ArxivQA) and table retrieval tasks (TabFQuAD).\n\n# BiPali: Pairing with a language model.\n\nIn the PaliGemma model architecture, SigLIP-generated patch embeddings are fed to a text language model to obtain LLM contextualized output patch embeddings.8 We average pool these representations to obtain a single dense vector, effectively creating a PaliGemma bi-encoder model (BiPali). After fine-tuning on the training dataset, we obtain a model that performs slightly worse in English than the tuned BiSigLIP variant. This can be explained by the fact that contrary to SigLIP, the original PaliGemma is not trained on contrastive matching tasks, but rather on next token prediction. Our contrastive fine-tuning phase on 100K images to transform PaliGemma into a bi-encoder is 5 orders of magnitude smaller than SigLIP\u2019s original contrastive training. However, we see notable improvements in French tasks, indicating that BiPali\u2019s LLM (Gemma 2B) helps multilingual text understanding. This is particularly notable as our training dataset does not contain non-English samples.\n\n# ColPali: Adding Late Interaction.\n\nOne benefit of inputting image patch embeddings through a language model is that they are natively mapped to a latent space similar to textual input (query). This enables leveraging the ColBERT strategy to compute interactions between text tokens and image patches, which enables a step-change improvement in performance compared to BiPali. Results in Table 2 show that our ColPali model also largely outperforms the strong baselines based on Unstructured and captioning, as well as all evaluated text-image embedding models. The difference is particularly stark on the more visually complex benchmark tasks, such as InfographicVQA, ArxivQA, and TabFQuAD representing respectively infographics, figures, and tables. However, text-centric documents are also better retrieved by the ColPali models across all evaluated domains and languages, making our approach the overall best-performing document-retrieval model.\n\n# Negative Results.\n\nFor extensiveness, we also train ColSigLIP, a late interaction variant of the BiSigLIP model but obtain abysmal performances. We attribute this to the large gaps w.r.t. SigLIP\u2019s pre-training, in which only a pooled latent representation is used in the contrastive loss, which does not optimize the representations of individual patch and token embeddings. Similarly, we train a BiSigLIPP aliGemma variant, in which we retrieve the image representations from the SigLIP model that has been further updated by PaliGemma fine-tuning, and use the text representations from PaliGemma\u2019s text model. After fine-tuning on our dataset, performance is severely inferior to SigLIP vanilla which simply encodes with SigLIP\u2019s original text and vision components. This indicates a logical misalignment between SigLIP embeddings, and Gemma embeddings after PaliGemma training. We detail these results in Table 5.\n\n# 5.2 Latencies & Memory Footprint\n\n# Online Querying. (R2)\n\nLogically, querying latencies differ between ColPali and a BGE-M3 embedding model. For BGE, encoding takes about 22 ms for 15 tokens, while encoding a query with ColPali\u2019s language model takes about 30 ms9. For smaller corpus sizes, computing the late interaction operation induces marginally small overheads (\u2248 1 ms per 1000 pages in the corpus), and the cosine similarity computation between bi-encoder vectors.\n\n9Computed for a batch size of 1 (online), and averaged over 1000 queries. See subsection B.5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e34ef8ca-0ecb-491d-987c-4e0ca7295f5c": {"__data__": {"id_": "e34ef8ca-0ecb-491d-987c-4e0ca7295f5c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71fe3a08-92b1-4c0a-bbd7-310d27d456fd", "node_type": "4", "metadata": {}, "hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45662eb7-0374-4eaa-81a5-1d6206504c70", "node_type": "1", "metadata": {}, "hash": "231a44d96b44ef7cef97f4d52fc7bf0c58617c4a316ef1d4aacccf09c35c8c27", "class_name": "RelatedNodeInfo"}}, "text": "# Table 2: Comprehensive evaluation of baseline models and our proposed method on ViDoRe.\n\nResults are presented using NDCG@5 metrics, and illustrate the impact of different components. Text-only metrics are not computed for benchmarks with only visual elements.\n\n|Model|Metrics| | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy Gov.| |Health.|Avg.| |\n|- BM25|-|34.1|-|-|44.0|59.6|90.4|78.3|78.8|82.6| | |\n|- BGE-M3|-|28.4\u21935.7|-|-|36.1\u21937.9|68.5\u21918.9|88.4\u21932.0|76.8\u21931.5|77.7\u21931.1|84.6\u21912.0| | |\n|- BM25|31.6|36.8|62.9|46.5|62.7|64.3|92.8|85.9|83.9|87.2|65.5| |\n|- BGE-M3|31.4\u21930.2|25.7\u219311.1|60.1\u21932.8|70.8\u219124.3|50.5\u219312.2|73.2\u21918.9|90.2\u21932.6|83.6\u21932.3|84.9\u21911.0|91.1\u21913.9|66.1\u21910.6| |\n|- BM25|40.1|38.4|70.0|35.4|61.5|60.9|88.0|84.7|82.7|89.2|65.1| |\n|- BGE-M3|35.7\u21934.4|32.9\u21935.4|71.9\u21911.9|69.1\u219133.7|43.8\u219317.7|73.1\u219112.2|88.8\u21910.8|83.3\u21931.4|80.4\u21932.3|91.3\u21912.1|67.0\u21911.9| |\n|Contrastive VLMs|Jina-CLIP|25.4|11.9|35.5|20.2|3.3|3.8|15.2|19.7|21.4|20.8|17.7|\n|Nomic-vision|17.1|10.7|30.1|16.3|2.7|1.1|12.9|10.9|11.4|15.7|12.9| |\n|SigLIP (Vanilla)|43.2|30.3|64.1|58.1|26.2|18.7|62.5|65.7|66.1|79.1|51.4| |\n|Ours|SigLIP (Vanilla)|43.2|30.3|64.1|58.1|26.2|18.7|62.5|65.7|66.1|79.1|51.4|\n|BiSigLIP (+fine-tuning)|58.5\u219115.3|32.9\u21912.6|70.5\u21916.4|62.7\u21914.6|30.5\u21914.3|26.5\u21917.8|74.3\u219111.8|73.7\u21918.0|74.2\u21918.1|82.3\u21913.2|58.6\u21917.2| |\n|BiPali (+LLM)|56.5\u2193-2.0|30.0\u2193-2.9|67.4\u2193-3.1|76.9\u219114.2|33.4\u21912.9|43.7\u219117.2|71.2\u2193-3.1|61.9\u2193-11.7|73.8\u2193-0.4|73.6\u2193-8.8|58.8\u21910.2| |\n|ColPali (+Late Inter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45662eb7-0374-4eaa-81a5-1d6206504c70": {"__data__": {"id_": "45662eb7-0374-4eaa-81a5-1d6206504c70", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71fe3a08-92b1-4c0a-bbd7-310d27d456fd", "node_type": "4", "metadata": {}, "hash": "65084fe76c7e7346fef80b1fe52a19cdda120251e9f2f18e7780148ee649afe3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e34ef8ca-0ecb-491d-987c-4e0ca7295f5c", "node_type": "1", "metadata": {}, "hash": "e95fa380b755b4c34d72a363da79a8c57174e8d488e9a48e2fd996853cc786e8", "class_name": "RelatedNodeInfo"}}, "text": ")|79.1\u219122.6|54.4\u219124.5|81.8\u219114.4|83.9\u21917.0|65.8\u219132.4|73.2\u219129.5|96.2\u219125.0|91.0\u219129.1|92.7\u219118.9|94.4\u219120.8|81.3\u219122.5| |\n\n# 5.3 Interpretability\n\nBy superimposing the late interaction heatmap on top of the original image, we can visualize the most salient image patches with respect to each term of the query, yielding interpretable insights into model focus zones. As epitomized in Figure 1, we observe ColPali exhibits strong OCR capabilities as both the words \"hourly\" and \"hours\" present a high similarity score with the query token &lt;_hour&gt;. We also note particular focus on other non-trivial image features such as the x-axis representing hours being salient. Other visualization examples with similar trends of the model transcending pure OCR are shown in Appendix C.\n\n# 6 Ablation study\n\nShould we scale models or patch numbers? We train a variant of PaliGemma with half the number of image patches (512). While there is a clear performance degradation w.r.t. to the 1024-patch ColPali model (Figure 4), memory usage is much lower.\n\nWhile another PaliGemma variant exists with 2048 patches, the different training datamix and the large memory requirements make this model impractical for both training.", "mimetype": "text/plain", "start_char_idx": 1511, "end_char_idx": 2719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e1bb9a6-a278-48c2-975a-b568abdba7d9": {"__data__": {"id_": "4e1bb9a6-a278-48c2-975a-b568abdba7d9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05a406b2-8f89-470b-abb2-77826c6f0f7a", "node_type": "4", "metadata": {}, "hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84", "class_name": "RelatedNodeInfo"}}, "text": "# Relative NDCG@5 (%)\n\n| |ColPali|Idefics2|No Mem.|Full IB|Train|TabF|\n|---|---|---|---|---|---|---|\n|(512)|(64)|Tokens|Loss|Vision|Tuning| |\n\nFigure 4: Relative NDCG@5 performance gain w.r.t. the default ColPali (1024 patches). TabFQuAD fine-tuning measures the performance difference on the TabFQuAD task after the introduction of targeted data in the training set. All other results refer to performance deltas averaged on all ViDoRe tasks.\n\nIdefics2-8B (Lauren\u00e7on et al., 2024), a VLM with a similar architecture and based on a Mistral-7B (Jiang et al., 2023) language backbone and a SigLIP vision encoder paired with a perceiver resampler. The most notable differences with PaliGemma lie in the size of the language model (2B and 7B resp.) and the number of image patches (between 512 and 2048 for PaliGemma, and 64 post-resampling for Idefics212). Our results (Figure 4) suggest language model size has a strong impact on performance, and along with the trained resampler enables more efficient representations for smaller numbers of image embeddings - ColIdefics2 with 64 patches edges out ColPali with 512 patches. Scaling the number of patches of the smaller ColPali model from 512 to 1024, enables largely surpassing the 60-patch ColIdefics2 while being about twice as fast in terms of training and inference latency. These results suggest there are tradeoffs between performance (R1), latencies during online querying (R2) and offline indexation phases (R3), and index memory size.\n\n# Should we fine-tune the vision component?\n\nWe run our contrastive finetuning on a ColPali model in which we also train the vision encoder and the projection layer. Results in Figure 4 show this leads to no significant improvements.\n\n# Do \"query augmentation\" tokens help?\n\nIn ColBERT, special tokens are concatenated to the input query to serve as soft query augmentation buffers. Training without these tokens, we observe no significant performance difference (Figure 4) in the English benchmarks. However, performance on the French tasks seems to improve (Table 5) and inference time.\n\nWith the option of adding 4 sub-image crops of 64 tokens each to the sequence, for a total of 320 tokens.\n\n# Is the Pairwise CE loss best?\n\nTraining with an in-batch negative contrastive loss, instead of the pairwise CE loss that only considers the hardest negative sample, leads to a slight performance degradation (\u22122.4%) on the aggregated benchmark.\n\n# Can the model adapt to new tasks?\n\nContrary to more complex multi-step retrieval pipelines, ColPali can be trained end-to-end, directly optimizing the downstream retrieval task which greatly facilitates fine-tuning to boost performance on specialized domains, multilingual retrieval, or specific visual elements the model struggles with. To demonstrate, we add 1552 samples representing French tables and associated queries to the training set. This represents the only French data in the training set, with all other examples being kept unchanged. We see significant NDCG@5 improvements (Figure 4) and even starker Recall@1 gains (+6.63%) on the TabFQuAD benchmark, with no performance degradation on the rest of the benchmark tasks (+0.34%).\n\n# 7 Conclusions\n\nThrough the conception of a new benchmark ViDoRe, we established the limits of both modern industrial document retrieval pipelines and off-the-shelf image-text contrastive models for visually rich document retrieval. We introduced ColPali, a novel retrieval model that leverages the latest generative Vision Language models to create highly performing multi-vector embeddings purely from visual document features. ColPali largely outperforms the best existing document retrieval methods while enabling faster corpus indexing time and maintaining low querying latencies, suggesting a very high potential for industrial document retrieval applications. We hope to encourage future work by publicly releasing the ViDoRe benchmark and all models and baselines from our study.\n\n# Future Work\n\nFurther performance gains could be obtained by exploring sub-image decomposition (Liu et al., 2023a), optimal image patch resampling strategies (Lauren\u00e7on et al., 2024), or hard-negative mining. Subsequently, our vision is to combine visual retrieval and visually grounded query answering to create RAG systems that purely function from visual features. An interesting line of research could be attempting to generate answers leveraging information stored in the indexed multi-vector patch embeddings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86fb7173-cbac-4724-94ec-75b78372f4dc": {"__data__": {"id_": "86fb7173-cbac-4724-94ec-75b78372f4dc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df552c0a-6b1d-4193-aab0-2b7a2700446d", "node_type": "4", "metadata": {}, "hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0", "class_name": "RelatedNodeInfo"}}, "text": "# Limitations\n\nFocus. In this work, we evaluate models on document retrieval tasks, covering several modalities (figures, text, tables, infographics). We however primarily focus on PDF-type documents, and evaluating systems on image retrieval with documents stemming from web page screenshots or handwritten documents might be an interesting generalization. We also focus on high-resource languages (English and French) and although we have shown the capacity of the ColPali model to generalize to languages outside of its fine-tuning set, it is unclear how the model would perform on languages that are not as represented in the model\u2019s language backbone. Finally, our setup assumes relevant documents exist, but abstention methods for Information Retrieval systems might be interesting to explore in more practical settings in which confidence estimation might be important (Gisserot-Boukhlef et al., 2024).\n\nSupport. This work relies on multi-vector retrieving derived from the ColBERT late interaction mechanism. Although some vector databases support late interaction engines13, many widely used vector retrieval frameworks do not propose native multi-vector support, and some engineering infrastructure efforts may be required to adapt them to work with ColPali (or ColBERT) models.\n\nData. In the creation of ViDoRe, we partially rely on synthetic query generation based on a commercial large language model, which may induce some amount of bias in the generated queries. To compensate for this, we have iterated on the prompting strategy and given real query examples to the models to help ground generation in realistic settings. We have further manually verified all synthetic queries through a lengthy process to validate their relevance and their quality. Our benchmark also includes many benchmark tasks with no synthetic data, and result trends observed between all tasks are correlated, further confirming the coherence of our benchmark design.\n\n# Ethical Considerations\n\nCarbon Footprint. Our work fully leverages prior pretrained models and training is not particularly compute-intensive. Furthermore, we rely on low-rank adapters to further reduce the computational resources needed, both during training and for storage. Overall, a training run represents about 40 hours of Mi250x AMD GPUs. Our experiments, in total, represent 1405 Mi250x GPU hours from highly efficient compute clusters running on low-carbon nuclear energy, representing a total of around 15kg CO2 eq.\n\nImpact. We believe our work could have a strong impact on improving industrial document retrieval systems. Our method is efficient, performs well, and the additional support towards visually rich information from documents could go a long way in unlocking knowledge sources previously difficult to index or query.\n\nResource Release. For transparency, and to foster future work, we release our comprehensive benchmark under open license and host a public leaderboard14. Our models are released under the same usage license as the base model (Gemma Research license for ColPali, Apache2.0 for ColIdefics2) and should be used as intended by the VLM license.\n\n# Acknowledgements\n\nThis work is partially supported by Illuin Technology, and by a grant from ANRT France. This work was performed using HPC resources from the CINES ADASTRA through Grant 2024-AD011015443. We extend our warm thanks to Jonathan Dong, Caio Corro, Victor Pellegrain and Ender Konukoglu for their valuable feedback on the paper.\n\n# References\n\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. 2023. Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. Publisher: arXiv Version Number: 5.\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a Visual Language Model for Few-Shot Learning. Publisher: arXiv Version Number: 2.\n\nAnthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku.\n\n14https://huggingface.co/spaces/vidore/vidore-leaderboard", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "735ebfdb-b7a1-45c6-8f1f-06e30707ee1a": {"__data__": {"id_": "735ebfdb-b7a1-45c6-8f1f-06e30707ee1a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c69e55d-dddd-4ce2-a020-e57b47de7bd3", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bc0d70f-c3e4-49b4-8e6d-0fa59a80d689", "node_type": "1", "metadata": {}, "hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha. 2021. DocFormer: End-to-End Transformer for Document Understanding. arXiv preprint. Version Number: 2.\n\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. Publisher: arXiv Version Number: 3.\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv preprint. Version Number: 3.\n\nBurton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422\u2013426. Place: New York, NY, USA Publisher: Association for Computing Machinery.\n\n\u0141ukasz Borchmann, Micha\u0142 Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina Szyndler, and Filip Grali\u0144ski. 2021. DUE: End-to-End Document Understanding Benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv preprint. Version Number: 3.\n\nXi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. 2023. PaLI-3 Vision Language Models: Smaller, Faster, Stronger. arXiv preprint. Version Number: 2.\n\nCohere. 2024. Introducing Rerank 3: A New Foundation Model for Efficient Enterprise Search & Retrieval.\n\nTimoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. 2023. Vision Transformers Need Registers. Publisher: [object Object] Version Number: 2.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Publisher: arXiv Version Number: 2.\n\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX: Exceeding YOLO Series in 2021. arXiv preprint. Version Number: 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bc0d70f-c3e4-49b4-8e6d-0fa59a80d689": {"__data__": {"id_": "1bc0d70f-c3e4-49b4-8e6d-0fa59a80d689", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c69e55d-dddd-4ce2-a020-e57b47de7bd3", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "735ebfdb-b7a1-45c6-8f1f-06e30707ee1a", "node_type": "1", "metadata": {}, "hash": "73a1d174247c6265657797b59c09c64dc8a0dbbe89ce49cf76e1b363e58e5267", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a04d290-5e86-4169-9af0-fb8f595c6ab0", "node_type": "1", "metadata": {}, "hash": "a33129b68cd64ada00b9243cecd3233f935c166bfedda87a4cb2921a33d65367", "class_name": "RelatedNodeInfo"}}, "text": "2023. Vision Transformers Need Registers. Publisher: [object Object] Version Number: 2.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Publisher: arXiv Version Number: 2.\n\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. 2021. YOLOX: Exceeding YOLO Series in 2021. arXiv preprint. Version Number: 2.\n\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv preprint. Version Number: 4.\n\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, C\u00e9line Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: A simple yet effective abstention mechanism. Preprint, arXiv:2402.12997.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Publisher: arXiv Version Number: 2.\n\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Publisher: arXiv Version Number: 3.", "mimetype": "text/plain", "start_char_idx": 2063, "end_char_idx": 4986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a04d290-5e86-4169-9af0-fb8f595c6ab0": {"__data__": {"id_": "4a04d290-5e86-4169-9af0-fb8f595c6ab0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c69e55d-dddd-4ce2-a020-e57b47de7bd3", "node_type": "4", "metadata": {}, "hash": "14ae5975b0436d45b2b28f1fa1dc287dffb696b009c69d971ff36bfc0f453377", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bc0d70f-c3e4-49b4-8e6d-0fa59a80d689", "node_type": "1", "metadata": {}, "hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "class_name": "RelatedNodeInfo"}}, "text": "Version Number: 4.\n\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, C\u00e9line Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: A simple yet effective abstention mechanism. Preprint, arXiv:2402.12997.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Publisher: arXiv Version Number: 2.\n\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. Publisher: arXiv Version Number: 3.\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix.", "mimetype": "text/plain", "start_char_idx": 4371, "end_char_idx": 5288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "297ad5ff-c67b-4105-b259-25e3e2d66900": {"__data__": {"id_": "297ad5ff-c67b-4105-b259-25e3e2d66900", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e416a557-24dd-476e-bf15-e562a4b7fd96", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e80eb6aa-28a8-4422-afb2-12f3e4ba0e92", "node_type": "1", "metadata": {}, "hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nand William El Sayed. 2023. Mistral 7B. Publisher: arXiv Version Number: 1.\n\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint. Version Number: 3.\n\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.\n\nGeewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2021. OCR-free Document Understanding Transformer. arXiv preprint. Version Number: 5.\n\nAndreas Koukounas, Georgios Mastrapas, Michael G\u00fcnther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Mart\u00ednez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, and Han Xiao. 2024. Jina CLIP: Your CLIP Model Is Also Your Text Retriever. arXiv preprint. Version Number: 1.\n\nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. 2024. What matters when building vision-language models? arXiv preprint ArXiv:2405.02246 [cs].\n\nJinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Y. Zhao. 2023. Rethinking the Role of Token Retrieval in Multi-Vector Retrieval. arXiv preprint. Version Number: 3.\n\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models. Preprint, arXiv:2403.00231.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. 2014. Microsoft COCO: Common Objects in Context. arXiv preprint. Version Number: 3.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv preprint. Version Number: 2.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. Publisher: arXiv Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e80eb6aa-28a8-4422-afb2-12f3e4ba0e92": {"__data__": {"id_": "e80eb6aa-28a8-4422-afb2-12f3e4ba0e92", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e416a557-24dd-476e-bf15-e562a4b7fd96", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "297ad5ff-c67b-4105-b259-25e3e2d66900", "node_type": "1", "metadata": {}, "hash": "f30400ea5c39a8dda2862c4f5a6ed31adb53965d1dfd780b04bd6e1abb680842", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4f56589-3301-49e5-97ed-055008cbc288", "node_type": "1", "metadata": {}, "hash": "5b6cd1ec36c79c626e603648645c27d2c8bdf7405d134fa5ee8d442654faaccb", "class_name": "RelatedNodeInfo"}}, "text": "Preprint, arXiv:2403.00231.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. 2014. Microsoft COCO: Common Objects in Context. arXiv preprint. Version Number: 3.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. arXiv preprint. Version Number: 2.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. Publisher: arXiv Version Number: 1.\n\nLucas Beyer*, Andreas Steiner*, Andr\u00e9 Susano Pinto*, Alexander Kolesnikov*, Xiao Wang*, Xiaohua Zhai*, Daniel Salz, Maxim Neumann, Ibrahim Al-abdulmohsin, Michael Tschannen, Jeremiah Harmsen, Daniel Keysers, Neil Houlsby, Xi Chen, Emanuele Bugliarello, Thomas Unterthiner, Keran Rong, Matthias Minderer, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Julian Eisenschlos, Manoj Kumar, Matko Bo\u0161njak, Matthias Bauer, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Paul Voigtlaender, Pinelopi Papalampidi, Olivier Henaff, Skanda Kopula, Xi Xiong, Radu Soricut, Model release contributors and general support, Tris Warkentin, Kat Black, Luiz Gustavo Martins, Glenn Cameron, Raj Gundluru, Manvinder Singh, Meg Risdal, Nilay Chauhan, Nate Keating, Nesh Devanathan, Elisa Bandy, Joe Fernandez, Antonia Paterson, Jenny Brennan, Tom Eccles, Pankil Botadra, Ben Bariach, Lav Rai, Minwoo Park, Dustin Luong, Daniel Vlasic, Bo Wu, Wenming Ye, Divyashree Sreepathihalli, Kiranbir Sodhia, Alek Andreev, Armand Joulin, Surya Bhupatiraju, Minh Giang, Joelle Barral, and Zoubin Ghahramani. 2024. PaliGemma.\n\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V Jawahar. 2021. InfographicVQA. arXiv preprint. Version Number: 2.\n\nMinesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2020. DocVQA: A Dataset for VQA on Document Images.\n\nNiklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and Nils Reimers. 2022. MTEB: Massive Text Embedding Benchmark. arXiv preprint. Version Number: 3.\n\nNomic. 2024. Nomic Embed Vision: Expanding The Nomic Latent Space.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. Publisher: arXiv Version Number: 1.\n\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint. Version Number: 1.\n\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109\u2013126. National Institute of Standards and Technology (NIST).\n\nKeshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint. Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 1567, "end_char_idx": 4642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4f56589-3301-49e5-97ed-055008cbc288": {"__data__": {"id_": "b4f56589-3301-49e5-97ed-055008cbc288", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e416a557-24dd-476e-bf15-e562a4b7fd96", "node_type": "4", "metadata": {}, "hash": "1613006ce89686e0f27855fb23d26c09bde84558aedb7566f6ce143660abd1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e80eb6aa-28a8-4422-afb2-12f3e4ba0e92", "node_type": "1", "metadata": {}, "hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "class_name": "RelatedNodeInfo"}}, "text": "2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv preprint. Version Number: 1.\n\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109\u2013126. National Institute of Standards and Technology (NIST).\n\nKeshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: An Efficient Engine for Late Interaction Retrieval. arXiv preprint. Version Number: 1.\n\nR. Smith. 2007. An Overview of the Tesseract OCR Engine. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2, pages 629\u2013633, Curitiba, Parana, Brazil. IEEE. ISSN: 1520-5363.", "mimetype": "text/plain", "start_char_idx": 4019, "end_char_idx": 4855, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2685f31f-3fe8-454c-a288-5c7af5a46e0a": {"__data__": {"id_": "2685f31f-3fe8-454c-a288-5c7af5a46e0a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a7e2428-b6bd-4887-ae8a-60bb48d803ff", "node_type": "4", "metadata": {}, "hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "853f116c-4b54-41f6-b1a7-268131d96c17", "node_type": "1", "metadata": {}, "hash": "275baea2efd06dc7d04371ac8ef8f549db38d8eac1441e281d806a98e93a9ea8", "class_name": "RelatedNodeInfo"}}, "text": "# References\n\nKaren Sparck Jones. 1972. A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL. Journal of Documentation, 28(1):11\u201321.\n\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. 2022. Unifying Vision, Text, and Layout for Universal Document Processing. arXiv preprint. Version Number: 3.\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. arXiv preprint. Version Number: 4.\n\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset. arXiv preprint. Version Number: 2.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint. Version Number: 2.\n\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. arXiv preprint. ArXiv:2002.10957 [cs].\n\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. FILIP: Fine-grained Interactive Language-Image Pre-Training. arXiv preprint. Version Number: 1.\n\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. arXiv preprint. Version Number: 3.\n\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid Loss for Language Image Pre-Training. Publisher: [object Object] Version Number: 4.\n\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty. 2023. Retrieving Multimodal Information for Augmented Generation: A Survey. arXiv preprint. Version Number: 3.\n\nFengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. 2022. Towards Complex Document Understanding By Discrete Reasoning. Publisher: arXiv Version Number: 3.\n\n# A Benchmark Datasets\n\n# A.1 Academic Datasets\n\nDocVQA (Mathew et al., 2020) includes collected images from the UCSF Industry Documents Library. Questions and answers were manually annotated.\n\nInfoVQA (Mathew et al., 2021) includes infographics collected from the Internet using the search query \u201cinfographics\u201d. Questions and answers were manually annotated.\n\nTAT-DQA (Zhu et al., 2022) is a large-scale Document VQA dataset that was constructed from publicly available real-world financial reports. It focuses on rich tabular and textual content requiring numerical reasoning. Questions and answers were manually annotated by human experts in finance.\n\narXivQA (Li et al., 2024) is a VQA dataset based on figures extracted from arXiv publications. The questions were generated synthetically using GPT-4 Vision.\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "853f116c-4b54-41f6-b1a7-268131d96c17": {"__data__": {"id_": "853f116c-4b54-41f6-b1a7-268131d96c17", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a7e2428-b6bd-4887-ae8a-60bb48d803ff", "node_type": "4", "metadata": {}, "hash": "fc61456a9c6e431f31a11a5efd63b04abfb50ca444eb1afdc28a92e597bb2e8b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2685f31f-3fe8-454c-a288-5c7af5a46e0a", "node_type": "1", "metadata": {}, "hash": "f5cd5448c9452cbe6dbac1ac03e442b08b61542ba0c8aa8aaaffc80752815b7e", "class_name": "RelatedNodeInfo"}}, "text": "Questions and answers were manually annotated.\n\nInfoVQA (Mathew et al., 2021) includes infographics collected from the Internet using the search query \u201cinfographics\u201d. Questions and answers were manually annotated.\n\nTAT-DQA (Zhu et al., 2022) is a large-scale Document VQA dataset that was constructed from publicly available real-world financial reports. It focuses on rich tabular and textual content requiring numerical reasoning. Questions and answers were manually annotated by human experts in finance.\n\narXivQA (Li et al., 2024) is a VQA dataset based on figures extracted from arXiv publications. The questions were generated synthetically using GPT-4 Vision.\n\nTabFQuAD (Table French Question Answering Dataset) is designed to evaluate TableQA models in realistic industry settings. We create additional queries to augment the existing human-annotated ones using the same method described in subsection A.2.\n\n# A.2 Practical Datasets\n\nMethodology. Creating a relevant retrieval dataset close to real use cases is a major challenge as the dataset needs to be both sufficiently large for effective fine-tuning and sufficiently diverse to cover a broad range of modalities (full text, tables, charts, ...), domains (industry, healthcare, ...), and query-document interactions (extractive questions, open-ended questions, ...). Our approach to building this dataset involves several steps: (1) we use a web crawler to collect publicly available documents on various themes and sources, (2) we convert these PDFs into a series of images, one per page, and (3) we generate queries related to each image using a VLM.\n\nWeb-Crawler. We implemented a web crawler to efficiently collect large volumes of documents related to a given topic. The crawler is seeded with a user-defined query (e.g. \"artificial intelligence\") and then uses GPT-3.5 Turbo to brainstorm related topics and subtopics. This query augmentation strategy aims at both broadening and deepening the search. GPT-3.5 Turbo is further used to generate diverse search queries from each subtopic.", "mimetype": "text/plain", "start_char_idx": 2614, "end_char_idx": 4670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da4b72a2-878f-4d0c-80d7-7687880e93ff": {"__data__": {"id_": "da4b72a2-878f-4d0c-80d7-7687880e93ff", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac76fdec-72fb-4c2e-b550-fd6e2ee7e59d", "node_type": "4", "metadata": {}, "hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb", "class_name": "RelatedNodeInfo"}}, "text": "# Query Generation and Document Collection\n\nQuery set is then consumed by a pool of parallel workers whose job is to fetch the associated most relevant documents. We use SerpAPI along with a filetype filter (PDF documents only) to programmatically scrape Google Search rankings. Each file is hashed and stored in a Bloom filter (Bloom, 1970) shared among workers to avoid duplicate documents in the final corpus. Unique scraped files are downloaded, and inserted into a SQLite database along with additional metadata.\n\n# Datamix\n\nUsing the web crawler, we collected approximately 1,000 documents for each of the following four seeds: \"energy\", \"government reports\", \"healthcare industry\", and \"artificial intelligence\". These seeds were meticulously hand-picked to align with real-use cases for retrieval models and visually rich pages. We also removed all documents containing any private information. At this stage, we randomly selected 900 files for the training set and 100 files for the test set, ensuring that data leakage into the test set was avoided during subsequent processing steps.\n\n# Query Generation\n\nTo increase the efficiency of our query generation scheme and to limit API calls, we generate at most 3 questions per image. From all the documents collected, we randomly sample 10,000 images per theme and call Claude-3 Sonnet with the following prompt:\n\nRemember that the question is asked by a user to get some information from a large documentary corpus that contains multimodal data. Generate a question that could be asked by a user without knowing the existence and the content of the corpus. Generate as well the answer to the question, which should be found in the page. And the format of the answer should be a list of words answering the question. Generate at most THREE pairs of questions and answers per page in a dictionary with the following format, answer ONLY this dictionary NOTHING ELSE:\n\n{\n\"questions\": [\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n}\n]\n}\n\nwhere XXXXXX is the question and ['YYYYYY'] is the corresponding list of answers that could be as long as needed.\n\nNote: If there are no questions to ask about the page, return an empty list. Focus on making relevant questions concerning the page.\n\n# Human Validation\n\nWe manually validate every single synthetically created query in ViDoRe to ensure quality, query relevance, and consistency with the benchmark objective of evaluating retrieval in practical industrial settings. During this step, we randomly assign document-pair queries to 4 volunteers.\n\n15 https://serpapi.com/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0af7a4a-4e38-4f22-80eb-fa6ade1d194b": {"__data__": {"id_": "c0af7a4a-4e38-4f22-80eb-fa6ade1d194b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a40403a-cf57-4f3d-8f5d-d0982706f8c7", "node_type": "4", "metadata": {}, "hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073", "class_name": "RelatedNodeInfo"}}, "text": "# B     Implementation details\n\n# B.1     Codebase\n\nThe codebase is written in PyTorch16 and leverages HuggingFace tooling for model implementations and trainers17.\n\n# B.2     Pairwise CE loss\n\nOur in-batch contrastive loss L is defined as the softmaxed cross-entropy of the positive scores\n\nsk+ = LI (dk, qk) w.r.t. to the maximal negative scores sk\u2212 = maxl,l\u0338 =k LI (qk, pl).\n\nFor numerical stability, we reformulate the loss with the softplus function, leading to:\n\nL = bk=11 Xsoftplus sk \u2212 skb\n\n# B.3     Hyperparameters\n\nHyperparameters are tuned on a validation split composed of 2% of the training dataset. We find bi-encoder methods to be more sensible to learning rate variations than late interaction-based models and achieve the best performance for all models with a learning rate of 5e \u2212 5. We experiment with LoRA rank and \u03b1 values and do not notice particular improvements past r = \u03b1 = 32. Per-device batch sizes are kept small due to long sequence lengths that complicate scaling past b = 4. Simulating larger batch sizes for in-batch negative sampling should enable even better results. We find the best results with global batch size b = 32 for 1 epoch on our training set.\n\n# B.4     Embedding size\n\nMinimizing storage footprint can be essential to industrial retrieval systems if databases contain millions of documents. With this criterion in view, we have compared the embedding sizes of the models in our study. As shown in Table 3, ColPali\u2019s embedding size is an order of magnitude larger than BM25 and two orders of magnitude larger than BGE-M3. However, this study is limited to the naive method of storing ColPali\u2019s multi-vector embeddings. In practical scenarios, using cluster centroids can reduce the size of ColPali multi-vector embeddings by up to an order of magnitude (Santhanam et al., 2022) and make it a competitive retrieval system.\n\n# B.5     Latency computations\n\nAll latency computations are done on a NVIDIA L4 GPU. Queries are encoded independently (batch size of 1) to simulate online querying, and pages are encoded with a batch size of 4 for PaliGemma derived models, and 8 for BGE-M3. Reported times include image and text processing time before the model forward pass, as well as query-to-index matching times. We note an interesting feature of ColPali is that all documents have the same sequence length, leading to prior knowledge of runtime and memory consumptions. Query latency experiments are averaged over 1000 queries, and indexing times are measured for a 100 page document. Per page time is obtained by diving total time by 100, corresponding to inverse page throughput.\n\n# B.6     Captioning\n\nExamples of captions generated for visually rich document chunks with Claude-3 Sonnet are shown in Figure 6 and Figure 5. The prompt used for generating the description is the following:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3f9715c-96e9-4cc6-8f3f-422b10c1d0be": {"__data__": {"id_": "e3f9715c-96e9-4cc6-8f3f-422b10c1d0be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67b0ea24-89ba-4dce-899f-bb1a0545acca", "node_type": "4", "metadata": {}, "hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160", "class_name": "RelatedNodeInfo"}}, "text": "# Figure 5: Example from the \"Energy\" test set.\n\nCaption: The image depicts the hourly energy generation profile, illustrating the contributions of various energy sources over 24 hours. The data is presented as a stacked bar chart, with the x-axis representing the hours of the day from 1 to 2, and the y-axis showing the average hourly generation in MW. The bars are segmented into different colors, each representing a distinct energy source: nuclear, bio, geothermal, solar, wind, hydro, natural gas, and other imports. The chart provides insights into the temporal variations in energy generation across different sources, highlighting the interplay between baseload and intermittent sources throughout the day.\n\n# Figure 6: Example from the \"Government Reports\" test set.\n\nCaption: The image shows a table titled \"System of Record\" which outlines the different types of documents or records maintained across various systems or departments within an organization related to project management and construction. The rows list documents like project plans, budgets, schedules, contracts, purchase orders, invoices, change requests, bid submissions, drawings, manuals, meeting minutes, and reports. The columns indicate the system or department responsible for maintaining each record, such as County Servers, Project View, OnBase, CGI Advantage Financial System, and Purchasing Department. The table uses \"W\" and \"T\" markers to denote which system or department serves as the primary source (writer) or storage location (trailer) for each type of document.\n\n# More similarity maps\n\nIn Figure 7, ColPali assigns a high similarity to all patches with the word \"Kazakhstan\" when given the token <_Kazakhstan>. Moreover, our model seems to exhibit world knowledge capabilities as the patch around the word \"Kashagan\" - an offshore oil field in Kazakhstan - also shows a high similarity score. On the other hand, in Figure 8, we observe that ColPali is also capable of complex image understanding. Not only are the patches containing the word \"formulations\" highly similar to the query token _formula, but so is the upper-left molecule structure.\n\nIt is also interesting to highlight that both similarity maps showcase a few white patches with high similarity scores. This behavior might first seem surprising as the white patches should not carry a meaningful signal from the original images. We believe the vectors associated with these patches share a similar role with the ViT registers (Darcet et al., 2023), i.e. these patches were repurposed for internal computations and stored the global information from the whole image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ea73fd7-98d1-420e-9002-36c8ca5d41b5": {"__data__": {"id_": "7ea73fd7-98d1-420e-9002-36c8ca5d41b5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67af852a-0306-4028-b539-2d92b33a67d4", "node_type": "4", "metadata": {}, "hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28", "class_name": "RelatedNodeInfo"}}, "text": "# Historique de production\n\n# Production totale des hydrocarbures liquides\n\n# Kazakhstan (1965-2019)\n\nQuery: \"Quelle partie de la production p\u00e9troli\u00e8re du Kazakhstan provient de champs en mer ?\"\n\n# Ferroelectrics\n\n# Lead Zirconium Titanate\n\nPb(Zr,Ti)O3\n\n1952 Shirane; Pb(Zr,Ti)O3 solid solutions\n\n1955 Jalte coor Berlincourt; Gerson: Complete Study PZT formulations\n\nCurie temperature 170-360\n\nQuery: What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2545564b-758d-4ed4-a62d-fefb63b7ee30": {"__data__": {"id_": "2545564b-758d-4ed4-a62d-fefb63b7ee30", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8305e55-bbdc-4c79-bd10-fa997110aca4", "node_type": "4", "metadata": {}, "hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96201a9a-d237-41eb-8749-c74a4443ee6e", "node_type": "1", "metadata": {}, "hash": "62816e0b531a77b544cbbfd001f5c8c7767e6aa087dc8bc5a6d4f92170b12b5e", "class_name": "RelatedNodeInfo"}}, "text": "# D. Additional results\n\n# D.1 Other Metrics\n\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy|Gov.|Health.|Avg.| |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Unstructured Text only|BM25|-|26.6|-|-|34.6|45.0|86.0|70.0|68.0|74.0|-|\n| |BGE-M3|-|22.8\u21933.8|-|-|26.1\u21938.5|51.0\u21916.0|81.0\u21935.0|72.0\u21912.0|67.0\u21931.0|77.0\u21913.0|-|\n|Unstructured + OCR|BM25|26.7|28.9|54.0|30.4|50.0|52.0|86.0|77.0|74.0|80.0|55.9|\n| |BGE-M3|28.1\u21911.4|22.9\u21936.0|53.8\u21930.2|55.7\u219125.3|38.6\u219311.4|56.0\u21914.0|82.0\u21934.0|79.0\u21912.0|76.0\u21912.0|83.0\u21913.0|57.5\u21911.6|\n|Unstructured + Captioning|BM25|35.5|30.2|61.5|24.3|49.0|47.0|79.0|76.0|75.0|81.0|55.9|\n| |BGE-M3|29.3\u21936.2|26.0\u21934.2|62.1\u21910.6|58.6\u219134.3|30.6\u219318.4|55.0\u21918.0|80.0\u21911.0|78.0\u21912.0|69.0\u21936.0|83.0\u21912.0|57.2\u21911.3|\n|Contrastive VLMs|Jina-CLIP|19.4|7.3|26.7|12.5|1.6|2.0|11.0|13.0|15.0|17.0|12.6|\n| |Nomic-vision|10.4|6.7|22.1|9.6|1.6|0.0|9.0|9.0|7.0|13.0|8.8|\n| |SigLIP (Vanilla)|34.2|21.3|51.8|46.1|17.9|13.0|50.0|51.0|47.0|65.0|39.7|\n|Ours|(Copied) SigLIP (Vanilla)|34.2|21.3|51.8|46.1|17.9|13.0|50.0|51.0|47.0|65.0|39.7|\n| |BiSigLIP (+fine-tuning)|49.2\u219115.0|23.8\u21912.5|59.0\u21917.2|52.1\u21916.0|20.7\u21912.8|16.0\u21913.0|62.0\u219112.0|61.0\u219110.0|55.0\u21918.0|72.0\u21917.0|47.1\u21917.4|\n| |BiPali (+LLM)|46.4\u2193-2.8|20.0\u2193-3.8|54.6\u2193-4.4|63.2\u219111.1|20.4\u2193-0.4|34.0\u219118.0|59.0\u2193-3.0|45.0\u2193-16.0|57.0\u21912.0|56.0\u2193-16.0|45.6\u2193-1.5|\n| |ColPali (+Late Inter.)|72.4\u219126.0|45.6\u219125.6|74.6\u219120.0|75.4\u219112.1|53.1\u219132.7|55.0\u219121.0|93.0\u219134.0|85.0\u219140.0|85.0\u219128.0|88.0\u219132.0|72.7\u219127.1|\n\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96201a9a-d237-41eb-8749-c74a4443ee6e": {"__data__": {"id_": "96201a9a-d237-41eb-8749-c74a4443ee6e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8305e55-bbdc-4c79-bd10-fa997110aca4", "node_type": "4", "metadata": {}, "hash": "5a84302400432bdaaec669bf49e2b1d79db3de30178607c50d44c240d6135777", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2545564b-758d-4ed4-a62d-fefb63b7ee30", "node_type": "1", "metadata": {}, "hash": "00119033c6c21ba634c0f31ec079ed765db6f54b4f9569e4c2947c753f6644b0", "class_name": "RelatedNodeInfo"}}, "text": ")|72.4\u219126.0|45.6\u219125.6|74.6\u219120.0|75.4\u219112.1|53.1\u219132.7|55.0\u219121.0|93.0\u219134.0|85.0\u219140.0|85.0\u219128.0|88.0\u219132.0|72.7\u219127.1|\n\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe. Results are presented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\n\n# D.2 Model Variants\n\n| |ArxivQ|DocQ|InfoQ|TabF|TATQ|Shift|AI|Energy|Gov.|Health.|Avg.|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|ColSigLIP (PaliGemma)|3.1|3.0|5.1|6.2|2.5|1.0|3.4|3.4|2.3|2.2|3.2|\n|BiSigLIP (PaliGemma)|18.5|14.6|33.4|39.5|16.1|5.2|27.6|32.6|36.6|35.7|26.0|\n|ColSigLIP (Original)|2.6|2.2|2.3|5.7|1.8|1.0|2.6|4.1|1.4|1.5|2.5|\n|ColPali (No Mem. Tokens)|80.4|53.2|82.4|77.4|65.7|63.4|97.0|89.9|93.6|92.4|79.6|\n|ColPali (Best)|79.1|54.4|81.8|83.9|65.8|73.2|96.2|91.0|92.7|94.4|81.3|\n\nTable 5: Evaluation of some \"negative results\" and ablations on ViDoRe; ColPali for reference. Results are presented using NDCG@5 metrics. Text-only metrics are not computed for benchmarks with only visual elements.", "mimetype": "text/plain", "start_char_idx": 1308, "end_char_idx": 2354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a2ad900-a01d-4a43-bb31-4d8a4ed2ac8c": {"__data__": {"id_": "4a2ad900-a01d-4a43-bb31-4d8a4ed2ac8c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96225294-ca1b-498d-b55b-aa07443ea6ba", "node_type": "4", "metadata": {}, "hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2", "class_name": "RelatedNodeInfo"}}, "text": "# ViDoRe examples\n\n# Energy\n\n|Query|Response|\n|---|---|\n|What types of accounts or products allow investors to defer paying taxes?| |\n|What is the projected peak electricity demand in California for the year 2030?| |\n|What is the estimated total savings for a PV system in Durham under the net metering (flat rate) billing option over the system\u2019s useful life of 25 years?|Projected 2030 electricity capacities|\n\n# Artificial Intelligence\n\n|Query|Response|\n|---|---|\n|What are some common outcome areas targeted by TAII for different age groups?| |\n|What did the robot monitor to determine when to activate or deactivate the blower motor and blinker?| |\n|What is the key approach used in the PDP architecture?| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee2674ba-3a3e-4751-87e0-b8970a287dd4": {"__data__": {"id_": "ee2674ba-3a3e-4751-87e0-b8970a287dd4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "543a6e39-ed15-4284-881c-370c3c5f837c", "node_type": "4", "metadata": {}, "hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451", "class_name": "RelatedNodeInfo"}}, "text": "# Healthcare Industry\n\n# Query: What is the chemical formula for the ferroelectric material Lead Zirconium Titanate (PZT)?\n\nFerroelectrics\n\nLojd Circon Um\n\nPblzo Tlal\n\n1952 Shlrano Sufuri\n\n# Query: What government entities are involved in public financing for health care in the US?\n\nUCLA Health System Financing\n\n# Government Reports\n\n# Query: What does the AVPU scale stand for in assessing the level of consciousness of a seriously ill child?\n\n# Query: What are some mandates for the EPA under the Pollution Prevention Act?\n\n# Query: What is the strategy of KPMG Hazem Hassan?\n\n# Query: What is the trust signal score for the consumer industry best-in-class archetype?\n\n# Who we are?\n\nWE\n\nM dats\n\nGMOnsecy\n\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00f7ca7e-fe65-425e-8a64-eb1327dde448": {"__data__": {"id_": "00f7ca7e-fe65-425e-8a64-eb1327dde448", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63bfc2a6-ff00-4c9a-b939-d9038db64301", "node_type": "4", "metadata": {}, "hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc", "class_name": "RelatedNodeInfo"}}, "text": "# Shift\n\n# Query: Selon le graphique, quelle est la capacit\u00e9 d\u2019import et la consommation r\u00e9elle de carburants SAF (biocarburants durables pour l\u2019aviation) pr\u00e9vues en 2050 ?\n\n# Query: Quelle partie de la production p\u00e9troli\u00e8re du Kazakhstan provient de champs en mer ?\n\n# Query: Quels sont les pays ayant la plus grande part des d\u00e9couvertes cumul\u00e9es de p\u00e9trole brut en 2020 (en milliers de barils, hors d\u00e9couvertes cumul\u00e9es) ?\n\n|Lucmolo|Fetall|\n|---|---|\n|20| |", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"065b847f-d5c7-4b5e-946a-e19c0c678276": {"doc_hash": "bfcc1f49902460d080bfad58b81e30c31320269e8427000e2814958d34de5b35", "ref_doc_id": "647adfc9-c2e6-4a6b-809f-5c987bee7120"}, "29f51ced-9aed-4dfe-93b6-ca58679f7f64": {"doc_hash": "26e1fa254c7432f1670a6b5b6e565e26517cedc2d6ecbf67e66427cdc253773f", "ref_doc_id": "12b41c0e-ed75-4138-8753-69a3b62e6e92"}, "5f4009d5-cc2b-452c-934f-c7569f1aed73": {"doc_hash": "7fc356da9e40dde4c738955742399c3207efc2f5e1ba5de8c2bde049dbe42d40", "ref_doc_id": "12b41c0e-ed75-4138-8753-69a3b62e6e92"}, "c24eb29c-41d9-40f3-b229-83a1601d4d41": {"doc_hash": "b26b621570eeff5449636676bc4a0c54f4cf49039b7cdc081d215ce7911eba55", "ref_doc_id": "d2e35d18-03bf-473a-b3ce-b15c038338a9"}, "d456ee2b-d445-49c6-8858-238652abc5d5": {"doc_hash": "1d1dab5124af2d8f9c2ad37e26e34ec0d7eee86f16b0dae150e57d77af523141", "ref_doc_id": "eefce1db-0425-4101-89d5-462df76903d5"}, "baea2fc5-c1f5-41f2-b6c4-1bb7f54e93c5": {"doc_hash": "b0482959b541d16831c426844d238166f2abdfeb36ae840d44d93b61f64f805d", "ref_doc_id": "eefce1db-0425-4101-89d5-462df76903d5"}, "5df8a21a-7f9b-4377-9bfb-96bf9b162416": {"doc_hash": "1b4b73d5e8c0549d82cb9750ace716f19e46184a6151184e8e630241fd274adb", "ref_doc_id": "1271b560-90d9-4728-8aca-459ac3f3a978"}, "0d35fd88-d6c8-4280-a0c5-e51815e859de": {"doc_hash": "c4da3b2d46edec6f7b9a663676c07303967672de1bf0f8a3d631c27821b94f65", "ref_doc_id": "1271b560-90d9-4728-8aca-459ac3f3a978"}, "d33e49d5-09d7-4aa2-8475-9df4452c7a71": {"doc_hash": "66d3c5eb8a06dd6fdfff50b2e4b3e30c44345f33e74426324f123183dd4739be", "ref_doc_id": "8e212b53-3c61-4de3-8729-495b3c2d4102"}, "edab247d-1dcf-4124-b515-daf5c16a9c58": {"doc_hash": "82fc455cd6a225486590d33c0b0ba119b9d79726be80b0e37f6b2baed3fe0027", "ref_doc_id": "5040f2d4-717e-4a69-a249-147390e2c170"}, "116c6535-8578-4b48-9088-ab73d523d52e": {"doc_hash": "ee029efb875e20c861d7e708f9ef47d1027e66dae839f2a4e630700d0a0cc271", "ref_doc_id": "5040f2d4-717e-4a69-a249-147390e2c170"}, "2d4c0faa-ece9-488f-ba42-a88e23a29076": {"doc_hash": "1c38e71427d5c92b8e1340d9a464cdf6e3b4077c4793ee33d4326b05f0800913", "ref_doc_id": "08a2bbe8-5a1c-4333-99d6-6b2c07ceab91"}, "c99364df-7b1f-4e9f-bd5a-7e117c434a74": {"doc_hash": "672a7cd4f4c06623a4b3425c7c97f11d550af807b4053fb364bae4e5ae6491b5", "ref_doc_id": "8f69f847-5fba-4a5c-b825-83a53aa31d7d"}, "01586bee-35d3-4c3e-a451-1f73944b245c": {"doc_hash": "f1b2ab53c37ff0f60710bec823833b2d11f9067488cff0f45fb605cf387d9528", "ref_doc_id": "8e240ef1-84e1-4611-943e-0fab87df0e42"}, "ff6e763a-f82e-4d33-8496-0596bd304fc5": {"doc_hash": "126a9db7b5740534c746ab8bf491d761f79ed61e69d2bbb5b28340187375d9d1", "ref_doc_id": "8e240ef1-84e1-4611-943e-0fab87df0e42"}, "852b9889-56c5-48ea-80fc-f40c85da03ea": {"doc_hash": "c58cc5755f7361081161d50b1a0ffc1f496718c5ca370affd2d5b88faa7ca33d", "ref_doc_id": "05888620-a24b-45ce-9a64-b4328b9fb53b"}, "607beb3c-c513-4d3a-9e3f-ab7f7536155c": {"doc_hash": "44ca02a9e4b0db24f891bdd31919653ef3f77118eaf411f65d21f2c119d5b2b8", "ref_doc_id": "05888620-a24b-45ce-9a64-b4328b9fb53b"}, "98449be0-2d90-4f31-a2e0-b4c315b87f4a": {"doc_hash": "27dc6e9a31c9d63494b5a69da08b106f1305e0466df0310057bfdeac18e2421a", "ref_doc_id": "165b8571-561f-49fa-a5d3-7f1bda24f647"}, "05fb2e3e-a7df-4d2d-85e0-6d8c8cbc4fbf": {"doc_hash": "85b331091d63ea0cdc50cfbd762ecf92295ff7231b65fd69d8e0a4cbc8cd56db", "ref_doc_id": "165b8571-561f-49fa-a5d3-7f1bda24f647"}, "2b467249-165b-413e-89c7-7517258708f8": {"doc_hash": "f87f2132fad07f59cb4affb97d2f6f0bd6e9889117ddc5bff697d7bfa4330e2f", "ref_doc_id": "a71ee94d-9a42-4a52-a668-e4654d1fc481"}, "7d1346be-639b-4e1d-9840-36b11a9b09d1": {"doc_hash": "2063ede2582d7b5fe6c9de2403b669244e5d3569b9bbc002c0421b4e8a41c7aa", "ref_doc_id": "a71ee94d-9a42-4a52-a668-e4654d1fc481"}, "a283d5aa-48c6-4da9-ba86-4605d13b9484": {"doc_hash": "d58032e4ee8e586e3079e22a7653c18a28ea239a22435290e3493c02d9d37241", "ref_doc_id": "97149929-3cc1-409a-a111-5a20b6a9ed80"}, "04477231-c1f4-46f8-ad28-380cd775e0e4": {"doc_hash": "b6bc9d9c1491e012cc554e4f7cfddc30836928b725ef77f49ff53db2000743bf", "ref_doc_id": "97149929-3cc1-409a-a111-5a20b6a9ed80"}, "f36b5f6f-020e-4270-a036-247fc1022486": {"doc_hash": "c5bf5f7cbf747f3f800b393ef22d6c1f0e10b40600363c10ccad128b4a6944d2", "ref_doc_id": "8dce5a1e-d154-4670-8609-36a570e8224b"}, "e0a07b23-c30f-4178-a2a5-06121ec85127": {"doc_hash": "08ed79cc1c4752fcc40708fefc847facc8e267b235ade0ab6439f955f443ffd3", "ref_doc_id": "8dce5a1e-d154-4670-8609-36a570e8224b"}, "961de226-256a-46a1-ad35-49d680567653": {"doc_hash": "7de4fa0f8609ccd0111b44dbc377b073e1bc5fe5e802e341e798794873eb63cc", "ref_doc_id": "836d23df-e4a4-48da-8924-544b586ed923"}, "586dc347-a24a-4b30-a587-f026fa90caba": {"doc_hash": "1be436630f6cb2c70dbbf62a82d8ca60bef9207c0178b1d6d94432f7f333a87b", "ref_doc_id": "836d23df-e4a4-48da-8924-544b586ed923"}, "1bdd3c28-6823-450c-ad2e-27a9a7e667d0": {"doc_hash": "b880dffe7d51cfb75890ee43480202e9537ed10de59c3f339b11d050f778947f", "ref_doc_id": "836d23df-e4a4-48da-8924-544b586ed923"}, "83d10d2e-a96b-4675-be01-9986d65617e5": {"doc_hash": "fe9d67ecb715a2253218ff3fc302532903b36f90563f1b0111d6335dcb33218e", "ref_doc_id": "704e614a-8cd1-41ab-827c-3a91cc8062d7"}, "7bed53cd-e14c-4188-9963-816778ce12e2": {"doc_hash": "a635630396efb22b2c52c3762f609b28883978b3e52d720c1dffed7a3e6a8c83", "ref_doc_id": "704e614a-8cd1-41ab-827c-3a91cc8062d7"}, "a5970011-ba6c-4b7d-9bd6-c742c47c6afe": {"doc_hash": "84430e8ca9c5eb206ae5ca18ba6aaddcf3c32910f906a028f5d9bd5e67a852bc", "ref_doc_id": "704e614a-8cd1-41ab-827c-3a91cc8062d7"}, "8f07e17b-19cc-4939-b664-1f871b0592cb": {"doc_hash": "03358c7197d100c55c6a208c8fa7ac4cf3ef5e181fb9dddcb89cd828023bdedf", "ref_doc_id": "704e614a-8cd1-41ab-827c-3a91cc8062d7"}, "ca5cc118-1846-4ab1-87d4-32afdc29c35e": {"doc_hash": "5d16368df214b5fd9f2af91d1e571cb086710ec9d2e3e70ba15769f0bba85d4c", "ref_doc_id": "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf"}, "a50ba741-b326-41e1-940a-2ec88539b1ea": {"doc_hash": "32003f6816b0332ac11db23ced60a7f2512351249a771367c76602b1b735a91c", "ref_doc_id": "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf"}, "4ca2c115-01e9-4c06-8074-61c72c562ba0": {"doc_hash": "30403ea75e12d401eedc9ffc86fa6c78b7f9996ae49838dcaccc79f72d313bba", "ref_doc_id": "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf"}, "09446021-cdbf-42a5-9946-3cfb4ba01b18": {"doc_hash": "3360f6122cbecb9382621beff68f3b03e69b7296e7db0fcf3aceb2edd2ee4ea6", "ref_doc_id": "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf"}, "2299c14e-0913-4401-8f0d-f3340bb219ce": {"doc_hash": "a899597338be4c8bf6daab246a79072b3a35397bdab2d4eacdd371d5156a710a", "ref_doc_id": "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f"}, "bc538e16-bfe1-4776-badb-f6ea1bb120d6": {"doc_hash": "0d0c4ad1019bd6531002ed16b1ac5e3fb06fd08dee32faa3e41f7a4a36608729", "ref_doc_id": "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f"}, "e8ae56e8-f265-4804-a3de-1cd660e4d613": {"doc_hash": "ba1bb88693546facd4d3fe17c622c29f6d6e61ab2a35b9e59c4de5518b2a8d21", "ref_doc_id": "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f"}, "4ccbcb06-5b9f-4e36-9c14-64db839c5284": {"doc_hash": "8a9a2dcd78139919080a709e56ba7cff536f47fdf1315219d950f5c3c5b0c828", "ref_doc_id": "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f"}, "19fa79e3-812a-4960-8c8c-657318a02e9b": {"doc_hash": "ce230988cd5a4e4631f05280b0486912926a5a9f6319cd6e50e61a1a7946858e", "ref_doc_id": "93a9d023-c5bc-49c5-adda-57b508974fb6"}, "ac978d6a-561f-440e-8ab4-87c053b5ae84": {"doc_hash": "41afc01a5db430e6194c244627ae0bf97ca7309273b389eabdc08a0df5365cd3", "ref_doc_id": "93a9d023-c5bc-49c5-adda-57b508974fb6"}, "b03f6dd5-1a57-4596-abd2-5dde0ff157b3": {"doc_hash": "81620294339a3d98091ab73662f68eda71b3c4098577a895c772f37624f3b0b1", "ref_doc_id": "93a9d023-c5bc-49c5-adda-57b508974fb6"}, "ec8577e8-581f-413b-9861-fd8595c05118": {"doc_hash": "0867c77b35eeb1450cb63a129da73e5e971535d9a32cf3922811b9e1c130905b", "ref_doc_id": "27bfdc15-3f59-4992-8659-4b532ba28f77"}, "39cc66d7-de5b-4722-9063-3359169b9c5d": {"doc_hash": "bc5f7514f2b00f508731dec7d32efeeb0af0fee031e3b0196fcae238a676937e", "ref_doc_id": "3888170d-5508-4326-85f4-e9f7a8471b80"}, "b15be2e4-6383-4a0f-99bf-b4306fa700da": {"doc_hash": "0f6868dcdca25630ae0aa6ce9340e0730e4a4e12ec47273cc4c8b04f2d88cdaa", "ref_doc_id": "d40fbb26-d7a8-4d3e-a2d0-3e472a150254"}, "13bd854f-0455-49ad-bac9-241c7e7d128b": {"doc_hash": "2bc924de312e4431c0d51e17a97656a382e9f2cc7bf1b9e3c2b7ec95b295ffc5", "ref_doc_id": "1117bc87-6ef8-452e-9754-9671a2ebb9ed"}, "47b05167-cf4e-4077-b313-7558c7a7c684": {"doc_hash": "53b2b2a6bc67e013b6a0dd1b7b43699b39f8bb737d785f93115a34db580d781d", "ref_doc_id": "cb0da17b-71fb-4820-8b80-188bccf29b5e"}, "7721da31-35e7-42ac-8614-fb1787dd6512": {"doc_hash": "2b9938b9900280880603f0a03c59d3b78ad5d7da3e60e932a41142448355d7ed", "ref_doc_id": "0435041d-c9ef-4eb1-9b39-a907d02384fc"}, "a01e21aa-0562-46d6-b271-efd856f04e28": {"doc_hash": "ba7103e4ea89e7935bf70fc6dc645ea175d4aa8aa55da2871f7f1e431a2285ea", "ref_doc_id": "e21f8e32-57bd-4673-bbf1-868381c1b8fe"}, "7cf826e0-c43c-4361-9807-04c1451cc7dc": {"doc_hash": "bff813d3ff236542431706732e5a6567d603b210e0b0006156f1f0a205d5c26a", "ref_doc_id": "20899be0-7bb6-432e-aca4-523e402f5c06"}, "5e215ca7-35ce-4ce7-b961-21430d8669e0": {"doc_hash": "2244d8f11f76fe87664863a0a49d695b72c53e3dd513cc7ade779cc01ab13249", "ref_doc_id": "d257f9ff-57fa-4031-b2f2-0656b3345edd"}, "c198dd33-1aa4-438b-b73f-2137bfdd6bd1": {"doc_hash": "c1cb0a2998f39629240082e28a0f2dfe58c9a7b5ccf18e8e1956e094834c5a57", "ref_doc_id": "93c933e3-1f25-45ef-9d39-ad622f69e126"}, "e08fec4b-a71b-4982-8e5b-013525c5e38f": {"doc_hash": "a1baa116df2c1a2e8d2afa44d98d19b9d9b505fd083eec1b78e775ceb3c1dd5c", "ref_doc_id": "0d3592d4-6aa8-4700-8c69-77087c11060c"}, "f1c6c617-50a5-43be-afee-8fbbf0172971": {"doc_hash": "91c37095f8789fcbe637a916e774ba7004016c4d3a1cbadfa5104b72f10ccaa3", "ref_doc_id": "6882b8ec-6e55-45c7-b489-7ac2ba1f3488"}, "e960c757-0e62-4490-a7b3-25340fe87a69": {"doc_hash": "336c0f82b5e9dab9b8b4bd6385f9778ff5ee13690ff9f2d680c83399a1b83581", "ref_doc_id": "bae8eb49-a2c5-46f1-b4ca-52407bc5c4a1"}, "493ebafb-758c-4f4c-a642-c4c4168463dd": {"doc_hash": "ff1f1c77793e5ac82a915f9aa2fd45b7bf4ac4661201102b943b866837389c5d", "ref_doc_id": "707987a8-0c39-47b7-94d1-9663fa24b198"}, "4c8018b0-29ea-431f-b59a-0ca241227a2a": {"doc_hash": "c25c8edbaaaf68044b891f828b4e416096ea5b17ed047129a0fcacd379051e44", "ref_doc_id": "3ef385b5-8499-4b8f-bf7c-44100a4762a6"}, "2df8e644-8c2a-46bd-b3be-1234f4651f62": {"doc_hash": "500892f44e905c113626d96caabfb15317bfe274102c290a6c4de6768bd8f9a8", "ref_doc_id": "b75f3b15-933c-4b74-8b49-c7f9042e2149"}, "62d127c4-9513-43cb-9ac2-fa1938efa088": {"doc_hash": "008e36d9d29f826e701b4fc47b5107d83107bbab67761f687ea117d1aa53ecfa", "ref_doc_id": "a81c6658-672d-41b7-8961-12fd3ce171d0"}, "deb131cd-aae1-4857-b406-f279b1f2d3db": {"doc_hash": "ceef9ffaba1320ba0ea39b985df66ebdc0c9f673ac17ce3a5303690c6145fe1f", "ref_doc_id": "e8cf0a5a-bf22-4d0f-92f3-f3de3bdb9fc1"}, "74cb8228-db5f-4d76-9fc6-053c154844b6": {"doc_hash": "9d34861c4eb5ce80836251ace57c2acdf448742f7d5edba5fca5732cb9672902", "ref_doc_id": "e0e94bd9-aefb-47aa-8195-61507e4b77b2"}, "c5baa6e4-1345-4cc4-81e5-c3c92629f018": {"doc_hash": "3f7e90c98f442de69726611fe0dc7598d36ccf0eff35c90b4062243f0d1c7ca5", "ref_doc_id": "29defb66-e9ea-4784-be99-3eeba7203251"}, "a9ecbf34-069f-415c-b2f6-225934450f8b": {"doc_hash": "369cfbe7a347f35eb4b4581c31e4903d6fccf9ca35e216108efd3d4b157daaa4", "ref_doc_id": "96498834-119d-4552-a4ce-fcb1e47fb499"}, "e22977f4-3c85-4a7f-96ef-304cdfe0cc68": {"doc_hash": "36c5aab85b7647309e9d9f8d5a67892475c6370b94ab98d69db89a496fd7e3ee", "ref_doc_id": "97291e4a-0512-49d5-bdc6-ff4fb59f6ee4"}, "5c2a9ae0-c729-4393-bb80-d14743f9a417": {"doc_hash": "a4f0a635e1b1b17fbb10bc4ae7ba18e10794eb10d45221ce23e37288fc98a924", "ref_doc_id": "eea54be3-50a9-4b54-a365-925aadb4a7e5"}, "42267297-7d4b-45d5-a248-3dac71197e42": {"doc_hash": "fc1f746d0081fd18b00ef65e941e75e7373f3f3d8d0697af1978143c7236093f", "ref_doc_id": "85fb42c9-7d17-4f14-bec1-af85400a4069"}, "c719ce5d-e93f-4d86-9f05-02ad026b6c91": {"doc_hash": "0ba35ac21622deb20aa5f08605be002263efcb1ba1840c18bee7fb0f5da9512a", "ref_doc_id": "db618e43-6a7f-4e87-82a8-37232cd32937"}, "9650164d-b5a6-439b-97b9-2a1fa6670990": {"doc_hash": "8eecd4e1791050bbc6ca43c2290d2e8d6c755cc1518848f57f09a91f22fe1f8e", "ref_doc_id": "352aeac5-eaca-435e-9d74-354004736df1"}, "d3207532-03e6-4e42-b890-2627f679299f": {"doc_hash": "755f15c2b1cb118af8652c3091b44d3420a10f5470890018b3da6c9a94333f6b", "ref_doc_id": "4a3a1a13-78a1-4a26-964f-294ff14386fc"}, "217b31f2-2d1f-404a-b7fb-53e3ea1b35d9": {"doc_hash": "56d382e2c45de695d2add60ad035339984f420aa95b7ce31a1d0f18ffd596ebe", "ref_doc_id": "7bb116f6-1482-47cf-8c0b-5ff01e9de263"}, "4e5f0992-86e9-47bb-8f10-28411a84b36a": {"doc_hash": "37f5c0757cf5d92a42a16bf5e4fa2f73dbd44e84de767f5d1187d698fbbe0e5f", "ref_doc_id": "32d5fd23-84ed-4d13-9938-cf101daf425a"}, "633523f4-9853-41c8-9fc9-318476d688c6": {"doc_hash": "5058351fbaf00008888cedd8cafb88828cff1095981ca6d7f681f28dad8a46dc", "ref_doc_id": "369fd348-471b-4887-ad19-f02ddfeb684e"}, "29d50115-8781-438f-ae4a-9f6513f02337": {"doc_hash": "8c539d4b071a43857cdba2034e6857e393f016a785071d1f70269fe71917de9a", "ref_doc_id": "0803b89a-0adf-4dab-9b17-68b99acfcf02"}, "22e377cd-03d0-4146-9341-916287373d16": {"doc_hash": "29dfbb4b10b1098486716af3301433abaa4e4497a9cd433703505680adc1dc0e", "ref_doc_id": "d2dd0f2c-3d39-414c-a7e0-a4cf7b3d0776"}, "25aebe7f-057b-4038-828a-5c4e1853ad4c": {"doc_hash": "c4009c09aa3b9f6a9d0db40cd0da90a91f3a69b0401cca67af8915d4dfb85dc9", "ref_doc_id": "6224201b-4efd-42a0-9cef-0946e4fe84e2"}, "9b464a67-3915-4958-ac68-2b7171839759": {"doc_hash": "4c5cb864d6f613f120deec2700bd6ccc6c7341aefd42caf31d6e4c5161b8a880", "ref_doc_id": "44b5c628-e4cf-4b05-a77a-3b6353959098"}, "16b66d2f-9b16-4dde-a651-5187d3229110": {"doc_hash": "febaf131872b92b5408d0a0f7d63821b65e48e564e3f1e0b37302973cc5a432f", "ref_doc_id": "b1619346-c4a4-4190-bacb-f7a0411ad965"}, "98e94121-f62c-42a6-a810-61d59ea5db9e": {"doc_hash": "fbe44af67d87f75f6b106a9776507fff79e758333c74c99b56a6b0c5bb79bcd6", "ref_doc_id": "ee3dba33-cbd5-4d14-a0bb-f1925fe3cc21"}, "7358e19d-b7ba-42a6-9cea-158701321ce5": {"doc_hash": "d37229e418afe3582ee15b1282bb60c9bee27913757842cf25bfaec60d546519", "ref_doc_id": "b4c59342-ca6f-4b8d-87da-db70670598cf"}, "70e3ccf5-6ee2-4db4-b4d6-0ec245e3c9aa": {"doc_hash": "242a2bab053cba454ac8d069063015d2f816de47e8f163cc4c1b5d6d4622ebc0", "ref_doc_id": "5b056c15-0557-4d5c-af51-5d23a349df35"}, "64b92cd4-420b-4128-b5e9-296c29676736": {"doc_hash": "3aa260254a0a0a05123cc571a14fd54efeb1f0b6ac0d588dc7de7830c1406be0", "ref_doc_id": "5b056c15-0557-4d5c-af51-5d23a349df35"}, "a083281a-7373-4f63-9f88-5c562a1b7bde": {"doc_hash": "1edb2752005f9ed77109ffa291db1940e596d6ae3634c5e11c8af92df3e22eb4", "ref_doc_id": "6bcfc07a-a4e6-4e06-b39d-548fdde1ebeb"}, "128d3dcd-ab94-4e35-9bb8-92e05593d61b": {"doc_hash": "d5ba69d898c3dd37a17c52d212586ec7cd3b4332e16fb9943fdf279e210037e6", "ref_doc_id": "1611ebf1-c46c-464a-9bed-27811fcdfd80"}, "630cf3fc-75fe-4726-bbfe-fb9acaea9405": {"doc_hash": "a42cdde41b843812bf8c8af30b37b84a6de6e3b886927316c324233f37f5bd66", "ref_doc_id": "764e6d7b-d79b-4603-8048-7d2ec800bb78"}, "1171fade-0afd-42a9-878c-52269c23e0b4": {"doc_hash": "bd86976017bbc6716eb3835a19378a59a44277f5abd00a0d62e99abdea53b997", "ref_doc_id": "14481d3d-e5d7-49f3-95a6-46d8add21e05"}, "42c24cbf-224a-40b8-80c9-0dc8f85d3cbd": {"doc_hash": "5204cff796020bebda61f7e60e96c5b1f787b65e156435ac18e60f69f882ee8d", "ref_doc_id": "4d2c2755-b053-4a4a-8be0-84cb5dd32d35"}, "2e749d08-9f11-4380-bbe4-13ccc6732bd1": {"doc_hash": "b54bc1ade45acf6c59489b276e2570de8fc8df4a20bea0ae9b86383dc92ff6a8", "ref_doc_id": "d2cf2a93-94b2-4c3a-b686-7f93daced2ce"}, "af7f5101-cf9f-4f08-b5fe-189709625aa0": {"doc_hash": "18d61403ba1629fa2ed53b54a3b1ce4dd95a5a9520e007672d7251594a5a918a", "ref_doc_id": "2c23edf9-18ca-4874-aecc-203868bdb030"}, "67fb1692-142e-448a-bbd1-3ecfea6d98ef": {"doc_hash": "ae947dfe014190bca29ef0b6940c911cad0ee6ca46cbb119349acc760c7b0e5e", "ref_doc_id": "c75fc2b8-5705-45ea-98b8-be05e7d95cba"}, "ac651916-78bc-4d6b-a755-57bae36cadeb": {"doc_hash": "a8c198a8ead11b65ae60f71aafba73c4f826423a5372f93b1e77b089946c9a38", "ref_doc_id": "ca1d2f0b-658c-468d-a97c-6a7b415fb786"}, "b70d2504-2dfa-4724-aad2-ccf9db730211": {"doc_hash": "eab12bcd2edcb0aa2a1af8c262b5c7ff7012c2266eeb72e3fb520bee71796be5", "ref_doc_id": "03d3d6e6-6e07-4e3b-b145-6eb8f9a9c6b1"}, "d6cf53ea-7f80-42e5-93ae-0373ca5ea885": {"doc_hash": "5b8733b0b0513872b5eb23220c39062796be433407b833535822dfd063b1bc1e", "ref_doc_id": "8176ef21-d338-4cf8-808c-bcc3235b67dd"}, "c9fe81c5-10bb-489e-92aa-cc2e38368695": {"doc_hash": "8892921da6c0dd547348c07d828bdbd84a05a120e440d029f8ebba881ef7bd0f", "ref_doc_id": "714e515a-097e-413a-95e7-7ff121ca2a03"}, "dd06978f-fa42-4655-9b44-9f34a02bbdb7": {"doc_hash": "0d3f07df09f4097e59329a0ae986d0260cb1155f3a256a75a7dd343ea399121e", "ref_doc_id": "799966c7-e2d0-4e58-8eed-8b7ebdee35ac"}, "6548d328-f710-450b-b483-8d8c90af5341": {"doc_hash": "75b869ebe60f99debcba265031d6b24889095f89d93f1eecc8ba4bff9cd759ba", "ref_doc_id": "c7dcd424-2e3f-4d7c-a2e9-ad7bdd79e735"}, "bddccd67-d3b9-4138-be50-242894ee5e52": {"doc_hash": "ba0dce2584d5816e99ba2e036f6502d2f98ac0c826d3b1cfc2624547b3c374dc", "ref_doc_id": "a0da184e-5c74-4c9e-83d3-143fadc4ca2f"}, "55fe216f-fb51-494f-8392-f785fb0b31aa": {"doc_hash": "f7116a3864a257d3857b2c2e97f4b8554e9662b4080bed8be20a9eed7950a063", "ref_doc_id": "e347ce2d-af15-4fa3-942b-d6cccb26d7c2"}, "8a7cf802-657f-4793-8802-4bf4961a98a1": {"doc_hash": "fdc0d91c026e8f59d0ea0d0ae3d8fd6ac85ad39db89ac6b4d00e4fcceec3f34a", "ref_doc_id": "65fe5291-1962-4b33-b64a-866c643c33fa"}, "16f77f09-cd24-4d9c-aac5-b5fa31ba1fdd": {"doc_hash": "c164dade06980564004b7695c40055d8425867e2ee69ec4d723f1f451320a1fc", "ref_doc_id": "65fe5291-1962-4b33-b64a-866c643c33fa"}, "b6ff1809-daf0-4e37-a83a-f99396530dff": {"doc_hash": "46ce6bfb76f997d85e5698f852cbc21c42ae2141fe25fe0714f5606165c95029", "ref_doc_id": "5cc62e09-56b1-4f5b-9090-66ab0aadf84a"}, "9f1956c8-e604-491c-b2a1-ec42671f2a84": {"doc_hash": "7f2b429f7585724bd87c0049714a78bfb5c09b7803a3cbe05906ed54c4fcf047", "ref_doc_id": "440b5a2d-2ee8-4a6d-83d4-f5fbd56ee8b3"}, "d46016e8-d65c-4ab0-960e-6212617389d3": {"doc_hash": "c932421134d347a11ef6db045d996480986f809560d46afcd03fe4ca7a1cbfa5", "ref_doc_id": "6ea119fa-bfc3-48cb-a3ff-45307b729cf3"}, "5776b7b2-d564-4075-9c58-c9053d3cf085": {"doc_hash": "58d60c259e4119a4f2b8ce11619da25d557684810bb9cd2f008eb051e310047a", "ref_doc_id": "a7fc1d3e-28a2-4af0-a9fc-eb41061ea974"}, "d86021b5-204c-44cb-87c3-e67b72ae1fba": {"doc_hash": "5b49e9462069692dbf96691617b56cbeb2aa15a68e8f3970aa8cbbee511ca005", "ref_doc_id": "208098e8-ddc5-47ce-8eea-ce740bce78c4"}, "8554ee82-8217-404b-b457-4f920c8a5ca6": {"doc_hash": "e98d446f03925d9d25db3a5f8cbef5514151c39b863a5c5ccaaa6674e134347d", "ref_doc_id": "8948cff9-9944-47d4-84fb-00fa4314d57e"}, "d62037c9-99f1-4bf4-8e2d-8cadf9e24324": {"doc_hash": "506b39824c90099ef5d83001b79e2e67692efec78577b74e5e23edd833e8dc60", "ref_doc_id": "963c113d-3f6f-479f-a228-aa9db5f64747"}, "4457a6d8-e566-405c-adeb-9afb91054a3d": {"doc_hash": "708a46f01bc47f519790f31d9beff218d118dc0e2e72a1284344fb76940241ca", "ref_doc_id": "963c113d-3f6f-479f-a228-aa9db5f64747"}, "a1e43c09-f40c-4298-91b8-1ce43da10abe": {"doc_hash": "9790bb154835ad6020a508d12ac18efb18d35aeadaa1e83dd23771bfac83b5d1", "ref_doc_id": "64d5847e-857a-4d86-bcb2-e1911a55defe"}, "9ba667f0-6b0d-401b-b146-9e8c59fbdcb7": {"doc_hash": "09222043bb7e69b3464eaf48dd6ff0fe32033112c711ca86e67d2301f9d48a27", "ref_doc_id": "64d5847e-857a-4d86-bcb2-e1911a55defe"}, "79927d35-332d-4a20-80dd-a7e950bcceca": {"doc_hash": "f4f8da5b6638ebff63da77b324999e11c068dbc787d245e6b3a2a76244bd1652", "ref_doc_id": "c2990194-8cef-40f0-9992-c5c6069dad44"}, "3e601421-6d25-4f5f-8e26-ef68f1769e04": {"doc_hash": "4f3077ab78d097b36ba333c66f91018acf1e416faee9eb4d9205a09995708ab8", "ref_doc_id": "c2990194-8cef-40f0-9992-c5c6069dad44"}, "975de8f5-d5f4-469d-acd2-50c34de9b1fa": {"doc_hash": "94575e81e1d9c56c376550ae0747f97b66c6286ae9ab743116df6ea0441ab9b0", "ref_doc_id": "2ec05c59-a930-4469-b083-26ba92bfdd93"}, "c75a001f-0c52-477d-a08d-88042578ac06": {"doc_hash": "8614b4ba37f7ef7ac8de77b05e5998374a02797bd1f282f9fc20ee470fab79fe", "ref_doc_id": "2ec05c59-a930-4469-b083-26ba92bfdd93"}, "18f16d88-133c-442b-8c75-1af9d0ac897c": {"doc_hash": "af195d51e838e72cd789b24fb798754902d00b0f8a79ccdc067e4ff3cacfcb63", "ref_doc_id": "468a45b4-6bf6-4eff-9d4a-907649c18f1b"}, "a3cb3843-2523-4991-b7d1-9b56e326719e": {"doc_hash": "47871a203f97532683f2e21fe7b1ed521dd12bf90b0cd6d2092e52d6c5771dd7", "ref_doc_id": "188a4754-1d4a-4a12-b66c-99fc4078f207"}, "467ed3b5-e90c-4240-b3f0-80d406d78bda": {"doc_hash": "a747839e8576b3928291fa21e410098478bd4bc7ad7abbea5b5ea96b5932915f", "ref_doc_id": "a16032be-2c9b-4c61-84c0-8217293d2517"}, "a2709435-5fdf-4050-9bd7-ad5ba6df4a48": {"doc_hash": "32da43775612fcce404ef1214ed5fbee77b5f49cd42d77c97c9333b90675942e", "ref_doc_id": "b9ef321e-f8fe-44c3-bcf5-704efe6d04cf"}, "60267760-356a-42a0-a0f7-b62a7ea0dde4": {"doc_hash": "54817c7a305bf0ea1dfe66b54554edb07f1e2969ef83d8ccb267f666b382072c", "ref_doc_id": "dfaab604-af68-41c7-b1ef-8904e2d6a91a"}, "c5b7c28a-0ca1-4675-8a64-d527fb828337": {"doc_hash": "86cfc773954be6f39aa552e8b3012e5a901ebb75af0f52f59297f449855d7d4e", "ref_doc_id": "e17aecf3-a572-4667-8a18-87d01123adc6"}, "ec377e55-ce64-4821-8992-24c16f743aa4": {"doc_hash": "93023d6f9b1a24e27f8ccdc010e6434c6446d7d082fbcf9d8719ff253e09dd25", "ref_doc_id": "bee98ea9-8b7e-403b-b2cb-e7e526fac9f3"}, "98793adc-6e5d-4174-a5b3-e3379e22851e": {"doc_hash": "0ce438a05fb6672823d5267568597c2b18562db0023240bf88a0fed4b5855e0e", "ref_doc_id": "5dcd931d-7360-4d7d-a208-b0cf6b88d3f2"}, "59db6a0c-2382-47e4-958f-13a833bfb60d": {"doc_hash": "092ef4b0c55e2507dd8c8a816a428fd1e4ac2ad83d5e4ebcea64ec48f0df3540", "ref_doc_id": "35e1c939-3db2-43c9-88d3-2afcaff459f7"}, "51be5534-ae6e-4fc3-be0e-404fdad4a874": {"doc_hash": "46273e7fd7dd4a5e8613dacedfdac36fcbf143a5773737880efc39e68b12bd2e", "ref_doc_id": "b96ec1ca-e8b1-45e6-8d0b-8548856adef6"}, "3d4a89d0-1ff4-4b4c-b010-34a64c4eb844": {"doc_hash": "81e19ef9c3484103d6d7df42405602e4b0964c4b00ff73d2f8e5dbede4bc9c84", "ref_doc_id": "3b2bb181-9a61-4184-8b42-1a9e96e2a2ca"}, "3a0130cd-477e-48ef-ad32-affcf781fabe": {"doc_hash": "62c6dabdcd084c9dcfa892da9150279df82ddae09cadcb5ab3735ec0c4d00c4f", "ref_doc_id": "ec7c7831-457e-46ba-a816-4a9770139dbc"}, "d078578a-31e1-4344-a6ad-ef3a528cbdb1": {"doc_hash": "b6b57703deb4a6fa6301469fa2d575e03b211359f3ff577ea1dc47f29b54dc8e", "ref_doc_id": "338268c9-79bf-497a-bcf1-49dacb39b48e"}, "86b84949-95ad-491f-916c-86a8c1b3d0e8": {"doc_hash": "9a1ba7535154d11016d91237a08baeedfff6b423274fe2038f4a498f01ffaae9", "ref_doc_id": "3ed954d9-ca30-4e47-b076-84b95d32108b"}, "f93b48fe-d89e-4d0c-a7d2-19341627af1c": {"doc_hash": "685904a8aac92e5be991ad8bbf259afa56be37afd60dc0da10a32b9cb5759df7", "ref_doc_id": "a0a7f33f-47fb-46b3-8332-fcc3a868a4ca"}, "647e389b-2499-4745-b1d6-34309a8d1d05": {"doc_hash": "dea4eeff56172ddc1bdd77f1308bfd8d4c9e28504c6ce40252d9372e4b6ac162", "ref_doc_id": "afb64066-9829-4bf6-84f7-37cfb5327063"}, "4e9bcaa8-b900-46f4-b20f-f977907fed8b": {"doc_hash": "20e4844026231903ec93ec439febcec2b46e8dd1e5bc373fc13c734b31110d84", "ref_doc_id": "fc34d6a5-4e38-49e6-9a32-c1d3b327d95f"}, "2632011e-478b-4382-94dd-9ba40515bbaf": {"doc_hash": "267a033d4923521bce5ef036505f10dc3d4628c1f82bc264181f4a2e0760748c", "ref_doc_id": "475e5dbf-5221-4388-b92b-48af66d45d70"}, "78968f00-3f45-4d24-8d6b-da87b31a300b": {"doc_hash": "3215e452ccf8f0c42b68f367d5435541141ecb727113fb49fbd0840aed81785d", "ref_doc_id": "5e2b690b-069d-473f-92b5-6f42188744fe"}, "113105fa-f455-4f67-b4b3-63893a3d9885": {"doc_hash": "87d26208ab2cf6752cc20e6aa578a45f5418d04d0d1c30d2daa832e3c5646fff", "ref_doc_id": "3fa42d2d-b2bf-458c-a7a9-f21998214119"}, "333738bf-9a26-49ea-aa02-d285910f3ede": {"doc_hash": "0bafb7a2715bc64634f2244e002182463e490954461a9b6f9078f7fc5548af10", "ref_doc_id": "7bec8982-5bda-43c7-8e25-7b2528c20693"}, "2174000f-bef5-4a9a-a95f-abcb0f2e34d6": {"doc_hash": "4bee1d4dfb3f0fa3e7881d1e27c31b494599506aacb0cc2cc6c7f247c12a84e6", "ref_doc_id": "acdc039c-fc47-4a43-b500-8fabdedbba78"}, "8b905824-6d28-43e7-b92a-d1ac49a6de75": {"doc_hash": "704930ef9336bed89cc4df4e7496564e5bfb6c034efda620bdc6b7d390934b39", "ref_doc_id": "d5d4b175-eb20-463f-8b6b-10d6d07c66cd"}, "fc35a834-8749-411f-a101-a41b4c276450": {"doc_hash": "35881124d2a20a3dc54aa36c505a5bf948c91f2198248f11a04deeb80f0d9a0f", "ref_doc_id": "d5d4b175-eb20-463f-8b6b-10d6d07c66cd"}, "d2b288ee-3eb4-4b24-83b8-4fcf7ca88042": {"doc_hash": "6dc46171aca6f95be98051b88b9151801353d8019d68e39685284b717acfe77f", "ref_doc_id": "d5d4b175-eb20-463f-8b6b-10d6d07c66cd"}, "9ead50d3-b982-476f-bf7c-a336ca49ffaf": {"doc_hash": "b718896182da26d2150a5a5cf97476616bb099bc957f31592e6b7b9274920c0f", "ref_doc_id": "a594c83a-1f35-4197-b839-36dc23daf492"}, "e4f4bc73-6784-4fcd-9991-b05455299885": {"doc_hash": "ddd29d568a7dcc982b8bc91df5cc936c0d7f3c652ba6e584ea34b70c2ed4db11", "ref_doc_id": "a594c83a-1f35-4197-b839-36dc23daf492"}, "aaf63c24-568b-4042-9470-9c26374b8d6c": {"doc_hash": "aaede1cc0cd0ded2d3ee9197244b4fa1fe0ad5392de682464a2d22d4e8e6d18b", "ref_doc_id": "a594c83a-1f35-4197-b839-36dc23daf492"}, "4fde08a8-70a6-413b-b7f4-848176d97431": {"doc_hash": "1e92068acaa0cb06d8bbf64871aee3bd02f8e93a10e6d605289758ff8117a579", "ref_doc_id": "5a2f4afc-1c83-4217-9fd9-20cdb7ed568e"}, "b972ba3f-c2ca-4331-b913-a5d6c79776f0": {"doc_hash": "b95c8fcba4748fe2d5cad3f1bafd73a66d2ad24054334004c41a03ec471e0113", "ref_doc_id": "5a2f4afc-1c83-4217-9fd9-20cdb7ed568e"}, "174eeaa9-7522-442d-bf5f-58881c2e1ef7": {"doc_hash": "3b15868a94b58c99faf3684425e73dce8f5874e4ee6c67c5c22e3f1882c357a3", "ref_doc_id": "bc05fd68-c0a4-49e3-8713-9dd61675966f"}, "73be25cd-c1c9-4077-8464-bb335acbd5ea": {"doc_hash": "91854c49a1eba0705cc5d96dbe0159257c912184f8f269bca4166d4a06bf4387", "ref_doc_id": "99554233-b8dd-4d76-a9a5-2f1933e60f8c"}, "7d975e5d-030c-4343-a9b6-aea2f6450c42": {"doc_hash": "78100ac5a296feb388e0e0b2050a2d2fe78fa5cf6334f305d59f52e5743bbbdd", "ref_doc_id": "e1502670-14ae-4b25-a506-eef06caab003"}, "b29baf98-2f39-4209-9fcb-cb911f38da8a": {"doc_hash": "3164f45305e21bf9198bb7eb4ff18e001fcf4d9bf646e44f074adc98a426fdf3", "ref_doc_id": "d43540c1-b004-4f4c-b6a0-539000df6522"}, "122cafec-d41e-4b4a-853e-e5f2f2c42f34": {"doc_hash": "a967d6d5eaef2c00c6558dc4caf5592e0a9826024c144f4848b1b7aafcf18bc1", "ref_doc_id": "06e0aba7-993a-432b-a014-8befc79c2344"}, "0dffcc6b-65d4-45b1-bcf8-788d3f5a318b": {"doc_hash": "ba0275052baf0a389c85bf6f86536393cf62512f293a0de7ccae5b7ed45fb630", "ref_doc_id": "06e0aba7-993a-432b-a014-8befc79c2344"}, "8cffdc72-4148-4604-abcf-769e44753198": {"doc_hash": "5acee882696475fb785d52905e23653f3adf026aae5d7a3fbb63e5025fd7bdb4", "ref_doc_id": "436ab925-3e02-4549-882d-b89e3aa38c86"}, "6df22e93-5190-40ab-b469-f9544af75f37": {"doc_hash": "a22622fe0f783774cba9fef4d05c1388a3992ff9941e82510aa55ae4c6545bb4", "ref_doc_id": "436ab925-3e02-4549-882d-b89e3aa38c86"}, "0dc69084-6ea9-4abe-82e9-d858612e5277": {"doc_hash": "6c0fb0d697b0e095f9fb712fd45e27389e2bb229d3674663318b3643e65a27e6", "ref_doc_id": "b214e52d-3193-41a0-a7d9-5ae726bfbcc9"}, "ceb623d5-af3f-4a33-86a6-d6c64c9c5bdd": {"doc_hash": "46e26e2d23722ef2e8540aaf85bc72c71d685dcd72e49f088fa94a1e3b8c3817", "ref_doc_id": "b214e52d-3193-41a0-a7d9-5ae726bfbcc9"}, "29286be9-d9d2-4f9d-b294-736eeeb0aec3": {"doc_hash": "ac7c6b90af6d696d229df873e33ffe5c5325e6f937e86612aa26318d14a6abed", "ref_doc_id": "5ef9dd3c-45de-41c4-8960-4619101e42dd"}, "d5817d98-5744-4dd2-acf1-3b393086b5f5": {"doc_hash": "6735d42e39492a4b704ec6a3b44035d9c70ab57cce24931a8acc67f6239a5331", "ref_doc_id": "5ef9dd3c-45de-41c4-8960-4619101e42dd"}, "3437e0f7-6bc0-4cdc-906c-114bbab9d8f0": {"doc_hash": "0672fb8c470ab22dc0da0aefbf98dce83ee52020cc7cae46beb68f5fdc50dd37", "ref_doc_id": "16b69735-06cb-4db9-941c-5790c02b4ebf"}, "705e5f2f-bd44-4f94-b431-581755811e7a": {"doc_hash": "cb4f23527b979181466111e2785a09980a3043b0934e46729d8a4b7f2b19244a", "ref_doc_id": "16b69735-06cb-4db9-941c-5790c02b4ebf"}, "f3a4e0c6-e964-45b1-a9d8-818f863a7422": {"doc_hash": "6c1d7d00e0cfa9a36483f012a6930b8c6219dc2f0e7797a1701d160ec12a1fd1", "ref_doc_id": "be8755f4-f435-49a8-b027-c6d8575843f1"}, "4d08392c-a989-427a-8810-d067508bca12": {"doc_hash": "58cf6bda5294783e3c81ef9cb51c07188b7d04e518e05b844e16e410d0bccf9a", "ref_doc_id": "a2d032fe-ef47-4568-b2f3-916ace081419"}, "4af695cc-042d-461e-a635-e0bce52dd39b": {"doc_hash": "3c5f782bbce8b05e707fdb9a8e5c44e647a8414d643a8320faf366553139dc4c", "ref_doc_id": "5d321f45-6ab4-4a48-b6cf-e2220f2097f6"}, "887d1bed-2420-49c3-9e7c-4d54f4624b58": {"doc_hash": "66d230b25d91b8f01e864811f419c1560f084c055ef6d170d440ebde9d6a3725", "ref_doc_id": "c86bc1d5-a8d3-40e0-8cd6-34ea589877ce"}, "79ff01b0-efe1-4525-996c-31de767043d3": {"doc_hash": "19c79c0d2211c963517f064264cbcb307809fa68219255bee6c60317698c8838", "ref_doc_id": "6ba4857a-d1e3-4958-9a7f-efbfab50e460"}, "661da3a0-7bc8-47bd-ae73-02f2b86aab46": {"doc_hash": "816cce2ea2f630b2110576ff9b24dbb7c3937d183dc99963a681117b98d2ab94", "ref_doc_id": "a121382b-a36e-41e7-b53b-2a3028f2144c"}, "951501b3-014a-420f-9d16-32ff3ac4b082": {"doc_hash": "9a2e84f3d68d6c0982c0b71fab6feade97d72829f854362b7ba64056d4abb656", "ref_doc_id": "1a583551-0779-463a-96fa-acf6ceec8b90"}, "cf92f5aa-a7b0-45d9-8e32-bc5f94630e17": {"doc_hash": "f8dbc88e09cb64e5523bb64902c6b83b62aa0ef9572388c8e02abb662108fa3f", "ref_doc_id": "8192aa70-9635-4ee9-9ea2-12c392e44cf9"}, "267d3945-b3c0-4a2b-ab84-e20c30245adc": {"doc_hash": "65de157938ae22b9008751b6b23ab9d7e8295a3995229f55ea9216a15dd61df5", "ref_doc_id": "3f34c844-643c-445b-91bd-d00f90101431"}, "5a7d4bed-170f-4c96-bc1f-d280fa9abcc2": {"doc_hash": "7649078b1086b64e11cd3fcd2b1fb0b2673b1be01ab008644435430c8e013f78", "ref_doc_id": "a32db838-bb6d-4dea-ae93-9cdc8eec0ff3"}, "f70ff4be-fe0f-4ca4-9a94-cc51ab1b414b": {"doc_hash": "9d286599b7bc64a1db083132c8609c92172da89f39f2f58950ae2dcfac4d90d1", "ref_doc_id": "af6fe410-42af-4244-8cc5-ff8e24656a57"}, "9c2b6b53-991e-40c2-9908-b5605ec2482b": {"doc_hash": "0968e9ac761f356ad4ee6e750dd1eef4e7e02b2ea2ce62a462e02411e05ffd99", "ref_doc_id": "9680fa5d-2f1e-4f33-bbd8-dd53b819475a"}, "26e62c74-9e54-402f-90b9-9893dbf2b0c3": {"doc_hash": "9aac624bafbfbcbac94dc95d4994d9c51c373bb44f0019204da861af7b5f0dbd", "ref_doc_id": "66af1053-2d81-4eab-84ba-119e7abc6f22"}, "d40da6f0-f4bd-4377-a281-48b604d1fbe5": {"doc_hash": "615966631500f77e96ee0401d5de188eb8b62a5091223ab4d1264d953b37f695", "ref_doc_id": "9843422b-c27b-4756-8766-023566c8c7f3"}, "6cde4160-9ee2-4451-ab6e-ba93a62f2c11": {"doc_hash": "d24a84959e7caa44f69b1dc04a6ed6a1854b540fbd18deb8aeb1120a7067335f", "ref_doc_id": "2df8c5bc-7c37-4fce-9ebe-5c0a69d49b66"}, "90cee7f3-bfa3-43e5-a4b6-1c14e6a3e43d": {"doc_hash": "7853d0a97c2dcce2a5ce548012b23bb01e2e1d78d13418ebd3a6c5d697986f70", "ref_doc_id": "9d767e90-1eac-4a11-9a73-83995809e8c6"}, "3519d667-1b12-4f9d-be19-724b600f6ee2": {"doc_hash": "507cb34a267758bdb5fe0da647d4dd4eb0c26f7ef76213237818aa0ab0be1cff", "ref_doc_id": "d982e4f4-50c1-4ae1-9a0b-318cc7fd9764"}, "e8ffcf57-6c08-4645-b6ad-ad92fa9414cc": {"doc_hash": "3733aa14c47cebca85bc18906b304757f2ec49a536452a91248bc9aa49c77a01", "ref_doc_id": "b87e77a8-c678-4578-82de-2e705185ed55"}, "1d5d7654-dc8b-4710-8207-92b93a57af43": {"doc_hash": "6e5bc63f1348a691ef2ddbcc00ef3e8a2ee9f72d08b08a2ed44d1a180c2bd414", "ref_doc_id": "ae1afc5b-3dff-4620-a507-2dffa8c7b274"}, "6a64b7a3-1ad7-4f79-b740-68e026c0d9cc": {"doc_hash": "0fd8a4028f7a71bf76a948bad6409872953be95da7a6ff9445e2d8b403221aeb", "ref_doc_id": "9772db31-bc51-4b36-9d9d-0b29ff654126"}, "bcb2102a-d100-46c3-ae23-c5e90ae5c168": {"doc_hash": "7e35dbdd9ad7491b97ff8b5330151e3b7e77bdb2646adc9cc710b6285bedea28", "ref_doc_id": "d52e1993-8a76-4d9d-8209-c568521d16d1"}, "4710bec7-dc6d-4780-8a37-05f1149f825a": {"doc_hash": "53e44091801bbd289c7ae79dc75fabec184bed3227ae614605ab9645ca42ca4d", "ref_doc_id": "8df9836c-9846-4216-a16d-c853d8126b87"}, "f23a4765-136e-4f2b-bbf8-376dbaad96cb": {"doc_hash": "fefa5e77817ef64fb5f8c73758a790589e59a075c46dfa38a900c5046421b86d", "ref_doc_id": "a4d7e487-56e2-439b-8a0b-1dc0dc69003a"}, "c5a857e7-7649-4d20-ad31-691e2aa41f37": {"doc_hash": "6822d8749373c366c8f8af20b775d86d5a678780bc8a13f1eed78363003a50c6", "ref_doc_id": "36161822-dcb1-4104-a0b7-fc0fedd5c14c"}, "1ea4eff0-573c-4dec-b1d6-ca4dde98ebf3": {"doc_hash": "bd0746c1f3573f17f9f7578052617106658105c8cf9a37d10508982b8b21606f", "ref_doc_id": "ed524dc3-03d0-4245-be4d-2e27060b96ba"}, "c41a67a6-bd2b-46c4-90a7-ecfd7f0eca39": {"doc_hash": "13e978e0877dc33ccc0e2a70e4eb789efd40f2c5a3505e13016892e2dfffce74", "ref_doc_id": "ed524dc3-03d0-4245-be4d-2e27060b96ba"}, "92ef9da6-dce9-4880-aa29-b0c4c6fb2b42": {"doc_hash": "4bd841d884e989aea106b56dd455f6ff6f9da80f27083ae3f61291df45b68d6c", "ref_doc_id": "e8bd1842-3d30-45b7-b425-c7fa9bc97027"}, "d0ce007b-2a53-4663-8539-1c004097a251": {"doc_hash": "666ff94a1df8b0e8d310d23176a9815b81469bc5452b2c035b11a249e9ce1b5f", "ref_doc_id": "5f0e0e5a-4b83-4278-810c-c147423aa1c6"}, "a7bb57b4-6157-4d93-a1ac-12e9068f0479": {"doc_hash": "adc962d0910d4ecfe131532cfe33aeb59ca30ec142021696f404b7f37314da21", "ref_doc_id": "08b13d92-834f-4a87-99ba-afa1f83b90b1"}, "19b1cb52-b163-415a-82da-818d8839c468": {"doc_hash": "0bc9e7ac1010959abd8cd7af1def9546ac82c2a52996256eb264dc3c6e8c79e4", "ref_doc_id": "5086d743-c697-4cd5-b5ae-c4ba86a9087a"}, "81133736-a125-4eef-a80d-6b8701d87e11": {"doc_hash": "91447c3fd795955e967ed4b5d00ae902b979b2ab24f0fc2cea99b491e3c23449", "ref_doc_id": "227bf9fe-e7fb-4e34-a61c-43c4e48cbe74"}, "54eebd33-5aef-4bd8-8ac8-988ad0ae3d76": {"doc_hash": "c5069176207b02cd5ee3e4d7cdbfb8353904ecdf83d15b321d02987b26f08eb8", "ref_doc_id": "5da12076-0526-4c4a-9ab1-ea170fb0ddbc"}, "a267d57e-aec1-4575-908c-57be13d506f9": {"doc_hash": "dfff0ed000d4d4571335649d0bc1c27f72f45e4ef3bdd3790e3e6db45b3ba3bc", "ref_doc_id": "5da12076-0526-4c4a-9ab1-ea170fb0ddbc"}, "65a8c4d7-a3c2-4286-9491-29fbbe1f6203": {"doc_hash": "9a38431536cf6927d524eadddfd9b4b8404840a9802d41d1836523a0fa4c03aa", "ref_doc_id": "3d916a01-f698-49bf-a6e0-71eb48ca45b8"}, "3a1a4304-3c29-40e6-880a-73aa2b340154": {"doc_hash": "b5e1f22c3829b2c4f0781c04895b8ac795cf027f632d0c6074410b00483c3963", "ref_doc_id": "3d916a01-f698-49bf-a6e0-71eb48ca45b8"}, "7c1c924b-2582-40b6-86b9-3301725d8a2c": {"doc_hash": "e266cbfcdf5057b3f1e5ccfb39e066a71986abc5812132bcd7b39e7ef93c99ce", "ref_doc_id": "58d5eaa9-1676-4862-b0ad-d2b8a38fc48c"}, "1515b072-5689-48f8-9cfc-a4f341a0b9ac": {"doc_hash": "dc2dd3a225a87e6e47257050effdf105e731ce5b98557a9719c297b14d4bc43f", "ref_doc_id": "58d5eaa9-1676-4862-b0ad-d2b8a38fc48c"}, "7bc1cbfd-4621-49d8-8faf-31c43a6702c0": {"doc_hash": "de9d06839d80c8abb9773d69f7a5cb69146db8da5be7a45af60c7cefbd38b034", "ref_doc_id": "01c35645-577b-472a-88c7-f1b65921fa4f"}, "43a7fa61-a7a4-44b7-9e37-54038a7c7776": {"doc_hash": "2a19dfed6b756193b03c785489f6062e327341ff9fd05b0f94daf1d35791ab1c", "ref_doc_id": "01c35645-577b-472a-88c7-f1b65921fa4f"}, "d2ce38a1-a059-4439-b376-0d944b526178": {"doc_hash": "aa2ebe2f5eb2e2558228410d9d0b9b5580e0549f6092ceda31d019dd3467f194", "ref_doc_id": "3910924b-4439-4dda-993e-1ce4e29bada4"}, "0a5b1c88-f6a3-4938-b26d-83a7660cfbe9": {"doc_hash": "0fd56fc09b39a028cfab7986b55fe1fc830eac06a855c44bae5f9f48995a2f8d", "ref_doc_id": "3910924b-4439-4dda-993e-1ce4e29bada4"}, "33f1e18d-93eb-4ec2-b124-f8bb210de54b": {"doc_hash": "18cec91ca59afbe957046b81a94bb7e6fbeb3d5ecaf8e66089ba3c74f8674be4", "ref_doc_id": "d060d58d-e467-46a0-923f-0b499427aab0"}, "055c34f5-6bbc-43cf-b0e8-b633f0ab0e4d": {"doc_hash": "159a773a8124a5b593f639c2f985a5e0ae7a8818a15a513959822a805fb56051", "ref_doc_id": "d060d58d-e467-46a0-923f-0b499427aab0"}, "741fc647-3b0e-43db-bc96-9c82f210cdca": {"doc_hash": "040126f552aa07b544dfdc0b4c7a9d09ee1b5ecce5f36dbede8bab0d2ef60ca5", "ref_doc_id": "33068781-96be-4704-87a7-3d869e815ea6"}, "6d250dfc-909c-45e8-ba68-3ec2e4065a00": {"doc_hash": "4b9a37ea523f6590102ee6aea93ec6ffce44bb88b0ed1a11b3ba12753ff1f772", "ref_doc_id": "33068781-96be-4704-87a7-3d869e815ea6"}, "3d0758b1-6d69-4c17-a35b-1d9d4d7115d2": {"doc_hash": "a55ea42931caf115120f09c76fb27c0ded0cf549768c160da2769ed63a193151", "ref_doc_id": "33068781-96be-4704-87a7-3d869e815ea6"}, "e47ba44a-ae8c-4883-bff9-fc5d71ceb44e": {"doc_hash": "069b8c1cf6368ac46d7a87c49f40d27245e9882dc1a07397566a136d794f4b2a", "ref_doc_id": "ca7dbfd1-126d-400b-a8fb-91cfb991a071"}, "7bf77513-49e7-4389-918e-15b8f27ba614": {"doc_hash": "66bb18a4e97649c4eb06e48d6d1aed0d3e563f5468c25a2ed80c2e9e709d832e", "ref_doc_id": "ca7dbfd1-126d-400b-a8fb-91cfb991a071"}, "60388c28-7e4e-4aef-beaf-73fe5972f207": {"doc_hash": "ccf1e003d1376e220d89d730b162bd23dfcd8aa39464dcfa66ea9d0eb8859c3f", "ref_doc_id": "d8c51a62-1749-4fbb-b7f7-aadfbf9de368"}, "f8369aa2-87b4-4d80-8281-4f949f203b3a": {"doc_hash": "c881090d48b1e71f8566a940efc875d1b2e53740c548e5807a17ca52521c1d29", "ref_doc_id": "d8c51a62-1749-4fbb-b7f7-aadfbf9de368"}, "bcb41f78-4de8-4abd-971e-515670f27517": {"doc_hash": "13ccf5921018c62dd09b1fc9a97b1baefdedc5b6ffa84f30ec3633e74f3cfa62", "ref_doc_id": "7f328036-899e-4b64-8151-1a54855d4edc"}, "6db06705-b622-43dd-967c-4f9a181122ed": {"doc_hash": "2e60dac77fa4fe56fd5cc765e36c37791d1741529f4959590eac379a15f18827", "ref_doc_id": "7f328036-899e-4b64-8151-1a54855d4edc"}, "c2ca69d1-d01d-455c-93c9-93c433c113e2": {"doc_hash": "827075a7c98daab954d59991ea0828992151fa52748abfb998bf3d0214993b96", "ref_doc_id": "8a666ccd-6cf8-407f-adf0-9feb3d1651fe"}, "23b24d1c-41f5-4a92-815a-7277d5cbbd96": {"doc_hash": "7ce61766bc9794f49564b84581cd2c7cbe1ff5627a478013436f2392a298f4b8", "ref_doc_id": "8a666ccd-6cf8-407f-adf0-9feb3d1651fe"}, "47ba346a-f06a-40ca-b728-2f0edb57e776": {"doc_hash": "36b8f4353177e999760c3804fd524e598f8159efbbf3fc0b5ff0cc3ca3e507fd", "ref_doc_id": "977d42fe-75e3-4446-b8e4-da8163132ddf"}, "7f2c5204-542e-401a-b56f-aa2240d51da1": {"doc_hash": "80a695024ea1e4008e64643945ffc4c4e7cf4b5803e03946fbe74bb0f6443e8e", "ref_doc_id": "977d42fe-75e3-4446-b8e4-da8163132ddf"}, "781fcfb3-87c9-4673-9532-f8071b317876": {"doc_hash": "b4a2f0a5d9f9c2f124365665f3564a68bfa4e440943926509b7573a83a3f4a4b", "ref_doc_id": "44171e08-ebbb-4ad2-a72b-caad0fbbd2a6"}, "99e0e83b-9110-443a-9968-0bdc465380c9": {"doc_hash": "0a76c0f757b437dc42fdbc76bab8ce3d8ea48ae087f092e806a0e4f8ef1ce0d2", "ref_doc_id": "44171e08-ebbb-4ad2-a72b-caad0fbbd2a6"}, "2368db4f-c1a2-41d6-a762-a3c79ee118d4": {"doc_hash": "bec7239fc0bce1d8d6060925c567792ec37a42c6a1d75916422615f7a1467a71", "ref_doc_id": "01e24ca2-a278-40f6-8c6a-3d0ef01e0b02"}, "eedcddff-3e26-4c96-8831-a60eec764717": {"doc_hash": "19980326427eee599b674c73fcdb1abff8525269f4e97c56bdffb12cd483f7fb", "ref_doc_id": "01e24ca2-a278-40f6-8c6a-3d0ef01e0b02"}, "3790ea29-b5b2-4ad7-aab9-63896366bd50": {"doc_hash": "aea0705d8c3544bc3842f632296da10f02bb5cac7819fd5f018ddc90861bdba3", "ref_doc_id": "8d5a96b7-487c-441e-b123-806c07f99da9"}, "97ddde65-51d6-490b-93a1-0efb46f4ed45": {"doc_hash": "172fe42bcad8f8b033cf4f9add0229b5de18ea3de80a7235d76822053e8e988b", "ref_doc_id": "cbb2953e-3560-4cf4-9189-a5410ab1af28"}, "9f25a1fc-34d0-415d-a2a8-e23ee0713a18": {"doc_hash": "bf865a8bcd507e8ab29909439bf3adcd090879b48c49f2a8d958903b7ad5a04b", "ref_doc_id": "cbb2953e-3560-4cf4-9189-a5410ab1af28"}, "43b3f599-fc24-4403-a686-7c06efefc68a": {"doc_hash": "41dc22a4f22816595776408abf7369bb1b0e9a49b979e21945cadc7aa4dad584", "ref_doc_id": "be8eba27-7bc3-47d4-8558-d8523201f14f"}, "fcef5747-a054-4322-a292-ce41ab606a80": {"doc_hash": "1a53a1469db1136ffa86a9fd617905411f2b4421146165e7f0694415f83ca05d", "ref_doc_id": "be8eba27-7bc3-47d4-8558-d8523201f14f"}, "d1e9170e-2bb9-403c-a6d2-066da1b9bc4e": {"doc_hash": "67751d0374d91cab5986d3a3e76f5d3f70f3b36c24cde5372dd9e1d3179e9781", "ref_doc_id": "0893cc8e-00ef-42fe-88e6-f6b94837ff1c"}, "25b50957-dbac-43bf-a1ce-dd80bbd7bc97": {"doc_hash": "ee80ffcf6910b358d8ec75306aeba0493dc78bb612d88b63f6080505f4ab1a32", "ref_doc_id": "1cd878b3-5851-466f-8859-adbc4ecd5abc"}, "e79ad8fb-0856-4679-a88e-2e3e4c203205": {"doc_hash": "393fbadf1cd21ab7d3e876faf3f474e515fe6f3f0039f16b4a60627403b552f6", "ref_doc_id": "1cd878b3-5851-466f-8859-adbc4ecd5abc"}, "58a8dc5f-c7d3-43ce-aee1-c13b8812a2d9": {"doc_hash": "55a812c501e0c1d36342da868b084fa3c793f292d21aa0146deeaaf887426117", "ref_doc_id": "1cd878b3-5851-466f-8859-adbc4ecd5abc"}, "fc24fa4e-1de9-4f81-9956-8aaf42f92e37": {"doc_hash": "2c2c961fa9e40606b5bfe52bd02b43b536b4ba1e3b9967d39e30c7f0a8bb898b", "ref_doc_id": "8883192f-bd3d-4d39-9db0-3c6b53d2c370"}, "3ad2b352-cb24-4dd5-b871-e751654a4bd1": {"doc_hash": "e61f05cc47216b739ce9d4f6229342e5eb588716063e6b76193c3d0fd55d7b31", "ref_doc_id": "a57814ee-8ef7-4d20-a9cd-32cebb57863a"}, "d1b3832d-29f2-4e6c-93cf-c7caac1efd18": {"doc_hash": "648bb32a52f80b19806bca9e7ab177ebc6df73f6d70b312a3c25abb57ccfe9cb", "ref_doc_id": "a57814ee-8ef7-4d20-a9cd-32cebb57863a"}, "f0b658ca-9ab2-42de-b24b-a3d59d920b3c": {"doc_hash": "9d9359a80f697bd6e202981ed3009e2b6b63af799d4ab102fdb8fa489d5026e1", "ref_doc_id": "cea75d86-480b-43d7-aeb0-259c84c50a44"}, "160351b5-0d0a-49f6-b976-42352f557a6f": {"doc_hash": "0db40346e4092cb54ae318e0c89a80cc72035b80e890568e2ef0021fc695153f", "ref_doc_id": "13207cf7-80cc-45b9-8997-079d14cc8bd7"}, "6cc6ad34-a930-41e9-9e98-1733168b1f41": {"doc_hash": "95b68d9b6cdb3f0a49b38d5e7e42a72acb111d735c9b87ffe936c2365a27bffb", "ref_doc_id": "27d57489-58df-48c0-a64d-784b7f0f3bd0"}, "fb5e8f63-1d9c-4bcf-bbcd-da255b66ada3": {"doc_hash": "99e226e0fa99df284ae15f3020c20d279345ae1541dc01878f55801b3ca7ec65", "ref_doc_id": "2ffef089-a4de-4b8f-a525-488eee533914"}, "1caa19d0-3bc2-4a72-98ff-d5378e7ced30": {"doc_hash": "bce4c02837a8ff6d51c138a54343ba3916ae72c6a749f0adf09e38afb03c0605", "ref_doc_id": "4282d3af-4e60-4356-8ad2-9ec5b836bd06"}, "31aec8b5-35a1-4ca5-8be4-606298653879": {"doc_hash": "a86415da615d60465fa29ad36e4b15e9c5bea5e6acf570f38bac654df4db5a95", "ref_doc_id": "cc275abf-a109-47c0-b1d2-15d0d38f05db"}, "e32392f5-2c26-48b8-aa48-4373917f9ad5": {"doc_hash": "d9273460b912981f9a86228a74bf99a55bde4f17a69f63e4170734af3f302c88", "ref_doc_id": "51209ac6-cbd7-4a11-909a-bd7205f89928"}, "d38e6b77-cc4a-468d-9f68-fa67e3e83c87": {"doc_hash": "e7a3778d49620a14ab3b9f76c8792f11b16f377dbd62cc53eaa22692025ed6d8", "ref_doc_id": "51209ac6-cbd7-4a11-909a-bd7205f89928"}, "05181800-79a3-481f-ab2a-fe3d08d2e5b0": {"doc_hash": "feedbd39a3140d75773689b39d8a55354d7434525a4e34c77631ed9bc51c1d48", "ref_doc_id": "6ddaad7f-9806-4bdb-894b-505a9065a993"}, "97bbba19-6b25-4c7f-aa99-a7c329a705f8": {"doc_hash": "e5013c3cc2d59ad20308d4ab2346f3c5cbee47d711f4acd5671413a9beeaeb17", "ref_doc_id": "6ddaad7f-9806-4bdb-894b-505a9065a993"}, "dfafd0f9-1b2a-4fbe-83d3-769c2291dc09": {"doc_hash": "0803a83c613879d8cd3dd68afdd2add098e6ff752bc3d46d5e871e9b23797a7c", "ref_doc_id": "909bdcc3-15c2-439f-bd37-95be22277e45"}, "076d8f6b-83ba-45be-8157-b0b6e0d740a0": {"doc_hash": "89e00e303d6c5825d11091575977e7775100d7fcd099f9e93dc1c06c964f953f", "ref_doc_id": "909bdcc3-15c2-439f-bd37-95be22277e45"}, "4b99e58f-ccb2-49ef-88ef-0ba35369304e": {"doc_hash": "7d576f9ff0b5cfa69644bf0709dbe4f6d03f85da7b92ceaee5bedeadd154038d", "ref_doc_id": "606a5e5e-6633-4800-9cc1-0635f0abfa8e"}, "2debd237-4d76-4f46-888b-1db1e41e731d": {"doc_hash": "e7ebaa7d5ee5dbb61685331b49fef53aa0539198badc20eefee516c1cb539e2e", "ref_doc_id": "606a5e5e-6633-4800-9cc1-0635f0abfa8e"}, "605d7da6-ac98-4b03-bbe9-0613cb404ac4": {"doc_hash": "831051f99c468ae1691c851455449653fc8da9b343aa4ab8f352b6cc9fe472be", "ref_doc_id": "d1140389-bb3a-478e-a038-e98a3e801170"}, "b6e8f320-c088-4606-bb5a-d8f5a0111dc7": {"doc_hash": "aeb2c50a7f5c321ace5d2aa50457b710bdf7f6ac3ef875f9fb4fc1f82eb022fa", "ref_doc_id": "d1140389-bb3a-478e-a038-e98a3e801170"}, "76c45c39-fefd-41f1-9c5f-b39e23834c3d": {"doc_hash": "2146a342b23647c16227a3c649c4640749095ed8c0bab0c27def2e904e6aec18", "ref_doc_id": "f90606e5-a9bf-4a1d-a445-5b774319de09"}, "34712188-7980-449e-b225-0bbf29eba6f1": {"doc_hash": "2edbb9ea8c392c95b9379193a477c4ed77ac61b7d11c44db3c7e158263903d36", "ref_doc_id": "f90606e5-a9bf-4a1d-a445-5b774319de09"}, "fd3179a6-e607-47c4-a0e6-c2ea8407a2e5": {"doc_hash": "519ae1b7a78c6798f88c719c3d05fa97dd9037a62829a3328a8ea8f984984bfc", "ref_doc_id": "7c3e870f-d94e-47cc-b7ba-8706db2f3342"}, "2a0c09e1-86f3-42f2-adb3-63023afe5d99": {"doc_hash": "229d9b43ea89147a770cf3522aafe7eb4d9c4ea2e53e8ccda39d97d393632ab1", "ref_doc_id": "78b896a5-8c17-485f-acca-a5632ef5469f"}, "48d16606-c594-43de-95b7-3b79dc53d267": {"doc_hash": "bf2602b2b7fe82a85d4f9faf7d758e94eff35cea82a9ea247d63435b39d85c21", "ref_doc_id": "78b896a5-8c17-485f-acca-a5632ef5469f"}, "9eafdbdd-1374-487d-8d9f-85c99153a83f": {"doc_hash": "54dfd43c2ba4ee91560ad4d9a52696292f6e69cc2d869e8883759cede42c1e77", "ref_doc_id": "91942c04-768a-4427-af6b-f5766072669e"}, "8762bcd4-a342-485a-88bb-5d515840b091": {"doc_hash": "fe5f6dbb5d74c672e67254ff328756778cc6fc5023b4054cc8489596bfc38b0c", "ref_doc_id": "247bcc3c-166c-4e1d-b7bf-b89b12432dbb"}, "478082de-98b2-46b9-be0f-95c59654c1b8": {"doc_hash": "e76739623baa5fcf682b9c33a5511948c6d9203a024726179a105cfadb2ad1ae", "ref_doc_id": "504d03f2-f63a-4a78-ba26-f2174a4d0dad"}, "180c4401-7074-49ab-9b0c-06c1aba6b6fc": {"doc_hash": "99b945e16d01fe063dbe12c46fb117f78475820bbcf6c0527f290d430c5d5b9d", "ref_doc_id": "1df3586a-38fb-48fd-afdc-238e74bea111"}, "d2217b17-f678-461b-a9bc-4e22334a20e8": {"doc_hash": "044201f20dd9b5d29809017320db76e3581396b8ed18030e7e3802293fbbfb84", "ref_doc_id": "3edafba9-7c7c-4bcb-b1da-05614d434e70"}, "a7f8599a-7dc5-43be-bed7-1aa35ed60e4e": {"doc_hash": "38a8eece9967cc965b937d21e27677caf7ee82ca9c24d47ee28fcde4ac7f7913", "ref_doc_id": "c48223cc-c3ad-4000-9e21-920f781c5716"}, "a370f997-4a72-4679-b584-230c58db6314": {"doc_hash": "aa7c8eb8dc05faf0fbb78afbec4cf457260e17e15cae3cc47656b6c87969a004", "ref_doc_id": "12d43681-7207-4c64-9271-ca3c8011311b"}, "f924674e-40ee-4e62-a586-b97e6a9dec19": {"doc_hash": "d67b1121ae2a3ff08f7de851a368029e0c8fb3504b2c22c79ba4ae49ad1bf427", "ref_doc_id": "12d43681-7207-4c64-9271-ca3c8011311b"}, "f94073b5-a48d-414c-a361-f450ac221235": {"doc_hash": "e690ae3682bd762e08fe1e861b3c61a143393a45f4a9a7be9b16b289595843e6", "ref_doc_id": "df9266e1-aed0-40e8-baa0-50c1176bc9ac"}, "cc99360c-5dbe-44cd-b6f8-3da4bee0da3d": {"doc_hash": "e69b8705d956dbc76040ed25903de3809a9661fb368d96f6491756888ed893cc", "ref_doc_id": "df9266e1-aed0-40e8-baa0-50c1176bc9ac"}, "3c000852-de47-4fd2-b35d-eceafb2c40ba": {"doc_hash": "ea94901b6c655470c8066e3ceafb039c304cb455a0529f37e8e75ac2f4e6c1dd", "ref_doc_id": "6dafd937-f703-40eb-9b35-fdfa06374640"}, "ea7433e7-3b31-46a7-8b7d-49dd52a67092": {"doc_hash": "81319cc83f5a56d3ca1858fbb2e4ba0c59eeeff70b55f0c067c41b0eca04e90f", "ref_doc_id": "2b115dac-cb32-4bae-99fe-9cc1bd788c9c"}, "23671350-d939-421a-a34c-3f554d4b7732": {"doc_hash": "c5aeee8e51fff1bcb6cb99241f1e893c728396333a4d24a0efab1fc403c2cff7", "ref_doc_id": "62d3220a-2116-4a29-bcf9-e84cd6910dc3"}, "e053b369-8a3e-46cf-aa99-440feb5677fe": {"doc_hash": "795d4e6147d7ef0a264cf8edf49a090b0e67ecc5b52dfdac621d8658da66a5aa", "ref_doc_id": "2d51054d-6664-4ee0-ba71-88f5bacfdcd8"}, "11523e6e-5fd3-4123-b275-375c917397d1": {"doc_hash": "ec8f3e92d03f212e15fc1ee093304c5c56a59057d1e24d28fb6995c53857284b", "ref_doc_id": "4690e286-763b-4f23-b1b3-009c74df0c5e"}, "013a6a6e-d551-4110-8f1f-950c49371b8c": {"doc_hash": "cd963e880376a43878aa41ffc00a7def3bb305f7f5d609c224f01f6e61aef383", "ref_doc_id": "f5bf4f04-3d68-4d59-be14-3da4c8094397"}, "3bf5c54d-8ba5-4445-a070-6b43818060c8": {"doc_hash": "e75ca82b7ad9fb23556941b75a0c12acc7cd33dd5d500f49f78d262c8b5865af", "ref_doc_id": "0749e919-af60-4d3b-8b3c-333a623117a3"}, "5105cbbc-f838-4a7d-80cb-4364636a05fe": {"doc_hash": "717db3dc80ab17989cbb83a074eeb8b7ef4cbf4f63240c7b907d760ca4209e7c", "ref_doc_id": "0749e919-af60-4d3b-8b3c-333a623117a3"}, "a945534c-7420-448d-8e87-c6d33d2c3cfa": {"doc_hash": "0429d99943f9d6d8c094800a690026cdc927533d81075ff36f8499a93d307c9d", "ref_doc_id": "ab5c9b84-0d38-4cbd-bf8e-62a83f09b1d1"}, "a9e90fea-f607-4f35-9541-9f0482a78a9b": {"doc_hash": "a7bc7b5f21911d462f67bdd612b7379ad858110d63c55881eba390c21d07dc6b", "ref_doc_id": "ab5c9b84-0d38-4cbd-bf8e-62a83f09b1d1"}, "996b2f85-5c85-455c-8ace-695a13f8abc9": {"doc_hash": "8d02807ac3dc858dcf07e96b21aa722b4b369f4a50dc5374b42535985e366811", "ref_doc_id": "fdd9df16-251a-4233-8269-5b21371850ae"}, "52fbd477-65f5-481e-b6b1-62ae318a58d6": {"doc_hash": "1fa162292cb36c68142847ed8a0de8094212b13aba7abeb3a16213bbc7a598dd", "ref_doc_id": "d6278ccd-0a3b-4828-be53-7a715d1aa418"}, "a2839109-e68f-48d8-a71d-0bd82f5fc770": {"doc_hash": "2cba3219eb9f2c4f83acadb3bc4957d8f6690f1eab027d9d20224fc1a478ff66", "ref_doc_id": "c276e4dd-7eb0-47ab-9fe1-01c8d3880ea0"}, "6bb4d31e-11ae-420a-9df9-c37af4cef012": {"doc_hash": "776bc26180beac1d5565122c13be264d6291da5fec37ed2ef132b5719e7129e7", "ref_doc_id": "4429fcb9-241a-44ce-bbbb-28be8a228e9c"}, "bba56623-699f-4a36-89ce-8558b05ab7dd": {"doc_hash": "63d2699393c6a015a3ffdfb16f33eaa9cc7c09dfa8ebf6ffdc0bdd49bddfdb3c", "ref_doc_id": "4429fcb9-241a-44ce-bbbb-28be8a228e9c"}, "40825576-2e7f-43cf-88cc-408564d9c22d": {"doc_hash": "157afc320a23c8ef0428896138c1e20581c840434919dbc4c8b405ff974ea0cf", "ref_doc_id": "de6ba9c8-ec35-456c-a781-e8ecc34e7bec"}, "af5df226-9f09-4a63-97b3-4c3512500483": {"doc_hash": "6aaca95c81d5aefc82f1fac2d4dc00602e4df9a67da3560c1fe580be2a70bd56", "ref_doc_id": "de6ba9c8-ec35-456c-a781-e8ecc34e7bec"}, "b622cef4-8291-4992-8708-e2387de255ec": {"doc_hash": "5c84d4a6bc155dde2a41552f7347960c40a875bbb095ae5f21048a67b54ad305", "ref_doc_id": "4c57bff8-dbe3-4a5c-b732-d9d5217d4aab"}, "de61dcce-8a06-4fb5-bf5a-90071c8c0101": {"doc_hash": "6ad968ffead6e7dd19f6190a8f7da0d3402090de8a4cbe76f4bf9db55db44ea6", "ref_doc_id": "1239de6d-e7d8-4883-be0f-190f81e639b1"}, "2fcbd6b2-9a38-45e8-a1d5-8cdfd676801c": {"doc_hash": "2ec9ece8afc5c5d8abddd879efa40e88ac1741063503f3f03e2f860b6cff1248", "ref_doc_id": "1239de6d-e7d8-4883-be0f-190f81e639b1"}, "c77a97ec-8bdc-407f-97c5-63f6a09cce52": {"doc_hash": "93d897ceb1935907b516c7e1d4a07063e9b0c49a7e4ba0d20d864f45be7e757e", "ref_doc_id": "b56ed615-ceeb-4964-bc9e-bb355b4355a4"}, "1d6de4ff-b77e-4757-826c-0492e208a5d2": {"doc_hash": "f82795d07466fc6acffefde58102ac77f1ad88d9d33ea3b21c8088e56e8e7107", "ref_doc_id": "b56ed615-ceeb-4964-bc9e-bb355b4355a4"}, "84b0b2eb-0474-4cfa-92e4-6b2a309e16a6": {"doc_hash": "915a8525568c07096ce9701c7dfa85d0c4d1b3f6d2042e6a5ace4b65159b8761", "ref_doc_id": "f5b49194-ea72-4320-91dc-edc64de669a3"}, "0267f680-2bfa-42ef-9f4a-7d8f0a609a22": {"doc_hash": "0ba4a599132e93700236a9c83df4465840babe561be31cc955948dbe9d6864d6", "ref_doc_id": "c28fea0f-0f26-468f-9e3a-cab565d71d0a"}, "f3e7fafd-088c-4f6e-a8c6-1a9b3ed09427": {"doc_hash": "b9db3ce904fc8c8f599d57b3012640ea2856f4e2dafddabaaabf0f696309c2ad", "ref_doc_id": "a1d3396a-359c-4e00-acc0-82ff635a98ab"}, "3b30fde4-09c2-4aec-ac2f-b8b391196e19": {"doc_hash": "532330e410fc1189c39c3d54ba237e42dd9faf016adb6a08e73d5b98d3e212a5", "ref_doc_id": "a5f9c112-b2e1-475f-b844-66038abff1f2"}, "f917a2d9-acc0-4957-ac3c-cbf5e0ab70c5": {"doc_hash": "3737ba1df978209266acc85ad6746d4288097ea36fbd672d1a760d7901d636f4", "ref_doc_id": "07f910cd-30a7-4e0a-9609-1f16f392929a"}, "2d1016ca-1c49-4391-83dc-ecd0f24fbfef": {"doc_hash": "cfcf10e14223db6ee0e854a5a22a99d51a4f1485539a50ffdc3a8ec60fed16f0", "ref_doc_id": "4e9b9781-5e82-456a-80e0-5c8d45b01b1e"}, "59f71801-587b-40eb-8617-935de9c539ce": {"doc_hash": "cbdc7b352db77d46117fea7a20c46331c966237148634735c2af018d3c9d8e10", "ref_doc_id": "8d1ec2ad-747e-4253-bd91-4f9272e45223"}, "960b7b58-d6f1-41fe-ae5a-e5cf598e6725": {"doc_hash": "75d41e4ec5f6884fb49732d810a5cd26a8043802ad30496f1b7c2e3ca735ec42", "ref_doc_id": "8b5862e5-b71f-42f5-8b94-01fca00edd3f"}, "ac8e2224-55ac-40d3-9f53-94409eb8c01d": {"doc_hash": "63b6b895217dd71a9564a7155a2d66645a049400e5f098800a130fbc8c0ded5e", "ref_doc_id": "b7108cf2-e11c-4ad3-a4d0-017e967b34c0"}, "ba2fe01d-c6f7-4a63-911e-845a1bc74cd6": {"doc_hash": "3775ee5756b6d0b44860229420bd4251953676f3eab459a8fa6a5f05da3c88f3", "ref_doc_id": "4323689b-3faf-4181-b1c1-fdf3ec3d431d"}, "9e2a3aa7-b0d5-403f-83fb-12fe8ab67bf3": {"doc_hash": "61c68fb2429e1b2bcaaa6e38fcc367cf2a72ae77b6b82417ff527767f271ccec", "ref_doc_id": "4323689b-3faf-4181-b1c1-fdf3ec3d431d"}, "74e498c4-57e1-43de-9e64-3e1fa97cc358": {"doc_hash": "ce95fb33d368c459c60bbb0a735f79bbbcfd23bed5701594a97d6285dbffeadb", "ref_doc_id": "e2d43306-33c1-47b5-99c2-03c72d333ba4"}, "b59250d8-29d8-4934-81f4-dfdd751378ea": {"doc_hash": "e4cb3d6a29338b814167bb124b78bb173cc6bbf31c40f0b5bc63f55be3eac60c", "ref_doc_id": "64ff6217-aa25-4300-af02-ab77711ac109"}, "c382a921-4270-441f-a39f-01dc0475a6ef": {"doc_hash": "9e003fc4eb548fe6e4aae2e44cfd20c91883ef5cf586584df17a78f3ffd2ed16", "ref_doc_id": "2ffee998-698f-47b8-84d1-f94c271167ac"}, "1e822eff-880a-4580-bd2d-828068c0a735": {"doc_hash": "a610145554cc82b0a53d174ad917aba240efa8c4de766e49ced6beae306d01ad", "ref_doc_id": "016912ae-85f0-4be1-898f-0279dd464857"}, "fbce0d03-07a7-405a-a7d4-778ffdb84cc7": {"doc_hash": "4cdedaa7e6c74fe63626a9bb5fbf4e1c2af54307f29460f632c86d7bf8bae88e", "ref_doc_id": "016912ae-85f0-4be1-898f-0279dd464857"}, "33accfda-5170-4b59-9787-b813702a79cc": {"doc_hash": "50e989b819aedfc803a47edd6e573968379aafc8544f756ae796fa0975a7dcc0", "ref_doc_id": "eca9605d-a892-4b3d-8858-7c72e3405936"}, "88a967fc-b8b6-4a22-b679-930c0d669887": {"doc_hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b", "ref_doc_id": "4fdbe99b-a72f-4ea9-9d58-1efbbc0e8af8"}, "96c3832a-9271-46fd-ad6f-bef965528748": {"doc_hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d", "ref_doc_id": "7da3d723-b838-417c-bbe8-a24a50b96ea4"}, "69557b56-d695-4be0-b06c-a16756271a7a": {"doc_hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f", "ref_doc_id": "60e079ef-a199-49c3-ac39-40f24a2eccfa"}, "352ba65b-fb57-40bf-8ddf-ce1e29548366": {"doc_hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe", "ref_doc_id": "128bdb06-8cf0-4df1-b56d-fdd8ffd2769d"}, "deef38ef-2baa-47ec-a91c-8b57e945d812": {"doc_hash": "b5899512ab126ca7b0cc5fe2a825e271dfbea6ea3e49469597376829a500a0db", "ref_doc_id": "689b0e3e-8117-4195-8759-dc5bb0960102"}, "34b1e964-5f20-4079-b6a6-3bd00159c71d": {"doc_hash": "7e3111e040a674730fa702d7902f35c16b8c433334150ac0ae151ea4fab47ce1", "ref_doc_id": "689b0e3e-8117-4195-8759-dc5bb0960102"}, "4bdfb0db-8258-4e14-90b3-cf81cd649223": {"doc_hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4", "ref_doc_id": "a2b2a7b7-fcec-4af1-b893-a7a5f1b6fb6d"}, "f5068953-bb3b-455b-82ef-0de4ed741b8b": {"doc_hash": "e95fa380b755b4c34d72a363da79a8c57174e8d488e9a48e2fd996853cc786e8", "ref_doc_id": "292d5589-3184-4ff3-b433-03daeaee1cba"}, "4579b779-7f65-4b09-8941-acf64d4fcb5d": {"doc_hash": "231a44d96b44ef7cef97f4d52fc7bf0c58617c4a316ef1d4aacccf09c35c8c27", "ref_doc_id": "292d5589-3184-4ff3-b433-03daeaee1cba"}, "d2912b69-f359-4b7d-a54e-36e1420fd862": {"doc_hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84", "ref_doc_id": "b0570d2b-7f87-4bb8-a029-0c83c23aea38"}, "49cb3b07-c632-4a00-9571-c1ad136785cb": {"doc_hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0", "ref_doc_id": "69d4d7ff-482f-4aa8-afeb-114aae7cfe25"}, "505ae009-11ad-461a-a326-c1f403176281": {"doc_hash": "73a1d174247c6265657797b59c09c64dc8a0dbbe89ce49cf76e1b363e58e5267", "ref_doc_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15"}, "3a81662e-6745-490d-ae37-d99a2d958098": {"doc_hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "ref_doc_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15"}, "5b105eae-7e29-4e0a-afc9-2d7b65c3c77b": {"doc_hash": "a33129b68cd64ada00b9243cecd3233f935c166bfedda87a4cb2921a33d65367", "ref_doc_id": "4b8f6a82-8d7d-4def-8709-c09dc876bd15"}, "72929673-b437-4081-94a9-57ce6c25073d": {"doc_hash": "f30400ea5c39a8dda2862c4f5a6ed31adb53965d1dfd780b04bd6e1abb680842", "ref_doc_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4"}, "fffe6a54-4d42-4a14-a05c-9b64cca9e222": {"doc_hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "ref_doc_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4"}, "ab1b2ed2-72ac-4b0d-b667-e034487aa3b8": {"doc_hash": "5b6cd1ec36c79c626e603648645c27d2c8bdf7405d134fa5ee8d442654faaccb", "ref_doc_id": "7394abf3-6e8c-4750-86fd-81dbee960fa4"}, "a4adfed4-ccf9-4e72-9c12-86d00ae9a256": {"doc_hash": "f5cd5448c9452cbe6dbac1ac03e442b08b61542ba0c8aa8aaaffc80752815b7e", "ref_doc_id": "9133d5b3-b632-40b6-b387-ac0329d734a4"}, "e2659d62-fb92-4038-b514-3c9317d1abe4": {"doc_hash": "275baea2efd06dc7d04371ac8ef8f549db38d8eac1441e281d806a98e93a9ea8", "ref_doc_id": "9133d5b3-b632-40b6-b387-ac0329d734a4"}, "7b8fc04a-98f5-47f5-9d1c-7e379229a881": {"doc_hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb", "ref_doc_id": "554e003a-18f7-4226-84fe-3021466167a3"}, "b470ba55-e6cd-4cee-97f2-c562e542ebd7": {"doc_hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073", "ref_doc_id": "817bba4f-a97f-4a48-90a0-de713b5685c6"}, "b32d07e1-df88-4758-9b50-025a29a84a7d": {"doc_hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160", "ref_doc_id": "506cce11-d72a-405c-a08a-4ef5f19ad9bc"}, "6daebd2d-2c2d-4d8c-b144-b95afa7451fd": {"doc_hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28", "ref_doc_id": "0996ba57-66b0-4db1-b8b3-cd9fa2e3b876"}, "408e5691-099f-4223-b137-d9c86f1dfbfd": {"doc_hash": "00119033c6c21ba634c0f31ec079ed765db6f54b4f9569e4c2947c753f6644b0", "ref_doc_id": "297ac6a3-89db-4283-91b6-8ff124b88fd0"}, "db357411-c164-4101-8211-5070391e40d4": {"doc_hash": "62816e0b531a77b544cbbfd001f5c8c7767e6aa087dc8bc5a6d4f92170b12b5e", "ref_doc_id": "297ac6a3-89db-4283-91b6-8ff124b88fd0"}, "cdd7138c-81c1-4e18-89c3-02b02d81d930": {"doc_hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2", "ref_doc_id": "765b95d7-7a8e-473e-9cbe-e435fe78268a"}, "5fd0f356-20b5-4be9-94f4-f45f8e44c155": {"doc_hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451", "ref_doc_id": "c478a956-8f9e-492d-a6c1-6ae1cd7e6a9a"}, "9ff732f0-d8c2-445f-8291-0881fa48edeb": {"doc_hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc", "ref_doc_id": "43782e9d-aa09-436f-a65d-7b7a5b82c66e"}, "9bbd7ac7-afbc-4c94-a72c-a7c0e8d1e597": {"doc_hash": "43fcbaf6b2647b08c4e3722db9672a4e54bd90085ad50eb701650750434c509a", "ref_doc_id": "df00ac50-61b0-4923-b855-15bb7066d27f"}, "3a17bcae-60fa-41d6-807e-d6394d06e32e": {"doc_hash": "6cd12d9c0749d39671f9a9e094212289c34c2ba057b4e0502e77f2ce6ce02fed", "ref_doc_id": "d8850640-e1e5-4d63-a1b6-b0f874a8eac7"}, "9777acee-6cc8-4661-a04b-592b172a6a01": {"doc_hash": "ca9d3583962f6888b09659920290d0b59a977ab0da9c6582d6a3eee21d1ccd89", "ref_doc_id": "1aa02e05-6795-4fb4-992d-ce56d9fc6590"}, "9e074fbb-0a90-4dae-9a3a-25e6e604a35f": {"doc_hash": "95ac358bc219b476177ea99f84bc1a2107242b498f089f0379af0bdb6a80075e", "ref_doc_id": "8db11412-4ed1-4c39-91dc-1c155d197d36"}, "35314791-4a1e-416d-908c-f7668f875876": {"doc_hash": "40d2e806636370700174ff136ff612f035ba871b5f7658fbfb9283bee1d6c3aa", "ref_doc_id": "bf4f6a01-18f1-4d3a-8c8d-8710bf99147f"}, "880ade31-999f-4e82-a6ef-78550859d536": {"doc_hash": "477b973cce4001a36ccc3d84301b49e39432cc81d849c2d90506a3ceee57d707", "ref_doc_id": "fa86fa59-3492-42b9-a42b-f32b374b009b"}, "79f530ba-9140-4a7d-adba-5a41c28b0da4": {"doc_hash": "a21f87be1d7c7e0fbc2b066f74f81c60f0919308e1e456d4be2cbd27ca4d411c", "ref_doc_id": "e9bd26e1-2124-42b9-afcc-d7a70506bee3"}, "de2f1616-35f7-41da-a1a7-40d3d6a9b107": {"doc_hash": "5bfa94367e0221f9505312a5a1e82a071e116a67c28ec1b9a78b8eb479d97d5f", "ref_doc_id": "e9bd26e1-2124-42b9-afcc-d7a70506bee3"}, "d3fc9fef-176e-4336-8bfe-29cf61ac25b8": {"doc_hash": "ec56454cc530e91ce85c97b623342e819a871cf7204e355611ecad6ce788cdd7", "ref_doc_id": "5d441ac7-0382-4003-85c4-e87e55ba43c0"}, "3443094a-9a77-4792-b5a2-25acba5e3435": {"doc_hash": "4c17104a17166eb34db6f9d78783869750b2ea9d9fbd1559950461ece6e7cf85", "ref_doc_id": "fc7115c2-4292-4f6a-bd89-2f0a8e055c4b"}, "53787251-5f15-4db9-bbff-7827715814c3": {"doc_hash": "2c95329d32d4e57f1527e36c9da1ca96f3d4da5c7cecca1c1e6205c88be8d859", "ref_doc_id": "fc7115c2-4292-4f6a-bd89-2f0a8e055c4b"}, "e64f9311-e1c0-4873-a0d6-64d5389d9d07": {"doc_hash": "d34461b1bd3cf6914fe9c7bf3b9255f656c71b0688588b8978638bee9f7119d4", "ref_doc_id": "2841b048-9911-4aa9-9cb8-fe075bc96d24"}, "aba90739-dd09-4f5c-a892-7283ccf6bc35": {"doc_hash": "dd4671b0f9841c801fe37c048221da59ab02b65007d35b3db431fc011fe82166", "ref_doc_id": "227f9f2f-a929-45ce-bc7c-f8e455f35aab"}, "b998e5ba-567b-4c7a-b8dc-7895b7d20daf": {"doc_hash": "f3fe476189f27b793bfb6d78a96edcc314af5b9da128933069d6b945ebe5eb4b", "ref_doc_id": "227f9f2f-a929-45ce-bc7c-f8e455f35aab"}, "656d7a6c-7def-4ace-9419-982f795e1876": {"doc_hash": "a5fb88a0ebaca8929f42a5b242596167bfb7f220431752fef37aef4118cbb63e", "ref_doc_id": "7bb4dab8-4e1c-4152-831e-706fc4f6a767"}, "23c21fb7-566e-4a53-a9e5-f97ba185cbf7": {"doc_hash": "dbec30e2c433bd39229707136922f715ef7ec11d0c2566b3bc4089fd3cc30db5", "ref_doc_id": "7bb4dab8-4e1c-4152-831e-706fc4f6a767"}, "80255d6f-d21a-4c7f-a0fb-26b518e6e821": {"doc_hash": "521d70979914d589b6c7dc989b592f7dd9d8cb7bcfff2ccf6a5e2031c90b1d81", "ref_doc_id": "ed2aa957-10f0-4448-8439-f32d70a39b30"}, "db47444a-ca0a-430b-b735-8340c41c98e0": {"doc_hash": "ccc47e3856ac196c782e5b0f07af7f3141afe7fb085fb1efecc3b69d2fa0ece6", "ref_doc_id": "ed2aa957-10f0-4448-8439-f32d70a39b30"}, "f9ea7240-5429-44fe-9077-77b5f784458f": {"doc_hash": "fb39b5c54c31f7bb5526d2cb89556b807e7134584254eb4fda41e73488d39a4f", "ref_doc_id": "abb0738f-c997-4c04-8e83-ebbc06f5917c"}, "05dfea33-ecc4-4f46-907a-b9270bc86f1a": {"doc_hash": "744f503866973b69fd6a11da0aabd2c6c77e342f7ac0a3cb412266792945590f", "ref_doc_id": "6caf45d1-9568-467b-8133-777c06fe22dc"}, "741bb11f-cf11-4931-b14e-83a90fe593ee": {"doc_hash": "c1d8a6560e25abecacfaf1945ec66b3426da9b2d501cf17a214fb03aef0d9e9b", "ref_doc_id": "6caf45d1-9568-467b-8133-777c06fe22dc"}, "43dc2345-3388-4c14-bc4d-012d3535b56f": {"doc_hash": "b6e2fa6032d0c12df9393f2a7cc890d1e638a35a026a7592785236f30b086b32", "ref_doc_id": "c1c1c936-ad0d-4985-b574-0d86fa19e532"}, "d4865e2f-e05b-430d-91a8-5e3ac06e19b9": {"doc_hash": "f7e26326e210a83af2f0fe2e653e2984673b97158326405d1ac4429d0d167bc0", "ref_doc_id": "d5404f4d-381e-4f20-8783-5eee5b041b01"}, "abde9292-bfc8-4e86-8f87-28c69ee56044": {"doc_hash": "1c801c609686d2530e94c2b39fe86460771b36f385b8259a3b74185de7f198ce", "ref_doc_id": "ef0e2448-0867-45cd-bc5e-34ea0e7dd7b9"}, "845b5e1f-77a1-49f1-91de-75733653c480": {"doc_hash": "70f2910e9f1cdc59c0b1ac5ea864b5dc031c5674807e5f24d7547c96ca7def2a", "ref_doc_id": "13bdbcba-8f85-411d-969c-93052675888b"}, "de79ae85-0b90-44c7-b1e3-a8dd0b493621": {"doc_hash": "d6f3588fd8a1c175f54e65061c0b69a346a67b45e9d5291036d404f297aea383", "ref_doc_id": "94184d0a-fd12-463c-b4de-5841883be5ce"}, "8be1cd02-866d-4572-9482-5d1733527fe4": {"doc_hash": "2f1fc5e372c2f21485fa4218db2884f66f5168be6133defe4f91126ecd00373f", "ref_doc_id": "94184d0a-fd12-463c-b4de-5841883be5ce"}, "5c398b0a-1cfa-476e-9030-0e02a96e661f": {"doc_hash": "6cd504a0dc010b57dd1ed78973e52f96e52b17c4a04ee49a0b29e3bf95c670ff", "ref_doc_id": "9dc2f831-c9ad-4c45-b3e4-070693cb9752"}, "d39bcd39-4bef-4b1d-a0d8-16048d54422b": {"doc_hash": "4b04b1f7fd7966ff907c9654274fbe062334797b2b8e47f82fa62b121328ddd2", "ref_doc_id": "ce43e60e-deda-42d5-8fc0-9b6da3642657"}, "5dd3cd0e-e0ae-4e47-a32d-66eb1323bbfa": {"doc_hash": "bfdd2f481bb89bcfd8746f355bb94cf4207885db81135a3bf31a85c8b8e80eff", "ref_doc_id": "01deaba2-86c6-4349-8b4a-bb4bf0319949"}, "e85ed627-7757-4f92-8f7c-834505713d79": {"doc_hash": "f23eaceafaa435742b4eea5fe4176c20ab90df3ea485a9bc31f9b4a2a682e261", "ref_doc_id": "c1f1200e-fad5-4c03-abf0-51801fbcd78b"}, "8072bb11-87b7-432e-87c7-c20a8465a5a1": {"doc_hash": "b7e04afede7c93e0006b51be8f9913932df5f050142c55fff1402c65676edbfc", "ref_doc_id": "8298f5eb-1298-4b53-81cf-c7f00d278920"}, "80208e01-50fc-4a12-92de-7bfa947db644": {"doc_hash": "abce1c3e04774ca26ca0e6891c328356f20734faf88b7fc6f83b01d881906309", "ref_doc_id": "ecf63efa-60fe-4978-8b13-3eec2d512d6f"}, "d19c8e45-3266-4bbe-8c59-0d5148085cec": {"doc_hash": "a9acbb67c1eaf44d9a96dd1c2a3b299a8e9eea05db6605300d23f5b8882dfe62", "ref_doc_id": "c74d7cae-3088-4e9e-8f69-ac5f0c4715bc"}, "4c70ac67-851a-45b9-b24d-80d83ac0c01f": {"doc_hash": "17b29c624a32f5e3d16a2738005a0d3dee5ba1916f506dfcf45808914c005900", "ref_doc_id": "c74d7cae-3088-4e9e-8f69-ac5f0c4715bc"}, "ffd27e79-57c3-4fc0-97de-1ccbc8d80683": {"doc_hash": "9b999a59ee573f0f0f891624231460be8aa1d814dd8d8a9cf517195c20026b9c", "ref_doc_id": "88975579-24bc-415d-b4ed-068de3542f92"}, "d9ba4453-af93-4380-bbcc-3c90e268eda7": {"doc_hash": "d40df4c740845e7e0a62407275e2af8c4fd1120ac6162a6d1f82ee80d46bb376", "ref_doc_id": "d640b365-9338-49ad-b5b7-2db345f53525"}, "2c4a8c9f-7d12-4e17-a46c-bb3a86b65124": {"doc_hash": "1403f4bf7efd9e36252b8d8f65506e66fb4130f0fd842433264be023777e2da3", "ref_doc_id": "d640b365-9338-49ad-b5b7-2db345f53525"}, "40df9145-a397-4196-b6cd-bf363bef1d65": {"doc_hash": "8342c405966443bc69c5bedfa5e7a68b4124c566f46676c7b7dd1db8768a361f", "ref_doc_id": "0d7a532a-8a66-45cd-9cc2-a9ffa73084f4"}, "861c327a-777a-475d-a6d1-837b5ff6ae80": {"doc_hash": "c46c84f5bc4187f18054c58e36e7b7ebd1353f85c1d27f8b5d870e494b20e02b", "ref_doc_id": "0d7a532a-8a66-45cd-9cc2-a9ffa73084f4"}, "9a322d3f-2b43-4061-b1d1-abe99b7ac834": {"doc_hash": "dee12f430b67421c89682681a6362891224f528b6bbd603e8e1f36476e09d746", "ref_doc_id": "6b5ae73e-e59f-47fa-ab4e-839672fe048a"}, "c52b97d3-0a8c-4772-8bdb-e4bd66770eb7": {"doc_hash": "09bf31a27bf8071377952fae53d6a5e0026e654b696f25d20b083406362668c3", "ref_doc_id": "3676db4c-277e-469b-bbe7-d56d40f92dd7"}, "0fd9c239-438b-4139-8580-13f8333ebeeb": {"doc_hash": "caa54a4cc92251dd6fda6f9c3a4845f0e8d71fa9b9afd4d28c879ae0a741ed81", "ref_doc_id": "2105a07e-8707-4901-be51-9087aa3ddc9f"}, "4bc28f09-664d-492c-8ebb-7fb9d4d3e9d3": {"doc_hash": "5fc69a8fd6ca503c2de22d1ae78f5a22de235da51d685366551580caf3f986c5", "ref_doc_id": "2105a07e-8707-4901-be51-9087aa3ddc9f"}, "aabe4c2c-e3f1-4d40-bfda-019374197927": {"doc_hash": "61d473ca9ca8eab7e31ea8e8dfb7d39108289ddd5f472ded6b4a983094de2189", "ref_doc_id": "78600fff-960f-468b-9296-3f47b3e5d22c"}, "ab6d0bda-a7ca-4a52-a085-88545b1778cb": {"doc_hash": "cfb4acdcfc298f769db3d1f2f4ad3a2ca087b56519725cf19ce8ff63af195bc1", "ref_doc_id": "78600fff-960f-468b-9296-3f47b3e5d22c"}, "1a0ba75f-2552-46b7-ab96-05e693a4a6e0": {"doc_hash": "e35730fe7f2037f5916589de3e7c5f95c28dc53d73d71fc87c05599846357185", "ref_doc_id": "52d90776-0151-4b29-bac9-cb34572422e3"}, "882bef19-bc76-4b65-94a7-6ef9dbc509d3": {"doc_hash": "b167060d3e02c1aa5c9b028089b2fea3ec495bfffed01b0dc64f55233a428ab2", "ref_doc_id": "52d90776-0151-4b29-bac9-cb34572422e3"}, "b81694da-c410-4267-9bb8-c3834314e00d": {"doc_hash": "0e26f273e04a09cd7e094b93c4d56e9d57c5dd62b90570ceb334f25b45452c85", "ref_doc_id": "ea7c5271-9630-4231-b068-5ab591a699d2"}, "c0476b83-bc22-48a5-9142-0b55717d8e1c": {"doc_hash": "cc20109f681c04a30d3b25a023be9469c21d8fee666129cce2361487fc49e00e", "ref_doc_id": "ea7c5271-9630-4231-b068-5ab591a699d2"}, "54649484-f0af-41af-a4e8-a218e850c908": {"doc_hash": "117f928a7be65079b0d87ef5f03429628264dbd1cdab3686ddade481443b2705", "ref_doc_id": "7afd3e10-1ca2-471d-a621-75c1dde510a2"}, "ccb4e6f5-e09e-46d0-9a7f-aa5810c42bd5": {"doc_hash": "0353d1f1215b0209253960b88a53ba2e1b53297db8e174f848dfa575e92a25e8", "ref_doc_id": "7afd3e10-1ca2-471d-a621-75c1dde510a2"}, "405eea68-78d0-4531-b0ae-3f57b94509a5": {"doc_hash": "7e72037e15945596f47d8dddb15e1e469d0e6aa744b102f593eb81c4f56e2378", "ref_doc_id": "6f9f3699-81e4-492d-9b2b-1d8d500e14d4"}, "d60ec123-3928-49b7-8155-2e418d84de73": {"doc_hash": "d5ee7d3902f4eaa7280aeb3181aef2ef8e7bdff1db5bfa94c1fe464fc04fd51e", "ref_doc_id": "6f9f3699-81e4-492d-9b2b-1d8d500e14d4"}, "b0936e70-74fd-460a-be27-067e462b545c": {"doc_hash": "89313fcf134c073708156bb3d48ca34acc98c5370cec3108a89faddace685642", "ref_doc_id": "89859f62-6d25-475a-ac37-d8d46e974607"}, "848fda35-49cd-4b13-be0a-325521247386": {"doc_hash": "38c4b1c9aa1d9cf3eef6b0754cde66b5b7abdf592779586c9eb5a25c870e6f3f", "ref_doc_id": "89859f62-6d25-475a-ac37-d8d46e974607"}, "761e7da4-8852-4cbc-825c-af3c81d9974e": {"doc_hash": "dae2cc0a9c52e9eb6307e26fe031f5c93b9e5f3b7100e55f839287c440a86e43", "ref_doc_id": "0825c0e2-b0fb-4a2b-8fef-ac7f7fbaf7f2"}, "e54863c2-008e-46d4-be11-8da57d5c32e2": {"doc_hash": "bc136335635a9ad0598695106c1f37a87fc1b9e0cd6540244518600c0faf03f9", "ref_doc_id": "0825c0e2-b0fb-4a2b-8fef-ac7f7fbaf7f2"}, "f7126876-5083-46a7-b415-2f3bf5499a60": {"doc_hash": "2687cdbb666191bb1d7803f3d17c254043a54010cd7cdbb0e2fe5a0dfb77276f", "ref_doc_id": "1095d662-7e2e-4dc3-9f35-abcc461be4b8"}, "0ba60313-48a5-4580-92e2-218ec42bf988": {"doc_hash": "2b01132ef2d6a08ed9aabcf37f9126a9526fb8513be3d5625ac265f40d5eaefd", "ref_doc_id": "1095d662-7e2e-4dc3-9f35-abcc461be4b8"}, "42af3159-369b-4328-8b74-a84abe48f9b9": {"doc_hash": "519c1b89d266a3a9e5cc5e350d58ede0b7de8c2067e80068b20ddef439da6780", "ref_doc_id": "0106afbf-b82d-411a-8a1a-848383ba3673"}, "3ade21bc-d790-4fbb-bd03-8b3749b9ed00": {"doc_hash": "7360d14b81463b68efd96902bab3ab1449a6bcbc3381cc8ad19ee5630b23d622", "ref_doc_id": "0106afbf-b82d-411a-8a1a-848383ba3673"}, "a3356ba8-be4f-45a1-970d-deeae023650a": {"doc_hash": "5e903f64f5750a010e061b971b3ffbf97505820765bf9ef3a91d879c390f7906", "ref_doc_id": "25eafcc0-678b-4c42-8644-ded427c5a99e"}, "ae308dda-de14-447c-8e8e-cc6ffd4d72bf": {"doc_hash": "ae0c6cfb7b230678640cd4e82fe21d297e1fba9ecbdd52bf82121a51713afec4", "ref_doc_id": "e29c671f-515d-4c2c-9106-348dfebbd811"}, "b56f57da-cfe6-4957-b283-5eceed098ce7": {"doc_hash": "b4902f304e232fb392899fb1905c2fccd48f647c03035c4d00f1c94d41c217a3", "ref_doc_id": "e29c671f-515d-4c2c-9106-348dfebbd811"}, "8bf96c12-68d2-445a-b887-1bd900e22f0f": {"doc_hash": "83d694cb947b48837f4f17e68380b39f082d45e6364ed70c4f841fabd27134bc", "ref_doc_id": "87201f69-a510-4839-9328-3fae3cf3af3d"}, "52baddc7-461a-4be8-bde4-d00f50dba61e": {"doc_hash": "70bc055df6e11ad25b28f793e026dfeb95e65bcc3c0e1c647429bb434b3f1622", "ref_doc_id": "cba32d09-f544-49b8-9e82-99d49c199884"}, "6a1f7780-dbf9-4a95-89bc-ee6e1acc0428": {"doc_hash": "1c0dfa10c1831094692f0a3b7f0778ccd003ae11258356a3da94d6a25d442bef", "ref_doc_id": "9d501f35-0963-49d2-8548-97a247a39c4d"}, "ca538e47-e711-496a-a2e4-5c2563d39594": {"doc_hash": "b9f1cbdfdc16734c278f25d5a671596886bc36ac629c4c2e19d302c9a9563f6f", "ref_doc_id": "e2ba21c9-dfb1-4a39-bdd3-9f935d27a740"}, "222e5948-86c8-45a0-a2bd-0a54387ea385": {"doc_hash": "fe87bfd37d71fcfcfbdf2f29c4ba54030e25a749c4d139947b690daf9e3b385e", "ref_doc_id": "69e278a4-3f40-4360-92a6-5a4003d9e0a7"}, "281a1bc5-7ff7-4927-a82a-8094c6826990": {"doc_hash": "89f3e4cf3986e8c242336f906f0a1ae3ecd89b358d30f0cae50e8f60b91ee7ff", "ref_doc_id": "69e278a4-3f40-4360-92a6-5a4003d9e0a7"}, "0b937449-32e0-40aa-a9a6-b20817156dbc": {"doc_hash": "4a1ee98846c4a45e5e46c9d23d40eefe0f31d8d6e4816c8d6f5fae503b91b803", "ref_doc_id": "69e278a4-3f40-4360-92a6-5a4003d9e0a7"}, "3c90e079-7bb1-4622-85e9-c84cc94f8a57": {"doc_hash": "1de90c881aa437d28e5cfd0a7360f4d32ed0b269b78673e9fb7abf20e900ad3c", "ref_doc_id": "69e278a4-3f40-4360-92a6-5a4003d9e0a7"}, "ca2493cb-040a-4671-a442-2e97db0df167": {"doc_hash": "f4aa90d687a246a15655b453b042e3b03408d6941b686aac7a0ab296540752e7", "ref_doc_id": "c48e9425-f7f7-44af-ace4-d8a338d7ff09"}, "25d99cd4-75bc-46f0-9acc-037401c44e90": {"doc_hash": "793f8240c64851b07a9041f591263c6055ce9cadfcb829abff1e2666b6eaebd5", "ref_doc_id": "1d1bd8e4-9640-4f75-9005-1084c9a0cb54"}, "824efe4c-d185-48c9-80a6-750a96533fd8": {"doc_hash": "dc5cc02e04548069848e9f6e92147af60abc0b7d56d9170dc59ce44538b3051e", "ref_doc_id": "1d1bd8e4-9640-4f75-9005-1084c9a0cb54"}, "5d452405-eeb3-41f8-a264-c4bc2bb2c651": {"doc_hash": "e72059b7aea8e874356c035fb890633ad018654b642010795ef6dab15dd5a8e8", "ref_doc_id": "a2e421cd-b4e3-48c8-bec3-c25643c0e7f2"}, "bd3bef54-a320-43fe-857f-e9b70fd8274a": {"doc_hash": "735a28dc122cf88a02a51d1d72c749db101667d12c5d50f878882e271557f38d", "ref_doc_id": "a2e421cd-b4e3-48c8-bec3-c25643c0e7f2"}, "1045f041-ceaf-4ea2-8df8-cb1e6e1dcaf7": {"doc_hash": "48af8248b72b0de4da2990019cdfb2596b1841c26d21e7d5b4efe2a4207bd1cb", "ref_doc_id": "45e2e642-e9b3-4f35-854b-399234699578"}, "1a2ac2ef-70a2-4456-a2a5-ef3f94b111be": {"doc_hash": "f9713113a3bbed31273bf0fbc3afc7ce52578705e16d93e1050ea0c2d91213af", "ref_doc_id": "45e2e642-e9b3-4f35-854b-399234699578"}, "f6f19c5f-87e6-4514-90d2-2c5d3f41da7e": {"doc_hash": "c543747271af17967bc9a40186fd562ee716de7801ab4bf7fbd0df9fd38ca06b", "ref_doc_id": "1d508908-1077-4695-8eb8-1fc17ec5111a"}, "7940baf4-e86f-407e-aa36-785357169a41": {"doc_hash": "4ded2e632fd2658394733b0643061164e16e8e8174f0c83dd736f93fb5da5c1c", "ref_doc_id": "fa18b795-06f5-43ca-90f2-7d3038fc9dca"}, "8f149a63-6228-4fdb-b6f1-191c4f2e9f70": {"doc_hash": "7b10edc052aa86afd503a71b451c09aa16a18a809ed151a6d7fc7476ae240ee1", "ref_doc_id": "cdacf8a0-0029-4d74-81e1-b539424177ad"}, "fc7959ba-8435-41f2-87d8-fe95879d9025": {"doc_hash": "e3ddb11abcf69c2b08d164051e5da81dcb0f532efcbd40ad95b2efe73b2a3019", "ref_doc_id": "da8e26c2-cfac-4aff-a81e-18a8275c1324"}, "3cb937e8-489f-498a-9b14-261eea11cd5c": {"doc_hash": "8df8e7aa45d99a7e5f58092fe116d5ffecdc3c50c2a3367c32f544141a905d92", "ref_doc_id": "da8e26c2-cfac-4aff-a81e-18a8275c1324"}, "ea948b2f-6610-4b0a-9a51-e9c9bae2d7f3": {"doc_hash": "c35cd0b8038a09a9fd4e30cfdc7670cea824065c90fe0d34d648f16de5ba3271", "ref_doc_id": "6195b65b-3a36-4576-8446-afd7f548778e"}, "43398376-a4e5-4ca6-9391-1cf5529b8515": {"doc_hash": "daa33e7df5401fcd09e483271c0cde06f678b00833ac78b9330e5fd81651366d", "ref_doc_id": "6195b65b-3a36-4576-8446-afd7f548778e"}, "fc9d2460-10a1-4ae4-a1ea-367ad4c03055": {"doc_hash": "3a06aa8c641c664e327c2348536a1be91b81fbbb0859584d7a8b08b908e6fc42", "ref_doc_id": "1d2b7df0-0d7d-4d43-a998-ac9bd1d04b63"}, "88b3022d-8bd2-4efc-acda-3a9824764c2e": {"doc_hash": "acd18904baf5d40736f7158a630e6826afc3e9a13a6c10abce159ae93fd310e3", "ref_doc_id": "1d2b7df0-0d7d-4d43-a998-ac9bd1d04b63"}, "d2c95240-b83a-476a-a383-0e1e892880f6": {"doc_hash": "418494cf0e108a1a85c75fab24f54950004ca3740a72b5322914909514e0a687", "ref_doc_id": "5ab13869-9119-45b4-9a5b-02a035680d71"}, "c0459641-e4cf-4317-ab31-e4e5731e1788": {"doc_hash": "44643d1d664f462abc620283789554a6d76be917ea960c007fbbc31c6710fbc6", "ref_doc_id": "5ab13869-9119-45b4-9a5b-02a035680d71"}, "0563ede1-e8ae-4889-88fb-5ef44c749ca3": {"doc_hash": "eea87a95bc68ea2b8583e5f40f323ef05156788f2671622f5cb7487644922ae9", "ref_doc_id": "64030abb-a341-4bcc-98d2-cba573e009da"}, "b704b4ed-8e14-482d-9603-9bcc0ae4f5f2": {"doc_hash": "11712e66676abaae612793bd2a99299c669ef4abc0488c292a49bcc80c628590", "ref_doc_id": "64030abb-a341-4bcc-98d2-cba573e009da"}, "424f8222-10fa-41f6-9a00-edb25f5345a0": {"doc_hash": "afd79592340971e1d97f3f47e88642945b79ea97f1272fdfa8132cac969923ed", "ref_doc_id": "b2626821-8be7-4945-b2da-61289c2fa823"}, "a017080e-4f9f-4fb7-b889-967007504c33": {"doc_hash": "fc752325f8bbee02f56a02f7b5b53b9590852d9ec7ef149f113ff865e4492862", "ref_doc_id": "b2626821-8be7-4945-b2da-61289c2fa823"}, "e3116f52-5638-4cd4-b917-5773b1015fd9": {"doc_hash": "a1de1a690a1224ccf2f5eb543468008d9d892d1000b8780e823f7e631446a036", "ref_doc_id": "0cfde65f-f4e3-48dd-88ab-bb761abe7b29"}, "5a07e326-d505-4585-b191-d284dcf310fc": {"doc_hash": "67b5cb2535a0c0d464b8e1bc6424f430884146253ee8e1a5dd8841bf244d88b7", "ref_doc_id": "ce9541f0-9a1a-4bd2-b222-95148eec5c29"}, "b4613155-f9da-4648-9f09-d785155c3511": {"doc_hash": "7d2500597185348f3d0c1d897bf0c7b34a5e611db5f2bbbd939b14ac9d038294", "ref_doc_id": "e4ee5fc6-3fc3-4fdf-ae6a-36b0b1f8fb65"}, "44a7d982-5a77-4a88-b135-e89f523cb4ff": {"doc_hash": "7a79d374c11221f9324caf42a20292464fbeb305e54d911d097a8d4bacf780df", "ref_doc_id": "99a75429-7639-43fe-a607-2df150a76a97"}, "baebd827-e5b4-44aa-bca9-6b3c1119b967": {"doc_hash": "47264044eb824a8b9b9bb339ddd5e89b8d99e1c92aa07d4ec85a33bb993c29d5", "ref_doc_id": "607c7cef-b614-41f4-85c8-c65623ec5c7c"}, "d3466bb3-3f61-496f-8d37-00da4d6c4055": {"doc_hash": "b94cd3d7e38db0fb6f9974fcebefe03ac4e9e0c3fcc23a19795ef36a64b1a2d0", "ref_doc_id": "f01f4512-d31b-4b34-8b1a-1216f77f186b"}, "3478e146-d55f-4b21-b054-3dfc060ed083": {"doc_hash": "c16a490174fb8177d21e702081c64031adf26bfb3b14bb0a06eecb293690dd9d", "ref_doc_id": "b55d0c87-2b99-4ea2-8be7-afabaa0adf0e"}, "3a2dd02d-062b-4add-b346-06a08f64221e": {"doc_hash": "746879176f02a6a3f928dab901c625031679d8e3758cc72b998bc6f85ea14040", "ref_doc_id": "a7c35154-1998-45dd-8613-5af301ba4af1"}, "9ac3f944-8d52-4b76-bc05-813c301ce072": {"doc_hash": "4eaafebadacfeff653f3cbaea4ded7c678d5b24cf814d9365023bc56578ea39f", "ref_doc_id": "2fccbbd9-0eed-4502-8b00-ee971fadb32a"}, "e357e088-670c-4842-b540-903fd4293b87": {"doc_hash": "83cd132bd4cf8ca40037d9b6dbeced312f8ba52ffb5c8ff01a98e44e1ff7236c", "ref_doc_id": "de2b33cb-3dae-47db-9394-f5866ace0cca"}, "eb060969-cb55-4f3c-9cd6-bba0a7ae6583": {"doc_hash": "b9e9b44ba240a3ebc1d0992db3e4686979ff73d663fd12f81ebd9749b75872e0", "ref_doc_id": "d9daf3b4-5159-409c-a2ce-807ba0233721"}, "57284202-cce4-4812-b82f-1a190f444061": {"doc_hash": "86c386183b20baac7f3718fdf3e70e5ed5a83256848fb9442bba62c734311fab", "ref_doc_id": "14d8a03a-2214-4539-9c5b-fd6b58968c9a"}, "c0584527-9447-4c4d-84c9-4c2665f997a1": {"doc_hash": "1419a3f24b3a30837ff9bb21bf9cd2133e6daeabddff28cc936f6767f743b0f5", "ref_doc_id": "2a156122-9e72-40f0-862b-a4301d2b2da7"}, "bd64f525-e6d3-4968-84e7-fc957da12524": {"doc_hash": "e8e0b8b9b9a20b3c19ecc85f1ad4737cf0fee70034e740353c83f4c9e884b342", "ref_doc_id": "bd65fa12-aa34-45ce-ab9b-71ece6f70a7d"}, "ee471490-debd-45e2-9bf0-3462f2620c33": {"doc_hash": "56b14f7c7ff6b021881725526460734d05e17c5849bc1227a4bfbe22e49dd73e", "ref_doc_id": "e6e5bf31-41eb-43de-a6b6-746fc8d52230"}, "546c059c-6c52-4502-859a-4b00394f2b82": {"doc_hash": "2b4dde41a0ea4c8b6f21a53346ffb53a846631d00744a44306556a4bf11e7b12", "ref_doc_id": "0c58e43f-b0c4-4fca-b572-806b3bed2976"}, "0467414e-4fc9-40ec-9dfd-c00b29530a63": {"doc_hash": "ba0af43d98285bb9d0096db17cfb95d90df98c843f71ba948b0a4657a7a30ed5", "ref_doc_id": "2edb74fd-43a3-402c-8445-7cc985047c9c"}, "11fc4d0b-0562-4d8d-8d39-252ca1c001be": {"doc_hash": "dedfce62f143964dcfb15f3e8e2f0484e23cb0557f6abb99c6149eb204981d66", "ref_doc_id": "8451ab4b-8d01-4a34-9d4c-411736de8603"}, "2f20a9eb-ab77-48e2-b324-e350c9fcfff1": {"doc_hash": "931452753203d5c15e75889009824f964fdc473ed218cacd99034d85af289c16", "ref_doc_id": "80285fa3-ae7f-47d6-813d-9d6175f53962"}, "8960c3e1-cde1-47f0-9a23-dcd6abf5e25b": {"doc_hash": "542b656533ef5e5f1093cea0f4d01388bfc7c597d26810bd7201e4a5a904b298", "ref_doc_id": "7440c110-a5b0-4273-afef-b28801e2dc63"}, "f09e7d54-03bc-4ad6-a68a-83c81274ab42": {"doc_hash": "4c59a7472986b7a1d6efe57b62b1df0d569f1eb262ef80abf1db0042d1536789", "ref_doc_id": "8002965c-6e10-4907-a270-6fd9c8097c12"}, "452565f7-75e2-4ca7-b623-15a97207f8db": {"doc_hash": "7ce0d15098c2f2b17976a32d05e13556754dd51d37cebbc323ffdaadf76bcc7f", "ref_doc_id": "76874a1d-d565-4127-b90b-e80cf519a911"}, "df2c35d2-8225-4634-90d1-b9508e83f8ff": {"doc_hash": "c148c965ca26c9e38b91cab14ead02d2599f89b797b18c4a1abb3cec43bad28f", "ref_doc_id": "ad79f79d-97bc-4b79-b12d-2c4725c583d1"}, "32ea1ebd-52ba-469d-ab6d-7eaa44308208": {"doc_hash": "a1b4c097829f499134980cf61ef4d8930d01611d6e512c7590553acd42297776", "ref_doc_id": "4f807cbe-5f10-4af6-a953-35220137d510"}, "7e01ab6b-b497-433f-a662-349705bb2875": {"doc_hash": "74932dcbe4fc8a27798eb33ce54335c6dfb8ce12d69d4d598a750547786ff215", "ref_doc_id": "295a4f87-907b-4079-bcbf-c9b0c4db1365"}, "c4d64bef-9d7f-4f36-902b-87689ce47641": {"doc_hash": "e22de1d6e12fafc4835643b5ec5e2130fd379d54e6de0ab325e1b65688e35297", "ref_doc_id": "7c0d045c-6e11-4640-b7f6-8765969923c2"}, "6f21f7ac-39eb-4f70-9d31-ef7d81e659a3": {"doc_hash": "4592b511e5f743816648f695692d1ea0de646278a07affcf2feab3a3daad08a8", "ref_doc_id": "d0c468d8-81ce-4f7e-9325-113947e69fcf"}, "bf1d653b-7b06-438a-a543-83b31d4ff5b8": {"doc_hash": "943d43122d884fd197fd22aa9f6142324ed717a5d40015ad6119c2c86a6ea887", "ref_doc_id": "bf9320ae-56c5-4c79-b577-4ce45a5126d4"}, "c09b6203-cfe7-4842-a024-e4b41c88ec7f": {"doc_hash": "31862ee6a8bfa0de5dd1075cbe4fbb25c27595ba7db302aeab7f4cdbbcea97ed", "ref_doc_id": "31b7946c-683b-435a-b39f-e1d39da093bb"}, "1a249a43-a580-4a66-871f-16dce3eaab98": {"doc_hash": "ba62590c93c8f78ee04df47848d2778fa6463035708b0ceba2a1f82327e402ab", "ref_doc_id": "22987d10-3c54-4c7d-ba44-6a334a25d4b0"}, "2795b75d-d9c8-4843-8749-4c1efd28e370": {"doc_hash": "2abc41f1e0bcafb6bdce3017877c0ec115f156487336a41c0d82e102cfaa323c", "ref_doc_id": "8d71be8a-7b0f-4d96-bf7e-d9bab0db008f"}, "28fe55ae-7656-4cf4-a0fb-a48de7d7244d": {"doc_hash": "3232e6f41a70418fa3a68fd6f6573d84e0a05ab84314ff74b41f1cf98fb2d39b", "ref_doc_id": "3dcc8d03-06ab-4588-8d5c-bd0db6c406e9"}, "5b5b956a-bcfc-4bfc-a9c2-8516e983db37": {"doc_hash": "e5c0f9c7c068509aa45d4f7c401eab11048567f582068d3e60093a3b0624d69b", "ref_doc_id": "c2dd3d89-e0f2-49d4-846b-5a4c897db878"}, "8b496285-34e9-4dbb-a4b0-b66b02c55dc2": {"doc_hash": "7af86e8d5b81e27755c02e810635a1a8e2bdf8f34fa304f3b49ba8389ec16951", "ref_doc_id": "773f0249-f85d-4d43-8a80-9a8555b60715"}, "a02c6a00-5c8c-4119-8868-96e0fb10b81f": {"doc_hash": "001d5ea9f19565c0fc7fc14fe1dd6c828ee3a48efabc9c5385a128e8cebf704f", "ref_doc_id": "237efde8-02cf-4d81-8cc6-e1a7a0211568"}, "46f75e9d-83fc-49b8-85c2-70bd04da440b": {"doc_hash": "0d1884bdc35fac4b09b18f0d9b86b612d57df58bf2ad808a693b5ca01dca5036", "ref_doc_id": "20fc4542-1c44-48b6-b10b-dc2ec0b186a3"}, "9fc2a466-1999-48d6-89e2-45845a9c85ee": {"doc_hash": "6411cea6a13132c5b6a65f8b3da16deb5ba5757b72fd37b96a7a0d0b09d174c0", "ref_doc_id": "34b3522a-8d41-4f6c-b3ca-4d7ba8bac658"}, "630ad981-5fc1-40e9-b3c9-f4f20b9a1892": {"doc_hash": "8b929f0a9f63a0dbca12fcd822c6bda203b4410309ab34f1df13d533ecd48d76", "ref_doc_id": "9ce2d18d-74a7-4742-9fe8-6c0301d46ace"}, "c400e8ce-7b1c-4606-a672-93f7af5fdd04": {"doc_hash": "e7a661bfefb9d4a4f28a3e0be32587943fe4d096e10684ab8f0c8c6bbf9838d5", "ref_doc_id": "2ffb84b8-0c0a-45a5-a533-6a89fba850c5"}, "f3f4bd4c-d4b5-4225-a2ca-3f007dd6ae4c": {"doc_hash": "1c0abcc55a377818a8591dac45d8ec7a913b584aec740cf2ce08b9cb94c5805a", "ref_doc_id": "668fe9bf-02e0-4518-8a22-f3368608d1b2"}, "288fdf20-f0a5-424b-b42a-575dfef5fbd4": {"doc_hash": "ea941f07af0b50513c8daf855b8334b7190ca2d8fe6a27d0f65e23fa6f2824ba", "ref_doc_id": "50f74c09-f222-4af3-abec-6b4df3da63f5"}, "f2e99a4e-5875-4ea6-af0c-7b8605c3ebfd": {"doc_hash": "9c6dfd6ea55972f4e169f1ff5f31b93cef356a7af945d4f376392d31af58b6bb", "ref_doc_id": "999e4dc9-2f1a-45fb-9658-0cfb9cf3dd47"}, "68cc4810-eeef-43b8-b599-ad9278c709e5": {"doc_hash": "d00db3aa0a7fd4d29508c37c564a7a148ae817bb849b69a938191026d3e4a755", "ref_doc_id": "82ee5183-d20f-4fc9-88e1-e990b3526918"}, "fb6c8d10-77e3-43de-820a-dc448e147f50": {"doc_hash": "047df83f5e4a27b1ec589628df9b758817a542c32572f15c7c8996101548c492", "ref_doc_id": "2f91b837-fd94-4813-b2ab-a58be62c3e4a"}, "3c01b273-4bd8-4cc1-ae1f-02a2759a6e06": {"doc_hash": "d578232fc7bd33eb7cc18e4ef627fca48c3797652e5eea3e74b5fcbaaf57e69f", "ref_doc_id": "acf6df41-134f-4062-b5f9-bd04fbabffc2"}, "c9899291-6d8c-4e96-aef1-eea5415e583e": {"doc_hash": "5a92ec89f32253c8d434e6fcf95c195cbfcc11d974e9284cd5e866aa3ee99faa", "ref_doc_id": "4ed27b10-e7d9-4664-93ad-9c36d6f4f032"}, "2ae11819-bdd6-4442-9d42-0423ce0d1494": {"doc_hash": "8ace05bc01e03ea2eb6bdb0b5879d0f3658be44d44cba64d0ebcbcc7d8d0d487", "ref_doc_id": "e309dd5b-52ff-47f0-84f9-0be0a1e3ba07"}, "06baff55-bd19-4cce-87c0-3d09702493cd": {"doc_hash": "ab71543cdf960b54c1bfe8d1adbd95f231db01e325aeba88514682a7ad515f85", "ref_doc_id": "bf72bcf6-8b39-4a46-8f66-d38b9b5f67d3"}, "5df63d7b-8088-4114-ac3d-452a6e7f0b5d": {"doc_hash": "1955222934b46e71a632e4dfe7ee5793909be5bc8221764d779d77a751ca085e", "ref_doc_id": "b0c6bad1-4df4-4755-b2f4-bb537c8e219f"}, "196820ff-8c45-4d57-8d3e-0cdc452de927": {"doc_hash": "8354e7f9d2a88f968ca93e1aa4f2272f7b486a1f06d5ff70154191f836c7d0c2", "ref_doc_id": "b0c6bad1-4df4-4755-b2f4-bb537c8e219f"}, "9169bd3b-f98d-4f88-ae05-4e481444a99c": {"doc_hash": "5aed5d7cb47605b7105f0d8ead53e8882070c6721ea32344198afb7cc73c9e86", "ref_doc_id": "2de5b6e5-d385-4ca6-a646-b74ab2ec695c"}, "9076c44f-5e82-45dd-a754-e7eff254eb31": {"doc_hash": "9065115841e6377c9996153b0b691b854f41ee4e113bf102cd3379d66bb91150", "ref_doc_id": "26e71e46-ff4b-46bd-8f4d-2f3ef79ecfa3"}, "e574c12f-81c7-4ad3-8857-5365e8699520": {"doc_hash": "0898f9c7ef2dd18b4ff958b421d558e9c2c535262255b3a93d1ef0b4184a2675", "ref_doc_id": "3a5d6bcf-58d3-4ab8-9cf7-3115c85f7dbe"}, "18e7290b-76af-4900-8221-cc450bf6f102": {"doc_hash": "0fe1e827d70005411c956e23e7d9f194d9c7ac545dc9aa0dc40700b8303644bf", "ref_doc_id": "40f133c5-088a-4c4e-b570-e1f62fdc75ff"}, "cae01cb1-91df-44c6-a6b7-9aabdb757c16": {"doc_hash": "e1205d5f8720280f80ffb0024404797293cf62860d2e9187ab965b6c8fc7fe48", "ref_doc_id": "b5646738-50e3-4fc5-a9ab-3c137642ef66"}, "edb9d7be-7ea0-49cb-a3d1-eec7e6cb36ad": {"doc_hash": "f02b1c87706a5b3c3e2e5f98fce4eace4620411b801876ec050c66a2a06f25b4", "ref_doc_id": "7569abaa-5b91-4407-bc3e-b9c2d0e0be71"}, "b86b2985-a4d4-4c42-9b68-f42ddd237573": {"doc_hash": "36d1f60750eae27eb40dd1c715a79998f19656f36b8361ec43ffea14f3e02ca8", "ref_doc_id": "e18db6f5-dd36-4f85-a1f9-64045a62beb2"}, "b32431b4-6f96-402b-800a-c60e849ebfce": {"doc_hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b", "ref_doc_id": "3d7d3c32-a711-4c19-bd7f-92f59685556f"}, "d8734d82-4d9a-4e3a-9ae1-dca6f65891ab": {"doc_hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d", "ref_doc_id": "f0dd7bd6-67ea-48bb-9973-7b6ae430a892"}, "01172341-d76b-4ffd-8ebb-8e59a49f4b06": {"doc_hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f", "ref_doc_id": "f3a48c5f-496a-4b14-af08-fb39f03370a4"}, "6b5a02cc-35fc-4fa9-a9d2-b1f36a697e14": {"doc_hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe", "ref_doc_id": "21795d51-479b-40ad-a150-aac48fae5962"}, "7c0bdc56-1d2d-441b-8d21-46a0697db649": {"doc_hash": "b5899512ab126ca7b0cc5fe2a825e271dfbea6ea3e49469597376829a500a0db", "ref_doc_id": "609fd3b8-4f1b-4e43-887d-8b5e5cd945d4"}, "dabcc144-efb7-4e05-8d81-939d52f156a7": {"doc_hash": "7e3111e040a674730fa702d7902f35c16b8c433334150ac0ae151ea4fab47ce1", "ref_doc_id": "609fd3b8-4f1b-4e43-887d-8b5e5cd945d4"}, "c731ebae-4c4c-4083-baef-014f44f1c49e": {"doc_hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4", "ref_doc_id": "1af302f3-b0d3-431b-b95d-9472fd47cd27"}, "6979f13a-4f38-4cc6-a076-68ecd79ac039": {"doc_hash": "e95fa380b755b4c34d72a363da79a8c57174e8d488e9a48e2fd996853cc786e8", "ref_doc_id": "9bfaabf7-5c60-4c85-aca5-381b31f75774"}, "3e08bd7a-7bfd-48b9-8a95-5b88f47bbf53": {"doc_hash": "231a44d96b44ef7cef97f4d52fc7bf0c58617c4a316ef1d4aacccf09c35c8c27", "ref_doc_id": "9bfaabf7-5c60-4c85-aca5-381b31f75774"}, "0f449792-62dd-4874-8473-0cf1c56d6f99": {"doc_hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84", "ref_doc_id": "0748b6f2-0135-451c-ad7b-13b12f1db444"}, "44c71317-9345-41d4-a7e1-c383ceee95c9": {"doc_hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0", "ref_doc_id": "79ea9571-1a91-438d-9c62-5ecd2adebaac"}, "10f2c3de-efe1-47f3-b441-91df604ad5f4": {"doc_hash": "73a1d174247c6265657797b59c09c64dc8a0dbbe89ce49cf76e1b363e58e5267", "ref_doc_id": "7d34ef88-55ca-4bae-b6ad-0e4318895305"}, "11dd0dcd-7efc-4312-82a7-7dd8fbe7988c": {"doc_hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "ref_doc_id": "7d34ef88-55ca-4bae-b6ad-0e4318895305"}, "aef2f663-7aca-4877-96ac-748136df92bf": {"doc_hash": "a33129b68cd64ada00b9243cecd3233f935c166bfedda87a4cb2921a33d65367", "ref_doc_id": "7d34ef88-55ca-4bae-b6ad-0e4318895305"}, "e128f73c-e338-4ee5-ae2a-9f4d71047e8c": {"doc_hash": "f30400ea5c39a8dda2862c4f5a6ed31adb53965d1dfd780b04bd6e1abb680842", "ref_doc_id": "b7ac12c6-e24f-4cf0-a601-e16be87f36da"}, "14c2450b-f530-43dc-bda3-a3318ce76eac": {"doc_hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "ref_doc_id": "b7ac12c6-e24f-4cf0-a601-e16be87f36da"}, "7827d715-6f00-4efe-ba2f-59e8565404ad": {"doc_hash": "5b6cd1ec36c79c626e603648645c27d2c8bdf7405d134fa5ee8d442654faaccb", "ref_doc_id": "b7ac12c6-e24f-4cf0-a601-e16be87f36da"}, "01a93ebb-101f-4a55-a36d-908541ad9f71": {"doc_hash": "f5cd5448c9452cbe6dbac1ac03e442b08b61542ba0c8aa8aaaffc80752815b7e", "ref_doc_id": "d40b8c03-0d86-458a-97ba-d1986dc3e9ef"}, "e27675e7-a242-4582-8ef7-f5781e55f524": {"doc_hash": "275baea2efd06dc7d04371ac8ef8f549db38d8eac1441e281d806a98e93a9ea8", "ref_doc_id": "d40b8c03-0d86-458a-97ba-d1986dc3e9ef"}, "94e1d8eb-9958-4ed4-bfeb-0a065c1542bf": {"doc_hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb", "ref_doc_id": "357ee6c0-4806-49c3-94ea-903a94afb69f"}, "a4338db1-5a15-457d-9869-403c5356bc9a": {"doc_hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073", "ref_doc_id": "dc317cf1-b705-4d8d-9dc0-2958ee98be27"}, "05cf5c28-7526-4c1d-a1c3-276dd9605b86": {"doc_hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160", "ref_doc_id": "4044e15c-8f00-4041-aa79-2d4991feac5b"}, "8d31348e-70a6-4212-8eb2-d2909e182ac0": {"doc_hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28", "ref_doc_id": "07e617ae-fa18-4210-9b51-b394b5a0fffb"}, "a10cf413-1be5-4783-b0cd-339c5afbbe86": {"doc_hash": "00119033c6c21ba634c0f31ec079ed765db6f54b4f9569e4c2947c753f6644b0", "ref_doc_id": "6a3dd0d7-3a7e-4d6f-a94c-d3bbfa985eff"}, "cdd5d77a-f6d9-44ad-aa42-13edc4d90806": {"doc_hash": "62816e0b531a77b544cbbfd001f5c8c7767e6aa087dc8bc5a6d4f92170b12b5e", "ref_doc_id": "6a3dd0d7-3a7e-4d6f-a94c-d3bbfa985eff"}, "f55c64c3-7c35-4ce8-ab63-ecee2d0f70ae": {"doc_hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2", "ref_doc_id": "c54c1f00-fa98-437b-99f9-7fd9426472ee"}, "c526329e-1e8d-4f5c-b10f-bdd2e028b2fd": {"doc_hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451", "ref_doc_id": "e4dd0687-9f00-4b8a-9709-9c222256cabf"}, "5709460d-cf04-4f42-9d20-11f703381225": {"doc_hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc", "ref_doc_id": "3c4413cd-d62a-4201-aa1d-49a4583a0bb3"}, "4603d6d1-873c-4316-a400-cbdc1334f250": {"doc_hash": "5060d32492e31e64645973fe1546713c53bb014358a48fb7fd76c2b0f5a5885b", "ref_doc_id": "391aa0f7-6325-4517-ad24-15e3efcf61f6"}, "80e11702-b6cf-4b3b-a0fb-54f60ef6dbae": {"doc_hash": "16e27eaaf69437942915c62123c5784e5bfc970147b0d8f228783ac4b77fbd9d", "ref_doc_id": "d21e51f0-3ec7-4a3d-8868-1c39fb86ca72"}, "0a4d9f8a-bee4-4635-b18c-fb55e04dab08": {"doc_hash": "4aaffa8becbd32ca62dabc53c25eb75127e320f8396c15b679cff105e6b37c2f", "ref_doc_id": "71321f4c-a11b-43b1-9919-2350469415a0"}, "1d9388fb-2824-4705-a8db-703f8c20b973": {"doc_hash": "a7d3d67742f54d942a9e67f665be026392d2a3492a74763403d824bac426bfbe", "ref_doc_id": "57f2020b-d272-48b7-a5aa-76ce658bc649"}, "db28afe2-4eab-4dda-8289-89aaea5bcd85": {"doc_hash": "b5899512ab126ca7b0cc5fe2a825e271dfbea6ea3e49469597376829a500a0db", "ref_doc_id": "91b7c270-194e-4fb5-a8a0-e82033bced17"}, "9a392518-ba7b-4ee5-a096-cf3f2e2444e6": {"doc_hash": "7e3111e040a674730fa702d7902f35c16b8c433334150ac0ae151ea4fab47ce1", "ref_doc_id": "91b7c270-194e-4fb5-a8a0-e82033bced17"}, "a8091b6c-4a36-4312-9cd1-218af5842e12": {"doc_hash": "ccb0ee80a9b0fe90a1c9e25086ae3b22fda991339a7c03f6d3b15617aa5b05e4", "ref_doc_id": "39bb65a1-f964-42cd-a973-29a48f3bb2ae"}, "e34ef8ca-0ecb-491d-987c-4e0ca7295f5c": {"doc_hash": "e95fa380b755b4c34d72a363da79a8c57174e8d488e9a48e2fd996853cc786e8", "ref_doc_id": "71fe3a08-92b1-4c0a-bbd7-310d27d456fd"}, "45662eb7-0374-4eaa-81a5-1d6206504c70": {"doc_hash": "231a44d96b44ef7cef97f4d52fc7bf0c58617c4a316ef1d4aacccf09c35c8c27", "ref_doc_id": "71fe3a08-92b1-4c0a-bbd7-310d27d456fd"}, "4e1bb9a6-a278-48c2-975a-b568abdba7d9": {"doc_hash": "cd13506b1e1d3460d0c1230596433a60a03960c646adb8c15913a80693884b84", "ref_doc_id": "05a406b2-8f89-470b-abb2-77826c6f0f7a"}, "86fb7173-cbac-4724-94ec-75b78372f4dc": {"doc_hash": "76a6ee4a725e70768179b4db894fe8195d91c147778fb32140f40c664e47a8c0", "ref_doc_id": "df552c0a-6b1d-4193-aab0-2b7a2700446d"}, "735ebfdb-b7a1-45c6-8f1f-06e30707ee1a": {"doc_hash": "73a1d174247c6265657797b59c09c64dc8a0dbbe89ce49cf76e1b363e58e5267", "ref_doc_id": "5c69e55d-dddd-4ce2-a020-e57b47de7bd3"}, "1bc0d70f-c3e4-49b4-8e6d-0fa59a80d689": {"doc_hash": "619dee57a0e81e25d9396e18c31e810978a223eee41f8f81da3244e64c54237f", "ref_doc_id": "5c69e55d-dddd-4ce2-a020-e57b47de7bd3"}, "4a04d290-5e86-4169-9af0-fb8f595c6ab0": {"doc_hash": "a33129b68cd64ada00b9243cecd3233f935c166bfedda87a4cb2921a33d65367", "ref_doc_id": "5c69e55d-dddd-4ce2-a020-e57b47de7bd3"}, "297ad5ff-c67b-4105-b259-25e3e2d66900": {"doc_hash": "f30400ea5c39a8dda2862c4f5a6ed31adb53965d1dfd780b04bd6e1abb680842", "ref_doc_id": "e416a557-24dd-476e-bf15-e562a4b7fd96"}, "e80eb6aa-28a8-4422-afb2-12f3e4ba0e92": {"doc_hash": "3e4fa483e97ddbd4756db127123e767475a7000c02f70d2c2a37232da691ee18", "ref_doc_id": "e416a557-24dd-476e-bf15-e562a4b7fd96"}, "b4f56589-3301-49e5-97ed-055008cbc288": {"doc_hash": "5b6cd1ec36c79c626e603648645c27d2c8bdf7405d134fa5ee8d442654faaccb", "ref_doc_id": "e416a557-24dd-476e-bf15-e562a4b7fd96"}, "2685f31f-3fe8-454c-a288-5c7af5a46e0a": {"doc_hash": "f5cd5448c9452cbe6dbac1ac03e442b08b61542ba0c8aa8aaaffc80752815b7e", "ref_doc_id": "3a7e2428-b6bd-4887-ae8a-60bb48d803ff"}, "853f116c-4b54-41f6-b1a7-268131d96c17": {"doc_hash": "275baea2efd06dc7d04371ac8ef8f549db38d8eac1441e281d806a98e93a9ea8", "ref_doc_id": "3a7e2428-b6bd-4887-ae8a-60bb48d803ff"}, "da4b72a2-878f-4d0c-80d7-7687880e93ff": {"doc_hash": "0c958f8d579623fc568828275be8fa4dfcbf00e6bfb7d3e599debfd81b5b3bdb", "ref_doc_id": "ac76fdec-72fb-4c2e-b550-fd6e2ee7e59d"}, "c0af7a4a-4e38-4f22-80eb-fa6ade1d194b": {"doc_hash": "17d870bccb07d3063248275996375c01da49e25fed89413f23bdc586ba1cb073", "ref_doc_id": "1a40403a-cf57-4f3d-8f5d-d0982706f8c7"}, "e3f9715c-96e9-4cc6-8f3f-422b10c1d0be": {"doc_hash": "cbd692e6b93862570448ad201af998640b1b4e4b8da07f55c31c6c4cbf630160", "ref_doc_id": "67b0ea24-89ba-4dce-899f-bb1a0545acca"}, "7ea73fd7-98d1-420e-9002-36c8ca5d41b5": {"doc_hash": "9a5d8228334c07db6d916cdd7072dcfd7b0f603d54f7d0ad3b6f46c397306e28", "ref_doc_id": "67af852a-0306-4028-b539-2d92b33a67d4"}, "2545564b-758d-4ed4-a62d-fefb63b7ee30": {"doc_hash": "00119033c6c21ba634c0f31ec079ed765db6f54b4f9569e4c2947c753f6644b0", "ref_doc_id": "e8305e55-bbdc-4c79-bd10-fa997110aca4"}, "96201a9a-d237-41eb-8749-c74a4443ee6e": {"doc_hash": "62816e0b531a77b544cbbfd001f5c8c7767e6aa087dc8bc5a6d4f92170b12b5e", "ref_doc_id": "e8305e55-bbdc-4c79-bd10-fa997110aca4"}, "4a2ad900-a01d-4a43-bb31-4d8a4ed2ac8c": {"doc_hash": "7bd1dbc98fc8016bfe230fbd25a4e70db5de964802ae55a52420a8de12ab26d2", "ref_doc_id": "96225294-ca1b-498d-b55b-aa07443ea6ba"}, "ee2674ba-3a3e-4751-87e0-b8970a287dd4": {"doc_hash": "c8db7f64b9c24029b5a48be022b31636476026b7ba3ed6818068096cb95ef451", "ref_doc_id": "543a6e39-ed15-4284-881c-370c3c5f837c"}, "00f7ca7e-fe65-425e-8a64-eb1327dde448": {"doc_hash": "7f74ede30c0ee29afcee0a85d3fcc6fbe3b4c1bdabf0b1ae36c298d4650b6dbc", "ref_doc_id": "63bfc2a6-ff00-4c9a-b939-d9038db64301"}}, "docstore/ref_doc_info": {"647adfc9-c2e6-4a6b-809f-5c987bee7120": {"node_ids": ["065b847f-d5c7-4b5e-946a-e19c0c678276"], "metadata": {}}, "12b41c0e-ed75-4138-8753-69a3b62e6e92": {"node_ids": ["29f51ced-9aed-4dfe-93b6-ca58679f7f64", "5f4009d5-cc2b-452c-934f-c7569f1aed73"], "metadata": {}}, "d2e35d18-03bf-473a-b3ce-b15c038338a9": {"node_ids": ["c24eb29c-41d9-40f3-b229-83a1601d4d41"], "metadata": {}}, "eefce1db-0425-4101-89d5-462df76903d5": {"node_ids": ["d456ee2b-d445-49c6-8858-238652abc5d5", "baea2fc5-c1f5-41f2-b6c4-1bb7f54e93c5"], "metadata": {}}, "1271b560-90d9-4728-8aca-459ac3f3a978": {"node_ids": ["5df8a21a-7f9b-4377-9bfb-96bf9b162416", "0d35fd88-d6c8-4280-a0c5-e51815e859de"], "metadata": {}}, "8e212b53-3c61-4de3-8729-495b3c2d4102": {"node_ids": ["d33e49d5-09d7-4aa2-8475-9df4452c7a71"], "metadata": {}}, "5040f2d4-717e-4a69-a249-147390e2c170": {"node_ids": ["edab247d-1dcf-4124-b515-daf5c16a9c58", "116c6535-8578-4b48-9088-ab73d523d52e"], "metadata": {}}, "08a2bbe8-5a1c-4333-99d6-6b2c07ceab91": {"node_ids": ["2d4c0faa-ece9-488f-ba42-a88e23a29076"], "metadata": {}}, "8f69f847-5fba-4a5c-b825-83a53aa31d7d": {"node_ids": ["c99364df-7b1f-4e9f-bd5a-7e117c434a74"], "metadata": {}}, "8e240ef1-84e1-4611-943e-0fab87df0e42": {"node_ids": ["01586bee-35d3-4c3e-a451-1f73944b245c", "ff6e763a-f82e-4d33-8496-0596bd304fc5"], "metadata": {}}, "05888620-a24b-45ce-9a64-b4328b9fb53b": {"node_ids": ["852b9889-56c5-48ea-80fc-f40c85da03ea", "607beb3c-c513-4d3a-9e3f-ab7f7536155c"], "metadata": {}}, "165b8571-561f-49fa-a5d3-7f1bda24f647": {"node_ids": ["98449be0-2d90-4f31-a2e0-b4c315b87f4a", "05fb2e3e-a7df-4d2d-85e0-6d8c8cbc4fbf"], "metadata": {}}, "a71ee94d-9a42-4a52-a668-e4654d1fc481": {"node_ids": ["2b467249-165b-413e-89c7-7517258708f8", "7d1346be-639b-4e1d-9840-36b11a9b09d1"], "metadata": {}}, "97149929-3cc1-409a-a111-5a20b6a9ed80": {"node_ids": ["a283d5aa-48c6-4da9-ba86-4605d13b9484", "04477231-c1f4-46f8-ad28-380cd775e0e4"], "metadata": {}}, "8dce5a1e-d154-4670-8609-36a570e8224b": {"node_ids": ["f36b5f6f-020e-4270-a036-247fc1022486", "e0a07b23-c30f-4178-a2a5-06121ec85127"], "metadata": {}}, "836d23df-e4a4-48da-8924-544b586ed923": {"node_ids": ["961de226-256a-46a1-ad35-49d680567653", "586dc347-a24a-4b30-a587-f026fa90caba", "1bdd3c28-6823-450c-ad2e-27a9a7e667d0"], "metadata": {}}, "704e614a-8cd1-41ab-827c-3a91cc8062d7": {"node_ids": ["83d10d2e-a96b-4675-be01-9986d65617e5", "7bed53cd-e14c-4188-9963-816778ce12e2", "a5970011-ba6c-4b7d-9bd6-c742c47c6afe", "8f07e17b-19cc-4939-b664-1f871b0592cb"], "metadata": {}}, "6ac6e2ac-5516-431a-a6d9-69c7580f3cdf": {"node_ids": ["ca5cc118-1846-4ab1-87d4-32afdc29c35e", "a50ba741-b326-41e1-940a-2ec88539b1ea", "4ca2c115-01e9-4c06-8074-61c72c562ba0", "09446021-cdbf-42a5-9946-3cfb4ba01b18"], "metadata": {}}, "b8b1f18d-b60f-4202-b078-1ecfbbf37f6f": {"node_ids": ["2299c14e-0913-4401-8f0d-f3340bb219ce", "bc538e16-bfe1-4776-badb-f6ea1bb120d6", "e8ae56e8-f265-4804-a3de-1cd660e4d613", "4ccbcb06-5b9f-4e36-9c14-64db839c5284"], "metadata": {}}, "93a9d023-c5bc-49c5-adda-57b508974fb6": {"node_ids": ["19fa79e3-812a-4960-8c8c-657318a02e9b", "ac978d6a-561f-440e-8ab4-87c053b5ae84", "b03f6dd5-1a57-4596-abd2-5dde0ff157b3"], "metadata": {}}, "27bfdc15-3f59-4992-8659-4b532ba28f77": {"node_ids": ["ec8577e8-581f-413b-9861-fd8595c05118"], "metadata": {}}, "3888170d-5508-4326-85f4-e9f7a8471b80": {"node_ids": ["39cc66d7-de5b-4722-9063-3359169b9c5d"], "metadata": {}}, "d40fbb26-d7a8-4d3e-a2d0-3e472a150254": {"node_ids": ["b15be2e4-6383-4a0f-99bf-b4306fa700da"], "metadata": {}}, "1117bc87-6ef8-452e-9754-9671a2ebb9ed": {"node_ids": ["13bd854f-0455-49ad-bac9-241c7e7d128b"], "metadata": {}}, "cb0da17b-71fb-4820-8b80-188bccf29b5e": {"node_ids": ["47b05167-cf4e-4077-b313-7558c7a7c684"], "metadata": {}}, "0435041d-c9ef-4eb1-9b39-a907d02384fc": {"node_ids": ["7721da31-35e7-42ac-8614-fb1787dd6512"], "metadata": {}}, "e21f8e32-57bd-4673-bbf1-868381c1b8fe": {"node_ids": ["a01e21aa-0562-46d6-b271-efd856f04e28"], "metadata": {}}, "20899be0-7bb6-432e-aca4-523e402f5c06": {"node_ids": ["7cf826e0-c43c-4361-9807-04c1451cc7dc"], "metadata": {}}, "d257f9ff-57fa-4031-b2f2-0656b3345edd": {"node_ids": ["5e215ca7-35ce-4ce7-b961-21430d8669e0"], "metadata": {}}, "93c933e3-1f25-45ef-9d39-ad622f69e126": {"node_ids": ["c198dd33-1aa4-438b-b73f-2137bfdd6bd1"], "metadata": {}}, "0d3592d4-6aa8-4700-8c69-77087c11060c": {"node_ids": ["e08fec4b-a71b-4982-8e5b-013525c5e38f"], "metadata": {}}, "6882b8ec-6e55-45c7-b489-7ac2ba1f3488": {"node_ids": ["f1c6c617-50a5-43be-afee-8fbbf0172971"], "metadata": {}}, "bae8eb49-a2c5-46f1-b4ca-52407bc5c4a1": {"node_ids": ["e960c757-0e62-4490-a7b3-25340fe87a69"], "metadata": {}}, "707987a8-0c39-47b7-94d1-9663fa24b198": {"node_ids": ["493ebafb-758c-4f4c-a642-c4c4168463dd"], "metadata": {}}, "3ef385b5-8499-4b8f-bf7c-44100a4762a6": {"node_ids": ["4c8018b0-29ea-431f-b59a-0ca241227a2a"], "metadata": {}}, "b75f3b15-933c-4b74-8b49-c7f9042e2149": {"node_ids": ["2df8e644-8c2a-46bd-b3be-1234f4651f62"], "metadata": {}}, "a81c6658-672d-41b7-8961-12fd3ce171d0": {"node_ids": ["62d127c4-9513-43cb-9ac2-fa1938efa088"], "metadata": {}}, "e8cf0a5a-bf22-4d0f-92f3-f3de3bdb9fc1": {"node_ids": ["deb131cd-aae1-4857-b406-f279b1f2d3db"], "metadata": {}}, "e0e94bd9-aefb-47aa-8195-61507e4b77b2": {"node_ids": ["74cb8228-db5f-4d76-9fc6-053c154844b6"], "metadata": {}}, "29defb66-e9ea-4784-be99-3eeba7203251": {"node_ids": ["c5baa6e4-1345-4cc4-81e5-c3c92629f018"], "metadata": {}}, "96498834-119d-4552-a4ce-fcb1e47fb499": {"node_ids": ["a9ecbf34-069f-415c-b2f6-225934450f8b"], "metadata": {}}, "97291e4a-0512-49d5-bdc6-ff4fb59f6ee4": {"node_ids": ["e22977f4-3c85-4a7f-96ef-304cdfe0cc68"], "metadata": {}}, "eea54be3-50a9-4b54-a365-925aadb4a7e5": {"node_ids": ["5c2a9ae0-c729-4393-bb80-d14743f9a417"], "metadata": {}}, "85fb42c9-7d17-4f14-bec1-af85400a4069": {"node_ids": ["42267297-7d4b-45d5-a248-3dac71197e42"], "metadata": {}}, "db618e43-6a7f-4e87-82a8-37232cd32937": {"node_ids": ["c719ce5d-e93f-4d86-9f05-02ad026b6c91"], "metadata": {}}, "352aeac5-eaca-435e-9d74-354004736df1": {"node_ids": ["9650164d-b5a6-439b-97b9-2a1fa6670990"], "metadata": {}}, "4a3a1a13-78a1-4a26-964f-294ff14386fc": {"node_ids": ["d3207532-03e6-4e42-b890-2627f679299f"], "metadata": {}}, "7bb116f6-1482-47cf-8c0b-5ff01e9de263": {"node_ids": ["217b31f2-2d1f-404a-b7fb-53e3ea1b35d9"], "metadata": {}}, "32d5fd23-84ed-4d13-9938-cf101daf425a": {"node_ids": ["4e5f0992-86e9-47bb-8f10-28411a84b36a"], "metadata": {}}, "369fd348-471b-4887-ad19-f02ddfeb684e": {"node_ids": ["633523f4-9853-41c8-9fc9-318476d688c6"], "metadata": {}}, "0803b89a-0adf-4dab-9b17-68b99acfcf02": {"node_ids": ["29d50115-8781-438f-ae4a-9f6513f02337"], "metadata": {}}, "d2dd0f2c-3d39-414c-a7e0-a4cf7b3d0776": {"node_ids": ["22e377cd-03d0-4146-9341-916287373d16"], "metadata": {}}, "6224201b-4efd-42a0-9cef-0946e4fe84e2": {"node_ids": ["25aebe7f-057b-4038-828a-5c4e1853ad4c"], "metadata": {}}, "44b5c628-e4cf-4b05-a77a-3b6353959098": {"node_ids": ["9b464a67-3915-4958-ac68-2b7171839759"], "metadata": {}}, "b1619346-c4a4-4190-bacb-f7a0411ad965": {"node_ids": ["16b66d2f-9b16-4dde-a651-5187d3229110"], "metadata": {}}, "ee3dba33-cbd5-4d14-a0bb-f1925fe3cc21": {"node_ids": ["98e94121-f62c-42a6-a810-61d59ea5db9e"], "metadata": {}}, "b4c59342-ca6f-4b8d-87da-db70670598cf": {"node_ids": ["7358e19d-b7ba-42a6-9cea-158701321ce5"], "metadata": {}}, "5b056c15-0557-4d5c-af51-5d23a349df35": {"node_ids": ["70e3ccf5-6ee2-4db4-b4d6-0ec245e3c9aa", "64b92cd4-420b-4128-b5e9-296c29676736"], "metadata": {}}, "6bcfc07a-a4e6-4e06-b39d-548fdde1ebeb": {"node_ids": ["a083281a-7373-4f63-9f88-5c562a1b7bde"], "metadata": {}}, "1611ebf1-c46c-464a-9bed-27811fcdfd80": {"node_ids": ["128d3dcd-ab94-4e35-9bb8-92e05593d61b"], "metadata": {}}, "764e6d7b-d79b-4603-8048-7d2ec800bb78": {"node_ids": ["630cf3fc-75fe-4726-bbfe-fb9acaea9405"], "metadata": {}}, "14481d3d-e5d7-49f3-95a6-46d8add21e05": {"node_ids": ["1171fade-0afd-42a9-878c-52269c23e0b4"], "metadata": {}}, "4d2c2755-b053-4a4a-8be0-84cb5dd32d35": {"node_ids": ["42c24cbf-224a-40b8-80c9-0dc8f85d3cbd"], "metadata": {}}, "d2cf2a93-94b2-4c3a-b686-7f93daced2ce": {"node_ids": ["2e749d08-9f11-4380-bbe4-13ccc6732bd1"], "metadata": {}}, "2c23edf9-18ca-4874-aecc-203868bdb030": {"node_ids": ["af7f5101-cf9f-4f08-b5fe-189709625aa0"], "metadata": {}}, "c75fc2b8-5705-45ea-98b8-be05e7d95cba": {"node_ids": ["67fb1692-142e-448a-bbd1-3ecfea6d98ef"], "metadata": {}}, "ca1d2f0b-658c-468d-a97c-6a7b415fb786": {"node_ids": ["ac651916-78bc-4d6b-a755-57bae36cadeb"], "metadata": {}}, "03d3d6e6-6e07-4e3b-b145-6eb8f9a9c6b1": {"node_ids": ["b70d2504-2dfa-4724-aad2-ccf9db730211"], "metadata": {}}, "8176ef21-d338-4cf8-808c-bcc3235b67dd": {"node_ids": ["d6cf53ea-7f80-42e5-93ae-0373ca5ea885"], "metadata": {}}, "714e515a-097e-413a-95e7-7ff121ca2a03": {"node_ids": ["c9fe81c5-10bb-489e-92aa-cc2e38368695"], "metadata": {}}, "799966c7-e2d0-4e58-8eed-8b7ebdee35ac": {"node_ids": ["dd06978f-fa42-4655-9b44-9f34a02bbdb7"], "metadata": {}}, "c7dcd424-2e3f-4d7c-a2e9-ad7bdd79e735": {"node_ids": ["6548d328-f710-450b-b483-8d8c90af5341"], "metadata": {}}, "a0da184e-5c74-4c9e-83d3-143fadc4ca2f": {"node_ids": ["bddccd67-d3b9-4138-be50-242894ee5e52"], "metadata": {}}, "e347ce2d-af15-4fa3-942b-d6cccb26d7c2": {"node_ids": ["55fe216f-fb51-494f-8392-f785fb0b31aa"], "metadata": {}}, "65fe5291-1962-4b33-b64a-866c643c33fa": {"node_ids": ["8a7cf802-657f-4793-8802-4bf4961a98a1", "16f77f09-cd24-4d9c-aac5-b5fa31ba1fdd"], "metadata": {}}, "5cc62e09-56b1-4f5b-9090-66ab0aadf84a": {"node_ids": ["b6ff1809-daf0-4e37-a83a-f99396530dff"], "metadata": {}}, "440b5a2d-2ee8-4a6d-83d4-f5fbd56ee8b3": {"node_ids": ["9f1956c8-e604-491c-b2a1-ec42671f2a84"], "metadata": {}}, "6ea119fa-bfc3-48cb-a3ff-45307b729cf3": {"node_ids": ["d46016e8-d65c-4ab0-960e-6212617389d3"], "metadata": {}}, "a7fc1d3e-28a2-4af0-a9fc-eb41061ea974": {"node_ids": ["5776b7b2-d564-4075-9c58-c9053d3cf085"], "metadata": {}}, "208098e8-ddc5-47ce-8eea-ce740bce78c4": {"node_ids": ["d86021b5-204c-44cb-87c3-e67b72ae1fba"], "metadata": {}}, "8948cff9-9944-47d4-84fb-00fa4314d57e": {"node_ids": ["8554ee82-8217-404b-b457-4f920c8a5ca6"], "metadata": {}}, "963c113d-3f6f-479f-a228-aa9db5f64747": {"node_ids": ["d62037c9-99f1-4bf4-8e2d-8cadf9e24324", "4457a6d8-e566-405c-adeb-9afb91054a3d"], "metadata": {}}, "64d5847e-857a-4d86-bcb2-e1911a55defe": {"node_ids": ["a1e43c09-f40c-4298-91b8-1ce43da10abe", "9ba667f0-6b0d-401b-b146-9e8c59fbdcb7"], "metadata": {}}, "c2990194-8cef-40f0-9992-c5c6069dad44": {"node_ids": ["79927d35-332d-4a20-80dd-a7e950bcceca", "3e601421-6d25-4f5f-8e26-ef68f1769e04"], "metadata": {}}, "2ec05c59-a930-4469-b083-26ba92bfdd93": {"node_ids": ["975de8f5-d5f4-469d-acd2-50c34de9b1fa", "c75a001f-0c52-477d-a08d-88042578ac06"], "metadata": {}}, "468a45b4-6bf6-4eff-9d4a-907649c18f1b": {"node_ids": ["18f16d88-133c-442b-8c75-1af9d0ac897c"], "metadata": {}}, "188a4754-1d4a-4a12-b66c-99fc4078f207": {"node_ids": ["a3cb3843-2523-4991-b7d1-9b56e326719e"], "metadata": {}}, "a16032be-2c9b-4c61-84c0-8217293d2517": {"node_ids": ["467ed3b5-e90c-4240-b3f0-80d406d78bda"], "metadata": {}}, "b9ef321e-f8fe-44c3-bcf5-704efe6d04cf": {"node_ids": ["a2709435-5fdf-4050-9bd7-ad5ba6df4a48"], "metadata": {}}, "dfaab604-af68-41c7-b1ef-8904e2d6a91a": {"node_ids": ["60267760-356a-42a0-a0f7-b62a7ea0dde4"], "metadata": {}}, "e17aecf3-a572-4667-8a18-87d01123adc6": {"node_ids": ["c5b7c28a-0ca1-4675-8a64-d527fb828337"], "metadata": {}}, "bee98ea9-8b7e-403b-b2cb-e7e526fac9f3": {"node_ids": ["ec377e55-ce64-4821-8992-24c16f743aa4"], "metadata": {}}, "5dcd931d-7360-4d7d-a208-b0cf6b88d3f2": {"node_ids": ["98793adc-6e5d-4174-a5b3-e3379e22851e"], "metadata": {}}, "35e1c939-3db2-43c9-88d3-2afcaff459f7": {"node_ids": ["59db6a0c-2382-47e4-958f-13a833bfb60d"], "metadata": {}}, "b96ec1ca-e8b1-45e6-8d0b-8548856adef6": {"node_ids": ["51be5534-ae6e-4fc3-be0e-404fdad4a874"], "metadata": {}}, "3b2bb181-9a61-4184-8b42-1a9e96e2a2ca": {"node_ids": ["3d4a89d0-1ff4-4b4c-b010-34a64c4eb844"], "metadata": {}}, "ec7c7831-457e-46ba-a816-4a9770139dbc": {"node_ids": ["3a0130cd-477e-48ef-ad32-affcf781fabe"], "metadata": {}}, "338268c9-79bf-497a-bcf1-49dacb39b48e": {"node_ids": ["d078578a-31e1-4344-a6ad-ef3a528cbdb1"], "metadata": {}}, "3ed954d9-ca30-4e47-b076-84b95d32108b": {"node_ids": ["86b84949-95ad-491f-916c-86a8c1b3d0e8"], "metadata": {}}, "a0a7f33f-47fb-46b3-8332-fcc3a868a4ca": {"node_ids": ["f93b48fe-d89e-4d0c-a7d2-19341627af1c"], "metadata": {}}, "afb64066-9829-4bf6-84f7-37cfb5327063": {"node_ids": ["647e389b-2499-4745-b1d6-34309a8d1d05"], "metadata": {}}, "fc34d6a5-4e38-49e6-9a32-c1d3b327d95f": {"node_ids": ["4e9bcaa8-b900-46f4-b20f-f977907fed8b"], "metadata": {}}, "475e5dbf-5221-4388-b92b-48af66d45d70": {"node_ids": ["2632011e-478b-4382-94dd-9ba40515bbaf"], "metadata": {}}, "5e2b690b-069d-473f-92b5-6f42188744fe": {"node_ids": ["78968f00-3f45-4d24-8d6b-da87b31a300b"], "metadata": {}}, "3fa42d2d-b2bf-458c-a7a9-f21998214119": {"node_ids": ["113105fa-f455-4f67-b4b3-63893a3d9885"], "metadata": {}}, "7bec8982-5bda-43c7-8e25-7b2528c20693": {"node_ids": ["333738bf-9a26-49ea-aa02-d285910f3ede"], "metadata": {}}, "acdc039c-fc47-4a43-b500-8fabdedbba78": {"node_ids": ["2174000f-bef5-4a9a-a95f-abcb0f2e34d6"], "metadata": {}}, "d5d4b175-eb20-463f-8b6b-10d6d07c66cd": {"node_ids": ["8b905824-6d28-43e7-b92a-d1ac49a6de75", "fc35a834-8749-411f-a101-a41b4c276450", "d2b288ee-3eb4-4b24-83b8-4fcf7ca88042"], "metadata": {}}, "a594c83a-1f35-4197-b839-36dc23daf492": {"node_ids": ["9ead50d3-b982-476f-bf7c-a336ca49ffaf", "e4f4bc73-6784-4fcd-9991-b05455299885", "aaf63c24-568b-4042-9470-9c26374b8d6c"], "metadata": {}}, "5a2f4afc-1c83-4217-9fd9-20cdb7ed568e": {"node_ids": ["4fde08a8-70a6-413b-b7f4-848176d97431", "b972ba3f-c2ca-4331-b913-a5d6c79776f0"], "metadata": {}}, "bc05fd68-c0a4-49e3-8713-9dd61675966f": {"node_ids": ["174eeaa9-7522-442d-bf5f-58881c2e1ef7"], "metadata": {}}, "99554233-b8dd-4d76-a9a5-2f1933e60f8c": {"node_ids": ["73be25cd-c1c9-4077-8464-bb335acbd5ea"], "metadata": {}}, "e1502670-14ae-4b25-a506-eef06caab003": {"node_ids": ["7d975e5d-030c-4343-a9b6-aea2f6450c42"], "metadata": {}}, "d43540c1-b004-4f4c-b6a0-539000df6522": {"node_ids": ["b29baf98-2f39-4209-9fcb-cb911f38da8a"], "metadata": {}}, "06e0aba7-993a-432b-a014-8befc79c2344": {"node_ids": ["122cafec-d41e-4b4a-853e-e5f2f2c42f34", "0dffcc6b-65d4-45b1-bcf8-788d3f5a318b"], "metadata": {}}, "436ab925-3e02-4549-882d-b89e3aa38c86": {"node_ids": ["8cffdc72-4148-4604-abcf-769e44753198", "6df22e93-5190-40ab-b469-f9544af75f37"], "metadata": {}}, "b214e52d-3193-41a0-a7d9-5ae726bfbcc9": {"node_ids": ["0dc69084-6ea9-4abe-82e9-d858612e5277", "ceb623d5-af3f-4a33-86a6-d6c64c9c5bdd"], "metadata": {}}, "5ef9dd3c-45de-41c4-8960-4619101e42dd": {"node_ids": ["29286be9-d9d2-4f9d-b294-736eeeb0aec3", "d5817d98-5744-4dd2-acf1-3b393086b5f5"], "metadata": {}}, "16b69735-06cb-4db9-941c-5790c02b4ebf": {"node_ids": ["3437e0f7-6bc0-4cdc-906c-114bbab9d8f0", "705e5f2f-bd44-4f94-b431-581755811e7a"], "metadata": {}}, "be8755f4-f435-49a8-b027-c6d8575843f1": {"node_ids": ["f3a4e0c6-e964-45b1-a9d8-818f863a7422"], "metadata": {}}, "a2d032fe-ef47-4568-b2f3-916ace081419": {"node_ids": ["4d08392c-a989-427a-8810-d067508bca12"], "metadata": {}}, "5d321f45-6ab4-4a48-b6cf-e2220f2097f6": {"node_ids": ["4af695cc-042d-461e-a635-e0bce52dd39b"], "metadata": {}}, "c86bc1d5-a8d3-40e0-8cd6-34ea589877ce": {"node_ids": ["887d1bed-2420-49c3-9e7c-4d54f4624b58"], "metadata": {}}, "6ba4857a-d1e3-4958-9a7f-efbfab50e460": {"node_ids": ["79ff01b0-efe1-4525-996c-31de767043d3"], "metadata": {}}, "a121382b-a36e-41e7-b53b-2a3028f2144c": {"node_ids": ["661da3a0-7bc8-47bd-ae73-02f2b86aab46"], "metadata": {}}, "1a583551-0779-463a-96fa-acf6ceec8b90": {"node_ids": ["951501b3-014a-420f-9d16-32ff3ac4b082"], "metadata": {}}, "8192aa70-9635-4ee9-9ea2-12c392e44cf9": {"node_ids": ["cf92f5aa-a7b0-45d9-8e32-bc5f94630e17"], "metadata": {}}, "3f34c844-643c-445b-91bd-d00f90101431": {"node_ids": ["267d3945-b3c0-4a2b-ab84-e20c30245adc"], "metadata": {}}, "a32db838-bb6d-4dea-ae93-9cdc8eec0ff3": {"node_ids": ["5a7d4bed-170f-4c96-bc1f-d280fa9abcc2"], "metadata": {}}, "af6fe410-42af-4244-8cc5-ff8e24656a57": {"node_ids": ["f70ff4be-fe0f-4ca4-9a94-cc51ab1b414b"], "metadata": {}}, "9680fa5d-2f1e-4f33-bbd8-dd53b819475a": {"node_ids": ["9c2b6b53-991e-40c2-9908-b5605ec2482b"], "metadata": {}}, "66af1053-2d81-4eab-84ba-119e7abc6f22": {"node_ids": ["26e62c74-9e54-402f-90b9-9893dbf2b0c3"], "metadata": {}}, "9843422b-c27b-4756-8766-023566c8c7f3": {"node_ids": ["d40da6f0-f4bd-4377-a281-48b604d1fbe5"], "metadata": {}}, "2df8c5bc-7c37-4fce-9ebe-5c0a69d49b66": {"node_ids": ["6cde4160-9ee2-4451-ab6e-ba93a62f2c11"], "metadata": {}}, "9d767e90-1eac-4a11-9a73-83995809e8c6": {"node_ids": ["90cee7f3-bfa3-43e5-a4b6-1c14e6a3e43d"], "metadata": {}}, "d982e4f4-50c1-4ae1-9a0b-318cc7fd9764": {"node_ids": ["3519d667-1b12-4f9d-be19-724b600f6ee2"], "metadata": {}}, "b87e77a8-c678-4578-82de-2e705185ed55": {"node_ids": ["e8ffcf57-6c08-4645-b6ad-ad92fa9414cc"], "metadata": {}}, "ae1afc5b-3dff-4620-a507-2dffa8c7b274": {"node_ids": ["1d5d7654-dc8b-4710-8207-92b93a57af43"], "metadata": {}}, "9772db31-bc51-4b36-9d9d-0b29ff654126": {"node_ids": ["6a64b7a3-1ad7-4f79-b740-68e026c0d9cc"], "metadata": {}}, "d52e1993-8a76-4d9d-8209-c568521d16d1": {"node_ids": ["bcb2102a-d100-46c3-ae23-c5e90ae5c168"], "metadata": {}}, "8df9836c-9846-4216-a16d-c853d8126b87": {"node_ids": ["4710bec7-dc6d-4780-8a37-05f1149f825a"], "metadata": {}}, "a4d7e487-56e2-439b-8a0b-1dc0dc69003a": {"node_ids": ["f23a4765-136e-4f2b-bbf8-376dbaad96cb"], "metadata": {}}, "36161822-dcb1-4104-a0b7-fc0fedd5c14c": {"node_ids": ["c5a857e7-7649-4d20-ad31-691e2aa41f37"], "metadata": {}}, "ed524dc3-03d0-4245-be4d-2e27060b96ba": {"node_ids": ["1ea4eff0-573c-4dec-b1d6-ca4dde98ebf3", "c41a67a6-bd2b-46c4-90a7-ecfd7f0eca39"], "metadata": {}}, "e8bd1842-3d30-45b7-b425-c7fa9bc97027": {"node_ids": ["92ef9da6-dce9-4880-aa29-b0c4c6fb2b42"], "metadata": {}}, "5f0e0e5a-4b83-4278-810c-c147423aa1c6": {"node_ids": ["d0ce007b-2a53-4663-8539-1c004097a251"], "metadata": {}}, "08b13d92-834f-4a87-99ba-afa1f83b90b1": {"node_ids": ["a7bb57b4-6157-4d93-a1ac-12e9068f0479"], "metadata": {}}, "5086d743-c697-4cd5-b5ae-c4ba86a9087a": {"node_ids": ["19b1cb52-b163-415a-82da-818d8839c468"], "metadata": {}}, "227bf9fe-e7fb-4e34-a61c-43c4e48cbe74": {"node_ids": ["81133736-a125-4eef-a80d-6b8701d87e11"], "metadata": {}}, "5da12076-0526-4c4a-9ab1-ea170fb0ddbc": {"node_ids": ["54eebd33-5aef-4bd8-8ac8-988ad0ae3d76", "a267d57e-aec1-4575-908c-57be13d506f9"], "metadata": {}}, "3d916a01-f698-49bf-a6e0-71eb48ca45b8": {"node_ids": ["65a8c4d7-a3c2-4286-9491-29fbbe1f6203", "3a1a4304-3c29-40e6-880a-73aa2b340154"], "metadata": {}}, "58d5eaa9-1676-4862-b0ad-d2b8a38fc48c": {"node_ids": ["7c1c924b-2582-40b6-86b9-3301725d8a2c", "1515b072-5689-48f8-9cfc-a4f341a0b9ac"], "metadata": {}}, "01c35645-577b-472a-88c7-f1b65921fa4f": {"node_ids": ["7bc1cbfd-4621-49d8-8faf-31c43a6702c0", "43a7fa61-a7a4-44b7-9e37-54038a7c7776"], "metadata": {}}, "3910924b-4439-4dda-993e-1ce4e29bada4": {"node_ids": ["d2ce38a1-a059-4439-b376-0d944b526178", "0a5b1c88-f6a3-4938-b26d-83a7660cfbe9"], "metadata": {}}, "d060d58d-e467-46a0-923f-0b499427aab0": {"node_ids": ["33f1e18d-93eb-4ec2-b124-f8bb210de54b", "055c34f5-6bbc-43cf-b0e8-b633f0ab0e4d"], "metadata": {}}, "33068781-96be-4704-87a7-3d869e815ea6": {"node_ids": ["741fc647-3b0e-43db-bc96-9c82f210cdca", "6d250dfc-909c-45e8-ba68-3ec2e4065a00", "3d0758b1-6d69-4c17-a35b-1d9d4d7115d2"], "metadata": {}}, "ca7dbfd1-126d-400b-a8fb-91cfb991a071": {"node_ids": ["e47ba44a-ae8c-4883-bff9-fc5d71ceb44e", "7bf77513-49e7-4389-918e-15b8f27ba614"], "metadata": {}}, "d8c51a62-1749-4fbb-b7f7-aadfbf9de368": {"node_ids": ["60388c28-7e4e-4aef-beaf-73fe5972f207", "f8369aa2-87b4-4d80-8281-4f949f203b3a"], "metadata": {}}, "7f328036-899e-4b64-8151-1a54855d4edc": {"node_ids": ["bcb41f78-4de8-4abd-971e-515670f27517", "6db06705-b622-43dd-967c-4f9a181122ed"], "metadata": {}}, "8a666ccd-6cf8-407f-adf0-9feb3d1651fe": {"node_ids": ["c2ca69d1-d01d-455c-93c9-93c433c113e2", "23b24d1c-41f5-4a92-815a-7277d5cbbd96"], "metadata": {}}, "977d42fe-75e3-4446-b8e4-da8163132ddf": {"node_ids": ["47ba346a-f06a-40ca-b728-2f0edb57e776", "7f2c5204-542e-401a-b56f-aa2240d51da1"], "metadata": {}}, "44171e08-ebbb-4ad2-a72b-caad0fbbd2a6": {"node_ids": ["781fcfb3-87c9-4673-9532-f8071b317876", "99e0e83b-9110-443a-9968-0bdc465380c9"], "metadata": {}}, "01e24ca2-a278-40f6-8c6a-3d0ef01e0b02": {"node_ids": ["2368db4f-c1a2-41d6-a762-a3c79ee118d4", "eedcddff-3e26-4c96-8831-a60eec764717"], "metadata": {}}, "8d5a96b7-487c-441e-b123-806c07f99da9": {"node_ids": ["3790ea29-b5b2-4ad7-aab9-63896366bd50"], "metadata": {}}, "cbb2953e-3560-4cf4-9189-a5410ab1af28": {"node_ids": ["97ddde65-51d6-490b-93a1-0efb46f4ed45", "9f25a1fc-34d0-415d-a2a8-e23ee0713a18"], "metadata": {}}, "be8eba27-7bc3-47d4-8558-d8523201f14f": {"node_ids": ["43b3f599-fc24-4403-a686-7c06efefc68a", "fcef5747-a054-4322-a292-ce41ab606a80"], "metadata": {}}, "0893cc8e-00ef-42fe-88e6-f6b94837ff1c": {"node_ids": ["d1e9170e-2bb9-403c-a6d2-066da1b9bc4e"], "metadata": {}}, "1cd878b3-5851-466f-8859-adbc4ecd5abc": {"node_ids": ["25b50957-dbac-43bf-a1ce-dd80bbd7bc97", "e79ad8fb-0856-4679-a88e-2e3e4c203205", "58a8dc5f-c7d3-43ce-aee1-c13b8812a2d9"], "metadata": {}}, "8883192f-bd3d-4d39-9db0-3c6b53d2c370": {"node_ids": ["fc24fa4e-1de9-4f81-9956-8aaf42f92e37"], "metadata": {}}, "a57814ee-8ef7-4d20-a9cd-32cebb57863a": {"node_ids": ["3ad2b352-cb24-4dd5-b871-e751654a4bd1", "d1b3832d-29f2-4e6c-93cf-c7caac1efd18"], "metadata": {}}, "cea75d86-480b-43d7-aeb0-259c84c50a44": {"node_ids": ["f0b658ca-9ab2-42de-b24b-a3d59d920b3c"], "metadata": {}}, "13207cf7-80cc-45b9-8997-079d14cc8bd7": {"node_ids": ["160351b5-0d0a-49f6-b976-42352f557a6f"], "metadata": {}}, "27d57489-58df-48c0-a64d-784b7f0f3bd0": {"node_ids": ["6cc6ad34-a930-41e9-9e98-1733168b1f41"], "metadata": {}}, "2ffef089-a4de-4b8f-a525-488eee533914": {"node_ids": ["fb5e8f63-1d9c-4bcf-bbcd-da255b66ada3"], "metadata": {}}, "4282d3af-4e60-4356-8ad2-9ec5b836bd06": {"node_ids": ["1caa19d0-3bc2-4a72-98ff-d5378e7ced30"], "metadata": {}}, "cc275abf-a109-47c0-b1d2-15d0d38f05db": {"node_ids": ["31aec8b5-35a1-4ca5-8be4-606298653879"], "metadata": {}}, "51209ac6-cbd7-4a11-909a-bd7205f89928": {"node_ids": ["e32392f5-2c26-48b8-aa48-4373917f9ad5", "d38e6b77-cc4a-468d-9f68-fa67e3e83c87"], "metadata": {}}, "6ddaad7f-9806-4bdb-894b-505a9065a993": {"node_ids": ["05181800-79a3-481f-ab2a-fe3d08d2e5b0", "97bbba19-6b25-4c7f-aa99-a7c329a705f8"], "metadata": {}}, "909bdcc3-15c2-439f-bd37-95be22277e45": {"node_ids": ["dfafd0f9-1b2a-4fbe-83d3-769c2291dc09", "076d8f6b-83ba-45be-8157-b0b6e0d740a0"], "metadata": {}}, "606a5e5e-6633-4800-9cc1-0635f0abfa8e": {"node_ids": ["4b99e58f-ccb2-49ef-88ef-0ba35369304e", "2debd237-4d76-4f46-888b-1db1e41e731d"], "metadata": {}}, "d1140389-bb3a-478e-a038-e98a3e801170": {"node_ids": ["605d7da6-ac98-4b03-bbe9-0613cb404ac4", "b6e8f320-c088-4606-bb5a-d8f5a0111dc7"], "metadata": {}}, "f90606e5-a9bf-4a1d-a445-5b774319de09": {"node_ids": ["76c45c39-fefd-41f1-9c5f-b39e23834c3d", "34712188-7980-449e-b225-0bbf29eba6f1"], "metadata": {}}, "7c3e870f-d94e-47cc-b7ba-8706db2f3342": {"node_ids": ["fd3179a6-e607-47c4-a0e6-c2ea8407a2e5"], "metadata": {}}, "78b896a5-8c17-485f-acca-a5632ef5469f": {"node_ids": ["2a0c09e1-86f3-42f2-adb3-63023afe5d99", "48d16606-c594-43de-95b7-3b79dc53d267"], "metadata": {}}, "91942c04-768a-4427-af6b-f5766072669e": {"node_ids": ["9eafdbdd-1374-487d-8d9f-85c99153a83f"], "metadata": {}}, "247bcc3c-166c-4e1d-b7bf-b89b12432dbb": {"node_ids": ["8762bcd4-a342-485a-88bb-5d515840b091"], "metadata": {}}, "504d03f2-f63a-4a78-ba26-f2174a4d0dad": {"node_ids": ["478082de-98b2-46b9-be0f-95c59654c1b8"], "metadata": {}}, "1df3586a-38fb-48fd-afdc-238e74bea111": {"node_ids": ["180c4401-7074-49ab-9b0c-06c1aba6b6fc"], "metadata": {}}, "3edafba9-7c7c-4bcb-b1da-05614d434e70": {"node_ids": ["d2217b17-f678-461b-a9bc-4e22334a20e8"], "metadata": {}}, "c48223cc-c3ad-4000-9e21-920f781c5716": {"node_ids": ["a7f8599a-7dc5-43be-bed7-1aa35ed60e4e"], "metadata": {}}, "12d43681-7207-4c64-9271-ca3c8011311b": {"node_ids": ["a370f997-4a72-4679-b584-230c58db6314", "f924674e-40ee-4e62-a586-b97e6a9dec19"], "metadata": {}}, "df9266e1-aed0-40e8-baa0-50c1176bc9ac": {"node_ids": ["f94073b5-a48d-414c-a361-f450ac221235", "cc99360c-5dbe-44cd-b6f8-3da4bee0da3d"], "metadata": {}}, "6dafd937-f703-40eb-9b35-fdfa06374640": {"node_ids": ["3c000852-de47-4fd2-b35d-eceafb2c40ba"], "metadata": {}}, "2b115dac-cb32-4bae-99fe-9cc1bd788c9c": {"node_ids": ["ea7433e7-3b31-46a7-8b7d-49dd52a67092"], "metadata": {}}, "62d3220a-2116-4a29-bcf9-e84cd6910dc3": {"node_ids": ["23671350-d939-421a-a34c-3f554d4b7732"], "metadata": {}}, "2d51054d-6664-4ee0-ba71-88f5bacfdcd8": {"node_ids": ["e053b369-8a3e-46cf-aa99-440feb5677fe"], "metadata": {}}, "4690e286-763b-4f23-b1b3-009c74df0c5e": {"node_ids": ["11523e6e-5fd3-4123-b275-375c917397d1"], "metadata": {}}, "f5bf4f04-3d68-4d59-be14-3da4c8094397": {"node_ids": ["013a6a6e-d551-4110-8f1f-950c49371b8c"], "metadata": {}}, "0749e919-af60-4d3b-8b3c-333a623117a3": {"node_ids": ["3bf5c54d-8ba5-4445-a070-6b43818060c8", "5105cbbc-f838-4a7d-80cb-4364636a05fe"], "metadata": {}}, "ab5c9b84-0d38-4cbd-bf8e-62a83f09b1d1": {"node_ids": ["a945534c-7420-448d-8e87-c6d33d2c3cfa", "a9e90fea-f607-4f35-9541-9f0482a78a9b"], "metadata": {}}, "fdd9df16-251a-4233-8269-5b21371850ae": {"node_ids": ["996b2f85-5c85-455c-8ace-695a13f8abc9"], "metadata": {}}, "d6278ccd-0a3b-4828-be53-7a715d1aa418": {"node_ids": ["52fbd477-65f5-481e-b6b1-62ae318a58d6"], "metadata": {}}, "c276e4dd-7eb0-47ab-9fe1-01c8d3880ea0": {"node_ids": ["a2839109-e68f-48d8-a71d-0bd82f5fc770"], "metadata": {}}, "4429fcb9-241a-44ce-bbbb-28be8a228e9c": {"node_ids": ["6bb4d31e-11ae-420a-9df9-c37af4cef012", "bba56623-699f-4a36-89ce-8558b05ab7dd"], "metadata": {}}, "de6ba9c8-ec35-456c-a781-e8ecc34e7bec": {"node_ids": ["40825576-2e7f-43cf-88cc-408564d9c22d", "af5df226-9f09-4a63-97b3-4c3512500483"], "metadata": {}}, "4c57bff8-dbe3-4a5c-b732-d9d5217d4aab": {"node_ids": ["b622cef4-8291-4992-8708-e2387de255ec"], "metadata": {}}, "1239de6d-e7d8-4883-be0f-190f81e639b1": {"node_ids": ["de61dcce-8a06-4fb5-bf5a-90071c8c0101", "2fcbd6b2-9a38-45e8-a1d5-8cdfd676801c"], "metadata": {}}, "b56ed615-ceeb-4964-bc9e-bb355b4355a4": {"node_ids": ["c77a97ec-8bdc-407f-97c5-63f6a09cce52", "1d6de4ff-b77e-4757-826c-0492e208a5d2"], "metadata": {}}, "f5b49194-ea72-4320-91dc-edc64de669a3": {"node_ids": ["84b0b2eb-0474-4cfa-92e4-6b2a309e16a6"], "metadata": {}}, "c28fea0f-0f26-468f-9e3a-cab565d71d0a": {"node_ids": ["0267f680-2bfa-42ef-9f4a-7d8f0a609a22"], "metadata": {}}, "a1d3396a-359c-4e00-acc0-82ff635a98ab": {"node_ids": ["f3e7fafd-088c-4f6e-a8c6-1a9b3ed09427"], "metadata": {}}, "a5f9c112-b2e1-475f-b844-66038abff1f2": {"node_ids": ["3b30fde4-09c2-4aec-ac2f-b8b391196e19"], "metadata": {}}, "07f910cd-30a7-4e0a-9609-1f16f392929a": {"node_ids": ["f917a2d9-acc0-4957-ac3c-cbf5e0ab70c5"], "metadata": {}}, "4e9b9781-5e82-456a-80e0-5c8d45b01b1e": {"node_ids": ["2d1016ca-1c49-4391-83dc-ecd0f24fbfef"], "metadata": {}}, "8d1ec2ad-747e-4253-bd91-4f9272e45223": {"node_ids": ["59f71801-587b-40eb-8617-935de9c539ce"], "metadata": {}}, "8b5862e5-b71f-42f5-8b94-01fca00edd3f": {"node_ids": ["960b7b58-d6f1-41fe-ae5a-e5cf598e6725"], "metadata": {}}, "b7108cf2-e11c-4ad3-a4d0-017e967b34c0": {"node_ids": ["ac8e2224-55ac-40d3-9f53-94409eb8c01d"], "metadata": {}}, "4323689b-3faf-4181-b1c1-fdf3ec3d431d": {"node_ids": ["ba2fe01d-c6f7-4a63-911e-845a1bc74cd6", "9e2a3aa7-b0d5-403f-83fb-12fe8ab67bf3"], "metadata": {}}, "e2d43306-33c1-47b5-99c2-03c72d333ba4": {"node_ids": ["74e498c4-57e1-43de-9e64-3e1fa97cc358"], "metadata": {}}, "64ff6217-aa25-4300-af02-ab77711ac109": {"node_ids": ["b59250d8-29d8-4934-81f4-dfdd751378ea"], "metadata": {}}, "2ffee998-698f-47b8-84d1-f94c271167ac": {"node_ids": ["c382a921-4270-441f-a39f-01dc0475a6ef"], "metadata": {}}, "016912ae-85f0-4be1-898f-0279dd464857": {"node_ids": ["1e822eff-880a-4580-bd2d-828068c0a735", "fbce0d03-07a7-405a-a7d4-778ffdb84cc7"], "metadata": {}}, "eca9605d-a892-4b3d-8858-7c72e3405936": {"node_ids": ["33accfda-5170-4b59-9787-b813702a79cc"], "metadata": {}}, "4fdbe99b-a72f-4ea9-9d58-1efbbc0e8af8": {"node_ids": ["88a967fc-b8b6-4a22-b679-930c0d669887"], "metadata": {}}, "7da3d723-b838-417c-bbe8-a24a50b96ea4": {"node_ids": ["96c3832a-9271-46fd-ad6f-bef965528748"], "metadata": {}}, "60e079ef-a199-49c3-ac39-40f24a2eccfa": {"node_ids": ["69557b56-d695-4be0-b06c-a16756271a7a"], "metadata": {}}, "128bdb06-8cf0-4df1-b56d-fdd8ffd2769d": {"node_ids": ["352ba65b-fb57-40bf-8ddf-ce1e29548366"], "metadata": {}}, "689b0e3e-8117-4195-8759-dc5bb0960102": {"node_ids": ["deef38ef-2baa-47ec-a91c-8b57e945d812", "34b1e964-5f20-4079-b6a6-3bd00159c71d"], "metadata": {}}, "a2b2a7b7-fcec-4af1-b893-a7a5f1b6fb6d": {"node_ids": ["4bdfb0db-8258-4e14-90b3-cf81cd649223"], "metadata": {}}, "292d5589-3184-4ff3-b433-03daeaee1cba": {"node_ids": ["f5068953-bb3b-455b-82ef-0de4ed741b8b", "4579b779-7f65-4b09-8941-acf64d4fcb5d"], "metadata": {}}, "b0570d2b-7f87-4bb8-a029-0c83c23aea38": {"node_ids": ["d2912b69-f359-4b7d-a54e-36e1420fd862"], "metadata": {}}, "69d4d7ff-482f-4aa8-afeb-114aae7cfe25": {"node_ids": ["49cb3b07-c632-4a00-9571-c1ad136785cb"], "metadata": {}}, "4b8f6a82-8d7d-4def-8709-c09dc876bd15": {"node_ids": ["505ae009-11ad-461a-a326-c1f403176281", "3a81662e-6745-490d-ae37-d99a2d958098", "5b105eae-7e29-4e0a-afc9-2d7b65c3c77b"], "metadata": {}}, "7394abf3-6e8c-4750-86fd-81dbee960fa4": {"node_ids": ["72929673-b437-4081-94a9-57ce6c25073d", "fffe6a54-4d42-4a14-a05c-9b64cca9e222", "ab1b2ed2-72ac-4b0d-b667-e034487aa3b8"], "metadata": {}}, "9133d5b3-b632-40b6-b387-ac0329d734a4": {"node_ids": ["a4adfed4-ccf9-4e72-9c12-86d00ae9a256", "e2659d62-fb92-4038-b514-3c9317d1abe4"], "metadata": {}}, "554e003a-18f7-4226-84fe-3021466167a3": {"node_ids": ["7b8fc04a-98f5-47f5-9d1c-7e379229a881"], "metadata": {}}, "817bba4f-a97f-4a48-90a0-de713b5685c6": {"node_ids": ["b470ba55-e6cd-4cee-97f2-c562e542ebd7"], "metadata": {}}, "506cce11-d72a-405c-a08a-4ef5f19ad9bc": {"node_ids": ["b32d07e1-df88-4758-9b50-025a29a84a7d"], "metadata": {}}, "0996ba57-66b0-4db1-b8b3-cd9fa2e3b876": {"node_ids": ["6daebd2d-2c2d-4d8c-b144-b95afa7451fd"], "metadata": {}}, "297ac6a3-89db-4283-91b6-8ff124b88fd0": {"node_ids": ["408e5691-099f-4223-b137-d9c86f1dfbfd", "db357411-c164-4101-8211-5070391e40d4"], "metadata": {}}, "765b95d7-7a8e-473e-9cbe-e435fe78268a": {"node_ids": ["cdd7138c-81c1-4e18-89c3-02b02d81d930"], "metadata": {}}, "c478a956-8f9e-492d-a6c1-6ae1cd7e6a9a": {"node_ids": ["5fd0f356-20b5-4be9-94f4-f45f8e44c155"], "metadata": {}}, "43782e9d-aa09-436f-a65d-7b7a5b82c66e": {"node_ids": ["9ff732f0-d8c2-445f-8291-0881fa48edeb"], "metadata": {}}, "df00ac50-61b0-4923-b855-15bb7066d27f": {"node_ids": ["9bbd7ac7-afbc-4c94-a72c-a7c0e8d1e597"], "metadata": {}}, "d8850640-e1e5-4d63-a1b6-b0f874a8eac7": {"node_ids": ["3a17bcae-60fa-41d6-807e-d6394d06e32e"], "metadata": {}}, "1aa02e05-6795-4fb4-992d-ce56d9fc6590": {"node_ids": ["9777acee-6cc8-4661-a04b-592b172a6a01"], "metadata": {}}, "8db11412-4ed1-4c39-91dc-1c155d197d36": {"node_ids": ["9e074fbb-0a90-4dae-9a3a-25e6e604a35f"], "metadata": {}}, "bf4f6a01-18f1-4d3a-8c8d-8710bf99147f": {"node_ids": ["35314791-4a1e-416d-908c-f7668f875876"], "metadata": {}}, "fa86fa59-3492-42b9-a42b-f32b374b009b": {"node_ids": ["880ade31-999f-4e82-a6ef-78550859d536"], "metadata": {}}, "e9bd26e1-2124-42b9-afcc-d7a70506bee3": {"node_ids": ["79f530ba-9140-4a7d-adba-5a41c28b0da4", "de2f1616-35f7-41da-a1a7-40d3d6a9b107"], "metadata": {}}, "5d441ac7-0382-4003-85c4-e87e55ba43c0": {"node_ids": ["d3fc9fef-176e-4336-8bfe-29cf61ac25b8"], "metadata": {}}, "fc7115c2-4292-4f6a-bd89-2f0a8e055c4b": {"node_ids": ["3443094a-9a77-4792-b5a2-25acba5e3435", "53787251-5f15-4db9-bbff-7827715814c3"], "metadata": {}}, "2841b048-9911-4aa9-9cb8-fe075bc96d24": {"node_ids": ["e64f9311-e1c0-4873-a0d6-64d5389d9d07"], "metadata": {}}, "227f9f2f-a929-45ce-bc7c-f8e455f35aab": {"node_ids": ["aba90739-dd09-4f5c-a892-7283ccf6bc35", "b998e5ba-567b-4c7a-b8dc-7895b7d20daf"], "metadata": {}}, "7bb4dab8-4e1c-4152-831e-706fc4f6a767": {"node_ids": ["656d7a6c-7def-4ace-9419-982f795e1876", "23c21fb7-566e-4a53-a9e5-f97ba185cbf7"], "metadata": {}}, "ed2aa957-10f0-4448-8439-f32d70a39b30": {"node_ids": ["80255d6f-d21a-4c7f-a0fb-26b518e6e821", "db47444a-ca0a-430b-b735-8340c41c98e0"], "metadata": {}}, "abb0738f-c997-4c04-8e83-ebbc06f5917c": {"node_ids": ["f9ea7240-5429-44fe-9077-77b5f784458f"], "metadata": {}}, "6caf45d1-9568-467b-8133-777c06fe22dc": {"node_ids": ["05dfea33-ecc4-4f46-907a-b9270bc86f1a", "741bb11f-cf11-4931-b14e-83a90fe593ee"], "metadata": {}}, "c1c1c936-ad0d-4985-b574-0d86fa19e532": {"node_ids": ["43dc2345-3388-4c14-bc4d-012d3535b56f"], "metadata": {}}, "d5404f4d-381e-4f20-8783-5eee5b041b01": {"node_ids": ["d4865e2f-e05b-430d-91a8-5e3ac06e19b9"], "metadata": {}}, "ef0e2448-0867-45cd-bc5e-34ea0e7dd7b9": {"node_ids": ["abde9292-bfc8-4e86-8f87-28c69ee56044"], "metadata": {}}, "13bdbcba-8f85-411d-969c-93052675888b": {"node_ids": ["845b5e1f-77a1-49f1-91de-75733653c480"], "metadata": {}}, "94184d0a-fd12-463c-b4de-5841883be5ce": {"node_ids": ["de79ae85-0b90-44c7-b1e3-a8dd0b493621", "8be1cd02-866d-4572-9482-5d1733527fe4"], "metadata": {}}, "9dc2f831-c9ad-4c45-b3e4-070693cb9752": {"node_ids": ["5c398b0a-1cfa-476e-9030-0e02a96e661f"], "metadata": {}}, "ce43e60e-deda-42d5-8fc0-9b6da3642657": {"node_ids": ["d39bcd39-4bef-4b1d-a0d8-16048d54422b"], "metadata": {}}, "01deaba2-86c6-4349-8b4a-bb4bf0319949": {"node_ids": ["5dd3cd0e-e0ae-4e47-a32d-66eb1323bbfa"], "metadata": {}}, "c1f1200e-fad5-4c03-abf0-51801fbcd78b": {"node_ids": ["e85ed627-7757-4f92-8f7c-834505713d79"], "metadata": {}}, "8298f5eb-1298-4b53-81cf-c7f00d278920": {"node_ids": ["8072bb11-87b7-432e-87c7-c20a8465a5a1"], "metadata": {}}, "ecf63efa-60fe-4978-8b13-3eec2d512d6f": {"node_ids": ["80208e01-50fc-4a12-92de-7bfa947db644"], "metadata": {}}, "c74d7cae-3088-4e9e-8f69-ac5f0c4715bc": {"node_ids": ["d19c8e45-3266-4bbe-8c59-0d5148085cec", "4c70ac67-851a-45b9-b24d-80d83ac0c01f"], "metadata": {}}, "88975579-24bc-415d-b4ed-068de3542f92": {"node_ids": ["ffd27e79-57c3-4fc0-97de-1ccbc8d80683"], "metadata": {}}, "d640b365-9338-49ad-b5b7-2db345f53525": {"node_ids": ["d9ba4453-af93-4380-bbcc-3c90e268eda7", "2c4a8c9f-7d12-4e17-a46c-bb3a86b65124"], "metadata": {}}, "0d7a532a-8a66-45cd-9cc2-a9ffa73084f4": {"node_ids": ["40df9145-a397-4196-b6cd-bf363bef1d65", "861c327a-777a-475d-a6d1-837b5ff6ae80"], "metadata": {}}, "6b5ae73e-e59f-47fa-ab4e-839672fe048a": {"node_ids": ["9a322d3f-2b43-4061-b1d1-abe99b7ac834"], "metadata": {}}, "3676db4c-277e-469b-bbe7-d56d40f92dd7": {"node_ids": ["c52b97d3-0a8c-4772-8bdb-e4bd66770eb7"], "metadata": {}}, "2105a07e-8707-4901-be51-9087aa3ddc9f": {"node_ids": ["0fd9c239-438b-4139-8580-13f8333ebeeb", "4bc28f09-664d-492c-8ebb-7fb9d4d3e9d3"], "metadata": {}}, "78600fff-960f-468b-9296-3f47b3e5d22c": {"node_ids": ["aabe4c2c-e3f1-4d40-bfda-019374197927", "ab6d0bda-a7ca-4a52-a085-88545b1778cb"], "metadata": {}}, "52d90776-0151-4b29-bac9-cb34572422e3": {"node_ids": ["1a0ba75f-2552-46b7-ab96-05e693a4a6e0", "882bef19-bc76-4b65-94a7-6ef9dbc509d3"], "metadata": {}}, "ea7c5271-9630-4231-b068-5ab591a699d2": {"node_ids": ["b81694da-c410-4267-9bb8-c3834314e00d", "c0476b83-bc22-48a5-9142-0b55717d8e1c"], "metadata": {}}, "7afd3e10-1ca2-471d-a621-75c1dde510a2": {"node_ids": ["54649484-f0af-41af-a4e8-a218e850c908", "ccb4e6f5-e09e-46d0-9a7f-aa5810c42bd5"], "metadata": {}}, "6f9f3699-81e4-492d-9b2b-1d8d500e14d4": {"node_ids": ["405eea68-78d0-4531-b0ae-3f57b94509a5", "d60ec123-3928-49b7-8155-2e418d84de73"], "metadata": {}}, "89859f62-6d25-475a-ac37-d8d46e974607": {"node_ids": ["b0936e70-74fd-460a-be27-067e462b545c", "848fda35-49cd-4b13-be0a-325521247386"], "metadata": {}}, "0825c0e2-b0fb-4a2b-8fef-ac7f7fbaf7f2": {"node_ids": ["761e7da4-8852-4cbc-825c-af3c81d9974e", "e54863c2-008e-46d4-be11-8da57d5c32e2"], "metadata": {}}, "1095d662-7e2e-4dc3-9f35-abcc461be4b8": {"node_ids": ["f7126876-5083-46a7-b415-2f3bf5499a60", "0ba60313-48a5-4580-92e2-218ec42bf988"], "metadata": {}}, "0106afbf-b82d-411a-8a1a-848383ba3673": {"node_ids": ["42af3159-369b-4328-8b74-a84abe48f9b9", "3ade21bc-d790-4fbb-bd03-8b3749b9ed00"], "metadata": {}}, "25eafcc0-678b-4c42-8644-ded427c5a99e": {"node_ids": ["a3356ba8-be4f-45a1-970d-deeae023650a"], "metadata": {}}, "e29c671f-515d-4c2c-9106-348dfebbd811": {"node_ids": ["ae308dda-de14-447c-8e8e-cc6ffd4d72bf", "b56f57da-cfe6-4957-b283-5eceed098ce7"], "metadata": {}}, "87201f69-a510-4839-9328-3fae3cf3af3d": {"node_ids": ["8bf96c12-68d2-445a-b887-1bd900e22f0f"], "metadata": {}}, "cba32d09-f544-49b8-9e82-99d49c199884": {"node_ids": ["52baddc7-461a-4be8-bde4-d00f50dba61e"], "metadata": {}}, "9d501f35-0963-49d2-8548-97a247a39c4d": {"node_ids": ["6a1f7780-dbf9-4a95-89bc-ee6e1acc0428"], "metadata": {}}, "e2ba21c9-dfb1-4a39-bdd3-9f935d27a740": {"node_ids": ["ca538e47-e711-496a-a2e4-5c2563d39594"], "metadata": {}}, "69e278a4-3f40-4360-92a6-5a4003d9e0a7": {"node_ids": ["222e5948-86c8-45a0-a2bd-0a54387ea385", "281a1bc5-7ff7-4927-a82a-8094c6826990", "0b937449-32e0-40aa-a9a6-b20817156dbc", "3c90e079-7bb1-4622-85e9-c84cc94f8a57"], "metadata": {}}, "c48e9425-f7f7-44af-ace4-d8a338d7ff09": {"node_ids": ["ca2493cb-040a-4671-a442-2e97db0df167"], "metadata": {}}, "1d1bd8e4-9640-4f75-9005-1084c9a0cb54": {"node_ids": ["25d99cd4-75bc-46f0-9acc-037401c44e90", "824efe4c-d185-48c9-80a6-750a96533fd8"], "metadata": {}}, "a2e421cd-b4e3-48c8-bec3-c25643c0e7f2": {"node_ids": ["5d452405-eeb3-41f8-a264-c4bc2bb2c651", "bd3bef54-a320-43fe-857f-e9b70fd8274a"], "metadata": {}}, "45e2e642-e9b3-4f35-854b-399234699578": {"node_ids": ["1045f041-ceaf-4ea2-8df8-cb1e6e1dcaf7", "1a2ac2ef-70a2-4456-a2a5-ef3f94b111be"], "metadata": {}}, "1d508908-1077-4695-8eb8-1fc17ec5111a": {"node_ids": ["f6f19c5f-87e6-4514-90d2-2c5d3f41da7e"], "metadata": {}}, "fa18b795-06f5-43ca-90f2-7d3038fc9dca": {"node_ids": ["7940baf4-e86f-407e-aa36-785357169a41"], "metadata": {}}, "cdacf8a0-0029-4d74-81e1-b539424177ad": {"node_ids": ["8f149a63-6228-4fdb-b6f1-191c4f2e9f70"], "metadata": {}}, "da8e26c2-cfac-4aff-a81e-18a8275c1324": {"node_ids": ["fc7959ba-8435-41f2-87d8-fe95879d9025", "3cb937e8-489f-498a-9b14-261eea11cd5c"], "metadata": {}}, "6195b65b-3a36-4576-8446-afd7f548778e": {"node_ids": ["ea948b2f-6610-4b0a-9a51-e9c9bae2d7f3", "43398376-a4e5-4ca6-9391-1cf5529b8515"], "metadata": {}}, "1d2b7df0-0d7d-4d43-a998-ac9bd1d04b63": {"node_ids": ["fc9d2460-10a1-4ae4-a1ea-367ad4c03055", "88b3022d-8bd2-4efc-acda-3a9824764c2e"], "metadata": {}}, "5ab13869-9119-45b4-9a5b-02a035680d71": {"node_ids": ["d2c95240-b83a-476a-a383-0e1e892880f6", "c0459641-e4cf-4317-ab31-e4e5731e1788"], "metadata": {}}, "64030abb-a341-4bcc-98d2-cba573e009da": {"node_ids": ["0563ede1-e8ae-4889-88fb-5ef44c749ca3", "b704b4ed-8e14-482d-9603-9bcc0ae4f5f2"], "metadata": {}}, "b2626821-8be7-4945-b2da-61289c2fa823": {"node_ids": ["424f8222-10fa-41f6-9a00-edb25f5345a0", "a017080e-4f9f-4fb7-b889-967007504c33"], "metadata": {}}, "0cfde65f-f4e3-48dd-88ab-bb761abe7b29": {"node_ids": ["e3116f52-5638-4cd4-b917-5773b1015fd9"], "metadata": {}}, "ce9541f0-9a1a-4bd2-b222-95148eec5c29": {"node_ids": ["5a07e326-d505-4585-b191-d284dcf310fc"], "metadata": {}}, "e4ee5fc6-3fc3-4fdf-ae6a-36b0b1f8fb65": {"node_ids": ["b4613155-f9da-4648-9f09-d785155c3511"], "metadata": {}}, "99a75429-7639-43fe-a607-2df150a76a97": {"node_ids": ["44a7d982-5a77-4a88-b135-e89f523cb4ff"], "metadata": {}}, "607c7cef-b614-41f4-85c8-c65623ec5c7c": {"node_ids": ["baebd827-e5b4-44aa-bca9-6b3c1119b967"], "metadata": {}}, "f01f4512-d31b-4b34-8b1a-1216f77f186b": {"node_ids": ["d3466bb3-3f61-496f-8d37-00da4d6c4055"], "metadata": {}}, "b55d0c87-2b99-4ea2-8be7-afabaa0adf0e": {"node_ids": ["3478e146-d55f-4b21-b054-3dfc060ed083"], "metadata": {}}, "a7c35154-1998-45dd-8613-5af301ba4af1": {"node_ids": ["3a2dd02d-062b-4add-b346-06a08f64221e"], "metadata": {}}, "2fccbbd9-0eed-4502-8b00-ee971fadb32a": {"node_ids": ["9ac3f944-8d52-4b76-bc05-813c301ce072"], "metadata": {}}, "de2b33cb-3dae-47db-9394-f5866ace0cca": {"node_ids": ["e357e088-670c-4842-b540-903fd4293b87"], "metadata": {}}, "d9daf3b4-5159-409c-a2ce-807ba0233721": {"node_ids": ["eb060969-cb55-4f3c-9cd6-bba0a7ae6583"], "metadata": {}}, "14d8a03a-2214-4539-9c5b-fd6b58968c9a": {"node_ids": ["57284202-cce4-4812-b82f-1a190f444061"], "metadata": {}}, "2a156122-9e72-40f0-862b-a4301d2b2da7": {"node_ids": ["c0584527-9447-4c4d-84c9-4c2665f997a1"], "metadata": {}}, "bd65fa12-aa34-45ce-ab9b-71ece6f70a7d": {"node_ids": ["bd64f525-e6d3-4968-84e7-fc957da12524"], "metadata": {}}, "e6e5bf31-41eb-43de-a6b6-746fc8d52230": {"node_ids": ["ee471490-debd-45e2-9bf0-3462f2620c33"], "metadata": {}}, "0c58e43f-b0c4-4fca-b572-806b3bed2976": {"node_ids": ["546c059c-6c52-4502-859a-4b00394f2b82"], "metadata": {}}, "2edb74fd-43a3-402c-8445-7cc985047c9c": {"node_ids": ["0467414e-4fc9-40ec-9dfd-c00b29530a63"], "metadata": {}}, "8451ab4b-8d01-4a34-9d4c-411736de8603": {"node_ids": ["11fc4d0b-0562-4d8d-8d39-252ca1c001be"], "metadata": {}}, "80285fa3-ae7f-47d6-813d-9d6175f53962": {"node_ids": ["2f20a9eb-ab77-48e2-b324-e350c9fcfff1"], "metadata": {}}, "7440c110-a5b0-4273-afef-b28801e2dc63": {"node_ids": ["8960c3e1-cde1-47f0-9a23-dcd6abf5e25b"], "metadata": {}}, "8002965c-6e10-4907-a270-6fd9c8097c12": {"node_ids": ["f09e7d54-03bc-4ad6-a68a-83c81274ab42"], "metadata": {}}, "76874a1d-d565-4127-b90b-e80cf519a911": {"node_ids": ["452565f7-75e2-4ca7-b623-15a97207f8db"], "metadata": {}}, "ad79f79d-97bc-4b79-b12d-2c4725c583d1": {"node_ids": ["df2c35d2-8225-4634-90d1-b9508e83f8ff"], "metadata": {}}, "4f807cbe-5f10-4af6-a953-35220137d510": {"node_ids": ["32ea1ebd-52ba-469d-ab6d-7eaa44308208"], "metadata": {}}, "295a4f87-907b-4079-bcbf-c9b0c4db1365": {"node_ids": ["7e01ab6b-b497-433f-a662-349705bb2875"], "metadata": {}}, "7c0d045c-6e11-4640-b7f6-8765969923c2": {"node_ids": ["c4d64bef-9d7f-4f36-902b-87689ce47641"], "metadata": {}}, "d0c468d8-81ce-4f7e-9325-113947e69fcf": {"node_ids": ["6f21f7ac-39eb-4f70-9d31-ef7d81e659a3"], "metadata": {}}, "bf9320ae-56c5-4c79-b577-4ce45a5126d4": {"node_ids": ["bf1d653b-7b06-438a-a543-83b31d4ff5b8"], "metadata": {}}, "31b7946c-683b-435a-b39f-e1d39da093bb": {"node_ids": ["c09b6203-cfe7-4842-a024-e4b41c88ec7f"], "metadata": {}}, "22987d10-3c54-4c7d-ba44-6a334a25d4b0": {"node_ids": ["1a249a43-a580-4a66-871f-16dce3eaab98"], "metadata": {}}, "8d71be8a-7b0f-4d96-bf7e-d9bab0db008f": {"node_ids": ["2795b75d-d9c8-4843-8749-4c1efd28e370"], "metadata": {}}, "3dcc8d03-06ab-4588-8d5c-bd0db6c406e9": {"node_ids": ["28fe55ae-7656-4cf4-a0fb-a48de7d7244d"], "metadata": {}}, "c2dd3d89-e0f2-49d4-846b-5a4c897db878": {"node_ids": ["5b5b956a-bcfc-4bfc-a9c2-8516e983db37"], "metadata": {}}, "773f0249-f85d-4d43-8a80-9a8555b60715": {"node_ids": ["8b496285-34e9-4dbb-a4b0-b66b02c55dc2"], "metadata": {}}, "237efde8-02cf-4d81-8cc6-e1a7a0211568": {"node_ids": ["a02c6a00-5c8c-4119-8868-96e0fb10b81f"], "metadata": {}}, "20fc4542-1c44-48b6-b10b-dc2ec0b186a3": {"node_ids": ["46f75e9d-83fc-49b8-85c2-70bd04da440b"], "metadata": {}}, "34b3522a-8d41-4f6c-b3ca-4d7ba8bac658": {"node_ids": ["9fc2a466-1999-48d6-89e2-45845a9c85ee"], "metadata": {}}, "9ce2d18d-74a7-4742-9fe8-6c0301d46ace": {"node_ids": ["630ad981-5fc1-40e9-b3c9-f4f20b9a1892"], "metadata": {}}, "2ffb84b8-0c0a-45a5-a533-6a89fba850c5": {"node_ids": ["c400e8ce-7b1c-4606-a672-93f7af5fdd04"], "metadata": {}}, "668fe9bf-02e0-4518-8a22-f3368608d1b2": {"node_ids": ["f3f4bd4c-d4b5-4225-a2ca-3f007dd6ae4c"], "metadata": {}}, "50f74c09-f222-4af3-abec-6b4df3da63f5": {"node_ids": ["288fdf20-f0a5-424b-b42a-575dfef5fbd4"], "metadata": {}}, "999e4dc9-2f1a-45fb-9658-0cfb9cf3dd47": {"node_ids": ["f2e99a4e-5875-4ea6-af0c-7b8605c3ebfd"], "metadata": {}}, "82ee5183-d20f-4fc9-88e1-e990b3526918": {"node_ids": ["68cc4810-eeef-43b8-b599-ad9278c709e5"], "metadata": {}}, "2f91b837-fd94-4813-b2ab-a58be62c3e4a": {"node_ids": ["fb6c8d10-77e3-43de-820a-dc448e147f50"], "metadata": {}}, "acf6df41-134f-4062-b5f9-bd04fbabffc2": {"node_ids": ["3c01b273-4bd8-4cc1-ae1f-02a2759a6e06"], "metadata": {}}, "4ed27b10-e7d9-4664-93ad-9c36d6f4f032": {"node_ids": ["c9899291-6d8c-4e96-aef1-eea5415e583e"], "metadata": {}}, "e309dd5b-52ff-47f0-84f9-0be0a1e3ba07": {"node_ids": ["2ae11819-bdd6-4442-9d42-0423ce0d1494"], "metadata": {}}, "bf72bcf6-8b39-4a46-8f66-d38b9b5f67d3": {"node_ids": ["06baff55-bd19-4cce-87c0-3d09702493cd"], "metadata": {}}, "b0c6bad1-4df4-4755-b2f4-bb537c8e219f": {"node_ids": ["5df63d7b-8088-4114-ac3d-452a6e7f0b5d", "196820ff-8c45-4d57-8d3e-0cdc452de927"], "metadata": {}}, "2de5b6e5-d385-4ca6-a646-b74ab2ec695c": {"node_ids": ["9169bd3b-f98d-4f88-ae05-4e481444a99c"], "metadata": {}}, "26e71e46-ff4b-46bd-8f4d-2f3ef79ecfa3": {"node_ids": ["9076c44f-5e82-45dd-a754-e7eff254eb31"], "metadata": {}}, "3a5d6bcf-58d3-4ab8-9cf7-3115c85f7dbe": {"node_ids": ["e574c12f-81c7-4ad3-8857-5365e8699520"], "metadata": {}}, "40f133c5-088a-4c4e-b570-e1f62fdc75ff": {"node_ids": ["18e7290b-76af-4900-8221-cc450bf6f102"], "metadata": {}}, "b5646738-50e3-4fc5-a9ab-3c137642ef66": {"node_ids": ["cae01cb1-91df-44c6-a6b7-9aabdb757c16"], "metadata": {}}, "7569abaa-5b91-4407-bc3e-b9c2d0e0be71": {"node_ids": ["edb9d7be-7ea0-49cb-a3d1-eec7e6cb36ad"], "metadata": {}}, "e18db6f5-dd36-4f85-a1f9-64045a62beb2": {"node_ids": ["b86b2985-a4d4-4c42-9b68-f42ddd237573"], "metadata": {}}, "3d7d3c32-a711-4c19-bd7f-92f59685556f": {"node_ids": ["b32431b4-6f96-402b-800a-c60e849ebfce"], "metadata": {}}, "f0dd7bd6-67ea-48bb-9973-7b6ae430a892": {"node_ids": ["d8734d82-4d9a-4e3a-9ae1-dca6f65891ab"], "metadata": {}}, "f3a48c5f-496a-4b14-af08-fb39f03370a4": {"node_ids": ["01172341-d76b-4ffd-8ebb-8e59a49f4b06"], "metadata": {}}, "21795d51-479b-40ad-a150-aac48fae5962": {"node_ids": ["6b5a02cc-35fc-4fa9-a9d2-b1f36a697e14"], "metadata": {}}, "609fd3b8-4f1b-4e43-887d-8b5e5cd945d4": {"node_ids": ["7c0bdc56-1d2d-441b-8d21-46a0697db649", "dabcc144-efb7-4e05-8d81-939d52f156a7"], "metadata": {}}, "1af302f3-b0d3-431b-b95d-9472fd47cd27": {"node_ids": ["c731ebae-4c4c-4083-baef-014f44f1c49e"], "metadata": {}}, "9bfaabf7-5c60-4c85-aca5-381b31f75774": {"node_ids": ["6979f13a-4f38-4cc6-a076-68ecd79ac039", "3e08bd7a-7bfd-48b9-8a95-5b88f47bbf53"], "metadata": {}}, "0748b6f2-0135-451c-ad7b-13b12f1db444": {"node_ids": ["0f449792-62dd-4874-8473-0cf1c56d6f99"], "metadata": {}}, "79ea9571-1a91-438d-9c62-5ecd2adebaac": {"node_ids": ["44c71317-9345-41d4-a7e1-c383ceee95c9"], "metadata": {}}, "7d34ef88-55ca-4bae-b6ad-0e4318895305": {"node_ids": ["10f2c3de-efe1-47f3-b441-91df604ad5f4", "11dd0dcd-7efc-4312-82a7-7dd8fbe7988c", "aef2f663-7aca-4877-96ac-748136df92bf"], "metadata": {}}, "b7ac12c6-e24f-4cf0-a601-e16be87f36da": {"node_ids": ["e128f73c-e338-4ee5-ae2a-9f4d71047e8c", "14c2450b-f530-43dc-bda3-a3318ce76eac", "7827d715-6f00-4efe-ba2f-59e8565404ad"], "metadata": {}}, "d40b8c03-0d86-458a-97ba-d1986dc3e9ef": {"node_ids": ["01a93ebb-101f-4a55-a36d-908541ad9f71", "e27675e7-a242-4582-8ef7-f5781e55f524"], "metadata": {}}, "357ee6c0-4806-49c3-94ea-903a94afb69f": {"node_ids": ["94e1d8eb-9958-4ed4-bfeb-0a065c1542bf"], "metadata": {}}, "dc317cf1-b705-4d8d-9dc0-2958ee98be27": {"node_ids": ["a4338db1-5a15-457d-9869-403c5356bc9a"], "metadata": {}}, "4044e15c-8f00-4041-aa79-2d4991feac5b": {"node_ids": ["05cf5c28-7526-4c1d-a1c3-276dd9605b86"], "metadata": {}}, "07e617ae-fa18-4210-9b51-b394b5a0fffb": {"node_ids": ["8d31348e-70a6-4212-8eb2-d2909e182ac0"], "metadata": {}}, "6a3dd0d7-3a7e-4d6f-a94c-d3bbfa985eff": {"node_ids": ["a10cf413-1be5-4783-b0cd-339c5afbbe86", "cdd5d77a-f6d9-44ad-aa42-13edc4d90806"], "metadata": {}}, "c54c1f00-fa98-437b-99f9-7fd9426472ee": {"node_ids": ["f55c64c3-7c35-4ce8-ab63-ecee2d0f70ae"], "metadata": {}}, "e4dd0687-9f00-4b8a-9709-9c222256cabf": {"node_ids": ["c526329e-1e8d-4f5c-b10f-bdd2e028b2fd"], "metadata": {}}, "3c4413cd-d62a-4201-aa1d-49a4583a0bb3": {"node_ids": ["5709460d-cf04-4f42-9d20-11f703381225"], "metadata": {}}, "391aa0f7-6325-4517-ad24-15e3efcf61f6": {"node_ids": ["4603d6d1-873c-4316-a400-cbdc1334f250"], "metadata": {}}, "d21e51f0-3ec7-4a3d-8868-1c39fb86ca72": {"node_ids": ["80e11702-b6cf-4b3b-a0fb-54f60ef6dbae"], "metadata": {}}, "71321f4c-a11b-43b1-9919-2350469415a0": {"node_ids": ["0a4d9f8a-bee4-4635-b18c-fb55e04dab08"], "metadata": {}}, "57f2020b-d272-48b7-a5aa-76ce658bc649": {"node_ids": ["1d9388fb-2824-4705-a8db-703f8c20b973"], "metadata": {}}, "91b7c270-194e-4fb5-a8a0-e82033bced17": {"node_ids": ["db28afe2-4eab-4dda-8289-89aaea5bcd85", "9a392518-ba7b-4ee5-a096-cf3f2e2444e6"], "metadata": {}}, "39bb65a1-f964-42cd-a973-29a48f3bb2ae": {"node_ids": ["a8091b6c-4a36-4312-9cd1-218af5842e12"], "metadata": {}}, "71fe3a08-92b1-4c0a-bbd7-310d27d456fd": {"node_ids": ["e34ef8ca-0ecb-491d-987c-4e0ca7295f5c", "45662eb7-0374-4eaa-81a5-1d6206504c70"], "metadata": {}}, "05a406b2-8f89-470b-abb2-77826c6f0f7a": {"node_ids": ["4e1bb9a6-a278-48c2-975a-b568abdba7d9"], "metadata": {}}, "df552c0a-6b1d-4193-aab0-2b7a2700446d": {"node_ids": ["86fb7173-cbac-4724-94ec-75b78372f4dc"], "metadata": {}}, "5c69e55d-dddd-4ce2-a020-e57b47de7bd3": {"node_ids": ["735ebfdb-b7a1-45c6-8f1f-06e30707ee1a", "1bc0d70f-c3e4-49b4-8e6d-0fa59a80d689", "4a04d290-5e86-4169-9af0-fb8f595c6ab0"], "metadata": {}}, "e416a557-24dd-476e-bf15-e562a4b7fd96": {"node_ids": ["297ad5ff-c67b-4105-b259-25e3e2d66900", "e80eb6aa-28a8-4422-afb2-12f3e4ba0e92", "b4f56589-3301-49e5-97ed-055008cbc288"], "metadata": {}}, "3a7e2428-b6bd-4887-ae8a-60bb48d803ff": {"node_ids": ["2685f31f-3fe8-454c-a288-5c7af5a46e0a", "853f116c-4b54-41f6-b1a7-268131d96c17"], "metadata": {}}, "ac76fdec-72fb-4c2e-b550-fd6e2ee7e59d": {"node_ids": ["da4b72a2-878f-4d0c-80d7-7687880e93ff"], "metadata": {}}, "1a40403a-cf57-4f3d-8f5d-d0982706f8c7": {"node_ids": ["c0af7a4a-4e38-4f22-80eb-fa6ade1d194b"], "metadata": {}}, "67b0ea24-89ba-4dce-899f-bb1a0545acca": {"node_ids": ["e3f9715c-96e9-4cc6-8f3f-422b10c1d0be"], "metadata": {}}, "67af852a-0306-4028-b539-2d92b33a67d4": {"node_ids": ["7ea73fd7-98d1-420e-9002-36c8ca5d41b5"], "metadata": {}}, "e8305e55-bbdc-4c79-bd10-fa997110aca4": {"node_ids": ["2545564b-758d-4ed4-a62d-fefb63b7ee30", "96201a9a-d237-41eb-8749-c74a4443ee6e"], "metadata": {}}, "96225294-ca1b-498d-b55b-aa07443ea6ba": {"node_ids": ["4a2ad900-a01d-4a43-bb31-4d8a4ed2ac8c"], "metadata": {}}, "543a6e39-ed15-4284-881c-370c3c5f837c": {"node_ids": ["ee2674ba-3a3e-4751-87e0-b8970a287dd4"], "metadata": {}}, "63bfc2a6-ff00-4c9a-b939-d9038db64301": {"node_ids": ["00f7ca7e-fe65-425e-8a64-eb1327dde448"], "metadata": {}}}}