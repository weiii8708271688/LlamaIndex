{"docstore/data": {"936122c6-bbe4-47a9-b179-1c52d61b8ab8": {"__data__": {"id_": "936122c6-bbe4-47a9-b179-1c52d61b8ab8", "embedding": null, "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5845b888-160d-42c4-b417-c8b5383922d1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef6afb73363bec2b7545f549f87f65a6f0b896bfbda58fac205da6f6ed18e115", "class_name": "RelatedNodeInfo"}}, "text": "ColPali : Efficient Document Retrieval with Vision Language Models\nManuel Faysse*1,3Hugues Sibille\u22171,4Tony Wu\u22171Bilel Omrani1\nGautier Viaud1C\u00e9line Hudelot3Pierre Colombo2,3\n1Illuin Technology2Equall.ai\n3CentraleSup\u00e9lec, Paris-Saclay4ETH Z\u00fcrich\nmanuel.faysse@centralesupelec.fr\nAbstract\nDocuments are visually rich structures that con-\nvey information through text, as well as tables,\nfigures, page layouts, or fonts. While mod-\nern document retrieval systems exhibit strong\nperformance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hin-\ndering their performance on practical document\nretrieval applications such as Retrieval Aug-\nmented Generation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e481ea7a-9638-479c-9958-7cc07d1465cb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9ae6faaf-3449-4b0f-8674-c3e403c758db": {"__data__": {"id_": "9ae6faaf-3449-4b0f-8674-c3e403c758db", "embedding": null, "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5845b888-160d-42c4-b417-c8b5383922d1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef6afb73363bec2b7545f549f87f65a6f0b896bfbda58fac205da6f6ed18e115", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "936122c6-bbe4-47a9-b179-1c52d61b8ab8", "node_type": "1", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "324f77e33d7a69a10c1d94e96aa9aa2462213bb9dc4d67a18af8a2b5bd6ac505", "class_name": "RelatedNodeInfo"}}, "text": "To benchmark current sys-\ntems on visually rich document retrieval, we in-\ntroduce the Visual Document Retrieval Bench-\nmark ViDoRe , composed of various page-level\nretrieving tasks spanning multiple domains,\nlanguages, and settings. The inherent short-\ncomings of modern systems motivate the in-\ntroduction of a new retrieval model architec-\nture, ColPali , which leverages the document\nunderstanding capabilities of recent Vision Lan-\nguage Models to produce high-quality contextu-\nalized embeddings solely from images of doc-\nument pages. Combined with a late interac-\ntion matching mechanism, ColPali largely out-\nperforms modern document retrieval pipelines\nwhile being drastically faster and end-to-end\ntrainable. We release all project artifacts at\nhttps://huggingface.co/vidore .\n1 Introduction\nDocument Retrieval consists in matching a user\nquery to relevant documents in a given corpus.", "mimetype": "text/plain", "start_char_idx": 679, "end_char_idx": 1575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e481ea7a-9638-479c-9958-7cc07d1465cb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c9bc29a3-4e6d-48a3-bf2a-c12ab7dad273": {"__data__": {"id_": "c9bc29a3-4e6d-48a3-bf2a-c12ab7dad273", "embedding": null, "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5845b888-160d-42c4-b417-c8b5383922d1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef6afb73363bec2b7545f549f87f65a6f0b896bfbda58fac205da6f6ed18e115", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ae6faaf-3449-4b0f-8674-c3e403c758db", "node_type": "1", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3bb2a08ce240ca18ae71da22b61a32a09dc8b928ac507d5b410eb84a446b8cda", "class_name": "RelatedNodeInfo"}}, "text": "It\nis central to many industrial applications, either as\na standalone ranking system (search engines) or\nas part of more complex information extraction or\nRetrieval Augmented Generation (RAG) pipelines.\nOver recent years, pretrained language models have\nenabled large improvements in text embedding\nmodels. In practical industrial settings, however,\nthe main performance bottleneck for efficient doc-\nument retrieval is not in embedding model perfor-\nmance but in the prior data ingestion pipeline. To\n*Equal Contribution\nFigure 1: For each term in a user query, ColPali iden-\ntifies the most relevant document image patches (high-\nlighted zones) and computes a query-to-page matching\nscore. We can then swiftly retrieve the most relevant\ndocuments from a large pre-indexed corpus.\nindex a standard PDF document, many steps are\nrequired. First, PDF parsers or Optical Charac-\nter Recognition (OCR) systems are used to extract\nwords from the pages.", "mimetype": "text/plain", "start_char_idx": 1576, "end_char_idx": 2523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e481ea7a-9638-479c-9958-7cc07d1465cb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "144ee7b8-5274-4ecf-ad41-3bf4690cdaa6": {"__data__": {"id_": "144ee7b8-5274-4ecf-ad41-3bf4690cdaa6", "embedding": null, "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5845b888-160d-42c4-b417-c8b5383922d1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef6afb73363bec2b7545f549f87f65a6f0b896bfbda58fac205da6f6ed18e115", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9bc29a3-4e6d-48a3-bf2a-c12ab7dad273", "node_type": "1", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "1672500e62ad5e58ccaa3b6112be2765c4485f733904b108715dd261ae7b78cf", "class_name": "RelatedNodeInfo"}}, "text": "Document layout detec-\ntion models can then be run to segment paragraphs,\ntitles, and other page objects such as tables, fig-\nures, and headers. A chunking strategy is then\ndefined to group text passages with some seman-\ntical coherence, and modern retrieval setups may\neven integrate a captioning step to describe visu-\nally rich elements in a natural language form, more\nsuitable for embedding models. In our experiments\n(Table 2), we typically find that optimizing the in-\ngestion pipeline yields much greater performance\non visually rich document retrieval than optimizing\nthe text embedding model.\nContribution 1: ViDoRe .In this work, we ar-\ngue that document retrieval systems should not\nbe evaluated solely on the capabilities of text em-\nbedding models (Bajaj et al., 2016; Thakur et al.,\n2021; Muennighoff et al., 2022), but should also\n1arXiv:2407.01449v3  [cs.IR]  7 Oct 2024", "mimetype": "text/plain", "start_char_idx": 2524, "end_char_idx": 3411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e481ea7a-9638-479c-9958-7cc07d1465cb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ca1986e3-4326-4915-8fb0-dd26cf3e400e": {"__data__": {"id_": "ca1986e3-4326-4915-8fb0-dd26cf3e400e", "embedding": null, "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5845b888-160d-42c4-b417-c8b5383922d1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef6afb73363bec2b7545f549f87f65a6f0b896bfbda58fac205da6f6ed18e115", "class_name": "RelatedNodeInfo"}}, "text": "ColPali : Efficient Document Retrieval with Vision Language Models\nManuel Faysse*1,3Hugues Sibille\u22171,4Tony Wu\u22171Bilel Omrani1\nGautier Viaud1C\u00e9line Hudelot3Pierre Colombo2,3\n1Illuin Technology2Equall.ai\n3CentraleSup\u00e9lec, Paris-Saclay4ETH Z\u00fcrich\nmanuel.faysse@centralesupelec.fr\nAbstract\nDocuments are visually rich structures that con-\nvey information through text, as well as tables,\nfigures, page layouts, or fonts. While mod-\nern document retrieval systems exhibit strong\nperformance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hin-\ndering their performance on practical document\nretrieval applications such as Retrieval Aug-\nmented Generation. To benchmark current sys-\ntems on visually rich document retrieval, we in-\ntroduce the Visual Document Retrieval Bench-\nmark ViDoRe , composed of various page-level\nretrieving tasks spanning multiple domains,\nlanguages, and settings. The inherent short-\ncomings of modern systems motivate the in-\ntroduction of a new retrieval model architec-\nture, ColPali , which leverages the document\nunderstanding capabilities of recent Vision Lan-\nguage Models to produce high-quality contextu-\nalized embeddings solely from images of doc-\nument pages. Combined with a late interac-\ntion matching mechanism, ColPali largely out-\nperforms modern document retrieval pipelines\nwhile being drastically faster and end-to-end\ntrainable. We release all project artifacts at\nhttps://huggingface.co/vidore .\n1 Introduction\nDocument Retrieval consists in matching a user\nquery to relevant documents in a given corpus. It\nis central to many industrial applications, either as\na standalone ranking system (search engines) or\nas part of more complex information extraction or\nRetrieval Augmented Generation (RAG) pipelines.\nOver recent years, pretrained language models have\nenabled large improvements in text embedding\nmodels. In practical industrial settings, however,\nthe main performance bottleneck for efficient doc-\nument retrieval is not in embedding model perfor-\nmance but in the prior data ingestion pipeline.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e481ea7a-9638-479c-9958-7cc07d1465cb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b27da0fb-cf4e-4487-affa-b9e4e8fe6e96": {"__data__": {"id_": "b27da0fb-cf4e-4487-affa-b9e4e8fe6e96", "embedding": null, "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5845b888-160d-42c4-b417-c8b5383922d1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef6afb73363bec2b7545f549f87f65a6f0b896bfbda58fac205da6f6ed18e115", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca1986e3-4326-4915-8fb0-dd26cf3e400e", "node_type": "1", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "33602be3d70da8b68d93db4a90ae6d89018245f6e24e9915a52033d8b8e5dc2a", "class_name": "RelatedNodeInfo"}}, "text": "To\n*Equal Contribution\nFigure 1: For each term in a user query, ColPali iden-\ntifies the most relevant document image patches (high-\nlighted zones) and computes a query-to-page matching\nscore. We can then swiftly retrieve the most relevant\ndocuments from a large pre-indexed corpus.\nindex a standard PDF document, many steps are\nrequired. First, PDF parsers or Optical Charac-\nter Recognition (OCR) systems are used to extract\nwords from the pages. Document layout detec-\ntion models can then be run to segment paragraphs,\ntitles, and other page objects such as tables, fig-\nures, and headers. A chunking strategy is then\ndefined to group text passages with some seman-\ntical coherence, and modern retrieval setups may\neven integrate a captioning step to describe visu-\nally rich elements in a natural language form, more\nsuitable for embedding models. In our experiments\n(Table 2), we typically find that optimizing the in-\ngestion pipeline yields much greater performance\non visually rich document retrieval than optimizing\nthe text embedding model.\nContribution 1: ViDoRe .In this work, we ar-\ngue that document retrieval systems should not\nbe evaluated solely on the capabilities of text em-\nbedding models (Bajaj et al., 2016; Thakur et al.,\n2021; Muennighoff et al., 2022), but should also\n1arXiv:2407.01449v3  [cs.IR]  7 Oct 2024", "mimetype": "text/plain", "start_char_idx": 2075, "end_char_idx": 3411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e481ea7a-9638-479c-9958-7cc07d1465cb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e481ea7a-9638-479c-9958-7cc07d1465cb": {"__data__": {"id_": "e481ea7a-9638-479c-9958-7cc07d1465cb", "embedding": null, "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5845b888-160d-42c4-b417-c8b5383922d1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef6afb73363bec2b7545f549f87f65a6f0b896bfbda58fac205da6f6ed18e115", "class_name": "RelatedNodeInfo"}}, "text": "ColPali : Efficient Document Retrieval with Vision Language Models\nManuel Faysse*1,3Hugues Sibille\u22171,4Tony Wu\u22171Bilel Omrani1\nGautier Viaud1C\u00e9line Hudelot3Pierre Colombo2,3\n1Illuin Technology2Equall.ai\n3CentraleSup\u00e9lec, Paris-Saclay4ETH Z\u00fcrich\nmanuel.faysse@centralesupelec.fr\nAbstract\nDocuments are visually rich structures that con-\nvey information through text, as well as tables,\nfigures, page layouts, or fonts. While mod-\nern document retrieval systems exhibit strong\nperformance on query-to-text matching, they\nstruggle to exploit visual cues efficiently, hin-\ndering their performance on practical document\nretrieval applications such as Retrieval Aug-\nmented Generation. To benchmark current sys-\ntems on visually rich document retrieval, we in-\ntroduce the Visual Document Retrieval Bench-\nmark ViDoRe , composed of various page-level\nretrieving tasks spanning multiple domains,\nlanguages, and settings. The inherent short-\ncomings of modern systems motivate the in-\ntroduction of a new retrieval model architec-\nture, ColPali , which leverages the document\nunderstanding capabilities of recent Vision Lan-\nguage Models to produce high-quality contextu-\nalized embeddings solely from images of doc-\nument pages. Combined with a late interac-\ntion matching mechanism, ColPali largely out-\nperforms modern document retrieval pipelines\nwhile being drastically faster and end-to-end\ntrainable. We release all project artifacts at\nhttps://huggingface.co/vidore .\n1 Introduction\nDocument Retrieval consists in matching a user\nquery to relevant documents in a given corpus. It\nis central to many industrial applications, either as\na standalone ranking system (search engines) or\nas part of more complex information extraction or\nRetrieval Augmented Generation (RAG) pipelines.\nOver recent years, pretrained language models have\nenabled large improvements in text embedding\nmodels. In practical industrial settings, however,\nthe main performance bottleneck for efficient doc-\nument retrieval is not in embedding model perfor-\nmance but in the prior data ingestion pipeline. To\n*Equal Contribution\nFigure 1: For each term in a user query, ColPali iden-\ntifies the most relevant document image patches (high-\nlighted zones) and computes a query-to-page matching\nscore. We can then swiftly retrieve the most relevant\ndocuments from a large pre-indexed corpus.\nindex a standard PDF document, many steps are\nrequired. First, PDF parsers or Optical Charac-\nter Recognition (OCR) systems are used to extract\nwords from the pages. Document layout detec-\ntion models can then be run to segment paragraphs,\ntitles, and other page objects such as tables, fig-\nures, and headers. A chunking strategy is then\ndefined to group text passages with some seman-\ntical coherence, and modern retrieval setups may\neven integrate a captioning step to describe visu-\nally rich elements in a natural language form, more\nsuitable for embedding models. In our experiments\n(Table 2), we typically find that optimizing the in-\ngestion pipeline yields much greater performance\non visually rich document retrieval than optimizing\nthe text embedding model.\nContribution 1: ViDoRe .In this work, we ar-\ngue that document retrieval systems should not\nbe evaluated solely on the capabilities of text em-\nbedding models (Bajaj et al., 2016; Thakur et al.,\n2021; Muennighoff et al., 2022), but should also\n1arXiv:2407.01449v3  [cs.IR]  7 Oct 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e481ea7a-9638-479c-9958-7cc07d1465cb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fb4e7299-13d2-4f24-93cc-8bceb1cb2f4d": {"__data__": {"id_": "fb4e7299-13d2-4f24-93cc-8bceb1cb2f4d", "embedding": null, "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9d8a5ed445f42beb1af7a184b187bcf0d331bd68273e9e5fe3981e97fc967a2e", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: ColPali simplifies document retrieval w.r.t. standard retrieval methods while achieving stronger perfor-\nmances with better latencies. Latencies and results are detailed in section 5 and subsection B.5.\nconsider the context and visual elements of the doc-\numents to be retrieved. To this end, we create and\nopenly release ViDoRe , a comprehensive bench-\nmark to evaluate systems on page-level document\nretrieval with a wide coverage of domains, visual\nelements, and languages. ViDoRe targets practical\ndocument retrieval settings, in which user queries\nmay require both textual and visual understanding\nto be correctly matched to relevant documents. We\nhighlight the shortcomings of current text-centric\nsystems in these settings.1\nContribution 2: ColPali .We propose a novel\nmodel architecture and training strategy based on\nVision Language Models (VLMs) to efficiently in-\ndex documents purely from their visual features,\nallowing for subsequent fast query matching with\nlate interaction mechanisms (Khattab and Zaharia,\n2020).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1039, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "aea9a68f-94a7-4088-952e-d4db1a7ca183", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "060b3ef8-9dfd-4716-87d1-c172674d9d98": {"__data__": {"id_": "060b3ef8-9dfd-4716-87d1-c172674d9d98", "embedding": null, "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9d8a5ed445f42beb1af7a184b187bcf0d331bd68273e9e5fe3981e97fc967a2e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb4e7299-13d2-4f24-93cc-8bceb1cb2f4d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "16d8c32cb8738e1ea29d433720a10e255fa0e2b3b308f1006064f697972627da", "class_name": "RelatedNodeInfo"}}, "text": "Our method, ColPali , outperforms all other\nretrieval systems on ViDoRe while being fast and\nend-to-end trainable. We release models and code\nathttps://huggingface.co/vidore .\n2 Problem Formulation & Related Work\nProblem Setting. In our setting, a retrieval system\nscores how relevant a document dfrom corpus Dis\n1The benchmark leaderboard is hosted pub-\nlicly at https://huggingface.co/spaces/vidore/\nvidore-leaderboard to encourage further developments.with respect to a query q. Computing the similarity\nscore s(q, d)\u2208Rfor each of the |D|documents\nin the corpus creates a ranking we can use to ex-\ntract the most relevant documents. In this work,\nwe focus on page-level retrieval: given a query, is\nthe correct document page retrieved by the system?\nFor coherence with existing literature, we further\nuse the term document to refer to individual pages,\ni.e. the atomic retrieved elements in our setting.", "mimetype": "text/plain", "start_char_idx": 1040, "end_char_idx": 1946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "aea9a68f-94a7-4088-952e-d4db1a7ca183", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "589a3a9d-81fa-4608-8cb1-9ccf67b460e1": {"__data__": {"id_": "589a3a9d-81fa-4608-8cb1-9ccf67b460e1", "embedding": null, "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9d8a5ed445f42beb1af7a184b187bcf0d331bd68273e9e5fe3981e97fc967a2e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "060b3ef8-9dfd-4716-87d1-c172674d9d98", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b9e7299f8fcd94057f6ff71bd304a9f4e7fc7db8b053686fba50bb0325aa7bae", "class_name": "RelatedNodeInfo"}}, "text": "the atomic retrieved elements in our setting. As\nwe focus on practical industrial retrieval applica-\ntions (RAG, search engines) with potentially large\ncorpora sizes, latency constraints are imposed on\nscoring systems. Most current retrieval systems\ncan be decomposed into (1) an offline indexation\nphase in which a document index is built and (2) an\nonline querying phase in which a query is matched\nto documents from the index and where low latency\nis vital to the user experience.\nEfficient document retrieval systems exhibit\njoint properties of high retrieval performance\n(R1), low latency during querying (R2), and high\nthroughput during indexation (R3).\n2.1 Textual Retrieval Methods\nDocument Retrieval in Text Space. Statistical\nmethods based on word frequency like TF-IDF\n(Sparck Jones, 1972) and BM25 (Robertson et al.,\n1994) are still widely used due to their simplicity\n2", "mimetype": "text/plain", "start_char_idx": 1901, "end_char_idx": 2783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "aea9a68f-94a7-4088-952e-d4db1a7ca183", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "487962b6-0855-408a-9602-8a5d26045ef2": {"__data__": {"id_": "487962b6-0855-408a-9602-8a5d26045ef2", "embedding": null, "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9d8a5ed445f42beb1af7a184b187bcf0d331bd68273e9e5fe3981e97fc967a2e", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: ColPali simplifies document retrieval w.r.t. standard retrieval methods while achieving stronger perfor-\nmances with better latencies. Latencies and results are detailed in section 5 and subsection B.5.\nconsider the context and visual elements of the doc-\numents to be retrieved. To this end, we create and\nopenly release ViDoRe , a comprehensive bench-\nmark to evaluate systems on page-level document\nretrieval with a wide coverage of domains, visual\nelements, and languages. ViDoRe targets practical\ndocument retrieval settings, in which user queries\nmay require both textual and visual understanding\nto be correctly matched to relevant documents. We\nhighlight the shortcomings of current text-centric\nsystems in these settings.1\nContribution 2: ColPali .We propose a novel\nmodel architecture and training strategy based on\nVision Language Models (VLMs) to efficiently in-\ndex documents purely from their visual features,\nallowing for subsequent fast query matching with\nlate interaction mechanisms (Khattab and Zaharia,\n2020). Our method, ColPali , outperforms all other\nretrieval systems on ViDoRe while being fast and\nend-to-end trainable. We release models and code\nathttps://huggingface.co/vidore .\n2 Problem Formulation & Related Work\nProblem Setting. In our setting, a retrieval system\nscores how relevant a document dfrom corpus Dis\n1The benchmark leaderboard is hosted pub-\nlicly at https://huggingface.co/spaces/vidore/\nvidore-leaderboard to encourage further developments.with respect to a query q. Computing the similarity\nscore s(q, d)\u2208Rfor each of the |D|documents\nin the corpus creates a ranking we can use to ex-\ntract the most relevant documents. In this work,\nwe focus on page-level retrieval: given a query, is\nthe correct document page retrieved by the system?\nFor coherence with existing literature, we further\nuse the term document to refer to individual pages,\ni.e. the atomic retrieved elements in our setting. As\nwe focus on practical industrial retrieval applica-\ntions (RAG, search engines) with potentially large\ncorpora sizes, latency constraints are imposed on\nscoring systems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "aea9a68f-94a7-4088-952e-d4db1a7ca183", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d094814c-2629-494e-8db3-fe3948f4b2e3": {"__data__": {"id_": "d094814c-2629-494e-8db3-fe3948f4b2e3", "embedding": null, "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9d8a5ed445f42beb1af7a184b187bcf0d331bd68273e9e5fe3981e97fc967a2e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "487962b6-0855-408a-9602-8a5d26045ef2", "node_type": "1", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "fbddf94ad021d25829e7dbe1e7e20ef3fbef9eec62f3c8b0d77b3ab19a75e612", "class_name": "RelatedNodeInfo"}}, "text": "Most current retrieval systems\ncan be decomposed into (1) an offline indexation\nphase in which a document index is built and (2) an\nonline querying phase in which a query is matched\nto documents from the index and where low latency\nis vital to the user experience.\nEfficient document retrieval systems exhibit\njoint properties of high retrieval performance\n(R1), low latency during querying (R2), and high\nthroughput during indexation (R3).\n2.1 Textual Retrieval Methods\nDocument Retrieval in Text Space. Statistical\nmethods based on word frequency like TF-IDF\n(Sparck Jones, 1972) and BM25 (Robertson et al.,\n1994) are still widely used due to their simplicity\n2", "mimetype": "text/plain", "start_char_idx": 2120, "end_char_idx": 2783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "aea9a68f-94a7-4088-952e-d4db1a7ca183", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "aea9a68f-94a7-4088-952e-d4db1a7ca183": {"__data__": {"id_": "aea9a68f-94a7-4088-952e-d4db1a7ca183", "embedding": null, "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8", "node_type": "4", "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9d8a5ed445f42beb1af7a184b187bcf0d331bd68273e9e5fe3981e97fc967a2e", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: ColPali simplifies document retrieval w.r.t. standard retrieval methods while achieving stronger perfor-\nmances with better latencies. Latencies and results are detailed in section 5 and subsection B.5.\nconsider the context and visual elements of the doc-\numents to be retrieved. To this end, we create and\nopenly release ViDoRe , a comprehensive bench-\nmark to evaluate systems on page-level document\nretrieval with a wide coverage of domains, visual\nelements, and languages. ViDoRe targets practical\ndocument retrieval settings, in which user queries\nmay require both textual and visual understanding\nto be correctly matched to relevant documents. We\nhighlight the shortcomings of current text-centric\nsystems in these settings.1\nContribution 2: ColPali .We propose a novel\nmodel architecture and training strategy based on\nVision Language Models (VLMs) to efficiently in-\ndex documents purely from their visual features,\nallowing for subsequent fast query matching with\nlate interaction mechanisms (Khattab and Zaharia,\n2020). Our method, ColPali , outperforms all other\nretrieval systems on ViDoRe while being fast and\nend-to-end trainable. We release models and code\nathttps://huggingface.co/vidore .\n2 Problem Formulation & Related Work\nProblem Setting. In our setting, a retrieval system\nscores how relevant a document dfrom corpus Dis\n1The benchmark leaderboard is hosted pub-\nlicly at https://huggingface.co/spaces/vidore/\nvidore-leaderboard to encourage further developments.with respect to a query q. Computing the similarity\nscore s(q, d)\u2208Rfor each of the |D|documents\nin the corpus creates a ranking we can use to ex-\ntract the most relevant documents. In this work,\nwe focus on page-level retrieval: given a query, is\nthe correct document page retrieved by the system?\nFor coherence with existing literature, we further\nuse the term document to refer to individual pages,\ni.e. the atomic retrieved elements in our setting. As\nwe focus on practical industrial retrieval applica-\ntions (RAG, search engines) with potentially large\ncorpora sizes, latency constraints are imposed on\nscoring systems. Most current retrieval systems\ncan be decomposed into (1) an offline indexation\nphase in which a document index is built and (2) an\nonline querying phase in which a query is matched\nto documents from the index and where low latency\nis vital to the user experience.\nEfficient document retrieval systems exhibit\njoint properties of high retrieval performance\n(R1), low latency during querying (R2), and high\nthroughput during indexation (R3).\n2.1 Textual Retrieval Methods\nDocument Retrieval in Text Space. Statistical\nmethods based on word frequency like TF-IDF\n(Sparck Jones, 1972) and BM25 (Robertson et al.,\n1994) are still widely used due to their simplicity\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "aea9a68f-94a7-4088-952e-d4db1a7ca183", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1da1bf8a-e192-436f-bc1a-16232efd2be8": {"__data__": {"id_": "1da1bf8a-e192-436f-bc1a-16232efd2be8", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}}, "text": "and efficiency. More recently, neural embedding\nmodels based on fine-tuned large language models\ndisplay state-of-the-art performance on a variety of\ntext embedding tasks and top the retrieval leader-\nboards (Muennighoff et al., 2022).\nNeural Retrievers. In bi-encoder models (Reimers\nand Gurevych, 2019; Karpukhin et al., 2020; Wang\net al., 2022), documents are independently mapped\noffline to a dense vector space. Queries are em-\nbedded online and matched to documents through\na fast cosine distance computation. A slower, but\nslightly more performant alternative, cross-encoder\nsystems (Wang et al., 2020; Cohere, 2024) concate-\nnate query and document as a single input sequence\nand iteratively attribute matching scores to each\npossible combination. This enables full attention\ncomputation between query and document terms\nbut comes at the cost of computational efficiency,\nas|D|encoding passes must be done online.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cf898f94-26f0-4099-b27b-ceebef62dc0f": {"__data__": {"id_": "cf898f94-26f0-4099-b27b-ceebef62dc0f", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1da1bf8a-e192-436f-bc1a-16232efd2be8", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c854c8f570252c3f764a5153462142bc298d1aaa9d6d7cc48761a0046090431", "class_name": "RelatedNodeInfo"}}, "text": "Multi-Vector retrieval via late interaction. In\nthe late interaction paradigm (Khattab and Zaharia,\n2020), an embedding is pre-computed and indexed\nper document token. At runtime, similarity can be\ncomputed with individual query token embeddings.\nThe idea is to benefit from the rich interaction be-\ntween individual query and document terms while\ntaking advantage of the offline computation and\nfast query matching enabled by bi-encoders.\nRetrieval Evaluation. Although benchmarks and\nleaderboards have been developed to evaluate text\nembedding models (Thakur et al., 2021; Muen-\nnighoff et al., 2022), as previously stated, much\nof the performance improvements in industrial use\ncases of embedding models stem from the prior\ndata ingestion pipeline. While documents often\nrely on visual elements to more efficiently convey\ninformation to human readers, text-only systems\nbarely tap into these visual cues.\nTo our knowledge, no benchmark evaluates docu-\nment retrieval methods by considering both textual\nand visual document features like a human would.", "mimetype": "text/plain", "start_char_idx": 922, "end_char_idx": 1976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "eb75fe4d-4e03-45f3-aa0f-38f4ac61ca1c": {"__data__": {"id_": "eb75fe4d-4e03-45f3-aa0f-38f4ac61ca1c", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf898f94-26f0-4099-b27b-ceebef62dc0f", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a2b800759e11e06d0db621935393173289554e0fe32c9bbf90572619729254aa", "class_name": "RelatedNodeInfo"}}, "text": "2.2 Integrating Visual features\nContrastive Vision Language Models. Mapping\nlatent representations of textual content to corre-\nsponding representations of visual content has been\ndone by aligning disjoint visual and text encoders\nthrough contrastive losses (Radford et al., 2021;\nZhai et al., 2023). While some OCR capabilities\nexist in these models, the visual component is often\nnot optimized for text understanding. The Fine-\ngrained Interactive Language-Image Pre-training(Yao et al., 2021) framework extends the late inter-\naction mechanism to cross-modal vision-language\nmodels, relying on max similarity operations be-\ntween text tokens and image patches.\nVisually Rich Document Understanding. To\ngo beyond text, some document-focused models\njointly encode text tokens alongside visual or docu-\nment layout features (Appalaraju et al., 2021; Kim\net al., 2021; Huang et al., 2022; Tang et al., 2022).", "mimetype": "text/plain", "start_char_idx": 1977, "end_char_idx": 2884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7f0828e9-b3b9-4c37-9d45-4821dac1db97": {"__data__": {"id_": "7f0828e9-b3b9-4c37-9d45-4821dac1db97", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb75fe4d-4e03-45f3-aa0f-38f4ac61ca1c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2920767117000799af226fbc24c96f404de46a28e534c3c8d045d3a4415d7442", "class_name": "RelatedNodeInfo"}}, "text": "Large Language transformer Models (LLMs) with\nstrong reasoning capabilities have recently been\ncombined with Vision Transformers (ViTs) (Doso-\nvitskiy et al., 2020) to create VLMs (Alayrac et al.,\n2022; Liu et al., 2023b; Bai et al., 2023; Lauren\u00e7on\net al., 2024) where image patch vectors from con-\ntrastively trained ViT models (Zhai et al., 2023) are\nfed as input embeddings to the language model and\nconcatenated with the text-token embeddings.\nPaliGemma. The PaliGemma-3B model (Beyer\net al., 2024) extends concepts from Pali3 (Chen\net al., 2023), and projects SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023) patch embeddings into\nGemma-2B\u2019s text vector space (Gemma Team\net al., 2024). Along with its reasonable size w.r.t.", "mimetype": "text/plain", "start_char_idx": 2885, "end_char_idx": 3614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d612872a-3784-44ec-aeda-36f337d6e67f": {"__data__": {"id_": "d612872a-3784-44ec-aeda-36f337d6e67f", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f0828e9-b3b9-4c37-9d45-4821dac1db97", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7daa6aa98dd0dcdd990d76ac3355a6b0fee55348540ea8342a6a258f7013297f", "class_name": "RelatedNodeInfo"}}, "text": "Along with its reasonable size w.r.t.\nother performant VLMs, an interesting property of\nPaliGemma\u2019s text model is that it is fine-tuned with\nfull-block attention on the prefix (instruction text\nand image tokens).\nVLMs display enhanced capabilities in Visual Ques-\ntion Answering, captioning, and document under-\nstanding (Yue et al., 2023), but are not optimized\nfor retrieval tasks.", "mimetype": "text/plain", "start_char_idx": 3577, "end_char_idx": 3960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "441bb3f0-852d-4f47-b772-c9f867f9f398": {"__data__": {"id_": "441bb3f0-852d-4f47-b772-c9f867f9f398", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}}, "text": "and efficiency. More recently, neural embedding\nmodels based on fine-tuned large language models\ndisplay state-of-the-art performance on a variety of\ntext embedding tasks and top the retrieval leader-\nboards (Muennighoff et al., 2022).\nNeural Retrievers. In bi-encoder models (Reimers\nand Gurevych, 2019; Karpukhin et al., 2020; Wang\net al., 2022), documents are independently mapped\noffline to a dense vector space. Queries are em-\nbedded online and matched to documents through\na fast cosine distance computation. A slower, but\nslightly more performant alternative, cross-encoder\nsystems (Wang et al., 2020; Cohere, 2024) concate-\nnate query and document as a single input sequence\nand iteratively attribute matching scores to each\npossible combination. This enables full attention\ncomputation between query and document terms\nbut comes at the cost of computational efficiency,\nas|D|encoding passes must be done online.\nMulti-Vector retrieval via late interaction. In\nthe late interaction paradigm (Khattab and Zaharia,\n2020), an embedding is pre-computed and indexed\nper document token. At runtime, similarity can be\ncomputed with individual query token embeddings.\nThe idea is to benefit from the rich interaction be-\ntween individual query and document terms while\ntaking advantage of the offline computation and\nfast query matching enabled by bi-encoders.\nRetrieval Evaluation. Although benchmarks and\nleaderboards have been developed to evaluate text\nembedding models (Thakur et al., 2021; Muen-\nnighoff et al., 2022), as previously stated, much\nof the performance improvements in industrial use\ncases of embedding models stem from the prior\ndata ingestion pipeline. While documents often\nrely on visual elements to more efficiently convey\ninformation to human readers, text-only systems\nbarely tap into these visual cues.\nTo our knowledge, no benchmark evaluates docu-\nment retrieval methods by considering both textual\nand visual document features like a human would.\n2.2 Integrating Visual features\nContrastive Vision Language Models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7c753d2e-50df-4577-8267-c678d33d2fdf": {"__data__": {"id_": "7c753d2e-50df-4577-8267-c678d33d2fdf", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "441bb3f0-852d-4f47-b772-c9f867f9f398", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ae7c5da7858944f69123e9862c8ae299f3b80dec84ad3e53462429bc164f47de", "class_name": "RelatedNodeInfo"}}, "text": "2.2 Integrating Visual features\nContrastive Vision Language Models. Mapping\nlatent representations of textual content to corre-\nsponding representations of visual content has been\ndone by aligning disjoint visual and text encoders\nthrough contrastive losses (Radford et al., 2021;\nZhai et al., 2023). While some OCR capabilities\nexist in these models, the visual component is often\nnot optimized for text understanding. The Fine-\ngrained Interactive Language-Image Pre-training(Yao et al., 2021) framework extends the late inter-\naction mechanism to cross-modal vision-language\nmodels, relying on max similarity operations be-\ntween text tokens and image patches.\nVisually Rich Document Understanding. To\ngo beyond text, some document-focused models\njointly encode text tokens alongside visual or docu-\nment layout features (Appalaraju et al., 2021; Kim\net al., 2021; Huang et al., 2022; Tang et al., 2022).\nLarge Language transformer Models (LLMs) with\nstrong reasoning capabilities have recently been\ncombined with Vision Transformers (ViTs) (Doso-\nvitskiy et al., 2020) to create VLMs (Alayrac et al.,\n2022; Liu et al., 2023b; Bai et al., 2023; Lauren\u00e7on\net al., 2024) where image patch vectors from con-\ntrastively trained ViT models (Zhai et al., 2023) are\nfed as input embeddings to the language model and\nconcatenated with the text-token embeddings.\nPaliGemma. The PaliGemma-3B model (Beyer\net al., 2024) extends concepts from Pali3 (Chen\net al., 2023), and projects SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023) patch embeddings into\nGemma-2B\u2019s text vector space (Gemma Team\net al., 2024). Along with its reasonable size w.r.t.\nother performant VLMs, an interesting property of\nPaliGemma\u2019s text model is that it is fine-tuned with\nfull-block attention on the prefix (instruction text\nand image tokens).", "mimetype": "text/plain", "start_char_idx": 1977, "end_char_idx": 3789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9b2a794c-758d-412d-8c7b-9ea1f4e6a2a1": {"__data__": {"id_": "9b2a794c-758d-412d-8c7b-9ea1f4e6a2a1", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c753d2e-50df-4577-8267-c678d33d2fdf", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "c48529f2aae760a841ffeb5c179d0186da87627da093fbc324bbdf3f77902cc5", "class_name": "RelatedNodeInfo"}}, "text": "VLMs display enhanced capabilities in Visual Ques-\ntion Answering, captioning, and document under-\nstanding (Yue et al., 2023), but are not optimized\nfor retrieval tasks.", "mimetype": "text/plain", "start_char_idx": 3790, "end_char_idx": 3960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2b97283b-f755-4510-a6b7-d30d99f25e5a": {"__data__": {"id_": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dd41d34-dc98-48de-95a7-d3a4c4254f9d", "node_type": "1", "metadata": {}, "hash": "e5e9b95ab9860a66bcb897baa1104a4e43c27b2a14ed30dd6f147c496a77202a", "class_name": "RelatedNodeInfo"}}, "text": "and efficiency. More recently, neural embedding\nmodels based on fine-tuned large language models\ndisplay state-of-the-art performance on a variety of\ntext embedding tasks and top the retrieval leader-\nboards (Muennighoff et al., 2022).\nNeural Retrievers. In bi-encoder models (Reimers\nand Gurevych, 2019; Karpukhin et al., 2020; Wang\net al., 2022), documents are independently mapped\noffline to a dense vector space. Queries are em-\nbedded online and matched to documents through\na fast cosine distance computation. A slower, but\nslightly more performant alternative, cross-encoder\nsystems (Wang et al., 2020; Cohere, 2024) concate-\nnate query and document as a single input sequence\nand iteratively attribute matching scores to each\npossible combination. This enables full attention\ncomputation between query and document terms\nbut comes at the cost of computational efficiency,\nas|D|encoding passes must be done online.\nMulti-Vector retrieval via late interaction. In\nthe late interaction paradigm (Khattab and Zaharia,\n2020), an embedding is pre-computed and indexed\nper document token. At runtime, similarity can be\ncomputed with individual query token embeddings.\nThe idea is to benefit from the rich interaction be-\ntween individual query and document terms while\ntaking advantage of the offline computation and\nfast query matching enabled by bi-encoders.\nRetrieval Evaluation. Although benchmarks and\nleaderboards have been developed to evaluate text\nembedding models (Thakur et al., 2021; Muen-\nnighoff et al., 2022), as previously stated, much\nof the performance improvements in industrial use\ncases of embedding models stem from the prior\ndata ingestion pipeline. While documents often\nrely on visual elements to more efficiently convey\ninformation to human readers, text-only systems\nbarely tap into these visual cues.\nTo our knowledge, no benchmark evaluates docu-\nment retrieval methods by considering both textual\nand visual document features like a human would.\n2.2 Integrating Visual features\nContrastive Vision Language Models. Mapping\nlatent representations of textual content to corre-\nsponding representations of visual content has been\ndone by aligning disjoint visual and text encoders\nthrough contrastive losses (Radford et al., 2021;\nZhai et al., 2023). While some OCR capabilities\nexist in these models, the visual component is often\nnot optimized for text understanding. The Fine-\ngrained Interactive Language-Image Pre-training(Yao et al., 2021) framework extends the late inter-\naction mechanism to cross-modal vision-language\nmodels, relying on max similarity operations be-\ntween text tokens and image patches.\nVisually Rich Document Understanding. To\ngo beyond text, some document-focused models\njointly encode text tokens alongside visual or docu-\nment layout features (Appalaraju et al., 2021; Kim\net al., 2021; Huang et al., 2022; Tang et al., 2022).\nLarge Language transformer Models (LLMs) with\nstrong reasoning capabilities have recently been\ncombined with Vision Transformers (ViTs) (Doso-\nvitskiy et al., 2020) to create VLMs (Alayrac et al.,\n2022; Liu et al., 2023b; Bai et al., 2023; Lauren\u00e7on\net al., 2024) where image patch vectors from con-\ntrastively trained ViT models (Zhai et al., 2023) are\nfed as input embeddings to the language model and\nconcatenated with the text-token embeddings.\nPaliGemma. The PaliGemma-3B model (Beyer\net al., 2024) extends concepts from Pali3 (Chen\net al., 2023), and projects SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023) patch embeddings into\nGemma-2B\u2019s text vector space (Gemma Team\net al., 2024). Along with its reasonable size w.r.t.\nother performant VLMs, an interesting property of\nPaliGemma\u2019s text model is that it is fine-tuned with\nfull-block attention on the prefix (instruction text\nand image tokens).\nVLMs display enhanced capabilities in Visual Ques-\ntion Answering, captioning, and document under-\nstanding (Yue et al., 2023), but are not optimized\nfor retrieval tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "843197ad-92c9-4210-9fa1-fd11a9d95e73": {"__data__": {"id_": "843197ad-92c9-4210-9fa1-fd11a9d95e73", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}}, "text": "PaliGemma. The PaliGemma-3B model (Beyer\net al., 2024) extends concepts from Pali3 (Chen\net al., 2023), and projects SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023) patch embeddings into\nGemma-2B\u2019s text vector space (Gemma Team\net al., 2024). Along with its reasonable size w.r.t.\nother performant VLMs, an interesting property of\nPaliGemma\u2019s text model is that it is fine-tuned with\nfull-block attention on the prefix (instruction text\nand image tokens).\nVLMs display enhanced capabilities in Visual Ques-\ntion Answering, captioning, and document under-\nstanding (Yue et al., 2023), but are not optimized\nfor retrieval tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5dd41d34-dc98-48de-95a7-d3a4c4254f9d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "06273651-20a8-4a8d-b3a9-3cb80c1f6fb3": {"__data__": {"id_": "06273651-20a8-4a8d-b3a9-3cb80c1f6fb3", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "843197ad-92c9-4210-9fa1-fd11a9d95e73", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f468f531e8ffe22007d8523f88807278c5508ccebe5d948bfeb7b845d31a0fcf", "class_name": "RelatedNodeInfo"}}, "text": "3 The ViDoRe Benchmark\nExisting benchmarks for contrastive vision-\nlanguage models primarily evaluate retrieval for\nnatural images (Lin et al., 2014; Borchmann et al.,\n2021; Thapliyal et al., 2022). On the other hand,\ntextual retrieval benchmarks (Muennighoff et al.,\n2022) are evaluated at at textual passage level and\nare not tailored for document retrieval tasks. We fill\nthe gap with ViDoRe , a comprehensive benchmark\nfor document retrieval using visual features.\n3.1 Benchmark Design\nViDoRe is designed to comprehensively evaluate\nretrieval systems on their capacity to match queries\nto relevant documents at the page level. This bench-\nmark encompasses multiple orthogonal subtasks,\nwith focuses on various modalities - text, figures,\ninfographics, tables; thematic domains - medical,\n3", "mimetype": "text/plain", "start_char_idx": 627, "end_char_idx": 1420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5dd41d34-dc98-48de-95a7-d3a4c4254f9d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6a0d5d51-b833-4a60-a353-5891e9338c8d": {"__data__": {"id_": "6a0d5d51-b833-4a60-a353-5891e9338c8d", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}}, "text": "PaliGemma. The PaliGemma-3B model (Beyer\net al., 2024) extends concepts from Pali3 (Chen\net al., 2023), and projects SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023) patch embeddings into\nGemma-2B\u2019s text vector space (Gemma Team\net al., 2024). Along with its reasonable size w.r.t.\nother performant VLMs, an interesting property of\nPaliGemma\u2019s text model is that it is fine-tuned with\nfull-block attention on the prefix (instruction text\nand image tokens).\nVLMs display enhanced capabilities in Visual Ques-\ntion Answering, captioning, and document under-\nstanding (Yue et al., 2023), but are not optimized\nfor retrieval tasks.\n3 The ViDoRe Benchmark\nExisting benchmarks for contrastive vision-\nlanguage models primarily evaluate retrieval for\nnatural images (Lin et al., 2014; Borchmann et al.,\n2021; Thapliyal et al., 2022). On the other hand,\ntextual retrieval benchmarks (Muennighoff et al.,\n2022) are evaluated at at textual passage level and\nare not tailored for document retrieval tasks. We fill\nthe gap with ViDoRe , a comprehensive benchmark\nfor document retrieval using visual features.\n3.1 Benchmark Design\nViDoRe is designed to comprehensively evaluate\nretrieval systems on their capacity to match queries\nto relevant documents at the page level. This bench-\nmark encompasses multiple orthogonal subtasks,\nwith focuses on various modalities - text, figures,\ninfographics, tables; thematic domains - medical,\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5dd41d34-dc98-48de-95a7-d3a4c4254f9d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5dd41d34-dc98-48de-95a7-d3a4c4254f9d": {"__data__": {"id_": "5dd41d34-dc98-48de-95a7-d3a4c4254f9d", "embedding": null, "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606", "node_type": "4", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "87eac614987a92545f80893ee41998d19124613d43004c47bb3de6a508ffdb68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b97283b-f755-4510-a6b7-d30d99f25e5a", "node_type": "1", "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a18605c1406d5e408f4c441589ec29dcac966989ccd8b651d9f5e47111ae4f90", "class_name": "RelatedNodeInfo"}}, "text": "PaliGemma. The PaliGemma-3B model (Beyer\net al., 2024) extends concepts from Pali3 (Chen\net al., 2023), and projects SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023) patch embeddings into\nGemma-2B\u2019s text vector space (Gemma Team\net al., 2024). Along with its reasonable size w.r.t.\nother performant VLMs, an interesting property of\nPaliGemma\u2019s text model is that it is fine-tuned with\nfull-block attention on the prefix (instruction text\nand image tokens).\nVLMs display enhanced capabilities in Visual Ques-\ntion Answering, captioning, and document under-\nstanding (Yue et al., 2023), but are not optimized\nfor retrieval tasks.\n3 The ViDoRe Benchmark\nExisting benchmarks for contrastive vision-\nlanguage models primarily evaluate retrieval for\nnatural images (Lin et al., 2014; Borchmann et al.,\n2021; Thapliyal et al., 2022). On the other hand,\ntextual retrieval benchmarks (Muennighoff et al.,\n2022) are evaluated at at textual passage level and\nare not tailored for document retrieval tasks. We fill\nthe gap with ViDoRe , a comprehensive benchmark\nfor document retrieval using visual features.\n3.1 Benchmark Design\nViDoRe is designed to comprehensively evaluate\nretrieval systems on their capacity to match queries\nto relevant documents at the page level. This bench-\nmark encompasses multiple orthogonal subtasks,\nwith focuses on various modalities - text, figures,\ninfographics, tables; thematic domains - medical,\n3", "mimetype": "text/plain", "start_char_idx": 3334, "end_char_idx": 4754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5dd41d34-dc98-48de-95a7-d3a4c4254f9d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "17b5ce96-f5a4-4ff3-b554-afc8e7270647": {"__data__": {"id_": "17b5ce96-f5a4-4ff3-b554-afc8e7270647", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}}, "text": "business, scientific, administrative; or languages -\nEnglish (eng), French (fra).\nDataset # Queries Domain\nAcademic Tasks\nDocVQA (eng) 500 (500) Industrial\nInfoVQA (eng) 500 (500) Infographics\nTAT-DQA (eng) 1600 (1600) Varied Modalities\narXiVQA (eng) 500 (500) Scientific Figures\nTabFQuAD (fra) 210 (210) Tables\nPractical Tasks\nEnergy (eng) 100 (1000) Scientific\nGovernment (eng) 100 (1000) Administrative\nHealthcare (eng) 100 (1000) Medical\nAI (eng) 100 (1000) Scientific\nShift Project (fra) 100 (1000) Environment\nTable 1: ViDoRe comprehensively evaluates multimodal\nretrieval methods. The size of the document corpus is\nindicated in parentheses.\nAcademic Tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7d091e6d-d673-4d1a-ba1f-e77eb2a832db": {"__data__": {"id_": "7d091e6d-d673-4d1a-ba1f-e77eb2a832db", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17b5ce96-f5a4-4ff3-b554-afc8e7270647", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "d776b04de43d67c6514ebfa9ea3111575ac75c25790e1f2d6a74500325f89df2", "class_name": "RelatedNodeInfo"}}, "text": "The size of the document corpus is\nindicated in parentheses.\nAcademic Tasks. We repurpose widely used visual\nquestion-answering benchmarks for retrieval tasks:\nfor each page-question-answer triplet, we use the\nquestion as the query, and the associated page as\nthe gold document (Table 1). These academic\ndatasets either focus on single specific modalities\n(Mathew et al., 2020, 2021; Li et al., 2024) or\ntarget more varied visually rich documents (Zhu\net al., 2022). Moreover, we consider TabFQuAD,\na human-labeled dataset on tables extracted from\nFrench industrial PDF documents released with\nthis work. Details can be found in subsection A.1.\nPractical tasks. We construct topic-specific re-\ntrieval benchmarks spanning multiple domains to\ngo beyond repurposed QA datasets and evaluate\nretrieval in more realistic industrial situations (e.g.\nRAG).", "mimetype": "text/plain", "start_char_idx": 588, "end_char_idx": 1437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "234c2977-6b8b-4bc7-8c77-6422aea915ce": {"__data__": {"id_": "234c2977-6b8b-4bc7-8c77-6422aea915ce", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d091e6d-d673-4d1a-ba1f-e77eb2a832db", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "5d0a6d172664180f4a35a5dacfb641f20851224328840195488724a825925ad4", "class_name": "RelatedNodeInfo"}}, "text": "RAG). To achieve this, we collect publicly acces-\nsible PDF documents and generate queries per-\ntaining to document pages using Claude-3 Sonnet,\na high-quality proprietary vision-language model\n(Anthropic, 2024). In total, we collect 1,000 doc-\nument pages per topic, which we associate with\n100 queries extensively filtered for quality and rele-\nvance by human annotators. The corpus topics are\nintentionally specific to maximize syntactic prox-\nimity between documents, creating challenging re-\ntrieval tasks and covering an array of orthogonal\ndomains (Table 1). Query-page pair examples are\nshown in Appendix E.2\nEvaluation Metrics. We evaluate performance on\nour benchmark (Requirement R1) using standard\n2Answers are generated alongside queries to (1) ground\nqueries and improve their quality and (2) provide resources to\nfoster future work.metrics from the retrieval literature (NDCG, Re-\ncall@K, MRR).", "mimetype": "text/plain", "start_char_idx": 1432, "end_char_idx": 2341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "89c39124-b4ea-4069-97ad-d515dfc2cb04": {"__data__": {"id_": "89c39124-b4ea-4069-97ad-d515dfc2cb04", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "234c2977-6b8b-4bc7-8c77-6422aea915ce", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a556d36d1ede60666b4a2f4997f733ba20598a9062ff0b9cda8374d698c276b4", "class_name": "RelatedNodeInfo"}}, "text": "We report NDCG@5 values as the\nmain performance metric in this work and release\nthe complete sets of results along with the models3.\nTo validate compliance with practical industrial\nconstraints, we also consider query latencies (R2)\nand indexing throughputs (R3).\n3.2 Assessing Current Systems\nUnstructured. We evaluate retrieval systems rep-\nresentative of those found in standard industrial\nRAG pipelines. As is common practice, we rely on\ntheUnstructured4off-the-shelf tool in the high-\nest resolution settings to construct high-quality text\nchunks from PDF documents. Unstructured or-\nchestrates the document parsing pipeline, relying\non deep learning vision models to detect titles and\ndocument layouts (Ge et al., 2021), OCR engines\n(Smith, 2007) to extract text in non-native PDFs,\nspecialized methods or models to detect and recon-\nstruct tables, and implements a chunking strategy\n(by-title ) that leverages the detected document\nstructure to preserve section boundaries when con-\ncatenating texts.", "mimetype": "text/plain", "start_char_idx": 2342, "end_char_idx": 3349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c6697468-8579-4b30-bb2c-b8fd75849702": {"__data__": {"id_": "c6697468-8579-4b30-bb2c-b8fd75849702", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89c39124-b4ea-4069-97ad-d515dfc2cb04", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "0bafbac94372ae80e6d86cd8941666226c565e487870aff2da42378911d42794", "class_name": "RelatedNodeInfo"}}, "text": "As is common practice, in our\nsimplest Unstructured configuration ( text-only ),\nonly textual elements are kept and figures, images,\nand tables are considered noisy information and\nare filtered out.\nUnstructured + X. While Unstructured is a\nstrong baseline by itself, we further augment\nUnstructured \u2019s output by integrating the visual\nelements. In ( + OCR ), tables, charts, and images\nare run through an OCR engine, processed by Un-\nstructured, and chunked independently. In ( + Cap-\ntioning ), we set up a fully-fledged captioning strat-\negy (Zhao et al., 2023), in which we feed visual\nelements to a strong proprietary Vision Language\nModel (Claude-3 Sonnet (Anthropic, 2024)) to ob-\ntain highly detailed textual descriptions of the ele-\nments.", "mimetype": "text/plain", "start_char_idx": 3350, "end_char_idx": 4098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b5e38a5b-ea8e-4432-931f-ee69ea9cdc5e": {"__data__": {"id_": "b5e38a5b-ea8e-4432-931f-ee69ea9cdc5e", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}}, "text": "business, scientific, administrative; or languages -\nEnglish (eng), French (fra).\nDataset # Queries Domain\nAcademic Tasks\nDocVQA (eng) 500 (500) Industrial\nInfoVQA (eng) 500 (500) Infographics\nTAT-DQA (eng) 1600 (1600) Varied Modalities\narXiVQA (eng) 500 (500) Scientific Figures\nTabFQuAD (fra) 210 (210) Tables\nPractical Tasks\nEnergy (eng) 100 (1000) Scientific\nGovernment (eng) 100 (1000) Administrative\nHealthcare (eng) 100 (1000) Medical\nAI (eng) 100 (1000) Scientific\nShift Project (fra) 100 (1000) Environment\nTable 1: ViDoRe comprehensively evaluates multimodal\nretrieval methods. The size of the document corpus is\nindicated in parentheses.\nAcademic Tasks. We repurpose widely used visual\nquestion-answering benchmarks for retrieval tasks:\nfor each page-question-answer triplet, we use the\nquestion as the query, and the associated page as\nthe gold document (Table 1). These academic\ndatasets either focus on single specific modalities\n(Mathew et al., 2020, 2021; Li et al., 2024) or\ntarget more varied visually rich documents (Zhu\net al., 2022). Moreover, we consider TabFQuAD,\na human-labeled dataset on tables extracted from\nFrench industrial PDF documents released with\nthis work. Details can be found in subsection A.1.\nPractical tasks. We construct topic-specific re-\ntrieval benchmarks spanning multiple domains to\ngo beyond repurposed QA datasets and evaluate\nretrieval in more realistic industrial situations (e.g.\nRAG). To achieve this, we collect publicly acces-\nsible PDF documents and generate queries per-\ntaining to document pages using Claude-3 Sonnet,\na high-quality proprietary vision-language model\n(Anthropic, 2024). In total, we collect 1,000 doc-\nument pages per topic, which we associate with\n100 queries extensively filtered for quality and rele-\nvance by human annotators.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "39aebc4c-9037-4c1c-8911-bd8e62a7510f": {"__data__": {"id_": "39aebc4c-9037-4c1c-8911-bd8e62a7510f", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5e38a5b-ea8e-4432-931f-ee69ea9cdc5e", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "35b01f065974e25fbe5f9314ffdca794eb4d3847dce35741a905338f6faae132", "class_name": "RelatedNodeInfo"}}, "text": "The corpus topics are\nintentionally specific to maximize syntactic prox-\nimity between documents, creating challenging re-\ntrieval tasks and covering an array of orthogonal\ndomains (Table 1). Query-page pair examples are\nshown in Appendix E.2\nEvaluation Metrics. We evaluate performance on\nour benchmark (Requirement R1) using standard\n2Answers are generated alongside queries to (1) ground\nqueries and improve their quality and (2) provide resources to\nfoster future work.metrics from the retrieval literature (NDCG, Re-\ncall@K, MRR). We report NDCG@5 values as the\nmain performance metric in this work and release\nthe complete sets of results along with the models3.\nTo validate compliance with practical industrial\nconstraints, we also consider query latencies (R2)\nand indexing throughputs (R3).\n3.2 Assessing Current Systems\nUnstructured. We evaluate retrieval systems rep-\nresentative of those found in standard industrial\nRAG pipelines. As is common practice, we rely on\ntheUnstructured4off-the-shelf tool in the high-\nest resolution settings to construct high-quality text\nchunks from PDF documents. Unstructured or-\nchestrates the document parsing pipeline, relying\non deep learning vision models to detect titles and\ndocument layouts (Ge et al., 2021), OCR engines\n(Smith, 2007) to extract text in non-native PDFs,\nspecialized methods or models to detect and recon-\nstruct tables, and implements a chunking strategy\n(by-title ) that leverages the detected document\nstructure to preserve section boundaries when con-\ncatenating texts. As is common practice, in our\nsimplest Unstructured configuration ( text-only ),\nonly textual elements are kept and figures, images,\nand tables are considered noisy information and\nare filtered out.\nUnstructured + X. While Unstructured is a\nstrong baseline by itself, we further augment\nUnstructured \u2019s output by integrating the visual\nelements. In ( + OCR ), tables, charts, and images\nare run through an OCR engine, processed by Un-\nstructured, and chunked independently.", "mimetype": "text/plain", "start_char_idx": 1806, "end_char_idx": 3823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "87bd1f7c-4867-42b2-ae7e-ad27b0fb797f": {"__data__": {"id_": "87bd1f7c-4867-42b2-ae7e-ad27b0fb797f", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39aebc4c-9037-4c1c-8911-bd8e62a7510f", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "08adb99ba2b8c8e332a32da0f8b6abf1c2d094f8c1bd9d39bfd87dd001959851", "class_name": "RelatedNodeInfo"}}, "text": "In ( + Cap-\ntioning ), we set up a fully-fledged captioning strat-\negy (Zhao et al., 2023), in which we feed visual\nelements to a strong proprietary Vision Language\nModel (Claude-3 Sonnet (Anthropic, 2024)) to ob-\ntain highly detailed textual descriptions of the ele-\nments.", "mimetype": "text/plain", "start_char_idx": 3824, "end_char_idx": 4098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ac3936c0-10e5-45bc-b8d7-f92f84922877": {"__data__": {"id_": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fb27f0c-84d9-4f15-9949-f545f20ff00c", "node_type": "1", "metadata": {}, "hash": "87698daebce1c25023e38d672b777dca8de5010182ee40cf931566db10661117", "class_name": "RelatedNodeInfo"}}, "text": "business, scientific, administrative; or languages -\nEnglish (eng), French (fra).\nDataset # Queries Domain\nAcademic Tasks\nDocVQA (eng) 500 (500) Industrial\nInfoVQA (eng) 500 (500) Infographics\nTAT-DQA (eng) 1600 (1600) Varied Modalities\narXiVQA (eng) 500 (500) Scientific Figures\nTabFQuAD (fra) 210 (210) Tables\nPractical Tasks\nEnergy (eng) 100 (1000) Scientific\nGovernment (eng) 100 (1000) Administrative\nHealthcare (eng) 100 (1000) Medical\nAI (eng) 100 (1000) Scientific\nShift Project (fra) 100 (1000) Environment\nTable 1: ViDoRe comprehensively evaluates multimodal\nretrieval methods. The size of the document corpus is\nindicated in parentheses.\nAcademic Tasks. We repurpose widely used visual\nquestion-answering benchmarks for retrieval tasks:\nfor each page-question-answer triplet, we use the\nquestion as the query, and the associated page as\nthe gold document (Table 1). These academic\ndatasets either focus on single specific modalities\n(Mathew et al., 2020, 2021; Li et al., 2024) or\ntarget more varied visually rich documents (Zhu\net al., 2022). Moreover, we consider TabFQuAD,\na human-labeled dataset on tables extracted from\nFrench industrial PDF documents released with\nthis work. Details can be found in subsection A.1.\nPractical tasks. We construct topic-specific re-\ntrieval benchmarks spanning multiple domains to\ngo beyond repurposed QA datasets and evaluate\nretrieval in more realistic industrial situations (e.g.\nRAG). To achieve this, we collect publicly acces-\nsible PDF documents and generate queries per-\ntaining to document pages using Claude-3 Sonnet,\na high-quality proprietary vision-language model\n(Anthropic, 2024). In total, we collect 1,000 doc-\nument pages per topic, which we associate with\n100 queries extensively filtered for quality and rele-\nvance by human annotators. The corpus topics are\nintentionally specific to maximize syntactic prox-\nimity between documents, creating challenging re-\ntrieval tasks and covering an array of orthogonal\ndomains (Table 1). Query-page pair examples are\nshown in Appendix E.2\nEvaluation Metrics. We evaluate performance on\nour benchmark (Requirement R1) using standard\n2Answers are generated alongside queries to (1) ground\nqueries and improve their quality and (2) provide resources to\nfoster future work.metrics from the retrieval literature (NDCG, Re-\ncall@K, MRR). We report NDCG@5 values as the\nmain performance metric in this work and release\nthe complete sets of results along with the models3.\nTo validate compliance with practical industrial\nconstraints, we also consider query latencies (R2)\nand indexing throughputs (R3).\n3.2 Assessing Current Systems\nUnstructured. We evaluate retrieval systems rep-\nresentative of those found in standard industrial\nRAG pipelines. As is common practice, we rely on\ntheUnstructured4off-the-shelf tool in the high-\nest resolution settings to construct high-quality text\nchunks from PDF documents. Unstructured or-\nchestrates the document parsing pipeline, relying\non deep learning vision models to detect titles and\ndocument layouts (Ge et al., 2021), OCR engines\n(Smith, 2007) to extract text in non-native PDFs,\nspecialized methods or models to detect and recon-\nstruct tables, and implements a chunking strategy\n(by-title ) that leverages the detected document\nstructure to preserve section boundaries when con-\ncatenating texts. As is common practice, in our\nsimplest Unstructured configuration ( text-only ),\nonly textual elements are kept and figures, images,\nand tables are considered noisy information and\nare filtered out.\nUnstructured + X. While Unstructured is a\nstrong baseline by itself, we further augment\nUnstructured \u2019s output by integrating the visual\nelements. In ( + OCR ), tables, charts, and images\nare run through an OCR engine, processed by Un-\nstructured, and chunked independently. In ( + Cap-\ntioning ), we set up a fully-fledged captioning strat-\negy (Zhao et al., 2023), in which we feed visual\nelements to a strong proprietary Vision Language\nModel (Claude-3 Sonnet (Anthropic, 2024)) to ob-\ntain highly detailed textual descriptions of the ele-\nments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5cae6908-9120-4146-b113-d8392842347a": {"__data__": {"id_": "5cae6908-9120-4146-b113-d8392842347a", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}}, "text": "As is common practice, in our\nsimplest Unstructured configuration ( text-only ),\nonly textual elements are kept and figures, images,\nand tables are considered noisy information and\nare filtered out.\nUnstructured + X. While Unstructured is a\nstrong baseline by itself, we further augment\nUnstructured \u2019s output by integrating the visual\nelements. In ( + OCR ), tables, charts, and images\nare run through an OCR engine, processed by Un-\nstructured, and chunked independently. In ( + Cap-\ntioning ), we set up a fully-fledged captioning strat-\negy (Zhao et al., 2023), in which we feed visual\nelements to a strong proprietary Vision Language\nModel (Claude-3 Sonnet (Anthropic, 2024)) to ob-\ntain highly detailed textual descriptions of the ele-\nments. Both strategies aim to integrate visual ele-\nments in the retrieval pipeline but incur significant\nlatency and resource costs (subsection 5.2).\nEmbedding Model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5fb27f0c-84d9-4f15-9949-f545f20ff00c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fd5a3dcb-9471-43a3-8dc6-bed5e39313f4": {"__data__": {"id_": "fd5a3dcb-9471-43a3-8dc6-bed5e39313f4", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cae6908-9120-4146-b113-d8392842347a", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b4881b05ad1015ab1145e9b9d1c72a0fafbdf798b78d8818c886f9e34aaab701", "class_name": "RelatedNodeInfo"}}, "text": "Embedding Model. To embed textual chunks, we\nevaluate Okapi BM25, the de facto standard sparse\nstatistical retrieval method, and the dense encoder\nof BGE-M3 (Chen et al., 2024), a multilingual\nneural method with SOTA performance in its size\ncategory. Chunks are embedded and scored inde-\npendently, and page-level scores are obtained by\n3https://huggingface.co/vidore\n4www.unstructured.io\n4", "mimetype": "text/plain", "start_char_idx": 893, "end_char_idx": 1283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5fb27f0c-84d9-4f15-9949-f545f20ff00c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e6865d62-7274-438a-9a93-5d0d50860367": {"__data__": {"id_": "e6865d62-7274-438a-9a93-5d0d50860367", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}}, "text": "As is common practice, in our\nsimplest Unstructured configuration ( text-only ),\nonly textual elements are kept and figures, images,\nand tables are considered noisy information and\nare filtered out.\nUnstructured + X. While Unstructured is a\nstrong baseline by itself, we further augment\nUnstructured \u2019s output by integrating the visual\nelements. In ( + OCR ), tables, charts, and images\nare run through an OCR engine, processed by Un-\nstructured, and chunked independently. In ( + Cap-\ntioning ), we set up a fully-fledged captioning strat-\negy (Zhao et al., 2023), in which we feed visual\nelements to a strong proprietary Vision Language\nModel (Claude-3 Sonnet (Anthropic, 2024)) to ob-\ntain highly detailed textual descriptions of the ele-\nments. Both strategies aim to integrate visual ele-\nments in the retrieval pipeline but incur significant\nlatency and resource costs (subsection 5.2).\nEmbedding Model. To embed textual chunks, we\nevaluate Okapi BM25, the de facto standard sparse\nstatistical retrieval method, and the dense encoder\nof BGE-M3 (Chen et al., 2024), a multilingual\nneural method with SOTA performance in its size\ncategory. Chunks are embedded and scored inde-\npendently, and page-level scores are obtained by\n3https://huggingface.co/vidore\n4www.unstructured.io\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5fb27f0c-84d9-4f15-9949-f545f20ff00c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5fb27f0c-84d9-4f15-9949-f545f20ff00c": {"__data__": {"id_": "5fb27f0c-84d9-4f15-9949-f545f20ff00c", "embedding": null, "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ef9d43845690208ea8374a0b4b2673c5ff8ad3c052dd202fa2a1d260c9fd36ea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac3936c0-10e5-45bc-b8d7-f92f84922877", "node_type": "1", "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "184aae3446c2bce37930c8292eb9f35227fb6cf6978f9230c8395d7bf76a525f", "class_name": "RelatedNodeInfo"}}, "text": "As is common practice, in our\nsimplest Unstructured configuration ( text-only ),\nonly textual elements are kept and figures, images,\nand tables are considered noisy information and\nare filtered out.\nUnstructured + X. While Unstructured is a\nstrong baseline by itself, we further augment\nUnstructured \u2019s output by integrating the visual\nelements. In ( + OCR ), tables, charts, and images\nare run through an OCR engine, processed by Un-\nstructured, and chunked independently. In ( + Cap-\ntioning ), we set up a fully-fledged captioning strat-\negy (Zhao et al., 2023), in which we feed visual\nelements to a strong proprietary Vision Language\nModel (Claude-3 Sonnet (Anthropic, 2024)) to ob-\ntain highly detailed textual descriptions of the ele-\nments. Both strategies aim to integrate visual ele-\nments in the retrieval pipeline but incur significant\nlatency and resource costs (subsection 5.2).\nEmbedding Model. To embed textual chunks, we\nevaluate Okapi BM25, the de facto standard sparse\nstatistical retrieval method, and the dense encoder\nof BGE-M3 (Chen et al., 2024), a multilingual\nneural method with SOTA performance in its size\ncategory. Chunks are embedded and scored inde-\npendently, and page-level scores are obtained by\n3https://huggingface.co/vidore\n4www.unstructured.io\n4", "mimetype": "text/plain", "start_char_idx": 3350, "end_char_idx": 4633, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5fb27f0c-84d9-4f15-9949-f545f20ff00c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0df44817-51ec-470c-bf74-d99aee327379": {"__data__": {"id_": "0df44817-51ec-470c-bf74-d99aee327379", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}}, "text": "max-pooling over the page\u2019s chunk scores.5\nContrastive VLMs. We also evaluate the strongest\navailable vision-language embedding models; Jina\nCLIP (Koukounas et al., 2024), Nomic Embed Vi-\nsion (Nomic, 2024), and SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023).\nResults. From a performance perspective, best re-\nsults are obtained by combining the Unstructured\nparser with visual information, either from caption-\ning strategies or by running OCR on the visual ele-\nments (Table 2). Little difference is seen between\nBM25 and BGE-M3 embeddings highlighting the\nvisual information bottleneck. Contrastive VLMs\nlag behind. Beyond retrieval performance (R1), the\nindexing latencies (R2) reported in Figure 3 illus-\ntrate that PDF parsing pipelines can be very lengthy,\nespecially when incorporating OCR or captioning\nstrategies.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "11226ad6-062b-49e0-81a9-0b2426d72f50": {"__data__": {"id_": "11226ad6-062b-49e0-81a9-0b2426d72f50", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0df44817-51ec-470c-bf74-d99aee327379", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8d886283763396b7bb46a986c6a88ea9be58dbb39de742818dcaccb4454dc1e4", "class_name": "RelatedNodeInfo"}}, "text": "Querying latencies at runtime (R3) are\nvery good for all evaluated systems ( \u226422ms on\nNVIDIA L4) due to fast query encoding and cosine\nsimilarity matching.\n0 1 2 3 4 5 6 7\nLatency (s)ColPali\n(0.39s)Siglip\n(0.12s)PDF Parser\n(7.22s)\nLayout Detection OCR Captioning Page Encoding\nFigure 3: Offline indexing with ColPali is much sim-\npler and faster compared to standard retrieval methods.\nIndexing speeds reported are computed on Nvidia L4\nGPUs and detailed in subsection B.5.\n4 Late interaction based Vision Retrieval\n4.1 Architecture\nVision-Language Models. Encouraged by their\nstrong document understanding capabilities, we\npropose adapting recent VLMs for retrieval. The\nkey concept is to leverage the alignment between\noutput embeddings of text and image tokens ac-\nquired during multi-modal finetuning.", "mimetype": "text/plain", "start_char_idx": 825, "end_char_idx": 1630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "134969f8-1eab-44cf-b9c9-d1fac50adc56": {"__data__": {"id_": "134969f8-1eab-44cf-b9c9-d1fac50adc56", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11226ad6-062b-49e0-81a9-0b2426d72f50", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3c24c90f029ef2b335658e343ec2e566800e4817f4b8b9abbf687459364c69e3", "class_name": "RelatedNodeInfo"}}, "text": "To this ex-\ntent, we introduce ColPali , a Paligemma-3B exten-\nsion that is capable of generating ColBERT-style\nmulti-vector representations of text and images\n(Figure 2). PaliGemma-3B is a strong candidate\ndue to its small size, the many released checkpoints\nfine-tuned for different image resolutions and tasks,\n5We empirically validated the max-pooling strategy over\nsub-page chunks to be more effective than concatenating all\npage chunks before embedding pagewise.and the promising performances on various doc-\nument understanding benchmarks. We add a pro-\njection layer to map the output language model-\ning embeddings to a vector space of reduced di-\nmension D= 128 as used in the ColBERT paper\n(Khattab and Zaharia, 2020) to keep lightweight\nbag-of-embedding representations.\nLate Interaction. Given query qand document\nd, we denote as Eq\u2208RNq\u00d7DandEd\u2208RNd\u00d7D\ntheir respective multi-vector representation in the\ncommon embedding space RD.", "mimetype": "text/plain", "start_char_idx": 1631, "end_char_idx": 2572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "85acb49a-24c0-4c43-86c0-f336620033da": {"__data__": {"id_": "85acb49a-24c0-4c43-86c0-f336620033da", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "134969f8-1eab-44cf-b9c9-d1fac50adc56", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "09db0dc796cfe2cc2bfe6cda7a49e2a74f16318bafe924088e24293fa3542232", "class_name": "RelatedNodeInfo"}}, "text": "The late interaction\noperator, LI(q, d), is the sum over all query vectors\nEq(j), of its maximum dot product \u27e8\u00b7|\u00b7\u27e9with each\nof the Nddocument embedding vectors Ed(1:Nd).\nLI(q, d) =X\ni\u2208[|1,Nq|]max\nj\u2208[|1,Nd|]\u27e8Eq(i)|Ed(j)\u27e9(1)\nContrastive Loss. The Late Interaction opera-\ntion is fully differentiable, enabling backpropaga-\ntion. Let a batch {qk, dk}k\u2208[|1,b|]composed of b\nquery-page pairs, where for all k\u2208[|1, b|], the\ndocument page dkis the document correspond-\ning to query qk. Following Khattab and Zaharia\n(2020), we define our in-batch contrastive loss Las\nthe softmaxed cross-entropy of the positive scores\ns+\nk=LI(qk, dk)w.r.t.", "mimetype": "text/plain", "start_char_idx": 2573, "end_char_idx": 3206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b640e831-6431-4554-986a-e0b8d144dccc": {"__data__": {"id_": "b640e831-6431-4554-986a-e0b8d144dccc", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85acb49a-24c0-4c43-86c0-f336620033da", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "d0e82a25d74e97ff5b333667ebc43813416a4ba14c1be6cdb301bfe2ab4b6f5f", "class_name": "RelatedNodeInfo"}}, "text": "to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, dl).\n4.2 Model training\nDataset. Our training dataset of 127,460 query-\npage pairs is comprised of train sets of openly\navailable academic datasets ( 63%) and a synthetic\ndataset made up of pages from web-crawled PDF\ndocuments and augmented with VLM-generated\n(Claude-3 Sonnet) pseudo-questions ( 37%).", "mimetype": "text/plain", "start_char_idx": 3207, "end_char_idx": 3563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6415abb1-dc4b-435b-afc7-3ab62b243533": {"__data__": {"id_": "6415abb1-dc4b-435b-afc7-3ab62b243533", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}}, "text": "max-pooling over the page\u2019s chunk scores.5\nContrastive VLMs. We also evaluate the strongest\navailable vision-language embedding models; Jina\nCLIP (Koukounas et al., 2024), Nomic Embed Vi-\nsion (Nomic, 2024), and SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023).\nResults. From a performance perspective, best re-\nsults are obtained by combining the Unstructured\nparser with visual information, either from caption-\ning strategies or by running OCR on the visual ele-\nments (Table 2). Little difference is seen between\nBM25 and BGE-M3 embeddings highlighting the\nvisual information bottleneck. Contrastive VLMs\nlag behind. Beyond retrieval performance (R1), the\nindexing latencies (R2) reported in Figure 3 illus-\ntrate that PDF parsing pipelines can be very lengthy,\nespecially when incorporating OCR or captioning\nstrategies. Querying latencies at runtime (R3) are\nvery good for all evaluated systems ( \u226422ms on\nNVIDIA L4) due to fast query encoding and cosine\nsimilarity matching.\n0 1 2 3 4 5 6 7\nLatency (s)ColPali\n(0.39s)Siglip\n(0.12s)PDF Parser\n(7.22s)\nLayout Detection OCR Captioning Page Encoding\nFigure 3: Offline indexing with ColPali is much sim-\npler and faster compared to standard retrieval methods.\nIndexing speeds reported are computed on Nvidia L4\nGPUs and detailed in subsection B.5.\n4 Late interaction based Vision Retrieval\n4.1 Architecture\nVision-Language Models. Encouraged by their\nstrong document understanding capabilities, we\npropose adapting recent VLMs for retrieval. The\nkey concept is to leverage the alignment between\noutput embeddings of text and image tokens ac-\nquired during multi-modal finetuning. To this ex-\ntent, we introduce ColPali , a Paligemma-3B exten-\nsion that is capable of generating ColBERT-style\nmulti-vector representations of text and images\n(Figure 2).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c470b584-a285-4064-8ea1-68a82d8ae5bf": {"__data__": {"id_": "c470b584-a285-4064-8ea1-68a82d8ae5bf", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6415abb1-dc4b-435b-afc7-3ab62b243533", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "bc1eae24f9b981e65e8ab2e1a82dbbc0d5683180240b061bf4351cee6aa35663", "class_name": "RelatedNodeInfo"}}, "text": "PaliGemma-3B is a strong candidate\ndue to its small size, the many released checkpoints\nfine-tuned for different image resolutions and tasks,\n5We empirically validated the max-pooling strategy over\nsub-page chunks to be more effective than concatenating all\npage chunks before embedding pagewise.and the promising performances on various doc-\nument understanding benchmarks. We add a pro-\njection layer to map the output language model-\ning embeddings to a vector space of reduced di-\nmension D= 128 as used in the ColBERT paper\n(Khattab and Zaharia, 2020) to keep lightweight\nbag-of-embedding representations.\nLate Interaction. Given query qand document\nd, we denote as Eq\u2208RNq\u00d7DandEd\u2208RNd\u00d7D\ntheir respective multi-vector representation in the\ncommon embedding space RD. The late interaction\noperator, LI(q, d), is the sum over all query vectors\nEq(j), of its maximum dot product \u27e8\u00b7|\u00b7\u27e9with each\nof the Nddocument embedding vectors Ed(1:Nd).\nLI(q, d) =X\ni\u2208[|1,Nq|]max\nj\u2208[|1,Nd|]\u27e8Eq(i)|Ed(j)\u27e9(1)\nContrastive Loss. The Late Interaction opera-\ntion is fully differentiable, enabling backpropaga-\ntion. Let a batch {qk, dk}k\u2208[|1,b|]composed of b\nquery-page pairs, where for all k\u2208[|1, b|], the\ndocument page dkis the document correspond-\ning to query qk. Following Khattab and Zaharia\n(2020), we define our in-batch contrastive loss Las\nthe softmaxed cross-entropy of the positive scores\ns+\nk=LI(qk, dk)w.r.t. to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, dl).\n4.2 Model training\nDataset.", "mimetype": "text/plain", "start_char_idx": 1803, "end_char_idx": 3293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "22647baa-2f22-448d-9739-711281ea1fd3": {"__data__": {"id_": "22647baa-2f22-448d-9739-711281ea1fd3", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c470b584-a285-4064-8ea1-68a82d8ae5bf", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "5da286d5b2c896ac5293bb6f5119e67356db0ea9e400cf6eee5223d4c1e6b6d1", "class_name": "RelatedNodeInfo"}}, "text": "4.2 Model training\nDataset. Our training dataset of 127,460 query-\npage pairs is comprised of train sets of openly\navailable academic datasets ( 63%) and a synthetic\ndataset made up of pages from web-crawled PDF\ndocuments and augmented with VLM-generated\n(Claude-3 Sonnet) pseudo-questions ( 37%).", "mimetype": "text/plain", "start_char_idx": 3266, "end_char_idx": 3563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0536d29a-907c-41e2-91df-2e0c62768193": {"__data__": {"id_": "0536d29a-907c-41e2-91df-2e0c62768193", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30b8747c-37f1-4b65-99ba-ca8566d3c53e", "node_type": "1", "metadata": {}, "hash": "93074af78c3aa33b1df4e97df84fcafa512ead2b5e1f4caf922aefd9965ea90c", "class_name": "RelatedNodeInfo"}}, "text": "max-pooling over the page\u2019s chunk scores.5\nContrastive VLMs. We also evaluate the strongest\navailable vision-language embedding models; Jina\nCLIP (Koukounas et al., 2024), Nomic Embed Vi-\nsion (Nomic, 2024), and SigLIP-So400m/14 (Al-\nabdulmohsin et al., 2023).\nResults. From a performance perspective, best re-\nsults are obtained by combining the Unstructured\nparser with visual information, either from caption-\ning strategies or by running OCR on the visual ele-\nments (Table 2). Little difference is seen between\nBM25 and BGE-M3 embeddings highlighting the\nvisual information bottleneck. Contrastive VLMs\nlag behind. Beyond retrieval performance (R1), the\nindexing latencies (R2) reported in Figure 3 illus-\ntrate that PDF parsing pipelines can be very lengthy,\nespecially when incorporating OCR or captioning\nstrategies. Querying latencies at runtime (R3) are\nvery good for all evaluated systems ( \u226422ms on\nNVIDIA L4) due to fast query encoding and cosine\nsimilarity matching.\n0 1 2 3 4 5 6 7\nLatency (s)ColPali\n(0.39s)Siglip\n(0.12s)PDF Parser\n(7.22s)\nLayout Detection OCR Captioning Page Encoding\nFigure 3: Offline indexing with ColPali is much sim-\npler and faster compared to standard retrieval methods.\nIndexing speeds reported are computed on Nvidia L4\nGPUs and detailed in subsection B.5.\n4 Late interaction based Vision Retrieval\n4.1 Architecture\nVision-Language Models. Encouraged by their\nstrong document understanding capabilities, we\npropose adapting recent VLMs for retrieval. The\nkey concept is to leverage the alignment between\noutput embeddings of text and image tokens ac-\nquired during multi-modal finetuning. To this ex-\ntent, we introduce ColPali , a Paligemma-3B exten-\nsion that is capable of generating ColBERT-style\nmulti-vector representations of text and images\n(Figure 2). PaliGemma-3B is a strong candidate\ndue to its small size, the many released checkpoints\nfine-tuned for different image resolutions and tasks,\n5We empirically validated the max-pooling strategy over\nsub-page chunks to be more effective than concatenating all\npage chunks before embedding pagewise.and the promising performances on various doc-\nument understanding benchmarks. We add a pro-\njection layer to map the output language model-\ning embeddings to a vector space of reduced di-\nmension D= 128 as used in the ColBERT paper\n(Khattab and Zaharia, 2020) to keep lightweight\nbag-of-embedding representations.\nLate Interaction. Given query qand document\nd, we denote as Eq\u2208RNq\u00d7DandEd\u2208RNd\u00d7D\ntheir respective multi-vector representation in the\ncommon embedding space RD. The late interaction\noperator, LI(q, d), is the sum over all query vectors\nEq(j), of its maximum dot product \u27e8\u00b7|\u00b7\u27e9with each\nof the Nddocument embedding vectors Ed(1:Nd).\nLI(q, d) =X\ni\u2208[|1,Nq|]max\nj\u2208[|1,Nd|]\u27e8Eq(i)|Ed(j)\u27e9(1)\nContrastive Loss. The Late Interaction opera-\ntion is fully differentiable, enabling backpropaga-\ntion. Let a batch {qk, dk}k\u2208[|1,b|]composed of b\nquery-page pairs, where for all k\u2208[|1, b|], the\ndocument page dkis the document correspond-\ning to query qk. Following Khattab and Zaharia\n(2020), we define our in-batch contrastive loss Las\nthe softmaxed cross-entropy of the positive scores\ns+\nk=LI(qk, dk)w.r.t. to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, dl).\n4.2 Model training\nDataset. Our training dataset of 127,460 query-\npage pairs is comprised of train sets of openly\navailable academic datasets ( 63%) and a synthetic\ndataset made up of pages from web-crawled PDF\ndocuments and augmented with VLM-generated\n(Claude-3 Sonnet) pseudo-questions ( 37%).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0536d29a-907c-41e2-91df-2e0c62768193", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6260c3fd-ce52-4908-92ea-f04f6d96b982": {"__data__": {"id_": "6260c3fd-ce52-4908-92ea-f04f6d96b982", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}}, "text": "Following Khattab and Zaharia\n(2020), we define our in-batch contrastive loss Las\nthe softmaxed cross-entropy of the positive scores\ns+\nk=LI(qk, dk)w.r.t. to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, dl).\n4.2 Model training\nDataset. Our training dataset of 127,460 query-\npage pairs is comprised of train sets of openly\navailable academic datasets ( 63%) and a synthetic\ndataset made up of pages from web-crawled PDF\ndocuments and augmented with VLM-generated\n(Claude-3 Sonnet) pseudo-questions ( 37%). Our\ntraining set is fully English by design, enabling us\nto study zero-shot generalization to non-English\nlanguages6. We explicitly verify no multi-page\nPDF document is used both ViDoRe and in the\ntrain set to prevent evaluation contamination. A\nvalidation set is created with 2%of the samples to\ntune hyperparameters.\nParameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "30b8747c-37f1-4b65-99ba-ca8566d3c53e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4e070bd5-9c75-4e71-ba69-aff6236ffa26": {"__data__": {"id_": "4e070bd5-9c75-4e71-ba69-aff6236ffa26", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6260c3fd-ce52-4908-92ea-f04f6d96b982", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "09fcd7457314a90557d4dd7953933f77f6ca29e3710d510cf2ed29c9fad71e4b", "class_name": "RelatedNodeInfo"}}, "text": "Parameters. All models are trained for 1 epoch on\nthe train set. Unless specified otherwise, we train\nmodels in bfloat16 format, use low-rank adapters\n(LoRA, Hu et al. (2021)) with \u03b1= 32 andr= 32\non the transformer layers from the language model,\n6Multilingual data is present in the pretraining corpus of\nthe language model (Gemma-2B) and potentially occurs dur-\ning PaliGemma-3B\u2019s multimodal training.\n5", "mimetype": "text/plain", "start_char_idx": 831, "end_char_idx": 1236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "30b8747c-37f1-4b65-99ba-ca8566d3c53e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "77a5d525-80de-4b81-8e7c-bf4aaf73e2c8": {"__data__": {"id_": "77a5d525-80de-4b81-8e7c-bf4aaf73e2c8", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}}, "text": "Following Khattab and Zaharia\n(2020), we define our in-batch contrastive loss Las\nthe softmaxed cross-entropy of the positive scores\ns+\nk=LI(qk, dk)w.r.t. to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, dl).\n4.2 Model training\nDataset. Our training dataset of 127,460 query-\npage pairs is comprised of train sets of openly\navailable academic datasets ( 63%) and a synthetic\ndataset made up of pages from web-crawled PDF\ndocuments and augmented with VLM-generated\n(Claude-3 Sonnet) pseudo-questions ( 37%). Our\ntraining set is fully English by design, enabling us\nto study zero-shot generalization to non-English\nlanguages6. We explicitly verify no multi-page\nPDF document is used both ViDoRe and in the\ntrain set to prevent evaluation contamination. A\nvalidation set is created with 2%of the samples to\ntune hyperparameters.\nParameters. All models are trained for 1 epoch on\nthe train set. Unless specified otherwise, we train\nmodels in bfloat16 format, use low-rank adapters\n(LoRA, Hu et al. (2021)) with \u03b1= 32 andr= 32\non the transformer layers from the language model,\n6Multilingual data is present in the pretraining corpus of\nthe language model (Gemma-2B) and potentially occurs dur-\ning PaliGemma-3B\u2019s multimodal training.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "30b8747c-37f1-4b65-99ba-ca8566d3c53e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "30b8747c-37f1-4b65-99ba-ca8566d3c53e": {"__data__": {"id_": "30b8747c-37f1-4b65-99ba-ca8566d3c53e", "embedding": null, "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f13a299522a5ac2c1204b196cd0547ee1e5f113f5a8962ceca9bcb296bedff1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0536d29a-907c-41e2-91df-2e0c62768193", "node_type": "1", "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "482814bcf3e47a334a28e53f9a3268490cb97578b99f8610991e4e1a2239265e", "class_name": "RelatedNodeInfo"}}, "text": "Following Khattab and Zaharia\n(2020), we define our in-batch contrastive loss Las\nthe softmaxed cross-entropy of the positive scores\ns+\nk=LI(qk, dk)w.r.t. to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, dl).\n4.2 Model training\nDataset. Our training dataset of 127,460 query-\npage pairs is comprised of train sets of openly\navailable academic datasets ( 63%) and a synthetic\ndataset made up of pages from web-crawled PDF\ndocuments and augmented with VLM-generated\n(Claude-3 Sonnet) pseudo-questions ( 37%). Our\ntraining set is fully English by design, enabling us\nto study zero-shot generalization to non-English\nlanguages6. We explicitly verify no multi-page\nPDF document is used both ViDoRe and in the\ntrain set to prevent evaluation contamination. A\nvalidation set is created with 2%of the samples to\ntune hyperparameters.\nParameters. All models are trained for 1 epoch on\nthe train set. Unless specified otherwise, we train\nmodels in bfloat16 format, use low-rank adapters\n(LoRA, Hu et al. (2021)) with \u03b1= 32 andr= 32\non the transformer layers from the language model,\n6Multilingual data is present in the pretraining corpus of\nthe language model (Gemma-2B) and potentially occurs dur-\ning PaliGemma-3B\u2019s multimodal training.\n5", "mimetype": "text/plain", "start_char_idx": 3052, "end_char_idx": 4288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "30b8747c-37f1-4b65-99ba-ca8566d3c53e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "75fa0034-cc52-41b2-9102-80626bdcb919": {"__data__": {"id_": "75fa0034-cc52-41b2-9102-80626bdcb919", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}}, "text": "as well as the final randomly initialized projection\nlayer, and use a paged_adamw_8bit optimizer. We\ntrain on an 8 GPU setup with data parallelism, a\nlearning rate of 5e\u22125with linear decay with 2.5%\nwarmup steps, and a batch size of 32.\nQuery Augmentation. As in Khattab and Za-\nharia (2020), we append 5 <unused0> tokens to\nthe query tokens to serve as a soft, differentiable\nquery expansion or re-weighting mechanism.\n5 Results\n5.1 Performance (R1)\nWe iteratively construct ColPali , starting from an\noff-the-shelf SigLIP model (Table 2).\nBiSigLIP: Improving a strong model. SigLIP7\nis a strong vision-language bi-encoder model, pre-\ntrained on the English split of WebLI (Chen et al.,\n2023), a corpus of billions of image-text pairs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a44dd696-5827-4690-a506-5b6031630fd2": {"__data__": {"id_": "a44dd696-5827-4690-a506-5b6031630fd2", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75fa0034-cc52-41b2-9102-80626bdcb919", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3d42a65e74a60905c20dd4fa23e7e8d14f56e776be0ab81bed3b3c5f1e561f90", "class_name": "RelatedNodeInfo"}}, "text": "We find that SigLIP largely outperforms both Jina\nCLIP and Nomic-vision on document retrieval\ntasks. Further fine-tuning the textual component\nof this model on our document-oriented dataset\n(BiSigLIP) yields clear improvements across the\nboard, particularly on figure retrieval (ArxivQA)\nand table retrieval tasks (TabFQuAD).\nBiPali: Pairing with a language model. In the\nPaliGemma model architecture, SigLIP-generated\npatch embeddings are fed to a text language model\nto obtain LLM contextualized output patch embed-\ndings.8We average pool these representations to\nobtain a single dense vector, effectively creating a\nPaliGemma bi-encoder model (BiPali). After fine-\ntuning on the training dataset, we obtain a model\nthat performs slightly worse in English than the\ntuned BiSigLIP variant.", "mimetype": "text/plain", "start_char_idx": 737, "end_char_idx": 1527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "fd8db8b0-3746-4518-adae-20691f8d96b9": {"__data__": {"id_": "fd8db8b0-3746-4518-adae-20691f8d96b9", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a44dd696-5827-4690-a506-5b6031630fd2", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "fad45eab38129535665307dcc542d49d1467f45a55d06824c337f562515c9c1a", "class_name": "RelatedNodeInfo"}}, "text": "This can be explained\nby the fact that contrary to SigLIP, the original\nPaliGemma is not trained on contrastive matching\ntasks, but rather on next token prediction. Our\ncontrastive fine-tuning phase on 100K images to\ntransform PaliGemma into a bi-encoder is 5 orders\nof magnitude smaller than SigLIP\u2019s original con-\ntrastive training. However, we see notable improve-\nments in French tasks, indicating that BiPali\u2019s LLM\n(Gemma 2B) helps multilingual text understanding.\nThis is particularly notable as our training dataset\ndoes not contain non-English samples.\n7https://huggingface.co/google/\nsiglip-so400m-patch14-384\n8Note that the SigLIP model used in PaliGemma slightly\ndiffers in terms of number patches - 1024 patches for\nPaliGemma\u2019s vision encoder, and 729 for the standalone\nSigLIP model.ColPali : Adding Late Interaction.", "mimetype": "text/plain", "start_char_idx": 1528, "end_char_idx": 2358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a02de361-6cad-42e5-84a3-5063865cdd01": {"__data__": {"id_": "a02de361-6cad-42e5-84a3-5063865cdd01", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd8db8b0-3746-4518-adae-20691f8d96b9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3d2a517074fa8e268d780c6b085147cf0aa63165ce26a6de99b37110c15272d2", "class_name": "RelatedNodeInfo"}}, "text": "One benefit\nof inputting image patch embeddings through a\nlanguage model is that they are natively mapped\nto a latent space similar to textual input (query).\nThis enables leveraging the ColBERT strategy to\ncompute interactions between text tokens and im-\nage patches, which enables a step-change improve-\nment in performance compared to BiPali. Re-\nsults in Table 2 show that our ColPali model also\nlargely outperforms the strong baselines based on\nUnstructured and captioning, as well as all evalu-\nated text-image embedding models. The difference\nis particularly stark on the more visually complex\nbenchmark tasks, such as InfographicVQA, Arx-\nivQA, and TabFQuAD representing respectively\ninfographics, figures, and tables. However, text-\ncentric documents are also better retrieved by the\nColPali models across all evaluated domains and\nlanguages, making our approach the overall best-\nperforming document-retrieval model.\nNegative Results.", "mimetype": "text/plain", "start_char_idx": 2359, "end_char_idx": 3302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6b68f57e-d476-4b0a-8ada-68ecf5c9941e": {"__data__": {"id_": "6b68f57e-d476-4b0a-8ada-68ecf5c9941e", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a02de361-6cad-42e5-84a3-5063865cdd01", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b78d5ec3c4ccb3cf20b580261511ddd500a50928bdfed410d4679e8f5d4a8ef4", "class_name": "RelatedNodeInfo"}}, "text": "Negative Results. For extensiveness, we also\ntrain ColSigLIP, a late interaction variant of the\nBiSigLIP model but obtain abysmal performances.\nWe attribute this to the large gaps w.r.t. SigLIP\u2019s\npre-training, in which only a pooled latent repre-\nsentation is used in the contrastive loss, which\ndoes not optimize the representations of individ-\nual patch and token embeddings. Similarly, we\ntrain a BiSigLIP PaliGemma variant, in which we\nretrieve the image representations from the SigLIP\nmodel that has been further updated by PaliGemma\nfine-tuning, and use the text representations from\nPaliGemma\u2019s text model.", "mimetype": "text/plain", "start_char_idx": 3285, "end_char_idx": 3899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "757ce21e-c414-4021-8ddd-9149246a3b5d": {"__data__": {"id_": "757ce21e-c414-4021-8ddd-9149246a3b5d", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}}, "text": "as well as the final randomly initialized projection\nlayer, and use a paged_adamw_8bit optimizer. We\ntrain on an 8 GPU setup with data parallelism, a\nlearning rate of 5e\u22125with linear decay with 2.5%\nwarmup steps, and a batch size of 32.\nQuery Augmentation. As in Khattab and Za-\nharia (2020), we append 5 <unused0> tokens to\nthe query tokens to serve as a soft, differentiable\nquery expansion or re-weighting mechanism.\n5 Results\n5.1 Performance (R1)\nWe iteratively construct ColPali , starting from an\noff-the-shelf SigLIP model (Table 2).\nBiSigLIP: Improving a strong model. SigLIP7\nis a strong vision-language bi-encoder model, pre-\ntrained on the English split of WebLI (Chen et al.,\n2023), a corpus of billions of image-text pairs.\nWe find that SigLIP largely outperforms both Jina\nCLIP and Nomic-vision on document retrieval\ntasks. Further fine-tuning the textual component\nof this model on our document-oriented dataset\n(BiSigLIP) yields clear improvements across the\nboard, particularly on figure retrieval (ArxivQA)\nand table retrieval tasks (TabFQuAD).\nBiPali: Pairing with a language model. In the\nPaliGemma model architecture, SigLIP-generated\npatch embeddings are fed to a text language model\nto obtain LLM contextualized output patch embed-\ndings.8We average pool these representations to\nobtain a single dense vector, effectively creating a\nPaliGemma bi-encoder model (BiPali). After fine-\ntuning on the training dataset, we obtain a model\nthat performs slightly worse in English than the\ntuned BiSigLIP variant. This can be explained\nby the fact that contrary to SigLIP, the original\nPaliGemma is not trained on contrastive matching\ntasks, but rather on next token prediction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1e8ee17c-4a31-4ffb-99c4-c99ea22cf7e4": {"__data__": {"id_": "1e8ee17c-4a31-4ffb-99c4-c99ea22cf7e4", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "757ce21e-c414-4021-8ddd-9149246a3b5d", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "08a3a649a9a6f74b08997013fd846c9d06e2566b83a612b56681ce681610f7d9", "class_name": "RelatedNodeInfo"}}, "text": "Our\ncontrastive fine-tuning phase on 100K images to\ntransform PaliGemma into a bi-encoder is 5 orders\nof magnitude smaller than SigLIP\u2019s original con-\ntrastive training. However, we see notable improve-\nments in French tasks, indicating that BiPali\u2019s LLM\n(Gemma 2B) helps multilingual text understanding.\nThis is particularly notable as our training dataset\ndoes not contain non-English samples.\n7https://huggingface.co/google/\nsiglip-so400m-patch14-384\n8Note that the SigLIP model used in PaliGemma slightly\ndiffers in terms of number patches - 1024 patches for\nPaliGemma\u2019s vision encoder, and 729 for the standalone\nSigLIP model.ColPali : Adding Late Interaction. One benefit\nof inputting image patch embeddings through a\nlanguage model is that they are natively mapped\nto a latent space similar to textual input (query).\nThis enables leveraging the ColBERT strategy to\ncompute interactions between text tokens and im-\nage patches, which enables a step-change improve-\nment in performance compared to BiPali. Re-\nsults in Table 2 show that our ColPali model also\nlargely outperforms the strong baselines based on\nUnstructured and captioning, as well as all evalu-\nated text-image embedding models. The difference\nis particularly stark on the more visually complex\nbenchmark tasks, such as InfographicVQA, Arx-\nivQA, and TabFQuAD representing respectively\ninfographics, figures, and tables. However, text-\ncentric documents are also better retrieved by the\nColPali models across all evaluated domains and\nlanguages, making our approach the overall best-\nperforming document-retrieval model.\nNegative Results. For extensiveness, we also\ntrain ColSigLIP, a late interaction variant of the\nBiSigLIP model but obtain abysmal performances.\nWe attribute this to the large gaps w.r.t. SigLIP\u2019s\npre-training, in which only a pooled latent repre-\nsentation is used in the contrastive loss, which\ndoes not optimize the representations of individ-\nual patch and token embeddings.", "mimetype": "text/plain", "start_char_idx": 1693, "end_char_idx": 3662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "90878eb0-7697-4008-a301-0bda7a07eb2c": {"__data__": {"id_": "90878eb0-7697-4008-a301-0bda7a07eb2c", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e8ee17c-4a31-4ffb-99c4-c99ea22cf7e4", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "770b970e6e44af53d56993e390a8a02d49232eaaa366bb93e72766902d692559", "class_name": "RelatedNodeInfo"}}, "text": "Similarly, we\ntrain a BiSigLIP PaliGemma variant, in which we\nretrieve the image representations from the SigLIP\nmodel that has been further updated by PaliGemma\nfine-tuning, and use the text representations from\nPaliGemma\u2019s text model.", "mimetype": "text/plain", "start_char_idx": 3663, "end_char_idx": 3899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "09180f96-566d-43af-a849-1b147f4366e2": {"__data__": {"id_": "09180f96-566d-43af-a849-1b147f4366e2", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a2120bb-a06b-4e42-9581-faf88bb253df", "node_type": "1", "metadata": {}, "hash": "2dc3d02857646ff4e19fecd40517ff52dc529b649773f9d0cbac68071ff8d5c0", "class_name": "RelatedNodeInfo"}}, "text": "as well as the final randomly initialized projection\nlayer, and use a paged_adamw_8bit optimizer. We\ntrain on an 8 GPU setup with data parallelism, a\nlearning rate of 5e\u22125with linear decay with 2.5%\nwarmup steps, and a batch size of 32.\nQuery Augmentation. As in Khattab and Za-\nharia (2020), we append 5 <unused0> tokens to\nthe query tokens to serve as a soft, differentiable\nquery expansion or re-weighting mechanism.\n5 Results\n5.1 Performance (R1)\nWe iteratively construct ColPali , starting from an\noff-the-shelf SigLIP model (Table 2).\nBiSigLIP: Improving a strong model. SigLIP7\nis a strong vision-language bi-encoder model, pre-\ntrained on the English split of WebLI (Chen et al.,\n2023), a corpus of billions of image-text pairs.\nWe find that SigLIP largely outperforms both Jina\nCLIP and Nomic-vision on document retrieval\ntasks. Further fine-tuning the textual component\nof this model on our document-oriented dataset\n(BiSigLIP) yields clear improvements across the\nboard, particularly on figure retrieval (ArxivQA)\nand table retrieval tasks (TabFQuAD).\nBiPali: Pairing with a language model. In the\nPaliGemma model architecture, SigLIP-generated\npatch embeddings are fed to a text language model\nto obtain LLM contextualized output patch embed-\ndings.8We average pool these representations to\nobtain a single dense vector, effectively creating a\nPaliGemma bi-encoder model (BiPali). After fine-\ntuning on the training dataset, we obtain a model\nthat performs slightly worse in English than the\ntuned BiSigLIP variant. This can be explained\nby the fact that contrary to SigLIP, the original\nPaliGemma is not trained on contrastive matching\ntasks, but rather on next token prediction. Our\ncontrastive fine-tuning phase on 100K images to\ntransform PaliGemma into a bi-encoder is 5 orders\nof magnitude smaller than SigLIP\u2019s original con-\ntrastive training. However, we see notable improve-\nments in French tasks, indicating that BiPali\u2019s LLM\n(Gemma 2B) helps multilingual text understanding.\nThis is particularly notable as our training dataset\ndoes not contain non-English samples.\n7https://huggingface.co/google/\nsiglip-so400m-patch14-384\n8Note that the SigLIP model used in PaliGemma slightly\ndiffers in terms of number patches - 1024 patches for\nPaliGemma\u2019s vision encoder, and 729 for the standalone\nSigLIP model.ColPali : Adding Late Interaction. One benefit\nof inputting image patch embeddings through a\nlanguage model is that they are natively mapped\nto a latent space similar to textual input (query).\nThis enables leveraging the ColBERT strategy to\ncompute interactions between text tokens and im-\nage patches, which enables a step-change improve-\nment in performance compared to BiPali. Re-\nsults in Table 2 show that our ColPali model also\nlargely outperforms the strong baselines based on\nUnstructured and captioning, as well as all evalu-\nated text-image embedding models. The difference\nis particularly stark on the more visually complex\nbenchmark tasks, such as InfographicVQA, Arx-\nivQA, and TabFQuAD representing respectively\ninfographics, figures, and tables. However, text-\ncentric documents are also better retrieved by the\nColPali models across all evaluated domains and\nlanguages, making our approach the overall best-\nperforming document-retrieval model.\nNegative Results. For extensiveness, we also\ntrain ColSigLIP, a late interaction variant of the\nBiSigLIP model but obtain abysmal performances.\nWe attribute this to the large gaps w.r.t. SigLIP\u2019s\npre-training, in which only a pooled latent repre-\nsentation is used in the contrastive loss, which\ndoes not optimize the representations of individ-\nual patch and token embeddings. Similarly, we\ntrain a BiSigLIP PaliGemma variant, in which we\nretrieve the image representations from the SigLIP\nmodel that has been further updated by PaliGemma\nfine-tuning, and use the text representations from\nPaliGemma\u2019s text model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "09180f96-566d-43af-a849-1b147f4366e2", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "48a393a9-9b2b-4f67-ba95-f8eac87f88f7": {"__data__": {"id_": "48a393a9-9b2b-4f67-ba95-f8eac87f88f7", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}}, "text": "However, text-\ncentric documents are also better retrieved by the\nColPali models across all evaluated domains and\nlanguages, making our approach the overall best-\nperforming document-retrieval model.\nNegative Results. For extensiveness, we also\ntrain ColSigLIP, a late interaction variant of the\nBiSigLIP model but obtain abysmal performances.\nWe attribute this to the large gaps w.r.t. SigLIP\u2019s\npre-training, in which only a pooled latent repre-\nsentation is used in the contrastive loss, which\ndoes not optimize the representations of individ-\nual patch and token embeddings. Similarly, we\ntrain a BiSigLIP PaliGemma variant, in which we\nretrieve the image representations from the SigLIP\nmodel that has been further updated by PaliGemma\nfine-tuning, and use the text representations from\nPaliGemma\u2019s text model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a2120bb-a06b-4e42-9581-faf88bb253df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c832fb63-660f-47b9-9c72-4a01c208e943": {"__data__": {"id_": "c832fb63-660f-47b9-9c72-4a01c208e943", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48a393a9-9b2b-4f67-ba95-f8eac87f88f7", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "c88312a568102e31fe37086dc3c53f7602c0d120bae8c65f3b1ad8d0444b1a85", "class_name": "RelatedNodeInfo"}}, "text": "After fine-tuning on\nour dataset, performance is severely inferior to\nSigLIP V anilla which simply encodes with SigLIP\u2019s\noriginal text and vision components. This indicates\na logical misalignment between SigLIP embed-\ndings, and Gemma embeddings after PaliGemma\ntraining. We detail these results in Table 5.\n5.2 Latencies & Memory Footprint\nOnline Querying. (R2) Logically, querying la-\ntencies differ between ColPali and a BGE-M3 em-\nbedding model. For BGE, encoding takes about\n22 ms for 15 tokens, while encoding a query with\nColPali \u2019s language model takes about 30 ms9. For\nsmaller corpus sizes, computing the late interaction\noperation induces marginally small overheads ( \u22481\nms per 1000 pages in the corpus), and the cosine\nsimilarity computation between bi-encoder vectors\n9Computed for a batch size of 1 (online), and averaged\nover 1000 queries. See subsection B.5\n6", "mimetype": "text/plain", "start_char_idx": 815, "end_char_idx": 1690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a2120bb-a06b-4e42-9581-faf88bb253df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "174f5784-520e-4f35-b397-a2b05a9071eb": {"__data__": {"id_": "174f5784-520e-4f35-b397-a2b05a9071eb", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}}, "text": "However, text-\ncentric documents are also better retrieved by the\nColPali models across all evaluated domains and\nlanguages, making our approach the overall best-\nperforming document-retrieval model.\nNegative Results. For extensiveness, we also\ntrain ColSigLIP, a late interaction variant of the\nBiSigLIP model but obtain abysmal performances.\nWe attribute this to the large gaps w.r.t. SigLIP\u2019s\npre-training, in which only a pooled latent repre-\nsentation is used in the contrastive loss, which\ndoes not optimize the representations of individ-\nual patch and token embeddings. Similarly, we\ntrain a BiSigLIP PaliGemma variant, in which we\nretrieve the image representations from the SigLIP\nmodel that has been further updated by PaliGemma\nfine-tuning, and use the text representations from\nPaliGemma\u2019s text model. After fine-tuning on\nour dataset, performance is severely inferior to\nSigLIP V anilla which simply encodes with SigLIP\u2019s\noriginal text and vision components. This indicates\na logical misalignment between SigLIP embed-\ndings, and Gemma embeddings after PaliGemma\ntraining. We detail these results in Table 5.\n5.2 Latencies & Memory Footprint\nOnline Querying. (R2) Logically, querying la-\ntencies differ between ColPali and a BGE-M3 em-\nbedding model. For BGE, encoding takes about\n22 ms for 15 tokens, while encoding a query with\nColPali \u2019s language model takes about 30 ms9. For\nsmaller corpus sizes, computing the late interaction\noperation induces marginally small overheads ( \u22481\nms per 1000 pages in the corpus), and the cosine\nsimilarity computation between bi-encoder vectors\n9Computed for a batch size of 1 (online), and averaged\nover 1000 queries. See subsection B.5\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a2120bb-a06b-4e42-9581-faf88bb253df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2a2120bb-a06b-4e42-9581-faf88bb253df": {"__data__": {"id_": "2a2120bb-a06b-4e42-9581-faf88bb253df", "embedding": null, "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66", "node_type": "4", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "83599d887121d77ee5d4e50091b7faba084e3da2438bdc87ce83c55bbe776247", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09180f96-566d-43af-a849-1b147f4366e2", "node_type": "1", "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "4f07761ab0441877405c5e45535385a8e9d1285c57009258828fc366c8fcde58", "class_name": "RelatedNodeInfo"}}, "text": "However, text-\ncentric documents are also better retrieved by the\nColPali models across all evaluated domains and\nlanguages, making our approach the overall best-\nperforming document-retrieval model.\nNegative Results. For extensiveness, we also\ntrain ColSigLIP, a late interaction variant of the\nBiSigLIP model but obtain abysmal performances.\nWe attribute this to the large gaps w.r.t. SigLIP\u2019s\npre-training, in which only a pooled latent repre-\nsentation is used in the contrastive loss, which\ndoes not optimize the representations of individ-\nual patch and token embeddings. Similarly, we\ntrain a BiSigLIP PaliGemma variant, in which we\nretrieve the image representations from the SigLIP\nmodel that has been further updated by PaliGemma\nfine-tuning, and use the text representations from\nPaliGemma\u2019s text model. After fine-tuning on\nour dataset, performance is severely inferior to\nSigLIP V anilla which simply encodes with SigLIP\u2019s\noriginal text and vision components. This indicates\na logical misalignment between SigLIP embed-\ndings, and Gemma embeddings after PaliGemma\ntraining. We detail these results in Table 5.\n5.2 Latencies & Memory Footprint\nOnline Querying. (R2) Logically, querying la-\ntencies differ between ColPali and a BGE-M3 em-\nbedding model. For BGE, encoding takes about\n22 ms for 15 tokens, while encoding a query with\nColPali \u2019s language model takes about 30 ms9. For\nsmaller corpus sizes, computing the late interaction\noperation induces marginally small overheads ( \u22481\nms per 1000 pages in the corpus), and the cosine\nsimilarity computation between bi-encoder vectors\n9Computed for a batch size of 1 (online), and averaged\nover 1000 queries. See subsection B.5\n6", "mimetype": "text/plain", "start_char_idx": 3085, "end_char_idx": 4775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "2a2120bb-a06b-4e42-9581-faf88bb253df", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "78bcb5a3-6a5f-4c83-b488-099980ee9594": {"__data__": {"id_": "78bcb5a3-6a5f-4c83-b488-099980ee9594", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}}, "text": "ArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.\nUnstructured Text only\n- BM25 - 34.1 - - 44.0 59.6 90.4 78.3 78.8 82.6 -\n- BGE-M3 - 28.4 \u21935.7- - 36.1 \u21937.968.5 \u21918.988.4 \u21932.076.8 \u21931.577.7 \u21931.184.6 \u21912.0 -\nUnstructured + OCR\n- BM25 31.6 36.8 62.9 46.5 62.7 64.3 92.8 85.9 83.9 87.2 65.5\n- BGE-M3 31.4 \u21930.225.7 \u219311.160.1 \u21932.870.8 \u219124.350.5 \u219312.273.2 \u21918.990.2 \u21932.683.6 \u21932.384.9 \u21911.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c140d60b-4a57-42ba-83c9-3b2347df533f": {"__data__": {"id_": "c140d60b-4a57-42ba-83c9-3b2347df533f", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78bcb5a3-6a5f-4c83-b488-099980ee9594", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "d43659bb1b8f786a183f0205ad77e5b3aea4fe7db7a2ea7240721824eeb99fa3", "class_name": "RelatedNodeInfo"}}, "text": "990.2 \u21932.683.6 \u21932.384.9 \u21911.091.1 \u21913.9 66.1 \u21910.6\nUnstructured + Captioning\n- BM25 40.1 38.4 70.0 35.4 61.5 60.9 88.0 84.7 82.7 89.2 65.1\n- BGE-M3 35.7 \u21934.432.9 \u21935.471.9 \u21911.969.1 \u219133.743.8 \u219317.773.1 \u219112.288.8 \u21910.883.3 \u21931.480.4 \u21932.391.3 \u21912.1 67.0 \u21911.9\nContrastive VLMs\nJina-CLIP 25.4 11.9 35.5 20.2 3.3 3.8 15.2 19.7 21.4 20.8 17.7\nNomic-vision 17.1 10.", "mimetype": "text/plain", "start_char_idx": 362, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d4b16900-3cf4-4abc-acdf-1d910caec986": {"__data__": {"id_": "d4b16900-3cf4-4abc-acdf-1d910caec986", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c140d60b-4a57-42ba-83c9-3b2347df533f", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b2ca5683137461044aad201bb753c47f7cecc76314dcb337abd5e04b2caf7801", "class_name": "RelatedNodeInfo"}}, "text": "8 17.7\nNomic-vision 17.1 10.7 30.1 16.3 2.7 1.1 12.9 10.9 11.4 15.7 12.9\nSigLIP (Vanilla) 43.2 30.3 64.1 58.1 26.2 18.7 62.5 65.7 66.1 79.1 51.4\nOurs\nSigLIP (Vanilla) 43.2 30.3 64.1 58.1 26.2 18.7 62.5 65.7 66.1 79.1 51.4\nBiSigLIP (+fine-tuning) 58.5 \u219115.332.9 \u21912.670.5 \u21916.462.7 \u21914.630.5 \u21914.326.5 \u21917.874.3 \u219111.873.7 \u21918.074.2 \u21918.182.3 \u21913.", "mimetype": "text/plain", "start_char_idx": 684, "end_char_idx": 1021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d49357f0-0cd5-462e-86d7-5ec81469804d": {"__data__": {"id_": "d49357f0-0cd5-462e-86d7-5ec81469804d", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4b16900-3cf4-4abc-acdf-1d910caec986", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ee0f974ac046b06f17c68ca681a8fd4ed5633d4728722b52a53f987d805fdb63", "class_name": "RelatedNodeInfo"}}, "text": "873.7 \u21918.074.2 \u21918.182.3 \u21913.2 58.6 \u21917.2\nBiPali (+LLM) 56.5 \u2193-2.030.0 \u2193-2.967.4 \u2193-3.176.9 \u219114.233.4 \u21912.943.7 \u219117.271.2 \u2193-3.161.9 \u2193-11.773.8 \u2193-0.473.6 \u2193-8.8 58.8 \u21910.2\nColPali (+Late Inter.) 79.1 \u219122.654.4 \u219124.581.8 \u219114.483.9 \u21917.065.8 \u219132.473.2 \u219129.596.2 \u219125.091.0 \u219129.192.7 \u219118.994.4 \u219120.8 81.3 \u219122.5\nTable 2: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using NDCG@5 metrics, and illustrate the impact of different components.", "mimetype": "text/plain", "start_char_idx": 994, "end_char_idx": 1474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6fccdadb-6bc1-40b1-8a3d-f5bcc5c33d31": {"__data__": {"id_": "6fccdadb-6bc1-40b1-8a3d-f5bcc5c33d31", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d49357f0-0cd5-462e-86d7-5ec81469804d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "15eb025e26b5daea795f48d68458477108928ac8f3617a2ca09c46f212b91d16", "class_name": "RelatedNodeInfo"}}, "text": "Text-only metrics are not\ncomputed for benchmarks with only visual elements.\nis even faster. Optimized late interaction engines\n(Santhanam et al., 2022; Lee et al., 2023) enable to\neasily scale corpus sizes to millions of documents\nwith reduced latency degradations.\nOffline Indexing. (R3) Standard retrieval methods\nusing bi-encoders represent each chunk as a single\nvector embedding, which is easy to store and fast\nto compute.", "mimetype": "text/plain", "start_char_idx": 1475, "end_char_idx": 1904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c633fd5a-bd69-4692-bc8c-07e17a64defd": {"__data__": {"id_": "c633fd5a-bd69-4692-bc8c-07e17a64defd", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}}, "text": "ArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.\nUnstructured Text only\n- BM25 - 34.1 - - 44.0 59.6 90.4 78.3 78.8 82.6 -\n- BGE-M3 - 28.4 \u21935.7- - 36.1 \u21937.968.5 \u21918.988.4 \u21932.076.8 \u21931.577.7 \u21931.184.6 \u21912.0 -\nUnstructured + OCR\n- BM25 31.6 36.8 62.9 46.5 62.7 64.3 92.8 85.9 83.9 87.2 65.5\n- BGE-M3 31.4 \u21930.225.7 \u219311.160.1 \u21932.870.8 \u219124.350.5 \u219312.273.2 \u21918.990.2 \u21932.683.6 \u21932.384.9 \u21911.091.1 \u21913.9 66.1 \u21910.6\nUnstructured + Captioning\n- BM25 40.1 38.4 70.0 35.4 61.5 60.9 88.0 84.7 82.7 89.2 65.1\n- BGE-M3 35.7 \u21934.432.9 \u21935.471.9 \u21911.969.1 \u219133.743.8 \u219317.773.1 \u219112.288.8 \u21910.883.3 \u21931.480.4 \u21932.391.3 \u21912.1 67.0 \u21911.9\nContrastive VLMs\nJina-CLIP 25.4 11.9 35.5 20.2 3.3 3.8 15.2 19.7 21.4 20.8 17.7\nNomic-vision 17.1 10.7 30.1 16.3 2.7 1.1 12.9 10.9 11.4 15.7 12.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d4413dab-3ea2-4bac-99b6-457f6271e095": {"__data__": {"id_": "d4413dab-3ea2-4bac-99b6-457f6271e095", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c633fd5a-bd69-4692-bc8c-07e17a64defd", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "23dfcd8f2f63dd9ca5456445469aaeb06c4d38df72a10ffb733aed380a356f16", "class_name": "RelatedNodeInfo"}}, "text": "1 12.9 10.9 11.4 15.7 12.9\nSigLIP (Vanilla) 43.2 30.3 64.1 58.1 26.2 18.7 62.5 65.7 66.1 79.1 51.4\nOurs\nSigLIP (Vanilla) 43.2 30.3 64.1 58.1 26.2 18.7 62.5 65.7 66.1 79.1 51.4\nBiSigLIP (+fine-tuning) 58.5 \u219115.332.9 \u21912.670.5 \u21916.462.7 \u21914.630.5 \u21914.326.5 \u21917.874.3 \u219111.873.7 \u21918.074.2 \u21918.182.3 \u21913.2 58.6 \u21917.2\nBiPali (+LLM) 56.5 \u2193-2.030.0 \u2193-2.967.4 \u2193-3.176.9 \u219114.233.4 \u21912.943.7 \u219117.271.2 \u2193-3.161.9 \u2193-11.773.8 \u2193-0.473.6 \u2193-8.8 58.8 \u21910.2\nColPali (+Late Inter.) 79.1 \u219122.654.4 \u219124.581.8 \u219114.483.9 \u21917.065.8 \u219132.473.2 \u219129.596.2 \u219125.091.0 \u219129.192.7 \u219118.994.4 \u219120.8 81.3 \u219122.5\nTable 2: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using NDCG@5 metrics, and illustrate the impact of different components. Text-only metrics are not\ncomputed for benchmarks with only visual elements.\nis even faster. Optimized late interaction engines\n(Santhanam et al., 2022; Lee et al., 2023) enable to\neasily scale corpus sizes to millions of documents\nwith reduced latency degradations.\nOffline Indexing.", "mimetype": "text/plain", "start_char_idx": 730, "end_char_idx": 1759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2d2bd23d-b78f-4e66-9d24-a545c04387bf": {"__data__": {"id_": "2d2bd23d-b78f-4e66-9d24-a545c04387bf", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4413dab-3ea2-4bac-99b6-457f6271e095", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7f4aad742860c7102def7552672bcbb0d22d88761680fc10dcb3d03f2ec78e9b", "class_name": "RelatedNodeInfo"}}, "text": "Offline Indexing. (R3) Standard retrieval methods\nusing bi-encoders represent each chunk as a single\nvector embedding, which is easy to store and fast\nto compute.", "mimetype": "text/plain", "start_char_idx": 1742, "end_char_idx": 1904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3": {"__data__": {"id_": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc", "node_type": "1", "metadata": {}, "hash": "d8ca5055b1b5514a12e351ae8e32a72004ebdc57689ce297d53f9ed40ba4656d", "class_name": "RelatedNodeInfo"}}, "text": "ArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.\nUnstructured Text only\n- BM25 - 34.1 - - 44.0 59.6 90.4 78.3 78.8 82.6 -\n- BGE-M3 - 28.4 \u21935.7- - 36.1 \u21937.968.5 \u21918.988.4 \u21932.076.8 \u21931.577.7 \u21931.184.6 \u21912.0 -\nUnstructured + OCR\n- BM25 31.6 36.8 62.9 46.5 62.7 64.3 92.8 85.9 83.9 87.2 65.5\n- BGE-M3 31.4 \u21930.225.7 \u219311.160.1 \u21932.870.8 \u219124.350.5 \u219312.273.2 \u21918.990.2 \u21932.683.6 \u21932.384.9 \u21911.091.1 \u21913.9 66.1 \u21910.6\nUnstructured + Captioning\n- BM25 40.1 38.4 70.0 35.4 61.5 60.9 88.0 84.7 82.7 89.2 65.1\n- BGE-M3 35.7 \u21934.432.9 \u21935.471.9 \u21911.969.1 \u219133.743.8 \u219317.773.1 \u219112.288.8 \u21910.883.3 \u21931.480.4 \u21932.391.3 \u21912.1 67.0 \u21911.9\nContrastive VLMs\nJina-CLIP 25.4 11.9 35.5 20.2 3.3 3.8 15.2 19.7 21.4 20.8 17.7\nNomic-vision 17.1 10.7 30.1 16.3 2.7 1.1 12.9 10.9 11.4 15.7 12.9\nSigLIP (Vanilla) 43.2 30.3 64.1 58.1 26.2 18.7 62.5 65.7 66.1 79.1 51.4\nOurs\nSigLIP (Vanilla) 43.2 30.3 64.1 58.1 26.2 18.7 62.5 65.7 66.1 79.1 51.4\nBiSigLIP (+fine-tuning) 58.5 \u219115.332.9 \u21912.670.5 \u21916.462.7 \u21914.630.5 \u21914.326.5 \u21917.874.3 \u219111.873.7 \u21918.074.2 \u21918.182.3 \u21913.2 58.6 \u21917.2\nBiPali (+LLM) 56.5 \u2193-2.030.0 \u2193-2.967.4 \u2193-3.176.9 \u219114.233.4 \u21912.943.7 \u219117.271.2 \u2193-3.161.9 \u2193-11.773.8 \u2193-0.473.6 \u2193-8.8 58.8 \u21910.2\nColPali (+Late Inter.) 79.1 \u219122.654.4 \u219124.581.8 \u219114.483.9 \u21917.065.8 \u219132.473.2 \u219129.596.2 \u219125.091.0 \u219129.192.7 \u219118.994.4 \u219120.8 81.3 \u219122.5\nTable 2: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using NDCG@5 metrics, and illustrate the impact of different components. Text-only metrics are not\ncomputed for benchmarks with only visual elements.\nis even faster. Optimized late interaction engines\n(Santhanam et al., 2022; Lee et al., 2023) enable to\neasily scale corpus sizes to millions of documents\nwith reduced latency degradations.\nOffline Indexing. (R3) Standard retrieval methods\nusing bi-encoders represent each chunk as a single\nvector embedding, which is easy to store and fast\nto compute.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c916a4d5-a0e2-48fb-8518-ce1362d0f304": {"__data__": {"id_": "c916a4d5-a0e2-48fb-8518-ce1362d0f304", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}}, "text": "Text-only metrics are not\ncomputed for benchmarks with only visual elements.\nis even faster. Optimized late interaction engines\n(Santhanam et al., 2022; Lee et al., 2023) enable to\neasily scale corpus sizes to millions of documents\nwith reduced latency degradations.\nOffline Indexing. (R3) Standard retrieval methods\nusing bi-encoders represent each chunk as a single\nvector embedding, which is easy to store and fast\nto compute. However, processing a PDF to get\nthe different chunks is the most time-consuming\npart (layout detection, OCR, chunking), and us-\ning captioning to handle multimodal data will only\nexacerbate this already lengthy process. On the\nother hand, ColPali directly encodes pages from\ntheir image representation. Although the encoder\nmodel is larger than standard retrieval encoders,\nskipping the preprocessing allows large speedups\nat indexing10(Figure 3).\nMemory Footprint. Our method requires stor-\ning a vector per image patch.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6c73cd9a-0437-468f-b198-d7ec0465a627": {"__data__": {"id_": "6c73cd9a-0437-468f-b198-d7ec0465a627", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c916a4d5-a0e2-48fb-8518-ce1362d0f304", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "cc0bf3bd54d0008d6e5b314e82b5d9e9c661a7f68a25efe44b7fa074c3d31dff", "class_name": "RelatedNodeInfo"}}, "text": "Memory Footprint. Our method requires stor-\ning a vector per image patch. We project each\nPaliGemma vector to a lower dimensional space\n(D=128) to maximize efficiency, leading to a mem-\nory footprint of 256KB per page (subsection B.4).\nImportantly, the memory footprint of the naive\nColBERT indexing strategy can be drastically im-\nproved through compression and clustering mecha-\n10Measures a NVIDIA L4 GPU, averaged on 100 pages,\nwith a batch size of 4 pages for ColPali and 8 text chunks for\nBi-Encoders. On average, a page is divided into 2.1 chunks.\nSee subsection B.5.nisms as proposed in the Performance-optimized\nLate Interaction Driver (Santhanam et al., 2022).\n5.3 Interpretability\nBy superimposing the late interaction heatmap on\ntop of the original image, we can visualize the most\nsalient image patches with respect to each term\nof the query, yielding interpretable insights into\nmodel focus zones.", "mimetype": "text/plain", "start_char_idx": 879, "end_char_idx": 1790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b43998cd-43e5-4912-ad92-947875b41cc1": {"__data__": {"id_": "b43998cd-43e5-4912-ad92-947875b41cc1", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c73cd9a-0437-468f-b198-d7ec0465a627", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7f4595bfc9f9748db854c05b73abd1b66fcf6a80dc1a05778925a4879a4d7969", "class_name": "RelatedNodeInfo"}}, "text": "As epitomized in Figure 1, we\nobserve ColPali exhibits strong OCR capabilities as\nboth the words \"hourly\" and\"hours\" present a high\nsimilarity score with the query token <_hour> . We\nalso note particular focus on other non-trivial image\nfeatures such as the x-axis representing hours being\nsalient. Other visualization examples with similar\ntrends of the model transcending pure OCR are\nshown in Appendix C.\n6 Ablation study\nShould we scale models or patch numbers ?\nWe train a variant of PaliGemma with half the num-\nber of image patches (512). While there is a clear\nperformance degradation w.r.t. to the 1024-patch\nColPali model (Figure 4), memory usage is much\nlower.11As an alternative to PaliGemma, we train\n11While another PaliGemma variant exists with 2048\npatches, the different training datamix and the large memory\nrequirements make this model impractical for both training\n7", "mimetype": "text/plain", "start_char_idx": 1791, "end_char_idx": 2677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ca0449d5-8085-47f2-881b-cf4766fef965": {"__data__": {"id_": "ca0449d5-8085-47f2-881b-cf4766fef965", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}}, "text": "Text-only metrics are not\ncomputed for benchmarks with only visual elements.\nis even faster. Optimized late interaction engines\n(Santhanam et al., 2022; Lee et al., 2023) enable to\neasily scale corpus sizes to millions of documents\nwith reduced latency degradations.\nOffline Indexing. (R3) Standard retrieval methods\nusing bi-encoders represent each chunk as a single\nvector embedding, which is easy to store and fast\nto compute. However, processing a PDF to get\nthe different chunks is the most time-consuming\npart (layout detection, OCR, chunking), and us-\ning captioning to handle multimodal data will only\nexacerbate this already lengthy process. On the\nother hand, ColPali directly encodes pages from\ntheir image representation. Although the encoder\nmodel is larger than standard retrieval encoders,\nskipping the preprocessing allows large speedups\nat indexing10(Figure 3).\nMemory Footprint. Our method requires stor-\ning a vector per image patch. We project each\nPaliGemma vector to a lower dimensional space\n(D=128) to maximize efficiency, leading to a mem-\nory footprint of 256KB per page (subsection B.4).\nImportantly, the memory footprint of the naive\nColBERT indexing strategy can be drastically im-\nproved through compression and clustering mecha-\n10Measures a NVIDIA L4 GPU, averaged on 100 pages,\nwith a batch size of 4 pages for ColPali and 8 text chunks for\nBi-Encoders. On average, a page is divided into 2.1 chunks.\nSee subsection B.5.nisms as proposed in the Performance-optimized\nLate Interaction Driver (Santhanam et al., 2022).\n5.3 Interpretability\nBy superimposing the late interaction heatmap on\ntop of the original image, we can visualize the most\nsalient image patches with respect to each term\nof the query, yielding interpretable insights into\nmodel focus zones. As epitomized in Figure 1, we\nobserve ColPali exhibits strong OCR capabilities as\nboth the words \"hourly\" and\"hours\" present a high\nsimilarity score with the query token <_hour> .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0fe00784-2507-44e6-87c4-a0898b696002": {"__data__": {"id_": "0fe00784-2507-44e6-87c4-a0898b696002", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca0449d5-8085-47f2-881b-cf4766fef965", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9314b3b0ff560e26136d0126ec168f026a1aa36fc2be27d8243dbefdd5d47cba", "class_name": "RelatedNodeInfo"}}, "text": "We\nalso note particular focus on other non-trivial image\nfeatures such as the x-axis representing hours being\nsalient. Other visualization examples with similar\ntrends of the model transcending pure OCR are\nshown in Appendix C.\n6 Ablation study\nShould we scale models or patch numbers ?\nWe train a variant of PaliGemma with half the num-\nber of image patches (512). While there is a clear\nperformance degradation w.r.t. to the 1024-patch\nColPali model (Figure 4), memory usage is much\nlower.11As an alternative to PaliGemma, we train\n11While another PaliGemma variant exists with 2048\npatches, the different training datamix and the large memory\nrequirements make this model impractical for both training\n7", "mimetype": "text/plain", "start_char_idx": 1971, "end_char_idx": 2677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc": {"__data__": {"id_": "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc", "embedding": null, "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de", "node_type": "4", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2b0e9076d241202a85b61478f8fa6b62c855de2d31ea1992dabff87bfd935a0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "node_type": "1", "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2f6f3e7c2d2d6da036ef60705d2b58bdd553eed479c49c9139059abe9b4ce3c9", "class_name": "RelatedNodeInfo"}}, "text": "Text-only metrics are not\ncomputed for benchmarks with only visual elements.\nis even faster. Optimized late interaction engines\n(Santhanam et al., 2022; Lee et al., 2023) enable to\neasily scale corpus sizes to millions of documents\nwith reduced latency degradations.\nOffline Indexing. (R3) Standard retrieval methods\nusing bi-encoders represent each chunk as a single\nvector embedding, which is easy to store and fast\nto compute. However, processing a PDF to get\nthe different chunks is the most time-consuming\npart (layout detection, OCR, chunking), and us-\ning captioning to handle multimodal data will only\nexacerbate this already lengthy process. On the\nother hand, ColPali directly encodes pages from\ntheir image representation. Although the encoder\nmodel is larger than standard retrieval encoders,\nskipping the preprocessing allows large speedups\nat indexing10(Figure 3).\nMemory Footprint. Our method requires stor-\ning a vector per image patch. We project each\nPaliGemma vector to a lower dimensional space\n(D=128) to maximize efficiency, leading to a mem-\nory footprint of 256KB per page (subsection B.4).\nImportantly, the memory footprint of the naive\nColBERT indexing strategy can be drastically im-\nproved through compression and clustering mecha-\n10Measures a NVIDIA L4 GPU, averaged on 100 pages,\nwith a batch size of 4 pages for ColPali and 8 text chunks for\nBi-Encoders. On average, a page is divided into 2.1 chunks.\nSee subsection B.5.nisms as proposed in the Performance-optimized\nLate Interaction Driver (Santhanam et al., 2022).\n5.3 Interpretability\nBy superimposing the late interaction heatmap on\ntop of the original image, we can visualize the most\nsalient image patches with respect to each term\nof the query, yielding interpretable insights into\nmodel focus zones. As epitomized in Figure 1, we\nobserve ColPali exhibits strong OCR capabilities as\nboth the words \"hourly\" and\"hours\" present a high\nsimilarity score with the query token <_hour> . We\nalso note particular focus on other non-trivial image\nfeatures such as the x-axis representing hours being\nsalient. Other visualization examples with similar\ntrends of the model transcending pure OCR are\nshown in Appendix C.\n6 Ablation study\nShould we scale models or patch numbers ?\nWe train a variant of PaliGemma with half the num-\nber of image patches (512). While there is a clear\nperformance degradation w.r.t. to the 1024-patch\nColPali model (Figure 4), memory usage is much\nlower.11As an alternative to PaliGemma, we train\n11While another PaliGemma variant exists with 2048\npatches, the different training datamix and the large memory\nrequirements make this model impractical for both training\n7", "mimetype": "text/plain", "start_char_idx": 1475, "end_char_idx": 4152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ad5c09a9-1935-4099-bd8c-3b462f24725a": {"__data__": {"id_": "ad5c09a9-1935-4099-bd8c-3b462f24725a", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}}, "text": "ColPali\n(512)Idefics2\n(64)No Mem.\nT okensFull IB\nLossTrain\nVisionT abF\nTuning40\n30\n20\n10\n010Relative NDCG@5 (%)Figure 4: Relative NDCG@5 performance gain w.r.t.\nthe default ColPali (1024 patches). TabFQuAD fine-\ntuning measures the performance difference on the\nTabFQuAD task after the introduction of targeted data\nin the training set. All other results refer to performance\ndeltas averaged on all ViDoRe tasks.\nIdefics2-8B (Lauren\u00e7on et al., 2024), a VLM with\na similar architecture and based on a Mistral-7B\n(Jiang et al., 2023) language backbone and a SigLIP\nvision encoder paired with a perceiver resampler.\nThe most notable differences with PaliGemma lie\nin the size of the language model (2B and 7B resp.)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3b1bc7e9-78e2-47e9-9f66-27a2d67dc01c": {"__data__": {"id_": "3b1bc7e9-78e2-47e9-9f66-27a2d67dc01c", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad5c09a9-1935-4099-bd8c-3b462f24725a", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "fc271c5fdeeda2da35dc1df2cf940ac4aec008cf1fafcd3521dab30fec5a4c82", "class_name": "RelatedNodeInfo"}}, "text": "and the number of image patches (between 512 and\n2048 for PaliGemma, and 64 post-resampling for\nIdefics212). Our results (Figure 4) suggest language\nmodel size has a strong impact on performance, and\nalong with the trained resampler enables more effi-\ncient representations for smaller numbers of image\nembeddings - ColIdefics2 with 64 patches edges\noutColPali with 512 patches. Scaling the number\nof patches of the smaller ColPali model from 512\nto 1024, enables largely surpassing the 60-patch\nColIdefics2 while being about twice as fast in terms\nof training and inference latency. These results sug-\ngest there are tradeoffs between performance (R1),\nlatencies during online querying (R2) and offline\nindexation phases (R3), and index memory size.\nShould we fine-tune the vision component?\nWe run our contrastive finetuning on a ColPali\nmodel in which we also train the vision encoder\nand the projection layer.", "mimetype": "text/plain", "start_char_idx": 713, "end_char_idx": 1626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ccad4441-f467-4b83-9023-75896ff3e1e4": {"__data__": {"id_": "ccad4441-f467-4b83-9023-75896ff3e1e4", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b1bc7e9-78e2-47e9-9f66-27a2d67dc01c", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "e3c36e999130ae615166dc2e2f275d6114ddb00284040e4ba8f7f1a1e12c4dc7", "class_name": "RelatedNodeInfo"}}, "text": "Results in Figure 4 show\nthis leads to no significant improvements.\nDo \"query augmentation\" tokens help?\nIn ColBERT, special tokens are concatenated to the\ninput query to serve as soft query augmentation\nbuffers. Training without these tokens, we observe\nno significant performance difference (Figure 4) in\nthe English benchmarks. However, performance\non the French tasks seems to improve (Table 5).\nand inference time.\n12With the option of adding 4 sub-image crops of 64 tokens\neach to the sequence, for a total of 320 tokensIs the Pairwise CE loss best?\nTraining with an in-batch negative contrastive loss,\ninstead of the pairwise CE loss that only considers\nthe hardest negative sample, leads to a slight per-\nformance degradation ( \u22122.4%) on the aggregated\nbenchmark.\nCan the model adapt to new tasks?", "mimetype": "text/plain", "start_char_idx": 1627, "end_char_idx": 2432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6c3d7274-f03e-406a-b9cf-4378af7bb951": {"__data__": {"id_": "6c3d7274-f03e-406a-b9cf-4378af7bb951", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccad4441-f467-4b83-9023-75896ff3e1e4", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7e1e79cb4eb5fa29caada6f172a9cb32b38cde2078239da06ebfd3b215b7134a", "class_name": "RelatedNodeInfo"}}, "text": "Can the model adapt to new tasks?\nContrary to more complex multi-step retrieval\npipelines, ColPali can be trained end-to-end, di-\nrectly optimizing the downstream retrieval task\nwhich greatly facilitates fine-tuning to boost per-\nformance on specialized domains, multilingual re-\ntrieval, or specific visual elements the model strug-\ngles with. To demonstrate, we add 1552 samples\nrepresenting French tables and associated queries\nto the training set. This represents the only French\ndata in the training set, with all other examples be-\ning kept unchanged. We see significant NDCG@5\nimprovements (Figure 4) and even starker Re-\ncall@1 gains ( +6.63%) on the TabFQuAD bench-\nmark, with no performance degradation on the rest\nof the benchmark tasks ( +0.34%).\n7 Conclusions\nThrough the conception of a new benchmark Vi-\nDoRe , we established the limits of both modern\nindustrial document retrieval pipelines and off-the-\nshelf image-text contrastive models for visually\nrich document retrieval.", "mimetype": "text/plain", "start_char_idx": 2399, "end_char_idx": 3392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "411a254f-99a9-4a62-a8f2-f712331cd2fc": {"__data__": {"id_": "411a254f-99a9-4a62-a8f2-f712331cd2fc", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c3d7274-f03e-406a-b9cf-4378af7bb951", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "66e24776c75a38587832ef405c929d6ba6517c3026ecc5f81cfd42182b0c20ff", "class_name": "RelatedNodeInfo"}}, "text": "We introduced ColPali , a\nnovel retrieval model that leverages the latest gen-\nerative Vision Language models to create highly\nperforming multi-vector embeddings purely from\nvisual document features. ColPali largely outper-\nforms the best existing document retrieval meth-\nods while enabling faster corpus indexing time and\nmaintaining low querying latencies, suggesting a\nvery high potential for industrial document retrieval\napplications. We hope to encourage future work by\npublicly releasing the ViDoRe benchmark and all\nmodels and baselines from our study.\nFuture Work.", "mimetype": "text/plain", "start_char_idx": 3393, "end_char_idx": 3967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "36818b3b-16c2-4667-864d-1f784cee487b": {"__data__": {"id_": "36818b3b-16c2-4667-864d-1f784cee487b", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}}, "text": "ColPali\n(512)Idefics2\n(64)No Mem.\nT okensFull IB\nLossTrain\nVisionT abF\nTuning40\n30\n20\n10\n010Relative NDCG@5 (%)Figure 4: Relative NDCG@5 performance gain w.r.t.\nthe default ColPali (1024 patches). TabFQuAD fine-\ntuning measures the performance difference on the\nTabFQuAD task after the introduction of targeted data\nin the training set. All other results refer to performance\ndeltas averaged on all ViDoRe tasks.\nIdefics2-8B (Lauren\u00e7on et al., 2024), a VLM with\na similar architecture and based on a Mistral-7B\n(Jiang et al., 2023) language backbone and a SigLIP\nvision encoder paired with a perceiver resampler.\nThe most notable differences with PaliGemma lie\nin the size of the language model (2B and 7B resp.)\nand the number of image patches (between 512 and\n2048 for PaliGemma, and 64 post-resampling for\nIdefics212). Our results (Figure 4) suggest language\nmodel size has a strong impact on performance, and\nalong with the trained resampler enables more effi-\ncient representations for smaller numbers of image\nembeddings - ColIdefics2 with 64 patches edges\noutColPali with 512 patches. Scaling the number\nof patches of the smaller ColPali model from 512\nto 1024, enables largely surpassing the 60-patch\nColIdefics2 while being about twice as fast in terms\nof training and inference latency. These results sug-\ngest there are tradeoffs between performance (R1),\nlatencies during online querying (R2) and offline\nindexation phases (R3), and index memory size.\nShould we fine-tune the vision component?\nWe run our contrastive finetuning on a ColPali\nmodel in which we also train the vision encoder\nand the projection layer. Results in Figure 4 show\nthis leads to no significant improvements.\nDo \"query augmentation\" tokens help?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "850fda2d-5eca-4223-9d74-8ee3af816c76": {"__data__": {"id_": "850fda2d-5eca-4223-9d74-8ee3af816c76", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36818b3b-16c2-4667-864d-1f784cee487b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8234e71935298efe59eb653f56683998bb7e34b425cc7389e59259ac2f714bf4", "class_name": "RelatedNodeInfo"}}, "text": "Do \"query augmentation\" tokens help?\nIn ColBERT, special tokens are concatenated to the\ninput query to serve as soft query augmentation\nbuffers. Training without these tokens, we observe\nno significant performance difference (Figure 4) in\nthe English benchmarks. However, performance\non the French tasks seems to improve (Table 5).\nand inference time.\n12With the option of adding 4 sub-image crops of 64 tokens\neach to the sequence, for a total of 320 tokensIs the Pairwise CE loss best?\nTraining with an in-batch negative contrastive loss,\ninstead of the pairwise CE loss that only considers\nthe hardest negative sample, leads to a slight per-\nformance degradation ( \u22122.4%) on the aggregated\nbenchmark.\nCan the model adapt to new tasks?\nContrary to more complex multi-step retrieval\npipelines, ColPali can be trained end-to-end, di-\nrectly optimizing the downstream retrieval task\nwhich greatly facilitates fine-tuning to boost per-\nformance on specialized domains, multilingual re-\ntrieval, or specific visual elements the model strug-\ngles with. To demonstrate, we add 1552 samples\nrepresenting French tables and associated queries\nto the training set. This represents the only French\ndata in the training set, with all other examples be-\ning kept unchanged. We see significant NDCG@5\nimprovements (Figure 4) and even starker Re-\ncall@1 gains ( +6.63%) on the TabFQuAD bench-\nmark, with no performance degradation on the rest\nof the benchmark tasks ( +0.34%).\n7 Conclusions\nThrough the conception of a new benchmark Vi-\nDoRe , we established the limits of both modern\nindustrial document retrieval pipelines and off-the-\nshelf image-text contrastive models for visually\nrich document retrieval. We introduced ColPali , a\nnovel retrieval model that leverages the latest gen-\nerative Vision Language models to create highly\nperforming multi-vector embeddings purely from\nvisual document features. ColPali largely outper-\nforms the best existing document retrieval meth-\nods while enabling faster corpus indexing time and\nmaintaining low querying latencies, suggesting a\nvery high potential for industrial document retrieval\napplications.", "mimetype": "text/plain", "start_char_idx": 1695, "end_char_idx": 3833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f1d07fbf-39c5-4a94-9ddf-af13035a45fe": {"__data__": {"id_": "f1d07fbf-39c5-4a94-9ddf-af13035a45fe", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "850fda2d-5eca-4223-9d74-8ee3af816c76", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9a1855d369a8dcfb401d76071e53994d206c3b5be667b4734128ac8619e4d721", "class_name": "RelatedNodeInfo"}}, "text": "We hope to encourage future work by\npublicly releasing the ViDoRe benchmark and all\nmodels and baselines from our study.\nFuture Work.", "mimetype": "text/plain", "start_char_idx": 3834, "end_char_idx": 3967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f5084595-6d0c-43a9-81e7-b82144e7cf7b": {"__data__": {"id_": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4de359e4-aaeb-463b-a39f-11cd4cf4d946", "node_type": "1", "metadata": {}, "hash": "8e677208633f79e6cf01c6e3ac18ec464c156318f726297a316381cc5904b6aa", "class_name": "RelatedNodeInfo"}}, "text": "ColPali\n(512)Idefics2\n(64)No Mem.\nT okensFull IB\nLossTrain\nVisionT abF\nTuning40\n30\n20\n10\n010Relative NDCG@5 (%)Figure 4: Relative NDCG@5 performance gain w.r.t.\nthe default ColPali (1024 patches). TabFQuAD fine-\ntuning measures the performance difference on the\nTabFQuAD task after the introduction of targeted data\nin the training set. All other results refer to performance\ndeltas averaged on all ViDoRe tasks.\nIdefics2-8B (Lauren\u00e7on et al., 2024), a VLM with\na similar architecture and based on a Mistral-7B\n(Jiang et al., 2023) language backbone and a SigLIP\nvision encoder paired with a perceiver resampler.\nThe most notable differences with PaliGemma lie\nin the size of the language model (2B and 7B resp.)\nand the number of image patches (between 512 and\n2048 for PaliGemma, and 64 post-resampling for\nIdefics212). Our results (Figure 4) suggest language\nmodel size has a strong impact on performance, and\nalong with the trained resampler enables more effi-\ncient representations for smaller numbers of image\nembeddings - ColIdefics2 with 64 patches edges\noutColPali with 512 patches. Scaling the number\nof patches of the smaller ColPali model from 512\nto 1024, enables largely surpassing the 60-patch\nColIdefics2 while being about twice as fast in terms\nof training and inference latency. These results sug-\ngest there are tradeoffs between performance (R1),\nlatencies during online querying (R2) and offline\nindexation phases (R3), and index memory size.\nShould we fine-tune the vision component?\nWe run our contrastive finetuning on a ColPali\nmodel in which we also train the vision encoder\nand the projection layer. Results in Figure 4 show\nthis leads to no significant improvements.\nDo \"query augmentation\" tokens help?\nIn ColBERT, special tokens are concatenated to the\ninput query to serve as soft query augmentation\nbuffers. Training without these tokens, we observe\nno significant performance difference (Figure 4) in\nthe English benchmarks. However, performance\non the French tasks seems to improve (Table 5).\nand inference time.\n12With the option of adding 4 sub-image crops of 64 tokens\neach to the sequence, for a total of 320 tokensIs the Pairwise CE loss best?\nTraining with an in-batch negative contrastive loss,\ninstead of the pairwise CE loss that only considers\nthe hardest negative sample, leads to a slight per-\nformance degradation ( \u22122.4%) on the aggregated\nbenchmark.\nCan the model adapt to new tasks?\nContrary to more complex multi-step retrieval\npipelines, ColPali can be trained end-to-end, di-\nrectly optimizing the downstream retrieval task\nwhich greatly facilitates fine-tuning to boost per-\nformance on specialized domains, multilingual re-\ntrieval, or specific visual elements the model strug-\ngles with. To demonstrate, we add 1552 samples\nrepresenting French tables and associated queries\nto the training set. This represents the only French\ndata in the training set, with all other examples be-\ning kept unchanged. We see significant NDCG@5\nimprovements (Figure 4) and even starker Re-\ncall@1 gains ( +6.63%) on the TabFQuAD bench-\nmark, with no performance degradation on the rest\nof the benchmark tasks ( +0.34%).\n7 Conclusions\nThrough the conception of a new benchmark Vi-\nDoRe , we established the limits of both modern\nindustrial document retrieval pipelines and off-the-\nshelf image-text contrastive models for visually\nrich document retrieval. We introduced ColPali , a\nnovel retrieval model that leverages the latest gen-\nerative Vision Language models to create highly\nperforming multi-vector embeddings purely from\nvisual document features. ColPali largely outper-\nforms the best existing document retrieval meth-\nods while enabling faster corpus indexing time and\nmaintaining low querying latencies, suggesting a\nvery high potential for industrial document retrieval\napplications. We hope to encourage future work by\npublicly releasing the ViDoRe benchmark and all\nmodels and baselines from our study.\nFuture Work.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "56903c5b-d594-4b5c-8dbc-74d0f7d4da96": {"__data__": {"id_": "56903c5b-d594-4b5c-8dbc-74d0f7d4da96", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}}, "text": "7 Conclusions\nThrough the conception of a new benchmark Vi-\nDoRe , we established the limits of both modern\nindustrial document retrieval pipelines and off-the-\nshelf image-text contrastive models for visually\nrich document retrieval. We introduced ColPali , a\nnovel retrieval model that leverages the latest gen-\nerative Vision Language models to create highly\nperforming multi-vector embeddings purely from\nvisual document features. ColPali largely outper-\nforms the best existing document retrieval meth-\nods while enabling faster corpus indexing time and\nmaintaining low querying latencies, suggesting a\nvery high potential for industrial document retrieval\napplications. We hope to encourage future work by\npublicly releasing the ViDoRe benchmark and all\nmodels and baselines from our study.\nFuture Work. Further performance gains could\nbe obtained by exploring sub-image decomposi-\ntion (Liu et al., 2023a), optimal image patch re-\nsampling strategies (Lauren\u00e7on et al., 2024), or\nhard-negative mining.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4de359e4-aaeb-463b-a39f-11cd4cf4d946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a4a94e42-ed52-4bbc-8f36-78f41cb5b77a": {"__data__": {"id_": "a4a94e42-ed52-4bbc-8f36-78f41cb5b77a", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56903c5b-d594-4b5c-8dbc-74d0f7d4da96", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "c203e8aceaa1eaa5669ae4175c21123bd042ef3f291e502ae7e6fb4fa1001035", "class_name": "RelatedNodeInfo"}}, "text": "Subsequently, our vision is\nto combine visual retrieval and visually grounded\nquery answering to create RAG systems that purely\nfunction from visual features. An interesting line of\nresearch could be attempting to generate answers\nleveraging information stored in the indexed multi-\nvector patch embeddings.\n8", "mimetype": "text/plain", "start_char_idx": 1009, "end_char_idx": 1318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4de359e4-aaeb-463b-a39f-11cd4cf4d946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ea8275fd-a8ec-4ac7-89cb-4bdb6a400ba3": {"__data__": {"id_": "ea8275fd-a8ec-4ac7-89cb-4bdb6a400ba3", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}}, "text": "7 Conclusions\nThrough the conception of a new benchmark Vi-\nDoRe , we established the limits of both modern\nindustrial document retrieval pipelines and off-the-\nshelf image-text contrastive models for visually\nrich document retrieval. We introduced ColPali , a\nnovel retrieval model that leverages the latest gen-\nerative Vision Language models to create highly\nperforming multi-vector embeddings purely from\nvisual document features. ColPali largely outper-\nforms the best existing document retrieval meth-\nods while enabling faster corpus indexing time and\nmaintaining low querying latencies, suggesting a\nvery high potential for industrial document retrieval\napplications. We hope to encourage future work by\npublicly releasing the ViDoRe benchmark and all\nmodels and baselines from our study.\nFuture Work. Further performance gains could\nbe obtained by exploring sub-image decomposi-\ntion (Liu et al., 2023a), optimal image patch re-\nsampling strategies (Lauren\u00e7on et al., 2024), or\nhard-negative mining. Subsequently, our vision is\nto combine visual retrieval and visually grounded\nquery answering to create RAG systems that purely\nfunction from visual features. An interesting line of\nresearch could be attempting to generate answers\nleveraging information stored in the indexed multi-\nvector patch embeddings.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4de359e4-aaeb-463b-a39f-11cd4cf4d946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4de359e4-aaeb-463b-a39f-11cd4cf4d946": {"__data__": {"id_": "4de359e4-aaeb-463b-a39f-11cd4cf4d946", "embedding": null, "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "913208b8-c3af-41c1-b72a-d253855c7f49", "node_type": "4", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dfda9a4e5771f29e3250ab8aa769cd78057c7a603cfd6d6f540a4b1e207b2b28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "fe6a0a17f2506f7863ec152d48e09ea662126dfbafd48de20dec7c40cad47bc8", "class_name": "RelatedNodeInfo"}}, "text": "7 Conclusions\nThrough the conception of a new benchmark Vi-\nDoRe , we established the limits of both modern\nindustrial document retrieval pipelines and off-the-\nshelf image-text contrastive models for visually\nrich document retrieval. We introduced ColPali , a\nnovel retrieval model that leverages the latest gen-\nerative Vision Language models to create highly\nperforming multi-vector embeddings purely from\nvisual document features. ColPali largely outper-\nforms the best existing document retrieval meth-\nods while enabling faster corpus indexing time and\nmaintaining low querying latencies, suggesting a\nvery high potential for industrial document retrieval\napplications. We hope to encourage future work by\npublicly releasing the ViDoRe benchmark and all\nmodels and baselines from our study.\nFuture Work. Further performance gains could\nbe obtained by exploring sub-image decomposi-\ntion (Liu et al., 2023a), optimal image patch re-\nsampling strategies (Lauren\u00e7on et al., 2024), or\nhard-negative mining. Subsequently, our vision is\nto combine visual retrieval and visually grounded\nquery answering to create RAG systems that purely\nfunction from visual features. An interesting line of\nresearch could be attempting to generate answers\nleveraging information stored in the indexed multi-\nvector patch embeddings.\n8", "mimetype": "text/plain", "start_char_idx": 3158, "end_char_idx": 4476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4de359e4-aaeb-463b-a39f-11cd4cf4d946", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "453e08b8-aa17-4d16-ab58-d4ca14d5fbf4": {"__data__": {"id_": "453e08b8-aa17-4d16-ab58-d4ca14d5fbf4", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}}, "text": "Limitations\nFocus. In this work, we evaluate models on doc-\nument retrieval tasks, covering several modalities\n(figures, text, tables, infographics). We however\nprimarily focus on PDF-type documents, and eval-\nuating systems on image retrieval with documents\nstemming from web page screenshots or hand-\nwritten documents might be an interesting general-\nization. We also focus on high-resource languages\n(English and French) and although we have shown\nthe capacity of the ColPali model to generalize to\nlanguages outside of its fine-tuning set, it is un-\nclear how the model would perform on languages\nthat are not as represented in the model\u2019s language\nbackbone. Finally, our setup assumes relevant doc-\numents exist, but abstention methods for Informa-\ntion Retrieval systems might be interesting to ex-\nplore in more practical settings in which confidence\nestimation might be important (Gisserot-Boukhlef\net al., 2024).\nSupport. This work relies on multi-vector retriev-\ning derived from the ColBERT late interaction\nmechanism.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2ae9d5fa-f827-4f7f-890c-9a08677dbedf": {"__data__": {"id_": "2ae9d5fa-f827-4f7f-890c-9a08677dbedf", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "453e08b8-aa17-4d16-ab58-d4ca14d5fbf4", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9975600fab920eebe046e70c6d7f646cd9044369bc21d176c6841bece6ef1e57", "class_name": "RelatedNodeInfo"}}, "text": "Although some vector databases sup-\nport late interaction engines13, many widely used\nvector retrieval frameworks do not propose native\nmulti-vector support, and some engineering infras-\ntructure efforts may be required to adapt them to\nwork with ColPali (or ColBERT) models.\nData. In the creation of ViDoRe , we partially rely\non synthetic query generation based on a commer-\ncial large language model, which may induce some\namount of bias in the generated queries. To com-\npensate for this, we have iterated on the prompt-\ning strategy and given real query examples to the\nmodels to help ground generation in realistic set-\ntings. We have further manually verified all syn-\nthetic queries through a lengthy process to validate\ntheir relevance and their quality. Our benchmark\nalso includes many benchmark tasks with no syn-\nthetic data, and result trends observed between all\ntasks are correlated, further confirming the coher-\nence of our benchmark design.\nEthical Considerations\nCarbon Footprint.", "mimetype": "text/plain", "start_char_idx": 1031, "end_char_idx": 2031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3642a03f-5ddc-4987-b350-382775aabfae": {"__data__": {"id_": "3642a03f-5ddc-4987-b350-382775aabfae", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ae9d5fa-f827-4f7f-890c-9a08677dbedf", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ce87014f1b40ff8959e30f61706a21d17d7d16fa337c0bf6066b461033daa13a", "class_name": "RelatedNodeInfo"}}, "text": "Ethical Considerations\nCarbon Footprint. Our work fully leverages prior\npretrained models and training is not particularly\ncompute-intensive. Furthermore, we rely on low-\nrank adapters to further reduce the computational\nresources needed, both during training and for\n13Vespa Engine, RAGatouille, QDrant, colbert.aistorage. Overall, a training run represents about\n40 hours of Mi250x AMD GPUs. Our experi-\nments, in total, represent 1405 Mi250x GPU hours\nfrom highly efficient compute clusters running on\nlow-carbon nuclear energy, representing a total of\naround 15kg CO2 eq.\nImpact. We believe our work could have a strong\nimpact on improving industrial document retrieval\nsystems. Our method is efficient, performs well,\nand the additional support towards visually rich in-\nformation from documents could go a long way in\nunlocking knowledge sources previously difficult\nto index or query.\nResource Release.", "mimetype": "text/plain", "start_char_idx": 1991, "end_char_idx": 2900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "63588345-229c-4833-b023-dcac6247f513": {"__data__": {"id_": "63588345-229c-4833-b023-dcac6247f513", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3642a03f-5ddc-4987-b350-382775aabfae", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "38aee263c1da034ac702f3be1d026fd0215b0e0c10a729904dc39c852098da00", "class_name": "RelatedNodeInfo"}}, "text": "Resource Release. For transparency, and to foster\nfuture work, we release our comprehensive bench-\nmark under open license and host a public leader-\nboard14. Our models are released under the same\nusage license as the base model (Gemma Research\nlicense for ColPali, Apache2.0 for ColIdefics2) and\nshould be used as intended by the VLM license.\nAcknowledgements\nThis work is partially supported by Illuin Tech-\nnology, and by a grant from ANRT France.\nThis work was performed using HPC resources\nfrom the CINES ADASTRA through Grant 2024-\nAD011015443. We extend our warm thanks to\nJonathan Dong, Caio Corro, Victor Pellegrain and\nEnder Konukoglu for their valuable feedback on\nthe paper.\nReferences\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander\nKolesnikov, and Lucas Beyer. 2023. Getting ViT\nin Shape: Scaling Laws for Compute-Optimal Model\nDesign.", "mimetype": "text/plain", "start_char_idx": 2883, "end_char_idx": 3731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "28dfb054-3092-4867-9fa2-4e944bf59627": {"__data__": {"id_": "28dfb054-3092-4867-9fa2-4e944bf59627", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63588345-229c-4833-b023-dcac6247f513", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8e63ba76b1607040f42f68e07e537145761f9ac2d356f0d1f0c7a32c32f7583e", "class_name": "RelatedNodeInfo"}}, "text": "Getting ViT\nin Shape: Scaling Laws for Compute-Optimal Model\nDesign. Publisher: arXiv Version Number: 5.", "mimetype": "text/plain", "start_char_idx": 3663, "end_char_idx": 3767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "64146070-c41e-40ea-a87a-29f7c1bdd873": {"__data__": {"id_": "64146070-c41e-40ea-a87a-29f7c1bdd873", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}}, "text": "Limitations\nFocus. In this work, we evaluate models on doc-\nument retrieval tasks, covering several modalities\n(figures, text, tables, infographics). We however\nprimarily focus on PDF-type documents, and eval-\nuating systems on image retrieval with documents\nstemming from web page screenshots or hand-\nwritten documents might be an interesting general-\nization. We also focus on high-resource languages\n(English and French) and although we have shown\nthe capacity of the ColPali model to generalize to\nlanguages outside of its fine-tuning set, it is un-\nclear how the model would perform on languages\nthat are not as represented in the model\u2019s language\nbackbone. Finally, our setup assumes relevant doc-\numents exist, but abstention methods for Informa-\ntion Retrieval systems might be interesting to ex-\nplore in more practical settings in which confidence\nestimation might be important (Gisserot-Boukhlef\net al., 2024).\nSupport. This work relies on multi-vector retriev-\ning derived from the ColBERT late interaction\nmechanism. Although some vector databases sup-\nport late interaction engines13, many widely used\nvector retrieval frameworks do not propose native\nmulti-vector support, and some engineering infras-\ntructure efforts may be required to adapt them to\nwork with ColPali (or ColBERT) models.\nData. In the creation of ViDoRe , we partially rely\non synthetic query generation based on a commer-\ncial large language model, which may induce some\namount of bias in the generated queries. To com-\npensate for this, we have iterated on the prompt-\ning strategy and given real query examples to the\nmodels to help ground generation in realistic set-\ntings. We have further manually verified all syn-\nthetic queries through a lengthy process to validate\ntheir relevance and their quality. Our benchmark\nalso includes many benchmark tasks with no syn-\nthetic data, and result trends observed between all\ntasks are correlated, further confirming the coher-\nence of our benchmark design.\nEthical Considerations\nCarbon Footprint. Our work fully leverages prior\npretrained models and training is not particularly\ncompute-intensive.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4acee7f5-5d6b-40a3-991a-9605b5d34a7f": {"__data__": {"id_": "4acee7f5-5d6b-40a3-991a-9605b5d34a7f", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64146070-c41e-40ea-a87a-29f7c1bdd873", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "d2576ba2c15ecfb4ddd39fc48ae8583a7645978241a5d59a67ae8461043feff0", "class_name": "RelatedNodeInfo"}}, "text": "Our work fully leverages prior\npretrained models and training is not particularly\ncompute-intensive. Furthermore, we rely on low-\nrank adapters to further reduce the computational\nresources needed, both during training and for\n13Vespa Engine, RAGatouille, QDrant, colbert.aistorage. Overall, a training run represents about\n40 hours of Mi250x AMD GPUs. Our experi-\nments, in total, represent 1405 Mi250x GPU hours\nfrom highly efficient compute clusters running on\nlow-carbon nuclear energy, representing a total of\naround 15kg CO2 eq.\nImpact. We believe our work could have a strong\nimpact on improving industrial document retrieval\nsystems. Our method is efficient, performs well,\nand the additional support towards visually rich in-\nformation from documents could go a long way in\nunlocking knowledge sources previously difficult\nto index or query.\nResource Release. For transparency, and to foster\nfuture work, we release our comprehensive bench-\nmark under open license and host a public leader-\nboard14. Our models are released under the same\nusage license as the base model (Gemma Research\nlicense for ColPali, Apache2.0 for ColIdefics2) and\nshould be used as intended by the VLM license.\nAcknowledgements\nThis work is partially supported by Illuin Tech-\nnology, and by a grant from ANRT France.\nThis work was performed using HPC resources\nfrom the CINES ADASTRA through Grant 2024-\nAD011015443. We extend our warm thanks to\nJonathan Dong, Caio Corro, Victor Pellegrain and\nEnder Konukoglu for their valuable feedback on\nthe paper.\nReferences\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander\nKolesnikov, and Lucas Beyer. 2023. Getting ViT\nin Shape: Scaling Laws for Compute-Optimal Model\nDesign. Publisher: arXiv Version Number: 5.", "mimetype": "text/plain", "start_char_idx": 2032, "end_char_idx": 3767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "380e57ef-28bb-4e56-ada0-bdfd232bca73": {"__data__": {"id_": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecad222b-3691-41bf-a6e2-76e7945e12eb", "node_type": "1", "metadata": {}, "hash": "afc9c894683e7a532de3fb8c15acc7bf40e05bcc060d5feed82138e27247dbb1", "class_name": "RelatedNodeInfo"}}, "text": "Limitations\nFocus. In this work, we evaluate models on doc-\nument retrieval tasks, covering several modalities\n(figures, text, tables, infographics). We however\nprimarily focus on PDF-type documents, and eval-\nuating systems on image retrieval with documents\nstemming from web page screenshots or hand-\nwritten documents might be an interesting general-\nization. We also focus on high-resource languages\n(English and French) and although we have shown\nthe capacity of the ColPali model to generalize to\nlanguages outside of its fine-tuning set, it is un-\nclear how the model would perform on languages\nthat are not as represented in the model\u2019s language\nbackbone. Finally, our setup assumes relevant doc-\numents exist, but abstention methods for Informa-\ntion Retrieval systems might be interesting to ex-\nplore in more practical settings in which confidence\nestimation might be important (Gisserot-Boukhlef\net al., 2024).\nSupport. This work relies on multi-vector retriev-\ning derived from the ColBERT late interaction\nmechanism. Although some vector databases sup-\nport late interaction engines13, many widely used\nvector retrieval frameworks do not propose native\nmulti-vector support, and some engineering infras-\ntructure efforts may be required to adapt them to\nwork with ColPali (or ColBERT) models.\nData. In the creation of ViDoRe , we partially rely\non synthetic query generation based on a commer-\ncial large language model, which may induce some\namount of bias in the generated queries. To com-\npensate for this, we have iterated on the prompt-\ning strategy and given real query examples to the\nmodels to help ground generation in realistic set-\ntings. We have further manually verified all syn-\nthetic queries through a lengthy process to validate\ntheir relevance and their quality. Our benchmark\nalso includes many benchmark tasks with no syn-\nthetic data, and result trends observed between all\ntasks are correlated, further confirming the coher-\nence of our benchmark design.\nEthical Considerations\nCarbon Footprint. Our work fully leverages prior\npretrained models and training is not particularly\ncompute-intensive. Furthermore, we rely on low-\nrank adapters to further reduce the computational\nresources needed, both during training and for\n13Vespa Engine, RAGatouille, QDrant, colbert.aistorage. Overall, a training run represents about\n40 hours of Mi250x AMD GPUs. Our experi-\nments, in total, represent 1405 Mi250x GPU hours\nfrom highly efficient compute clusters running on\nlow-carbon nuclear energy, representing a total of\naround 15kg CO2 eq.\nImpact. We believe our work could have a strong\nimpact on improving industrial document retrieval\nsystems. Our method is efficient, performs well,\nand the additional support towards visually rich in-\nformation from documents could go a long way in\nunlocking knowledge sources previously difficult\nto index or query.\nResource Release. For transparency, and to foster\nfuture work, we release our comprehensive bench-\nmark under open license and host a public leader-\nboard14. Our models are released under the same\nusage license as the base model (Gemma Research\nlicense for ColPali, Apache2.0 for ColIdefics2) and\nshould be used as intended by the VLM license.\nAcknowledgements\nThis work is partially supported by Illuin Tech-\nnology, and by a grant from ANRT France.\nThis work was performed using HPC resources\nfrom the CINES ADASTRA through Grant 2024-\nAD011015443. We extend our warm thanks to\nJonathan Dong, Caio Corro, Victor Pellegrain and\nEnder Konukoglu for their valuable feedback on\nthe paper.\nReferences\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander\nKolesnikov, and Lucas Beyer. 2023. Getting ViT\nin Shape: Scaling Laws for Compute-Optimal Model\nDesign. Publisher: arXiv Version Number: 5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3d1df480-1647-49a1-9aac-924a3a2eea6c": {"__data__": {"id_": "3d1df480-1647-49a1-9aac-924a3a2eea6c", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}}, "text": "Acknowledgements\nThis work is partially supported by Illuin Tech-\nnology, and by a grant from ANRT France.\nThis work was performed using HPC resources\nfrom the CINES ADASTRA through Grant 2024-\nAD011015443. We extend our warm thanks to\nJonathan Dong, Caio Corro, Victor Pellegrain and\nEnder Konukoglu for their valuable feedback on\nthe paper.\nReferences\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander\nKolesnikov, and Lucas Beyer. 2023. Getting ViT\nin Shape: Scaling Laws for Compute-Optimal Model\nDesign. Publisher: arXiv Version Number: 5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 540, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ecad222b-3691-41bf-a6e2-76e7945e12eb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "dfbea04f-757c-4b09-bbe8-37af0467422b": {"__data__": {"id_": "dfbea04f-757c-4b09-bbe8-37af0467422b", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d1df480-1647-49a1-9aac-924a3a2eea6c", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "c7221db9be5fcdcd9431af4014a2e93c4ac66eb0c5e193a402db6a5b2774e5b9", "class_name": "RelatedNodeInfo"}}, "text": "Publisher: arXiv Version Number: 5.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a Visual Language Model for Few-Shot\nLearning. Publisher: arXiv Version Number: 2.\nAnthropic. 2024. The Claude 3 Model Family: Opus,\nSonnet, Haiku.", "mimetype": "text/plain", "start_char_idx": 505, "end_char_idx": 1137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ecad222b-3691-41bf-a6e2-76e7945e12eb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "48ffee18-dc4d-492d-8205-d51e54be752f": {"__data__": {"id_": "48ffee18-dc4d-492d-8205-d51e54be752f", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfbea04f-757c-4b09-bbe8-37af0467422b", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f538dbec85efb8760ad740f05c3faee61a9f2e09370ac866e04c9e5c5e122dc6", "class_name": "RelatedNodeInfo"}}, "text": "2024. The Claude 3 Model Family: Opus,\nSonnet, Haiku.\n14https://huggingface.co/spaces/vidore/\nvidore-leaderboard\n9", "mimetype": "text/plain", "start_char_idx": 1084, "end_char_idx": 1198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ecad222b-3691-41bf-a6e2-76e7945e12eb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "21a0bd8e-2c7e-462b-878e-509b252673c7": {"__data__": {"id_": "21a0bd8e-2c7e-462b-878e-509b252673c7", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}}, "text": "Acknowledgements\nThis work is partially supported by Illuin Tech-\nnology, and by a grant from ANRT France.\nThis work was performed using HPC resources\nfrom the CINES ADASTRA through Grant 2024-\nAD011015443. We extend our warm thanks to\nJonathan Dong, Caio Corro, Victor Pellegrain and\nEnder Konukoglu for their valuable feedback on\nthe paper.\nReferences\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander\nKolesnikov, and Lucas Beyer. 2023. Getting ViT\nin Shape: Scaling Laws for Compute-Optimal Model\nDesign. Publisher: arXiv Version Number: 5.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a Visual Language Model for Few-Shot\nLearning. Publisher: arXiv Version Number: 2.\nAnthropic. 2024. The Claude 3 Model Family: Opus,\nSonnet, Haiku.\n14https://huggingface.co/spaces/vidore/\nvidore-leaderboard\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ecad222b-3691-41bf-a6e2-76e7945e12eb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ecad222b-3691-41bf-a6e2-76e7945e12eb": {"__data__": {"id_": "ecad222b-3691-41bf-a6e2-76e7945e12eb", "embedding": null, "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3109a076439d2cc99325dd87627118e052897f3005a98e878d9d29835ae488ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "380e57ef-28bb-4e56-ada0-bdfd232bca73", "node_type": "1", "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "df1fdbbca19439299aab943e73dbbd861b106b7434ea16effc188f54f2f57be5", "class_name": "RelatedNodeInfo"}}, "text": "Acknowledgements\nThis work is partially supported by Illuin Tech-\nnology, and by a grant from ANRT France.\nThis work was performed using HPC resources\nfrom the CINES ADASTRA through Grant 2024-\nAD011015443. We extend our warm thanks to\nJonathan Dong, Caio Corro, Victor Pellegrain and\nEnder Konukoglu for their valuable feedback on\nthe paper.\nReferences\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander\nKolesnikov, and Lucas Beyer. 2023. Getting ViT\nin Shape: Scaling Laws for Compute-Optimal Model\nDesign. Publisher: arXiv Version Number: 5.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a Visual Language Model for Few-Shot\nLearning. Publisher: arXiv Version Number: 2.\nAnthropic. 2024. The Claude 3 Model Family: Opus,\nSonnet, Haiku.\n14https://huggingface.co/spaces/vidore/\nvidore-leaderboard\n9", "mimetype": "text/plain", "start_char_idx": 3227, "end_char_idx": 4425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "ecad222b-3691-41bf-a6e2-76e7945e12eb", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d0a5531a-a42d-4bdc-a89a-74fe521fdc1f": {"__data__": {"id_": "d0a5531a-a42d-4bdc-a89a-74fe521fdc1f", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}}, "text": "Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,\nYusheng Xie, and R. Manmatha. 2021. DocFormer:\nEnd-to-End Transformer for Document Understand-\ning. arXiv preprint . Version Number: 2.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-VL: A Versatile\nVision-Language Model for Understanding, Local-\nization, Text Reading, and Beyond. Publisher: arXiv\nVersion Number: 3.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir\nRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,\nand Tong Wang. 2016.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "73255c79-8fc4-4d78-ae92-d22941b7f88d": {"__data__": {"id_": "73255c79-8fc4-4d78-ae92-d22941b7f88d", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0a5531a-a42d-4bdc-a89a-74fe521fdc1f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "377ab028614fea3837d2e686f568a03d9e080b4ae2d4ebd1333a1b221c64a3e5", "class_name": "RelatedNodeInfo"}}, "text": "2016. MS MARCO: A Human Gen-\nerated MAchine Reading COmprehension Dataset.\narXiv preprint . Version Number: 3.", "mimetype": "text/plain", "start_char_idx": 665, "end_char_idx": 775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b4977b46-bfbe-40aa-be97-d38421b4fbe0": {"__data__": {"id_": "b4977b46-bfbe-40aa-be97-d38421b4fbe0", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73255c79-8fc4-4d78-ae92-d22941b7f88d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "48f83f3e3cae8ec39f2b8a9d9d8b369be13dbd3a7bd416d3309aefcd03ac1ef3", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint . Version Number: 3.\nLucas Beyer, Andreas Steiner, Andr\u00e9 Susano Pinto,\nAlexander Kolesnikov, Xiao Wang, Daniel Salz,\nMaxim Neumann, Ibrahim Alabdulmohsin, Michael\nTschannen, Emanuele Bugliarello, Thomas Un-\nterthiner, Daniel Keysers, Skanda Koppula, Fangyu\nLiu, Adam Grycner, Alexey Gritsenko, Neil Houlsby,\nManoj Kumar, Keran Rong, Julian Eisenschlos,\nRishabh Kabra, Matthias Bauer, Matko Bo\u0161n-\njak, Xi Chen, Matthias Minderer, Paul V oigtlaen-\nder, Ioana Bica, Ivana Balazevic, Joan Puigcerver,\nPinelopi Papalampidi, Olivier Henaff, Xi Xiong,\nRadu Soricut, Jeremiah Harmsen, and Xiaohua Zhai.\n2024. Paligemma: A versatile 3b vlm for transfer.", "mimetype": "text/plain", "start_char_idx": 740, "end_char_idx": 1399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ef5d432b-e547-4104-a380-961c55fc2445": {"__data__": {"id_": "ef5d432b-e547-4104-a380-961c55fc2445", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4977b46-bfbe-40aa-be97-d38421b4fbe0", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f8c7d6f9ef6ad559f23fe58576c8a7c81d1d9609b98f5a742c2369770e1cad51", "class_name": "RelatedNodeInfo"}}, "text": "2024. Paligemma: A versatile 3b vlm for transfer.\nPreprint , arXiv:2407.07726.\nBurton H. Bloom. 1970. Space/time trade-offs in\nhash coding with allowable errors. Commun. ACM ,\n13(7):422\u2013426. Place: New York, NY , USA Pub-\nlisher: Association for Computing Machinery.\n\u0141ukasz Borchmann, Micha\u0142 Pietruszka, Tomasz Stanis-\nlawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina\nSzyndler, and Filip Grali \u00b4nski. 2021. DUE: End-to-\nEnd Document Understanding Benchmark. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2) .\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo,\nDefu Lian, and Zheng Liu. 2024.", "mimetype": "text/plain", "start_char_idx": 1350, "end_char_idx": 2000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ce5b57ca-1106-4307-ae6e-75da4d7f8d7e": {"__data__": {"id_": "ce5b57ca-1106-4307-ae6e-75da4d7f8d7e", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef5d432b-e547-4104-a380-961c55fc2445", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7a8895adf4321bebb6e1f736be3e0e4dadc7fb8586bd82369e31e94efb49b26f", "class_name": "RelatedNodeInfo"}}, "text": "2024. BGE M3-\nEmbedding: Multi-Lingual, Multi-Functionality,\nMulti-Granularity Text Embeddings Through Self-\nKnowledge Distillation. arXiv preprint . Version\nNumber: 3.\nXi Chen, Xiao Wang, Lucas Beyer, Alexander\nKolesnikov, Jialin Wu, Paul V oigtlaender, Basil\nMustafa, Sebastian Goodman, Ibrahim Alabdul-\nmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong,\nDaniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu,\nDaniel Keysers, Xiaohua Zhai, and Radu Soricut.\n2023. PaLI-3 Vision Language Models: Smaller,\nFaster, Stronger. arXiv preprint . Version Number: 2.\nCohere. 2024.", "mimetype": "text/plain", "start_char_idx": 1995, "end_char_idx": 2563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1c5cc9e1-69b7-41b2-aced-b2308cab16aa": {"__data__": {"id_": "1c5cc9e1-69b7-41b2-aced-b2308cab16aa", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce5b57ca-1106-4307-ae6e-75da4d7f8d7e", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "aff418c88d73f1a6e4dfef88d101bb94fde294ad0b0fd0b579cc6e0f74be7f4c", "class_name": "RelatedNodeInfo"}}, "text": "Version Number: 2.\nCohere. 2024. Introducing Rerank 3: A New Foun-\ndation Model for Efficient Enterprise Search & Re-\ntrieval.Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and\nPiotr Bojanowski. 2023. Vision Transformers Need\nRegisters. Publisher: [object Object] Version Num-\nber: 2.", "mimetype": "text/plain", "start_char_idx": 2531, "end_char_idx": 2814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b465b4ce-4a25-4a5d-bc59-2da1ee31b6db": {"__data__": {"id_": "b465b4ce-4a25-4a5d-bc59-2da1ee31b6db", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}}, "text": "Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,\nYusheng Xie, and R. Manmatha. 2021. DocFormer:\nEnd-to-End Transformer for Document Understand-\ning. arXiv preprint . Version Number: 2.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-VL: A Versatile\nVision-Language Model for Understanding, Local-\nization, Text Reading, and Beyond. Publisher: arXiv\nVersion Number: 3.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir\nRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,\nand Tong Wang. 2016. MS MARCO: A Human Gen-\nerated MAchine Reading COmprehension Dataset.\narXiv preprint . Version Number: 3.\nLucas Beyer, Andreas Steiner, Andr\u00e9 Susano Pinto,\nAlexander Kolesnikov, Xiao Wang, Daniel Salz,\nMaxim Neumann, Ibrahim Alabdulmohsin, Michael\nTschannen, Emanuele Bugliarello, Thomas Un-\nterthiner, Daniel Keysers, Skanda Koppula, Fangyu\nLiu, Adam Grycner, Alexey Gritsenko, Neil Houlsby,\nManoj Kumar, Keran Rong, Julian Eisenschlos,\nRishabh Kabra, Matthias Bauer, Matko Bo\u0161n-\njak, Xi Chen, Matthias Minderer, Paul V oigtlaen-\nder, Ioana Bica, Ivana Balazevic, Joan Puigcerver,\nPinelopi Papalampidi, Olivier Henaff, Xi Xiong,\nRadu Soricut, Jeremiah Harmsen, and Xiaohua Zhai.\n2024. Paligemma: A versatile 3b vlm for transfer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "279b3702-571a-42a2-b3d9-413ec8ca72da": {"__data__": {"id_": "279b3702-571a-42a2-b3d9-413ec8ca72da", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b465b4ce-4a25-4a5d-bc59-2da1ee31b6db", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "c85fb5fb425fbeb8aaf70560eb72f8ace1f5018743b1047cdaabcbce9d20e416", "class_name": "RelatedNodeInfo"}}, "text": "2024. Paligemma: A versatile 3b vlm for transfer.\nPreprint , arXiv:2407.07726.\nBurton H. Bloom. 1970. Space/time trade-offs in\nhash coding with allowable errors. Commun. ACM ,\n13(7):422\u2013426. Place: New York, NY , USA Pub-\nlisher: Association for Computing Machinery.\n\u0141ukasz Borchmann, Micha\u0142 Pietruszka, Tomasz Stanis-\nlawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina\nSzyndler, and Filip Grali \u00b4nski. 2021. DUE: End-to-\nEnd Document Understanding Benchmark. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2) .\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo,\nDefu Lian, and Zheng Liu. 2024. BGE M3-\nEmbedding: Multi-Lingual, Multi-Functionality,\nMulti-Granularity Text Embeddings Through Self-\nKnowledge Distillation. arXiv preprint . Version\nNumber: 3.\nXi Chen, Xiao Wang, Lucas Beyer, Alexander\nKolesnikov, Jialin Wu, Paul V oigtlaender, Basil\nMustafa, Sebastian Goodman, Ibrahim Alabdul-\nmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong,\nDaniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu,\nDaniel Keysers, Xiaohua Zhai, and Radu Soricut.\n2023. PaLI-3 Vision Language Models: Smaller,\nFaster, Stronger. arXiv preprint . Version Number: 2.\nCohere. 2024. Introducing Rerank 3: A New Foun-\ndation Model for Efficient Enterprise Search & Re-\ntrieval.Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and\nPiotr Bojanowski. 2023. Vision Transformers Need\nRegisters. Publisher: [object Object] Version Num-\nber: 2.", "mimetype": "text/plain", "start_char_idx": 1350, "end_char_idx": 2814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9a267d95-0a4a-4968-b5d4-a8df4675fd7c": {"__data__": {"id_": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "node_type": "1", "metadata": {}, "hash": "f791e08bf0f81c7b0b2572582343ff687548f6e60705c970c359b53f0a89ef45", "class_name": "RelatedNodeInfo"}}, "text": "Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,\nYusheng Xie, and R. Manmatha. 2021. DocFormer:\nEnd-to-End Transformer for Document Understand-\ning. arXiv preprint . Version Number: 2.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-VL: A Versatile\nVision-Language Model for Understanding, Local-\nization, Text Reading, and Beyond. Publisher: arXiv\nVersion Number: 3.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir\nRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,\nand Tong Wang. 2016. MS MARCO: A Human Gen-\nerated MAchine Reading COmprehension Dataset.\narXiv preprint . Version Number: 3.\nLucas Beyer, Andreas Steiner, Andr\u00e9 Susano Pinto,\nAlexander Kolesnikov, Xiao Wang, Daniel Salz,\nMaxim Neumann, Ibrahim Alabdulmohsin, Michael\nTschannen, Emanuele Bugliarello, Thomas Un-\nterthiner, Daniel Keysers, Skanda Koppula, Fangyu\nLiu, Adam Grycner, Alexey Gritsenko, Neil Houlsby,\nManoj Kumar, Keran Rong, Julian Eisenschlos,\nRishabh Kabra, Matthias Bauer, Matko Bo\u0161n-\njak, Xi Chen, Matthias Minderer, Paul V oigtlaen-\nder, Ioana Bica, Ivana Balazevic, Joan Puigcerver,\nPinelopi Papalampidi, Olivier Henaff, Xi Xiong,\nRadu Soricut, Jeremiah Harmsen, and Xiaohua Zhai.\n2024. Paligemma: A versatile 3b vlm for transfer.\nPreprint , arXiv:2407.07726.\nBurton H. Bloom. 1970. Space/time trade-offs in\nhash coding with allowable errors. Commun. ACM ,\n13(7):422\u2013426. Place: New York, NY , USA Pub-\nlisher: Association for Computing Machinery.\n\u0141ukasz Borchmann, Micha\u0142 Pietruszka, Tomasz Stanis-\nlawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina\nSzyndler, and Filip Grali \u00b4nski. 2021. DUE: End-to-\nEnd Document Understanding Benchmark. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2) .\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo,\nDefu Lian, and Zheng Liu. 2024. BGE M3-\nEmbedding: Multi-Lingual, Multi-Functionality,\nMulti-Granularity Text Embeddings Through Self-\nKnowledge Distillation. arXiv preprint . Version\nNumber: 3.\nXi Chen, Xiao Wang, Lucas Beyer, Alexander\nKolesnikov, Jialin Wu, Paul V oigtlaender, Basil\nMustafa, Sebastian Goodman, Ibrahim Alabdul-\nmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong,\nDaniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu,\nDaniel Keysers, Xiaohua Zhai, and Radu Soricut.\n2023. PaLI-3 Vision Language Models: Smaller,\nFaster, Stronger. arXiv preprint . Version Number: 2.\nCohere. 2024. Introducing Rerank 3: A New Foun-\ndation Model for Efficient Enterprise Search & Re-\ntrieval.Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and\nPiotr Bojanowski. 2023. Vision Transformers Need\nRegisters. Publisher: [object Object] Version Num-\nber: 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "77f521b5-368a-447b-90bc-864fc4e4f31c": {"__data__": {"id_": "77f521b5-368a-447b-90bc-864fc4e4f31c", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}}, "text": "2023. PaLI-3 Vision Language Models: Smaller,\nFaster, Stronger. arXiv preprint . Version Number: 2.\nCohere. 2024. Introducing Rerank 3: A New Foun-\ndation Model for Efficient Enterprise Search & Re-\ntrieval.Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and\nPiotr Bojanowski. 2023. Vision Transformers Need\nRegisters. Publisher: [object Object] Version Num-\nber: 2.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2020. An Image\nis Worth 16x16 Words: Transformers for Image\nRecognition at Scale. Publisher: arXiv Version\nNumber: 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "522c8145-c711-4502-bd3c-f8af6be87aad": {"__data__": {"id_": "522c8145-c711-4502-bd3c-f8af6be87aad", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77f521b5-368a-447b-90bc-864fc4e4f31c", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7152f94e0f1d28ee19cff6b4fd0b4fccdab5becf5d5d4e4a054556bc84d4785e", "class_name": "RelatedNodeInfo"}}, "text": "Publisher: arXiv Version\nNumber: 2.\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and\nJian Sun. 2021. YOLOX: Exceeding YOLO Series\nin 2021. arXiv preprint . Version Number: 2.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay\nKale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot,\nPier Giuseppe Sessa, Aakanksha Chowdhery, Adam\nRoberts, Aditya Barua, Alex Botev, Alex Castro-\nRos, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tac-\nchetti, Anna Bulanova, Antonia Paterson, Beth\nTsai, Bobak Shahriari, Charline Le Lan, Christo-\npher A. Choquette-Choo,", "mimetype": "text/plain", "start_char_idx": 656, "end_char_idx": 1282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a6b927fa-cc71-499d-948d-d51fc165f3b5": {"__data__": {"id_": "a6b927fa-cc71-499d-948d-d51fc165f3b5", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "522c8145-c711-4502-bd3c-f8af6be87aad", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "4b47553b8edc218759209532da88c79cd6b2d48c14bc5f69c2943bea208b67e3", "class_name": "RelatedNodeInfo"}}, "text": "Charline Le Lan, Christo-\npher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer,\nDaphne Ippolito, David Reid, Elena Buchatskaya,\nEric Ni, Eric Noland, Geng Yan, George Tucker,\nGeorge-Christian Muraru, Grigory Rozhdestvenskiy,\nHenryk Michalewski, Ian Tenney, Ivan Grishchenko,\nJacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-\nnan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin\nMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-\ncan, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,\nMachel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael\nSharman, Nikolai Chinaev, Nithum Thain, Olivier\nBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-\nley, Paul Michel, Petko Yotov,", "mimetype": "text/plain", "start_char_idx": 1233, "end_char_idx": 1925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4442909d-8978-4334-a45f-05989a1fa8a8": {"__data__": {"id_": "4442909d-8978-4334-a45f-05989a1fa8a8", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6b927fa-cc71-499d-948d-d51fc165f3b5", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "c9ab83f7c231ac901ffc7bab0f29ea4e0f9bda275a03bad10b9d574bf4ee3384", "class_name": "RelatedNodeInfo"}}, "text": "Oscar Wahltinez, Paige Bai-\nley, Paul Michel, Petko Yotov, Rahma Chaabouni,\nRamona Comanescu, Reena Jana, Rohan Anil, Ross\nMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,\nSebastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Kli-\nmenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao\nGong, Tris Warkentin, Ludovic Peran, Minh Giang,\nCl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray\nKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,\nDouglas Eck, Joelle Barral, Fernando Pereira, Eli\nCollins, Armand Joulin, Noah Fiedel, Evan Senter,", "mimetype": "text/plain", "start_char_idx": 1867, "end_char_idx": 2477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "50f0e63c-0b5c-4b1d-ac7c-30b84d84d9f0": {"__data__": {"id_": "50f0e63c-0b5c-4b1d-ac7c-30b84d84d9f0", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4442909d-8978-4334-a45f-05989a1fa8a8", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8e84dccd154939ac74eead0a72aa597796a625e0981988f14b954faba40bc772", "class_name": "RelatedNodeInfo"}}, "text": "Eli\nCollins, Armand Joulin, Noah Fiedel, Evan Senter,\nAlek Andreev, and Kathleen Kenealy. 2024. Gemma:\nOpen Models Based on Gemini Research and Tech-\nnology. arXiv preprint . Version Number: 4.\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Em-\nmanuel Malherbe, C\u00e9line Hudelot, and Pierre\nColombo. 2024. Towards trustworthy reranking: A\nsimple yet effective abstention mechanism. Preprint ,\narXiv:2402.12997.", "mimetype": "text/plain", "start_char_idx": 2424, "end_char_idx": 2829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8d9bf0f8-daf6-4e6d-adbd-d2fea4411f4d": {"__data__": {"id_": "8d9bf0f8-daf6-4e6d-adbd-d2fea4411f4d", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}}, "text": "2023. PaLI-3 Vision Language Models: Smaller,\nFaster, Stronger. arXiv preprint . Version Number: 2.\nCohere. 2024. Introducing Rerank 3: A New Foun-\ndation Model for Efficient Enterprise Search & Re-\ntrieval.Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and\nPiotr Bojanowski. 2023. Vision Transformers Need\nRegisters. Publisher: [object Object] Version Num-\nber: 2.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2020. An Image\nis Worth 16x16 Words: Transformers for Image\nRecognition at Scale. Publisher: arXiv Version\nNumber: 2.\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and\nJian Sun. 2021. YOLOX: Exceeding YOLO Series\nin 2021. arXiv preprint . Version Number: 2.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay\nKale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot,\nPier Giuseppe Sessa, Aakanksha Chowdhery, Adam\nRoberts, Aditya Barua, Alex Botev, Alex Castro-\nRos, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tac-\nchetti, Anna Bulanova, Antonia Paterson, Beth\nTsai, Bobak Shahriari, Charline Le Lan, Christo-\npher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer,\nDaphne Ippolito, David Reid, Elena Buchatskaya,\nEric Ni, Eric Noland,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "70c7a548-cac3-4e1c-ad39-5c0a7cc46ca4": {"__data__": {"id_": "70c7a548-cac3-4e1c-ad39-5c0a7cc46ca4", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d9bf0f8-daf6-4e6d-adbd-d2fea4411f4d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "92e66dd01cf6778ed2cb703a05a8bc03e5f8afac7563e5a48393dfc3f625500c", "class_name": "RelatedNodeInfo"}}, "text": "David Reid, Elena Buchatskaya,\nEric Ni, Eric Noland, Geng Yan, George Tucker,\nGeorge-Christian Muraru, Grigory Rozhdestvenskiy,\nHenryk Michalewski, Ian Tenney, Ivan Grishchenko,\nJacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-\nnan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin\nMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-\ncan, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,\nMachel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael\nSharman, Nikolai Chinaev, Nithum Thain, Olivier\nBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-\nley, Paul Michel, Petko Yotov, Rahma Chaabouni,\nRamona Comanescu, Reena Jana, Rohan Anil, Ross\nMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,\nSebastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Kli-\nmenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao\nGong, Tris Warkentin, Ludovic Peran, Minh Giang,\nCl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray\nKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,\nDouglas Eck, Joelle Barral, Fernando Pereira, Eli\nCollins, Armand Joulin, Noah Fiedel, Evan Senter,\nAlek Andreev, and Kathleen Kenealy. 2024. Gemma:\nOpen Models Based on Gemini Research and Tech-\nnology. arXiv preprint . Version Number: 4.\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Em-\nmanuel Malherbe, C\u00e9line Hudelot, and Pierre\nColombo.", "mimetype": "text/plain", "start_char_idx": 1327, "end_char_idx": 2718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1303e699-89ff-4ff0-bb47-0cfeb7019408": {"__data__": {"id_": "1303e699-89ff-4ff0-bb47-0cfeb7019408", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70c7a548-cac3-4e1c-ad39-5c0a7cc46ca4", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "aa2e29a253fe02b7f47deb722220d84f57d1e6c6cf444e9648dae8b4e3f8f3c8", "class_name": "RelatedNodeInfo"}}, "text": "2024. Towards trustworthy reranking: A\nsimple yet effective abstention mechanism. Preprint ,\narXiv:2402.12997.", "mimetype": "text/plain", "start_char_idx": 2719, "end_char_idx": 2829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "17613d62-f18f-4308-a7d9-725dd9a25ac1": {"__data__": {"id_": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7c51a72a7c6cbabf6a2b3cfae09a6915c65fb7ce8607ac4a6a4b974f64730562", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "341906a7-7b81-4e34-b04c-aff139b399d9", "node_type": "1", "metadata": {}, "hash": "205b21986ef2670db8a777ed8cca817fbcdc2525abcaf4bd8719cafaa573576c", "class_name": "RelatedNodeInfo"}}, "text": "2023. PaLI-3 Vision Language Models: Smaller,\nFaster, Stronger. arXiv preprint . Version Number: 2.\nCohere. 2024. Introducing Rerank 3: A New Foun-\ndation Model for Efficient Enterprise Search & Re-\ntrieval.Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and\nPiotr Bojanowski. 2023. Vision Transformers Need\nRegisters. Publisher: [object Object] Version Num-\nber: 2.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2020. An Image\nis Worth 16x16 Words: Transformers for Image\nRecognition at Scale. Publisher: arXiv Version\nNumber: 2.\nZheng Ge, Songtao Liu, Feng Wang, Zeming Li, and\nJian Sun. 2021. YOLOX: Exceeding YOLO Series\nin 2021. arXiv preprint . Version Number: 2.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay\nKale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot,\nPier Giuseppe Sessa, Aakanksha Chowdhery, Adam\nRoberts, Aditya Barua, Alex Botev, Alex Castro-\nRos, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tac-\nchetti, Anna Bulanova, Antonia Paterson, Beth\nTsai, Bobak Shahriari, Charline Le Lan, Christo-\npher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer,\nDaphne Ippolito, David Reid, Elena Buchatskaya,\nEric Ni, Eric Noland, Geng Yan, George Tucker,\nGeorge-Christian Muraru, Grigory Rozhdestvenskiy,\nHenryk Michalewski, Ian Tenney, Ivan Grishchenko,\nJacob Austin, James Keeling, Jane Labanowski,\nJean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-\nnan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin\nMao-Jones, Katherine Lee, Kathy Yu, Katie Milli-\ncan, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,\nMachel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael\nSharman, Nikolai Chinaev, Nithum Thain, Olivier\nBachem, Oscar Chang, Oscar Wahltinez, Paige Bai-\nley, Paul Michel, Petko Yotov, Rahma Chaabouni,\nRamona Comanescu, Reena Jana, Rohan Anil, Ross\nMcIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,\nSebastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Kli-\nmenko, Tom Hennigan, Vlad Feinberg, Wojciech\nStokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao\nGong, Tris Warkentin, Ludovic Peran, Minh Giang,\nCl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray\nKavukcuoglu, Demis Hassabis, Zoubin Ghahramani,\nDouglas Eck, Joelle Barral, Fernando Pereira, Eli\nCollins, Armand Joulin, Noah Fiedel, Evan Senter,\nAlek Andreev, and Kathleen Kenealy. 2024. Gemma:\nOpen Models Based on Gemini Research and Tech-\nnology. arXiv preprint . Version Number: 4.\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Em-\nmanuel Malherbe, C\u00e9line Hudelot, and Pierre\nColombo. 2024. Towards trustworthy reranking: A\nsimple yet effective abstention mechanism. Preprint ,\narXiv:2402.12997.", "mimetype": "text/plain", "start_char_idx": 2450, "end_char_idx": 5279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b97b614d-9aa8-4a34-80c4-257d7519da09": {"__data__": {"id_": "b97b614d-9aa8-4a34-80c4-257d7519da09", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}}, "text": "2024. Gemma:\nOpen Models Based on Gemini Research and Tech-\nnology. arXiv preprint . Version Number: 4.\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Em-\nmanuel Malherbe, C\u00e9line Hudelot, and Pierre\nColombo. 2024. Towards trustworthy reranking: A\nsimple yet effective abstention mechanism. Preprint ,\narXiv:2402.12997.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "341906a7-7b81-4e34-b04c-aff139b399d9", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "404f6ffc-bfc5-432e-af58-2b1965b50c95": {"__data__": {"id_": "404f6ffc-bfc5-432e-af58-2b1965b50c95", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}}, "text": "2024. Gemma:\nOpen Models Based on Gemini Research and Tech-\nnology. arXiv preprint . Version Number: 4.\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Em-\nmanuel Malherbe, C\u00e9line Hudelot, and Pierre\nColombo. 2024. Towards trustworthy reranking: A\nsimple yet effective abstention mechanism. Preprint ,\narXiv:2402.12997.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "341906a7-7b81-4e34-b04c-aff139b399d9", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "341906a7-7b81-4e34-b04c-aff139b399d9": {"__data__": {"id_": "341906a7-7b81-4e34-b04c-aff139b399d9", "embedding": null, "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26", "node_type": "4", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "f1d7a3e5a8dd052e977eb1ebeccd67d825437cdd8d1030e10f16520e63933bee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17613d62-f18f-4308-a7d9-725dd9a25ac1", "node_type": "1", "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "26f7cd56c3971389e7dd92ba9bbc44d8966385b8ba0fcfda4301606a4e6a4694", "class_name": "RelatedNodeInfo"}}, "text": "2024. Gemma:\nOpen Models Based on Gemini Research and Tech-\nnology. arXiv preprint . Version Number: 4.\nHippolyte Gisserot-Boukhlef, Manuel Faysse, Em-\nmanuel Malherbe, C\u00e9line Hudelot, and Pierre\nColombo. 2024. Towards trustworthy reranking: A\nsimple yet effective abstention mechanism. Preprint ,\narXiv:2402.12997.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\n10", "mimetype": "text/plain", "start_char_idx": 4964, "end_char_idx": 5425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "341906a7-7b81-4e34-b04c-aff139b399d9", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6f4f8de3-6858-45df-995c-66d1f7b22aff": {"__data__": {"id_": "6f4f8de3-6858-45df-995c-66d1f7b22aff", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}}, "text": "of Large Language Models. Publisher: arXiv Version\nNumber: 2.\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and\nFuru Wei. 2022. LayoutLMv3: Pre-training for Doc-\nument AI with Unified Text and Image Masking. Pub-\nlisher: arXiv Version Number: 3.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed. 2023. Mistral 7B. Publisher:\narXiv Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bb809562-7d23-4f7c-a220-c03efbe1727e": {"__data__": {"id_": "bb809562-7d23-4f7c-a220-c03efbe1727e", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f4f8de3-6858-45df-995c-66d1f7b22aff", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a15ef67c9f731cc60c48fdb46edfc735aa41815712ac9404090b4348fabeaea5", "class_name": "RelatedNodeInfo"}}, "text": "Mistral 7B. Publisher:\narXiv Version Number: 1.\nVladimir Karpukhin, Barlas O \u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense Passage Retrieval for\nOpen-Domain Question Answering. arXiv preprint .\nVersion Number: 3.\nOmar Khattab and Matei Zaharia. 2020. ColBERT:\nEfficient and Effective Passage Search via Contextu-\nalized Late Interaction over BERT.\nGeewook Kim, Teakgyu Hong, Moonbin Yim,\nJeongyeon Nam, Jinyoung Park, Jinyeong Yim, Won-\nseok Hwang, Sangdoo Yun, Dongyoon Han, and\nSeunghyun Park. 2021. OCR-free Document Un-\nderstanding Transformer. arXiv preprint . Version\nNumber: 5.", "mimetype": "text/plain", "start_char_idx": 579, "end_char_idx": 1216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0ffc365b-1710-4571-ac52-8d59489b0b9a": {"__data__": {"id_": "0ffc365b-1710-4571-ac52-8d59489b0b9a", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb809562-7d23-4f7c-a220-c03efbe1727e", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "247658fb845fa354239d9b24394da3ae5683da52669bbe074cb0ff8a19bef3b6", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint . Version\nNumber: 5.\nAndreas Koukounas, Georgios Mastrapas, Michael G\u00fcn-\nther, Bo Wang, Scott Martens, Isabelle Mohr, Saba\nSturua, Mohammad Kalim Akram, Joan Fontanals\nMart\u00ednez, Saahil Ognawala, Susana Guzman, Maxi-\nmilian Werk, Nan Wang, and Han Xiao. 2024. Jina\nCLIP: Your CLIP Model Is Also Your Text Retriever.\narXiv preprint . Version Number: 1.\nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and\nVictor Sanh. 2024. What matters when build-\ning vision-language models? arXiv preprint .\nArXiv:2405.02246 [cs].\nJinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu,\nTao Lei, Iftekhar Naim, Ming-Wei Chang, and Vin-\ncent Y .", "mimetype": "text/plain", "start_char_idx": 1181, "end_char_idx": 1815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d6edc881-f11e-4a5f-93ca-9015782f0478": {"__data__": {"id_": "d6edc881-f11e-4a5f-93ca-9015782f0478", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ffc365b-1710-4571-ac52-8d59489b0b9a", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "e74dc4f10ec00bf9aaba9bb4414fc57b553a9af7a0037dcea2bc6e67b8d5e42c", "class_name": "RelatedNodeInfo"}}, "text": "Zhao. 2023. Rethinking the Role of Token\nRetrieval in Multi-Vector Retrieval. arXiv preprint .\nVersion Number: 3.\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong\nFeng, Lingpeng Kong, and Qi Liu. 2024. Multimodal\narxiv: A dataset for improving scientific compre-\nhension of large vision-language models. Preprint ,\narXiv:2403.00231.\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona,\nDeva Ramanan, C. Lawrence Zitnick, and Piotr Dol-\nl\u00e1r. 2014. Microsoft COCO: Common Objects in\nContext. arXiv preprint . Version Number: 3.Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a.", "mimetype": "text/plain", "start_char_idx": 1816, "end_char_idx": 2455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "59ecdf1f-910e-432c-94a9-7f20ddde9645": {"__data__": {"id_": "59ecdf1f-910e-432c-94a9-7f20ddde9645", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6edc881-f11e-4a5f-93ca-9015782f0478", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "371572b28f65058a8c1e3fd072a0f98c24e9417648c7e751fd7cdaebadab5e72", "class_name": "RelatedNodeInfo"}}, "text": "2023a. Improved Baselines with Visual Instruc-\ntion Tuning. arXiv preprint . Version Number: 2.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual Instruction Tuning. Publisher:\narXiv Version Number: 1.\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimos-\nthenis Karatzas, Ernest Valveny, and C. V Jawahar.\n2021. InfographicVQA. arXiv preprint . Version\nNumber: 2.", "mimetype": "text/plain", "start_char_idx": 2449, "end_char_idx": 2833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bdea7c05-e6b0-48c5-b573-2e6c54f44b69": {"__data__": {"id_": "bdea7c05-e6b0-48c5-b573-2e6c54f44b69", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}}, "text": "of Large Language Models. Publisher: arXiv Version\nNumber: 2.\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and\nFuru Wei. 2022. LayoutLMv3: Pre-training for Doc-\nument AI with Unified Text and Image Masking. Pub-\nlisher: arXiv Version Number: 3.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed. 2023. Mistral 7B. Publisher:\narXiv Version Number: 1.\nVladimir Karpukhin, Barlas O \u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense Passage Retrieval for\nOpen-Domain Question Answering. arXiv preprint .\nVersion Number: 3.\nOmar Khattab and Matei Zaharia. 2020. ColBERT:\nEfficient and Effective Passage Search via Contextu-\nalized Late Interaction over BERT.\nGeewook Kim, Teakgyu Hong, Moonbin Yim,\nJeongyeon Nam, Jinyoung Park, Jinyeong Yim, Won-\nseok Hwang, Sangdoo Yun, Dongyoon Han, and\nSeunghyun Park. 2021. OCR-free Document Un-\nderstanding Transformer. arXiv preprint . Version\nNumber: 5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "08d29566-3d35-4713-9a87-50f5d5cb4ac6": {"__data__": {"id_": "08d29566-3d35-4713-9a87-50f5d5cb4ac6", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdea7c05-e6b0-48c5-b573-2e6c54f44b69", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b5e891d926f4cbc571de3b420f5d618e29e5f1b5f1c587fd9ace0341c18130e4", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint . Version\nNumber: 5.\nAndreas Koukounas, Georgios Mastrapas, Michael G\u00fcn-\nther, Bo Wang, Scott Martens, Isabelle Mohr, Saba\nSturua, Mohammad Kalim Akram, Joan Fontanals\nMart\u00ednez, Saahil Ognawala, Susana Guzman, Maxi-\nmilian Werk, Nan Wang, and Han Xiao. 2024. Jina\nCLIP: Your CLIP Model Is Also Your Text Retriever.\narXiv preprint . Version Number: 1.\nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and\nVictor Sanh. 2024. What matters when build-\ning vision-language models? arXiv preprint .\nArXiv:2405.02246 [cs].\nJinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu,\nTao Lei, Iftekhar Naim, Ming-Wei Chang, and Vin-\ncent Y . Zhao. 2023. Rethinking the Role of Token\nRetrieval in Multi-Vector Retrieval. arXiv preprint .\nVersion Number: 3.\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong\nFeng, Lingpeng Kong, and Qi Liu. 2024. Multimodal\narxiv: A dataset for improving scientific compre-\nhension of large vision-language models. Preprint ,\narXiv:2403.00231.\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona,\nDeva Ramanan, C. Lawrence Zitnick, and Piotr Dol-\nl\u00e1r. 2014. Microsoft COCO: Common Objects in\nContext. arXiv preprint . Version Number: 3.Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved Baselines with Visual Instruc-\ntion Tuning. arXiv preprint . Version Number: 2.", "mimetype": "text/plain", "start_char_idx": 1181, "end_char_idx": 2544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "91ee45e9-591e-4100-9e56-cef5fcf3d88b": {"__data__": {"id_": "91ee45e9-591e-4100-9e56-cef5fcf3d88b", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08d29566-3d35-4713-9a87-50f5d5cb4ac6", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "32bdc7ebb48d2e758a110c366308e13a619529797d44d210cc61be7512f526bd", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint . Version Number: 2.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual Instruction Tuning. Publisher:\narXiv Version Number: 1.\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimos-\nthenis Karatzas, Ernest Valveny, and C. V Jawahar.\n2021. InfographicVQA. arXiv preprint . Version\nNumber: 2.", "mimetype": "text/plain", "start_char_idx": 2509, "end_char_idx": 2833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a1dea19a-b472-4445-a1cb-f972803ddaa5": {"__data__": {"id_": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bce4097-a720-4346-9ef2-ee9216f9f290", "node_type": "1", "metadata": {}, "hash": "443ebb1424b2de529080b9310ae8d4ffd82f0eb29268957721e79c47f3e47605", "class_name": "RelatedNodeInfo"}}, "text": "of Large Language Models. Publisher: arXiv Version\nNumber: 2.\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and\nFuru Wei. 2022. LayoutLMv3: Pre-training for Doc-\nument AI with Unified Text and Image Masking. Pub-\nlisher: arXiv Version Number: 3.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed. 2023. Mistral 7B. Publisher:\narXiv Version Number: 1.\nVladimir Karpukhin, Barlas O \u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense Passage Retrieval for\nOpen-Domain Question Answering. arXiv preprint .\nVersion Number: 3.\nOmar Khattab and Matei Zaharia. 2020. ColBERT:\nEfficient and Effective Passage Search via Contextu-\nalized Late Interaction over BERT.\nGeewook Kim, Teakgyu Hong, Moonbin Yim,\nJeongyeon Nam, Jinyoung Park, Jinyeong Yim, Won-\nseok Hwang, Sangdoo Yun, Dongyoon Han, and\nSeunghyun Park. 2021. OCR-free Document Un-\nderstanding Transformer. arXiv preprint . Version\nNumber: 5.\nAndreas Koukounas, Georgios Mastrapas, Michael G\u00fcn-\nther, Bo Wang, Scott Martens, Isabelle Mohr, Saba\nSturua, Mohammad Kalim Akram, Joan Fontanals\nMart\u00ednez, Saahil Ognawala, Susana Guzman, Maxi-\nmilian Werk, Nan Wang, and Han Xiao. 2024. Jina\nCLIP: Your CLIP Model Is Also Your Text Retriever.\narXiv preprint . Version Number: 1.\nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and\nVictor Sanh. 2024. What matters when build-\ning vision-language models? arXiv preprint .\nArXiv:2405.02246 [cs].\nJinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu,\nTao Lei, Iftekhar Naim, Ming-Wei Chang, and Vin-\ncent Y . Zhao. 2023. Rethinking the Role of Token\nRetrieval in Multi-Vector Retrieval. arXiv preprint .\nVersion Number: 3.\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong\nFeng, Lingpeng Kong, and Qi Liu. 2024. Multimodal\narxiv: A dataset for improving scientific compre-\nhension of large vision-language models. Preprint ,\narXiv:2403.00231.\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona,\nDeva Ramanan, C. Lawrence Zitnick, and Piotr Dol-\nl\u00e1r. 2014. Microsoft COCO: Common Objects in\nContext. arXiv preprint . Version Number: 3.Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved Baselines with Visual Instruc-\ntion Tuning. arXiv preprint . Version Number: 2.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual Instruction Tuning. Publisher:\narXiv Version Number: 1.\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimos-\nthenis Karatzas, Ernest Valveny, and C. V Jawahar.\n2021. InfographicVQA. arXiv preprint . Version\nNumber: 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "bd409718-af93-42eb-ab96-73f34ed2934f": {"__data__": {"id_": "bd409718-af93-42eb-ab96-73f34ed2934f", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}}, "text": "2014. Microsoft COCO: Common Objects in\nContext. arXiv preprint . Version Number: 3.Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved Baselines with Visual Instruc-\ntion Tuning. arXiv preprint . Version Number: 2.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual Instruction Tuning. Publisher:\narXiv Version Number: 1.\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimos-\nthenis Karatzas, Ernest Valveny, and C. V Jawahar.\n2021. InfographicVQA. arXiv preprint . Version\nNumber: 2.\nMinesh Mathew, Dimosthenis Karatzas, and C. V . Jawa-\nhar. 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5bce4097-a720-4346-9ef2-ee9216f9f290", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0042aebc-0ac7-4408-9572-333d93d86b9d": {"__data__": {"id_": "0042aebc-0ac7-4408-9572-333d93d86b9d", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd409718-af93-42eb-ab96-73f34ed2934f", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "e15c7b18b4407028df227df10fcbc8722abaea2ab55df0a36818a93b0462bf5e", "class_name": "RelatedNodeInfo"}}, "text": "Jawa-\nhar. 2020. DocVQA: A Dataset for VQA on Docu-\nment Images.\nNiklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and\nNils Reimers. 2022. MTEB: Massive Text Embed-\nding Benchmark. arXiv preprint . Version Number:\n3.\nNomic. 2024. Nomic Embed Vision: Expanding The\nNomic Latent Space.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning Transferable Visual Models From Natural Lan-\nguage Supervision. Publisher: arXiv Version Num-\nber: 1.\nNils Reimers and Iryna Gurevych. 2019.", "mimetype": "text/plain", "start_char_idx": 572, "end_char_idx": 1198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5bce4097-a720-4346-9ef2-ee9216f9f290", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f0b22b9c-7538-4564-a7d5-d4999c69c4e8": {"__data__": {"id_": "f0b22b9c-7538-4564-a7d5-d4999c69c4e8", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0042aebc-0ac7-4408-9572-333d93d86b9d", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "09567df3ea211561f40da0187cf2469d75fa69d4a5d135db3731fbdbd5259052", "class_name": "RelatedNodeInfo"}}, "text": "Nils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence Embeddings using Siamese BERT-\nNetworks. arXiv preprint . Version Number: 1.\nStephen E. Robertson, Steve Walker, Susan Jones,\nMicheline Hancock-Beaulieu, and Mike Gatford.\n1994. Okapi at TREC-3. In Proceedings of The Third\nText REtrieval Conference, TREC 1994, Gaithers-\nburg, Maryland, USA, November 2-4, 1994 , volume\n500-225 of NIST Special Publication , pages 109\u2013\n126. National Institute of Standards and Technology\n(NIST).\nKeshav Santhanam, Omar Khattab, Christopher Potts,\nand Matei Zaharia. 2022. PLAID: An Efficient En-\ngine for Late Interaction Retrieval. arXiv preprint .\nVersion Number: 1.\nR. Smith. 2007. An Overview of the Tesseract OCR\nEngine.", "mimetype": "text/plain", "start_char_idx": 1160, "end_char_idx": 1882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5bce4097-a720-4346-9ef2-ee9216f9f290", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d25799d6-a0b5-4496-ad34-c37b368f9bba": {"__data__": {"id_": "d25799d6-a0b5-4496-ad34-c37b368f9bba", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0b22b9c-7538-4564-a7d5-d4999c69c4e8", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7e435bfb28fa155b2531697e33493cea1f1deb781aacc0bb787d074fef36bf35", "class_name": "RelatedNodeInfo"}}, "text": "R. Smith. 2007. An Overview of the Tesseract OCR\nEngine. In Ninth International Conference on Doc-\nument Analysis and Recognition (ICDAR 2007) Vol\n2, pages 629\u2013633, Curitiba, Parana, Brazil. IEEE.\nISSN: 1520-5363.\nKaren Sparck Jones. 1972. A STATISTICAL INTER-\nPRETATION OF TERM SPECIFICITY AND ITS\nAPPLICATION IN RETRIEV AL. Journal of Docu-\nmentation , 28(1):11\u201321.\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang,\nYang Liu, Chenguang Zhu, Michael Zeng, Cha\nZhang, and Mohit Bansal. 2022. Unifying Vision,\nText, and Layout for Universal Document Processing.\narXiv preprint . Version Number: 3.\n11", "mimetype": "text/plain", "start_char_idx": 1826, "end_char_idx": 2424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5bce4097-a720-4346-9ef2-ee9216f9f290", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "da2ad865-a1b8-494a-b9a5-b70724cfdabe": {"__data__": {"id_": "da2ad865-a1b8-494a-b9a5-b70724cfdabe", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}}, "text": "2014. Microsoft COCO: Common Objects in\nContext. arXiv preprint . Version Number: 3.Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved Baselines with Visual Instruc-\ntion Tuning. arXiv preprint . Version Number: 2.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual Instruction Tuning. Publisher:\narXiv Version Number: 1.\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimos-\nthenis Karatzas, Ernest Valveny, and C. V Jawahar.\n2021. InfographicVQA. arXiv preprint . Version\nNumber: 2.\nMinesh Mathew, Dimosthenis Karatzas, and C. V . Jawa-\nhar. 2020. DocVQA: A Dataset for VQA on Docu-\nment Images.\nNiklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and\nNils Reimers. 2022. MTEB: Massive Text Embed-\nding Benchmark. arXiv preprint . Version Number:\n3.\nNomic. 2024. Nomic Embed Vision: Expanding The\nNomic Latent Space.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning Transferable Visual Models From Natural Lan-\nguage Supervision. Publisher: arXiv Version Num-\nber: 1.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence Embeddings using Siamese BERT-\nNetworks. arXiv preprint . Version Number: 1.\nStephen E. Robertson, Steve Walker, Susan Jones,\nMicheline Hancock-Beaulieu, and Mike Gatford.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5bce4097-a720-4346-9ef2-ee9216f9f290", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2a779f7d-157c-451f-bec8-3a9f3a952901": {"__data__": {"id_": "2a779f7d-157c-451f-bec8-3a9f3a952901", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da2ad865-a1b8-494a-b9a5-b70724cfdabe", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "14b46c8dbb626d073af8f0d5823cb639f8a2ff79da7cf5cee5a549776c6bc687", "class_name": "RelatedNodeInfo"}}, "text": "1994. Okapi at TREC-3. In Proceedings of The Third\nText REtrieval Conference, TREC 1994, Gaithers-\nburg, Maryland, USA, November 2-4, 1994 , volume\n500-225 of NIST Special Publication , pages 109\u2013\n126. National Institute of Standards and Technology\n(NIST).\nKeshav Santhanam, Omar Khattab, Christopher Potts,\nand Matei Zaharia. 2022. PLAID: An Efficient En-\ngine for Late Interaction Retrieval. arXiv preprint .\nVersion Number: 1.\nR. Smith. 2007. An Overview of the Tesseract OCR\nEngine. In Ninth International Conference on Doc-\nument Analysis and Recognition (ICDAR 2007) Vol\n2, pages 629\u2013633, Curitiba, Parana, Brazil. IEEE.\nISSN: 1520-5363.\nKaren Sparck Jones. 1972. A STATISTICAL INTER-\nPRETATION OF TERM SPECIFICITY AND ITS\nAPPLICATION IN RETRIEV AL. Journal of Docu-\nmentation , 28(1):11\u201321.\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang,\nYang Liu, Chenguang Zhu, Michael Zeng, Cha\nZhang, and Mohit Bansal. 2022. Unifying Vision,\nText, and Layout for Universal Document Processing.\narXiv preprint . Version Number: 3.\n11", "mimetype": "text/plain", "start_char_idx": 1396, "end_char_idx": 2424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5bce4097-a720-4346-9ef2-ee9216f9f290", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5bce4097-a720-4346-9ef2-ee9216f9f290": {"__data__": {"id_": "5bce4097-a720-4346-9ef2-ee9216f9f290", "embedding": null, "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9f632b9-6938-49f3-90c6-27b726540ac6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a878e2d3e5026e73fc0da5330a970864a59abbdf5481e56476e8c6b1c934d806", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1dea19a-b472-4445-a1cb-f972803ddaa5", "node_type": "1", "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "25e84841628412439c6fa47709a30bfd5834aa5bf92ad042e9899acd407e356a", "class_name": "RelatedNodeInfo"}}, "text": "2014. Microsoft COCO: Common Objects in\nContext. arXiv preprint . Version Number: 3.Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved Baselines with Visual Instruc-\ntion Tuning. arXiv preprint . Version Number: 2.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual Instruction Tuning. Publisher:\narXiv Version Number: 1.\nMinesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimos-\nthenis Karatzas, Ernest Valveny, and C. V Jawahar.\n2021. InfographicVQA. arXiv preprint . Version\nNumber: 2.\nMinesh Mathew, Dimosthenis Karatzas, and C. V . Jawa-\nhar. 2020. DocVQA: A Dataset for VQA on Docu-\nment Images.\nNiklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and\nNils Reimers. 2022. MTEB: Massive Text Embed-\nding Benchmark. arXiv preprint . Version Number:\n3.\nNomic. 2024. Nomic Embed Vision: Expanding The\nNomic Latent Space.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning Transferable Visual Models From Natural Lan-\nguage Supervision. Publisher: arXiv Version Num-\nber: 1.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence Embeddings using Siamese BERT-\nNetworks. arXiv preprint . Version Number: 1.\nStephen E. Robertson, Steve Walker, Susan Jones,\nMicheline Hancock-Beaulieu, and Mike Gatford.\n1994. Okapi at TREC-3. In Proceedings of The Third\nText REtrieval Conference, TREC 1994, Gaithers-\nburg, Maryland, USA, November 2-4, 1994 , volume\n500-225 of NIST Special Publication , pages 109\u2013\n126. National Institute of Standards and Technology\n(NIST).\nKeshav Santhanam, Omar Khattab, Christopher Potts,\nand Matei Zaharia. 2022. PLAID: An Efficient En-\ngine for Late Interaction Retrieval. arXiv preprint .\nVersion Number: 1.\nR. Smith. 2007. An Overview of the Tesseract OCR\nEngine. In Ninth International Conference on Doc-\nument Analysis and Recognition (ICDAR 2007) Vol\n2, pages 629\u2013633, Curitiba, Parana, Brazil. IEEE.\nISSN: 1520-5363.\nKaren Sparck Jones. 1972. A STATISTICAL INTER-\nPRETATION OF TERM SPECIFICITY AND ITS\nAPPLICATION IN RETRIEV AL. Journal of Docu-\nmentation , 28(1):11\u201321.\nZineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang,\nYang Liu, Chenguang Zhu, Michael Zeng, Cha\nZhang, and Mohit Bansal. 2022. Unifying Vision,\nText, and Layout for Universal Document Processing.\narXiv preprint . Version Number: 3.\n11", "mimetype": "text/plain", "start_char_idx": 2310, "end_char_idx": 4734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "5bce4097-a720-4346-9ef2-ee9216f9f290", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a4010cca-a0f4-4c03-a2af-1cf80549465f": {"__data__": {"id_": "a4010cca-a0f4-4c03-a2af-1cf80549465f", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}}, "text": "Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA Heterogenous Benchmark for Zero-shot Evalua-\ntion of Information Retrieval Models. arXiv preprint .\nVersion Number: 4.\nAshish V . Thapliyal, Jordi Pont-Tuset, Xi Chen, and\nRadu Soricut. 2022. Crossmodal-3600: A Massively\nMultilingual Multimodal Evaluation Dataset. arXiv\npreprint . Version Number: 2.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text Embeddings by Weakly-\nSupervised Contrastive Pre-training. arXiv preprint .\nVersion Number: 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cc205667-9e0a-400d-8055-d241287e6349": {"__data__": {"id_": "cc205667-9e0a-400d-8055-d241287e6349", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4010cca-a0f4-4c03-a2af-1cf80549465f", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "887e3b5e3b48b284128f5654c0ac03339e31ebe25b59acd596878159abbd8d73", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint .\nVersion Number: 2.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. MiniLM: Deep Self-\nAttention Distillation for Task-Agnostic Compres-\nsion of Pre-Trained Transformers. arXiv preprint .\nArXiv:2002.10957 [cs].\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu,\nMinzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,\nXin Jiang, and Chunjing Xu. 2021. FILIP: Fine-\ngrained Interactive Language-Image Pre-Training.\narXiv preprint . Version Number: 1.", "mimetype": "text/plain", "start_char_idx": 582, "end_char_idx": 1061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "38ee5820-2509-46f2-b725-cb55f3e88168": {"__data__": {"id_": "38ee5820-2509-46f2-b725-cb55f3e88168", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc205667-9e0a-400d-8055-d241287e6349", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "298a9e693c72baf3a24448ca7ff7ada2cee48062c897ae8a87d19ff9265af9a4", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint . Version Number: 1.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao\nYu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,\nHuan Sun, Yu Su, and Wenhu Chen. 2023. MMMU:\nA Massive Multi-discipline Multimodal Understand-\ning and Reasoning Benchmark for Expert AGI. arXiv\npreprint . Version Number: 3.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,\nand Lucas Beyer. 2023. Sigmoid Loss for Language\nImage Pre-Training. Publisher: [object Object] Ver-\nsion Number: 4.", "mimetype": "text/plain", "start_char_idx": 1026, "end_char_idx": 1639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "439e17dc-9d7a-4928-ab30-c0bcbeffb9d3": {"__data__": {"id_": "439e17dc-9d7a-4928-ab30-c0bcbeffb9d3", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38ee5820-2509-46f2-b725-cb55f3e88168", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "5d7408a1604f1888ffc736c62d6b06b2ed19ff904998776ebcdb94fd4fd7d382", "class_name": "RelatedNodeInfo"}}, "text": "Publisher: [object Object] Ver-\nsion Number: 4.\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai\nJiao, Xuan Long Do, Chengwei Qin, Bosheng Ding,\nXiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq\nJoty. 2023. Retrieving Multimodal Information for\nAugmented Generation: A Survey. arXiv preprint .\nVersion Number: 3.\nFengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang,\nHaozhou Zhang, and Tat-Seng Chua. 2022. Towards\nComplex Document Understanding By Discrete Rea-\nsoning. Publisher: arXiv Version Number: 3.\nA Benchmark Datasets\nA.1 Academic Datasets\nDocVQA (Mathew et al., 2020) includes collected\nimages from the UCSF Industry Documents Li-\nbrary.", "mimetype": "text/plain", "start_char_idx": 1592, "end_char_idx": 2232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "56397c8d-a735-4e92-98aa-7280c4d91845": {"__data__": {"id_": "56397c8d-a735-4e92-98aa-7280c4d91845", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "439e17dc-9d7a-4928-ab30-c0bcbeffb9d3", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "c772de3f0e7adc9ca2f823cdd35363556a04612e2dee6580033a7cb3b72a710e", "class_name": "RelatedNodeInfo"}}, "text": "Questions and answers were manually anno-\ntated.InfoVQA (Mathew et al., 2021) includes infograph-\nics collected from the Internet using the search\nquery \u201c infographics \u201d. Questions and answers were\nmanually annotated.\nTAT-DQA (Zhu et al., 2022) is a large-scale Docu-\nment VQA dataset that was constructed from pub-\nlicly available real-world financial reports. It fo-\ncuses on rich tabular and textual content requiring\nnumerical reasoning. Questions and answers were\nmanually annotated by human experts in finance.\narXivQA (Li et al., 2024) is a VQA dataset based\non figures extracted from arXiv publications. The\nquestions were generated synthetically using GPT-\n4 Vision.\nTabFQuAD (Table French Question Answering\nDataset) is designed to evaluate TableQA models\nin realistic industry settings. We create additional\nqueries to augment the existing human-annotated\nones using the same method described in subsec-\ntion A.2.", "mimetype": "text/plain", "start_char_idx": 2233, "end_char_idx": 3157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6a0836e9-5cef-4080-aee6-8245acb184fe": {"__data__": {"id_": "6a0836e9-5cef-4080-aee6-8245acb184fe", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56397c8d-a735-4e92-98aa-7280c4d91845", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6e812d6baf09921201bc5c648a33c70d62606a88a1ab8ab4bc3f751ad2cccafc", "class_name": "RelatedNodeInfo"}}, "text": "A.2 Practical Datasets\nMethodology.", "mimetype": "text/plain", "start_char_idx": 3158, "end_char_idx": 3193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "12eed830-25b2-4743-b818-531a7567923e": {"__data__": {"id_": "12eed830-25b2-4743-b818-531a7567923e", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}}, "text": "Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA Heterogenous Benchmark for Zero-shot Evalua-\ntion of Information Retrieval Models. arXiv preprint .\nVersion Number: 4.\nAshish V . Thapliyal, Jordi Pont-Tuset, Xi Chen, and\nRadu Soricut. 2022. Crossmodal-3600: A Massively\nMultilingual Multimodal Evaluation Dataset. arXiv\npreprint . Version Number: 2.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text Embeddings by Weakly-\nSupervised Contrastive Pre-training. arXiv preprint .\nVersion Number: 2.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. MiniLM: Deep Self-\nAttention Distillation for Task-Agnostic Compres-\nsion of Pre-Trained Transformers. arXiv preprint .\nArXiv:2002.10957 [cs].\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu,\nMinzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,\nXin Jiang, and Chunjing Xu. 2021. FILIP: Fine-\ngrained Interactive Language-Image Pre-Training.\narXiv preprint . Version Number: 1.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao\nYu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,\nHuan Sun, Yu Su, and Wenhu Chen. 2023.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e6bba099-272d-4193-8426-e836167810be": {"__data__": {"id_": "e6bba099-272d-4193-8426-e836167810be", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12eed830-25b2-4743-b818-531a7567923e", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "282bf34c79de172b71853686af289e7207a8eeeecd2dec799d6cf3c2759d099d", "class_name": "RelatedNodeInfo"}}, "text": "2023. MMMU:\nA Massive Multi-discipline Multimodal Understand-\ning and Reasoning Benchmark for Expert AGI. arXiv\npreprint . Version Number: 3.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,\nand Lucas Beyer. 2023. Sigmoid Loss for Language\nImage Pre-Training. Publisher: [object Object] Ver-\nsion Number: 4.\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai\nJiao, Xuan Long Do, Chengwei Qin, Bosheng Ding,\nXiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq\nJoty. 2023. Retrieving Multimodal Information for\nAugmented Generation: A Survey. arXiv preprint .\nVersion Number: 3.\nFengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang,\nHaozhou Zhang, and Tat-Seng Chua. 2022. Towards\nComplex Document Understanding By Discrete Rea-\nsoning. Publisher: arXiv Version Number: 3.\nA Benchmark Datasets\nA.1 Academic Datasets\nDocVQA (Mathew et al., 2020) includes collected\nimages from the UCSF Industry Documents Li-\nbrary. Questions and answers were manually anno-\ntated.InfoVQA (Mathew et al., 2021) includes infograph-\nics collected from the Internet using the search\nquery \u201c infographics \u201d. Questions and answers were\nmanually annotated.\nTAT-DQA (Zhu et al., 2022) is a large-scale Docu-\nment VQA dataset that was constructed from pub-\nlicly available real-world financial reports. It fo-\ncuses on rich tabular and textual content requiring\nnumerical reasoning. Questions and answers were\nmanually annotated by human experts in finance.\narXivQA (Li et al., 2024) is a VQA dataset based\non figures extracted from arXiv publications. The\nquestions were generated synthetically using GPT-\n4 Vision.\nTabFQuAD (Table French Question Answering\nDataset) is designed to evaluate TableQA models\nin realistic industry settings.", "mimetype": "text/plain", "start_char_idx": 1330, "end_char_idx": 3030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b4ce308a-7cbb-41cd-b975-3c4907187238": {"__data__": {"id_": "b4ce308a-7cbb-41cd-b975-3c4907187238", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6bba099-272d-4193-8426-e836167810be", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ba11330b4e0f245cd12098918a4b80f66cb34f8308fb2be4b822d77b0cc796f5", "class_name": "RelatedNodeInfo"}}, "text": "We create additional\nqueries to augment the existing human-annotated\nones using the same method described in subsec-\ntion A.2.\nA.2 Practical Datasets\nMethodology.", "mimetype": "text/plain", "start_char_idx": 3031, "end_char_idx": 3193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "abee7795-8c88-40b1-b005-024a6605c0ed": {"__data__": {"id_": "abee7795-8c88-40b1-b005-024a6605c0ed", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e0e1ccc-96c7-482c-a028-0f47c5f704d6", "node_type": "1", "metadata": {}, "hash": "a6c0ecc879bca5c06325bca291da5e409a1c546fc953cc6af9eca06e0f143641", "class_name": "RelatedNodeInfo"}}, "text": "Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA Heterogenous Benchmark for Zero-shot Evalua-\ntion of Information Retrieval Models. arXiv preprint .\nVersion Number: 4.\nAshish V . Thapliyal, Jordi Pont-Tuset, Xi Chen, and\nRadu Soricut. 2022. Crossmodal-3600: A Massively\nMultilingual Multimodal Evaluation Dataset. arXiv\npreprint . Version Number: 2.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text Embeddings by Weakly-\nSupervised Contrastive Pre-training. arXiv preprint .\nVersion Number: 2.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. MiniLM: Deep Self-\nAttention Distillation for Task-Agnostic Compres-\nsion of Pre-Trained Transformers. arXiv preprint .\nArXiv:2002.10957 [cs].\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu,\nMinzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,\nXin Jiang, and Chunjing Xu. 2021. FILIP: Fine-\ngrained Interactive Language-Image Pre-Training.\narXiv preprint . Version Number: 1.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao\nYu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,\nHuan Sun, Yu Su, and Wenhu Chen. 2023. MMMU:\nA Massive Multi-discipline Multimodal Understand-\ning and Reasoning Benchmark for Expert AGI. arXiv\npreprint . Version Number: 3.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,\nand Lucas Beyer. 2023. Sigmoid Loss for Language\nImage Pre-Training. Publisher: [object Object] Ver-\nsion Number: 4.\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai\nJiao, Xuan Long Do, Chengwei Qin, Bosheng Ding,\nXiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq\nJoty. 2023. Retrieving Multimodal Information for\nAugmented Generation: A Survey. arXiv preprint .\nVersion Number: 3.\nFengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang,\nHaozhou Zhang, and Tat-Seng Chua. 2022. Towards\nComplex Document Understanding By Discrete Rea-\nsoning. Publisher: arXiv Version Number: 3.\nA Benchmark Datasets\nA.1 Academic Datasets\nDocVQA (Mathew et al., 2020) includes collected\nimages from the UCSF Industry Documents Li-\nbrary. Questions and answers were manually anno-\ntated.InfoVQA (Mathew et al., 2021) includes infograph-\nics collected from the Internet using the search\nquery \u201c infographics \u201d. Questions and answers were\nmanually annotated.\nTAT-DQA (Zhu et al., 2022) is a large-scale Docu-\nment VQA dataset that was constructed from pub-\nlicly available real-world financial reports. It fo-\ncuses on rich tabular and textual content requiring\nnumerical reasoning. Questions and answers were\nmanually annotated by human experts in finance.\narXivQA (Li et al., 2024) is a VQA dataset based\non figures extracted from arXiv publications. The\nquestions were generated synthetically using GPT-\n4 Vision.\nTabFQuAD (Table French Question Answering\nDataset) is designed to evaluate TableQA models\nin realistic industry settings. We create additional\nqueries to augment the existing human-annotated\nones using the same method described in subsec-\ntion A.2.\nA.2 Practical Datasets\nMethodology.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "53db8acf-dacc-43d1-9b71-da0733f1b590": {"__data__": {"id_": "53db8acf-dacc-43d1-9b71-da0733f1b590", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}}, "text": "Questions and answers were\nmanually annotated.\nTAT-DQA (Zhu et al., 2022) is a large-scale Docu-\nment VQA dataset that was constructed from pub-\nlicly available real-world financial reports. It fo-\ncuses on rich tabular and textual content requiring\nnumerical reasoning. Questions and answers were\nmanually annotated by human experts in finance.\narXivQA (Li et al., 2024) is a VQA dataset based\non figures extracted from arXiv publications. The\nquestions were generated synthetically using GPT-\n4 Vision.\nTabFQuAD (Table French Question Answering\nDataset) is designed to evaluate TableQA models\nin realistic industry settings. We create additional\nqueries to augment the existing human-annotated\nones using the same method described in subsec-\ntion A.2.\nA.2 Practical Datasets\nMethodology.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1e0e1ccc-96c7-482c-a028-0f47c5f704d6", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "87a07630-fd48-4cc8-a4fb-87b61d2c651b": {"__data__": {"id_": "87a07630-fd48-4cc8-a4fb-87b61d2c651b", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53db8acf-dacc-43d1-9b71-da0733f1b590", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "d48145082af5a218bddd782838909bcbbc772c1d3c1e4909f52e7d96d87b5199", "class_name": "RelatedNodeInfo"}}, "text": "A.2 Practical Datasets\nMethodology. Creating a relevant retrieval dataset\nclose to real use cases is a major challenge as the\ndataset needs to be both sufficiently large for effec-\ntive fine-tuning and sufficiently diverse to cover a\nbroad range of modalities (full text, tables, charts,\n...), domains (industry, healthcare, ...), and query-\ndocument interactions (extractive questions, open-\nended questions, ...). Our approach to building this\ndataset involves several steps: (1) we use a web\ncrawler to collect publicly available documents on\nvarious themes and sources, (2) we convert these\nPDFs into a series of images, one per page, and (3)\nwe generate queries related to each image using a\nVLM.\nWeb-Crawler. We implemented a web crawler to\nefficiently collect large volumes of documents re-\nlated to a given topic. The crawler is seeded with\na user-defined query (e.g. \"artificial intelligence\")\nand then uses GPT-3.5 Turbo to brainstorm related\ntopics and subtopics.", "mimetype": "text/plain", "start_char_idx": 754, "end_char_idx": 1728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1e0e1ccc-96c7-482c-a028-0f47c5f704d6", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e1a7dc18-5fe0-44bd-96ed-3b4443ea7a1c": {"__data__": {"id_": "e1a7dc18-5fe0-44bd-96ed-3b4443ea7a1c", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87a07630-fd48-4cc8-a4fb-87b61d2c651b", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "dbc0c256ac81167b7ee9e91690d888228888542aa99db804e452875123e58f01", "class_name": "RelatedNodeInfo"}}, "text": "This query augmentation\nstrategy aims at both broadening and deepening the\nsearch. GPT-3.5 Turbo is further used to generate\ndiverse search queries from each subtopic. This\nquery set is then consumed by a pool of parallel\nworkers whose job is to fetch the associated most\nrelevant documents. We use SerpAPI15along with\na filetype filter (PDF documents only) to program-\nmatically scrape Google Search rankings. Each\n15https://serpapi.com/\n12", "mimetype": "text/plain", "start_char_idx": 1729, "end_char_idx": 2170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1e0e1ccc-96c7-482c-a028-0f47c5f704d6", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "44a21df2-4dd0-47a9-96d3-07dc7b22ea17": {"__data__": {"id_": "44a21df2-4dd0-47a9-96d3-07dc7b22ea17", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}}, "text": "Questions and answers were\nmanually annotated.\nTAT-DQA (Zhu et al., 2022) is a large-scale Docu-\nment VQA dataset that was constructed from pub-\nlicly available real-world financial reports. It fo-\ncuses on rich tabular and textual content requiring\nnumerical reasoning. Questions and answers were\nmanually annotated by human experts in finance.\narXivQA (Li et al., 2024) is a VQA dataset based\non figures extracted from arXiv publications. The\nquestions were generated synthetically using GPT-\n4 Vision.\nTabFQuAD (Table French Question Answering\nDataset) is designed to evaluate TableQA models\nin realistic industry settings. We create additional\nqueries to augment the existing human-annotated\nones using the same method described in subsec-\ntion A.2.\nA.2 Practical Datasets\nMethodology. Creating a relevant retrieval dataset\nclose to real use cases is a major challenge as the\ndataset needs to be both sufficiently large for effec-\ntive fine-tuning and sufficiently diverse to cover a\nbroad range of modalities (full text, tables, charts,\n...), domains (industry, healthcare, ...), and query-\ndocument interactions (extractive questions, open-\nended questions, ...). Our approach to building this\ndataset involves several steps: (1) we use a web\ncrawler to collect publicly available documents on\nvarious themes and sources, (2) we convert these\nPDFs into a series of images, one per page, and (3)\nwe generate queries related to each image using a\nVLM.\nWeb-Crawler. We implemented a web crawler to\nefficiently collect large volumes of documents re-\nlated to a given topic. The crawler is seeded with\na user-defined query (e.g. \"artificial intelligence\")\nand then uses GPT-3.5 Turbo to brainstorm related\ntopics and subtopics. This query augmentation\nstrategy aims at both broadening and deepening the\nsearch. GPT-3.5 Turbo is further used to generate\ndiverse search queries from each subtopic. This\nquery set is then consumed by a pool of parallel\nworkers whose job is to fetch the associated most\nrelevant documents.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1e0e1ccc-96c7-482c-a028-0f47c5f704d6", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a2c2e9d6-4091-45dd-9754-0cb9cac0ec89": {"__data__": {"id_": "a2c2e9d6-4091-45dd-9754-0cb9cac0ec89", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44a21df2-4dd0-47a9-96d3-07dc7b22ea17", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b147e62109f748dcd9c8b52fc8e80c6daa68ebedff46487e8a8afc100dc8557c", "class_name": "RelatedNodeInfo"}}, "text": "We use SerpAPI15along with\na filetype filter (PDF documents only) to program-\nmatically scrape Google Search rankings. Each\n15https://serpapi.com/\n12", "mimetype": "text/plain", "start_char_idx": 2021, "end_char_idx": 2170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1e0e1ccc-96c7-482c-a028-0f47c5f704d6", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1e0e1ccc-96c7-482c-a028-0f47c5f704d6": {"__data__": {"id_": "1e0e1ccc-96c7-482c-a028-0f47c5f704d6", "embedding": null, "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0", "node_type": "4", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6c232121073528c6445206669ec11a7af2f37f4a3cde56cb6427c1073b8cab6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abee7795-8c88-40b1-b005-024a6605c0ed", "node_type": "1", "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "36ce3a46aa7cfba6021877eb4c02fa9959db3a2c515ecbe3f0ad9c4b119c285d", "class_name": "RelatedNodeInfo"}}, "text": "Questions and answers were\nmanually annotated.\nTAT-DQA (Zhu et al., 2022) is a large-scale Docu-\nment VQA dataset that was constructed from pub-\nlicly available real-world financial reports. It fo-\ncuses on rich tabular and textual content requiring\nnumerical reasoning. Questions and answers were\nmanually annotated by human experts in finance.\narXivQA (Li et al., 2024) is a VQA dataset based\non figures extracted from arXiv publications. The\nquestions were generated synthetically using GPT-\n4 Vision.\nTabFQuAD (Table French Question Answering\nDataset) is designed to evaluate TableQA models\nin realistic industry settings. We create additional\nqueries to augment the existing human-annotated\nones using the same method described in subsec-\ntion A.2.\nA.2 Practical Datasets\nMethodology. Creating a relevant retrieval dataset\nclose to real use cases is a major challenge as the\ndataset needs to be both sufficiently large for effec-\ntive fine-tuning and sufficiently diverse to cover a\nbroad range of modalities (full text, tables, charts,\n...), domains (industry, healthcare, ...), and query-\ndocument interactions (extractive questions, open-\nended questions, ...). Our approach to building this\ndataset involves several steps: (1) we use a web\ncrawler to collect publicly available documents on\nvarious themes and sources, (2) we convert these\nPDFs into a series of images, one per page, and (3)\nwe generate queries related to each image using a\nVLM.\nWeb-Crawler. We implemented a web crawler to\nefficiently collect large volumes of documents re-\nlated to a given topic. The crawler is seeded with\na user-defined query (e.g. \"artificial intelligence\")\nand then uses GPT-3.5 Turbo to brainstorm related\ntopics and subtopics. This query augmentation\nstrategy aims at both broadening and deepening the\nsearch. GPT-3.5 Turbo is further used to generate\ndiverse search queries from each subtopic. This\nquery set is then consumed by a pool of parallel\nworkers whose job is to fetch the associated most\nrelevant documents. We use SerpAPI15along with\na filetype filter (PDF documents only) to program-\nmatically scrape Google Search rankings. Each\n15https://serpapi.com/\n12", "mimetype": "text/plain", "start_char_idx": 2404, "end_char_idx": 4574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "1e0e1ccc-96c7-482c-a028-0f47c5f704d6", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "67b293de-51ac-4be7-b1d7-309f3bcf2f57": {"__data__": {"id_": "67b293de-51ac-4be7-b1d7-309f3bcf2f57", "embedding": null, "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c855f4f0-3f40-42b9-ab44-bc338badf868", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8d6f5a75d9e1c7045e162e8ea5aba9976c691d978f42b6d0b25db1680db1bed8", "class_name": "RelatedNodeInfo"}}, "text": "file is hashed and stored in a Bloom filter (Bloom,\n1970) shared among workers to avoid duplicate doc-\numents in the final corpus. Unique scraped files are\ndownloaded, and inserted into a SQLite database\nalong with additional metadata.\nDatamix. Using the web crawler, we collected\napproximately 1,000 documents for each of the\nfollowing four seeds: \"energy\" ,\"government re-\nports\" ,\"healthcare industry\" , and \"artificial intel-\nligence\" . These seeds were meticulously hand-\npicked to align with real-use cases for retrieval\nmodels and visually rich pages. We also removed\nall documents containing any private information.\nAt this stage, we randomly selected 900 files for\nthe training set and 100 files for the test set, ensur-\ning that data leakage into the test set was avoided\nduring subsequent processing steps.\nQuery Generation. To increase the efficiency of\nour query generation scheme and to limit API calls,\nwe generate at most 3 questions per image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c84ce928-12c8-4d14-af6f-bac7a49c486d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b27f9de5-bbe4-421f-b677-fa36200836bd": {"__data__": {"id_": "b27f9de5-bbe4-421f-b677-fa36200836bd", "embedding": null, "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c855f4f0-3f40-42b9-ab44-bc338badf868", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8d6f5a75d9e1c7045e162e8ea5aba9976c691d978f42b6d0b25db1680db1bed8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67b293de-51ac-4be7-b1d7-309f3bcf2f57", "node_type": "1", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "0ec688d9aede8a32ec322e95b2efd6d8a78d9bcf2aaefb8b97fbb73566a1285d", "class_name": "RelatedNodeInfo"}}, "text": "From\nall the documents collected, we randomly sample\n10,000 images per theme and call Claude-3 Sonnet\nwith the following prompt:You are an assistant specialized in\nMultimodal RAG tasks.\\n ,\u2192\nThe task is the following: given an image\nfrom a pdf page, you will have to ,\u2192\ngenerate questions that can be asked by a\nuser to retrieve information from ,\u2192\na large documentary corpus.\nThe question should be relevant to the\npage, and should not be too specific ,\u2192\nor too general. The question should be\nabout the subject of the page, and ,\u2192\nthe answer needs to be found in the page.\n\\n ,\u2192\nRemember that the question is asked by a\nuser to get some information from a ,\u2192\nlarge documentary corpus that contains\nmultimodal data. Generate a question ,\u2192\nthat could be asked by a user without\nknowing the existence and the content ,\u2192\nof the corpus. \\n\nGenerate as well the answer to the\nquestion, which should be found in the ,\u2192\npage.", "mimetype": "text/plain", "start_char_idx": 962, "end_char_idx": 1881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c84ce928-12c8-4d14-af6f-bac7a49c486d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "173cb8fb-4926-4e7c-b254-af74e55813dc": {"__data__": {"id_": "173cb8fb-4926-4e7c-b254-af74e55813dc", "embedding": null, "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c855f4f0-3f40-42b9-ab44-bc338badf868", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8d6f5a75d9e1c7045e162e8ea5aba9976c691d978f42b6d0b25db1680db1bed8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b27f9de5-bbe4-421f-b677-fa36200836bd", "node_type": "1", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "9f58fd46ee349331b7a0b018e32a2f52487c0c2b95cc84eec7078c2b5258e6fe", "class_name": "RelatedNodeInfo"}}, "text": "And the format of the answer should\nbe a list of words answering the ,\u2192\nquestion. \\n\nGenerate at most THREE pairs of questions\nand answers per page in a ,\u2192\ndictionary with the following format,\nanswer ONLY this dictionary ,\u2192\nNOTHING ELSE: \\n\n{\n\"questions\": [\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n]\n}\nwhere XXXXXX is the question and\n['YYYYYY '] is the corresponding list\nof answers,\u2192\n,\u2192\nthat could be as long as needed. \\n\nNote: If there are no questions to ask\nabout the page, return an empty list. ,\u2192\nFocus on making relevant questions\nconcerning the page. \\n ,\u2192\nHere is the page: \\n\nHuman Validation.", "mimetype": "text/plain", "start_char_idx": 1882, "end_char_idx": 2592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c84ce928-12c8-4d14-af6f-bac7a49c486d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3431851a-bda9-4cfa-a555-95089afb452e": {"__data__": {"id_": "3431851a-bda9-4cfa-a555-95089afb452e", "embedding": null, "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c855f4f0-3f40-42b9-ab44-bc338badf868", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8d6f5a75d9e1c7045e162e8ea5aba9976c691d978f42b6d0b25db1680db1bed8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "173cb8fb-4926-4e7c-b254-af74e55813dc", "node_type": "1", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a750e54154357743e60d3d66ad0e8d84fa571f47bb76ce71d50c85e38c8af30c", "class_name": "RelatedNodeInfo"}}, "text": "\\n ,\u2192\nHere is the page: \\n\nHuman Validation. We manually validate every\nsingle synthetically created query in ViDoRe to en-\nsure quality, query relevance, and consistency with\nthe benchmark objective of evaluating retrieval in\npractical industrial settings. During this step, we\nrandomly assign document-pair queries to 4 vol-\n13", "mimetype": "text/plain", "start_char_idx": 2548, "end_char_idx": 2877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c84ce928-12c8-4d14-af6f-bac7a49c486d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d12797e7-db32-46f3-a145-dec8296d1752": {"__data__": {"id_": "d12797e7-db32-46f3-a145-dec8296d1752", "embedding": null, "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c855f4f0-3f40-42b9-ab44-bc338badf868", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8d6f5a75d9e1c7045e162e8ea5aba9976c691d978f42b6d0b25db1680db1bed8", "class_name": "RelatedNodeInfo"}}, "text": "file is hashed and stored in a Bloom filter (Bloom,\n1970) shared among workers to avoid duplicate doc-\numents in the final corpus. Unique scraped files are\ndownloaded, and inserted into a SQLite database\nalong with additional metadata.\nDatamix. Using the web crawler, we collected\napproximately 1,000 documents for each of the\nfollowing four seeds: \"energy\" ,\"government re-\nports\" ,\"healthcare industry\" , and \"artificial intel-\nligence\" . These seeds were meticulously hand-\npicked to align with real-use cases for retrieval\nmodels and visually rich pages. We also removed\nall documents containing any private information.\nAt this stage, we randomly selected 900 files for\nthe training set and 100 files for the test set, ensur-\ning that data leakage into the test set was avoided\nduring subsequent processing steps.\nQuery Generation. To increase the efficiency of\nour query generation scheme and to limit API calls,\nwe generate at most 3 questions per image. From\nall the documents collected, we randomly sample\n10,000 images per theme and call Claude-3 Sonnet\nwith the following prompt:You are an assistant specialized in\nMultimodal RAG tasks.\\n ,\u2192\nThe task is the following: given an image\nfrom a pdf page, you will have to ,\u2192\ngenerate questions that can be asked by a\nuser to retrieve information from ,\u2192\na large documentary corpus.\nThe question should be relevant to the\npage, and should not be too specific ,\u2192\nor too general. The question should be\nabout the subject of the page, and ,\u2192\nthe answer needs to be found in the page.\n\\n ,\u2192\nRemember that the question is asked by a\nuser to get some information from a ,\u2192\nlarge documentary corpus that contains\nmultimodal data. Generate a question ,\u2192\nthat could be asked by a user without\nknowing the existence and the content ,\u2192\nof the corpus. \\n\nGenerate as well the answer to the\nquestion, which should be found in the ,\u2192\npage. And the format of the answer should\nbe a list of words answering the ,\u2192\nquestion.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1963, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c84ce928-12c8-4d14-af6f-bac7a49c486d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "04cbb945-790d-4e6b-bd07-e017beaba9ce": {"__data__": {"id_": "04cbb945-790d-4e6b-bd07-e017beaba9ce", "embedding": null, "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c855f4f0-3f40-42b9-ab44-bc338badf868", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8d6f5a75d9e1c7045e162e8ea5aba9976c691d978f42b6d0b25db1680db1bed8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d12797e7-db32-46f3-a145-dec8296d1752", "node_type": "1", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "43e816d44350deb77aae96e79087341affe7f276e6f2a626aae2e5df62f65c7e", "class_name": "RelatedNodeInfo"}}, "text": "\\n\nGenerate at most THREE pairs of questions\nand answers per page in a ,\u2192\ndictionary with the following format,\nanswer ONLY this dictionary ,\u2192\nNOTHING ELSE: \\n\n{\n\"questions\": [\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n]\n}\nwhere XXXXXX is the question and\n['YYYYYY '] is the corresponding list\nof answers,\u2192\n,\u2192\nthat could be as long as needed. \\n\nNote: If there are no questions to ask\nabout the page, return an empty list. ,\u2192\nFocus on making relevant questions\nconcerning the page. \\n ,\u2192\nHere is the page: \\n\nHuman Validation. We manually validate every\nsingle synthetically created query in ViDoRe to en-\nsure quality, query relevance, and consistency with\nthe benchmark objective of evaluating retrieval in\npractical industrial settings. During this step, we\nrandomly assign document-pair queries to 4 vol-\n13", "mimetype": "text/plain", "start_char_idx": 1964, "end_char_idx": 2877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c84ce928-12c8-4d14-af6f-bac7a49c486d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c84ce928-12c8-4d14-af6f-bac7a49c486d": {"__data__": {"id_": "c84ce928-12c8-4d14-af6f-bac7a49c486d", "embedding": null, "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c855f4f0-3f40-42b9-ab44-bc338badf868", "node_type": "4", "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8d6f5a75d9e1c7045e162e8ea5aba9976c691d978f42b6d0b25db1680db1bed8", "class_name": "RelatedNodeInfo"}}, "text": "file is hashed and stored in a Bloom filter (Bloom,\n1970) shared among workers to avoid duplicate doc-\numents in the final corpus. Unique scraped files are\ndownloaded, and inserted into a SQLite database\nalong with additional metadata.\nDatamix. Using the web crawler, we collected\napproximately 1,000 documents for each of the\nfollowing four seeds: \"energy\" ,\"government re-\nports\" ,\"healthcare industry\" , and \"artificial intel-\nligence\" . These seeds were meticulously hand-\npicked to align with real-use cases for retrieval\nmodels and visually rich pages. We also removed\nall documents containing any private information.\nAt this stage, we randomly selected 900 files for\nthe training set and 100 files for the test set, ensur-\ning that data leakage into the test set was avoided\nduring subsequent processing steps.\nQuery Generation. To increase the efficiency of\nour query generation scheme and to limit API calls,\nwe generate at most 3 questions per image. From\nall the documents collected, we randomly sample\n10,000 images per theme and call Claude-3 Sonnet\nwith the following prompt:You are an assistant specialized in\nMultimodal RAG tasks.\\n ,\u2192\nThe task is the following: given an image\nfrom a pdf page, you will have to ,\u2192\ngenerate questions that can be asked by a\nuser to retrieve information from ,\u2192\na large documentary corpus.\nThe question should be relevant to the\npage, and should not be too specific ,\u2192\nor too general. The question should be\nabout the subject of the page, and ,\u2192\nthe answer needs to be found in the page.\n\\n ,\u2192\nRemember that the question is asked by a\nuser to get some information from a ,\u2192\nlarge documentary corpus that contains\nmultimodal data. Generate a question ,\u2192\nthat could be asked by a user without\nknowing the existence and the content ,\u2192\nof the corpus. \\n\nGenerate as well the answer to the\nquestion, which should be found in the ,\u2192\npage. And the format of the answer should\nbe a list of words answering the ,\u2192\nquestion. \\n\nGenerate at most THREE pairs of questions\nand answers per page in a ,\u2192\ndictionary with the following format,\nanswer ONLY this dictionary ,\u2192\nNOTHING ELSE: \\n\n{\n\"questions\": [\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n{\n\"question\": \"XXXXXX\",\n\"answer\": [\"YYYYYY\"]\n},\n]\n}\nwhere XXXXXX is the question and\n['YYYYYY '] is the corresponding list\nof answers,\u2192\n,\u2192\nthat could be as long as needed. \\n\nNote: If there are no questions to ask\nabout the page, return an empty list. ,\u2192\nFocus on making relevant questions\nconcerning the page. \\n ,\u2192\nHere is the page: \\n\nHuman Validation. We manually validate every\nsingle synthetically created query in ViDoRe to en-\nsure quality, query relevance, and consistency with\nthe benchmark objective of evaluating retrieval in\npractical industrial settings. During this step, we\nrandomly assign document-pair queries to 4 vol-\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "c84ce928-12c8-4d14-af6f-bac7a49c486d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "5545bc25-a275-4b36-94e5-b71f10fc17d8": {"__data__": {"id_": "5545bc25-a275-4b36-94e5-b71f10fc17d8", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}}, "text": "unteer annotators and instruct them to filter out\nqueries that do not fit the above-listed criteria. We\nalso instruct annotators to flag any documents they\ndeem to contain PII information or content not\nsuited for an academic benchmark. No flag was\nraised during the entirety of the process, validating\nour prior PDF collection strategy. 100 queries per\ntopic are collected in this manner. Annotators are\ncolleagues and collaborators of the authors who\nvolunteered to help. Each annotator spent approxi-\nmately 3 hours filtering the larger query set down\nto 100 high-quality queries per topic.\nB Implementation details\nB.1 Codebase\nThe codebase is written in PyTorch16and leverages\nHuggingFace tooling for model implementations\nand trainers17.\nB.2 Pairwise CE loss\nOur in-batch contrastive loss Lis defined as the\nsoftmaxed cross-entropy of the positive scores\ns+\nk=LI(dk, qk)w.r.t.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d72fc83f-82fe-478e-b567-405349b260b7": {"__data__": {"id_": "d72fc83f-82fe-478e-b567-405349b260b7", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5545bc25-a275-4b36-94e5-b71f10fc17d8", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "e8197de589638fabec44bd8f5af3bfa92b60df979bb7079314374622383c4b21", "class_name": "RelatedNodeInfo"}}, "text": "to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, pl).\nFor numerical stability, we reformulate the loss\nwith the softplus function, leading to:\nL=1\nbbX\nk=1softplus\u0000\ns\u2212\nk\u2212s+\nk\u0001\n(2)\nB.3 Hyperparameters\nHyperparameters are tuned on a validation split\ncomposed of 2%of the training dataset. We find\nbi-encoder methods to be more sensible to learning\nrate variations than late interaction-based models\nand achieve the best performance for all models\nwith a learning rate of 5e\u22125. We experiment with\nLoRA rank and \u03b1values and do not notice partic-\nular improvements past r=\u03b1= 32 . Per-device\nbatch sizes are kept small due to long sequence\nlengths that complicate scaling past b= 4. Simu-\nlating larger batch sizes for in-batch negative sam-\npling should enable even better results.", "mimetype": "text/plain", "start_char_idx": 883, "end_char_idx": 1663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7ceabaf6-f4c3-4d42-b228-30dad5a402e4": {"__data__": {"id_": "7ceabaf6-f4c3-4d42-b228-30dad5a402e4", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d72fc83f-82fe-478e-b567-405349b260b7", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "18a0b866f5f4b84c12c048cb10d4aa05ae1e65e46cd630158e0ff57d49891523", "class_name": "RelatedNodeInfo"}}, "text": "We find\nthe best results with global batch size b= 32 for 1\nepoch on our training set.\nB.4 Embedding size\nMinimizing storage footprint can be essential to\nindustrial retrieval systems if databases contain mil-\n16https://pytorch.org/\n17https://huggingface.colions of documents. With this criterion in view, we\nhave compared the embedding sizes of the models\nin our study. As shown in Table 3, ColPali \u2019s em-\nbedding size is an order of magnitude larger than\nBM25 and two orders of magnitude larger than\nBGE-M3. However, this study is limited to the\nnaive method of storing ColPali \u2019s multi-vector em-\nbeddings. In practical scenarios, using cluster cen-\ntroids can reduce the size of ColPali multi-vector\nembeddings by up to an order of magnitude (San-\nthanam et al., 2022) and make it a competitive\nretrieval system.", "mimetype": "text/plain", "start_char_idx": 1664, "end_char_idx": 2480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "569cde60-c250-4f05-8f87-d003e6feff30": {"__data__": {"id_": "569cde60-c250-4f05-8f87-d003e6feff30", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ceabaf6-f4c3-4d42-b228-30dad5a402e4", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "07c18e7c2fa0ef57975d54288511fc2d48d4c28340cebb3b26a342e78fd3fdbb", "class_name": "RelatedNodeInfo"}}, "text": "Model Embedding size (KB)\nBGE-M3 8.60\nBM25 (dense emb.) 3.00\nBM25 (sparse emb.) 1.56 \u00b1 0.51\nColPali (float16) 256\nTable 3: Comparison of the embedding sizes for the\nDocVQA test set from ViDoRe w.r.t. different retrieval\nmodels. The lower the size the smaller the storage\nfootprint of the model. The mean \u00b1stdsize is given for\nthe sparse embeddings.\nB.5 Latency computations\nAll latency computations are done on a NVIDIA\nL4 GPU. Queries are encoded independently (batch\nsize of 1) to simulate online querying, and pages\nare encoded with a batch size of 4 for PaliGemma\nderived models, and 8 for BGE-M3. Reported\ntimes include image and text processing time be-\nfore the model forward pass, as well as query-to-\nindex matching times.", "mimetype": "text/plain", "start_char_idx": 2481, "end_char_idx": 3212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "2abba5b6-8fa5-4edf-87c5-c1690fe2a72b": {"__data__": {"id_": "2abba5b6-8fa5-4edf-87c5-c1690fe2a72b", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "569cde60-c250-4f05-8f87-d003e6feff30", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "1d93fa475b299744f02fb69445c100d45bdbc071f993fe36664f23437735d2cd", "class_name": "RelatedNodeInfo"}}, "text": "We note an interesting fea-\nture of ColPali is that all documents have the same\nsequence length, leading to prior knowledge of run-\ntime and memory consumptions. Query latency\nexperiments are averaged over 1000 queries, and\nindexing times are measured for a 100 page docu-\nment. Per page time is obtained by diving total time\nby 100, corresponding to inverse page throughput.\nB.6 Captioning\nExamples of captions generated for visually rich\ndocument chunks with Claude-3 Sonnet are shown\nin Figure 6 and Figure 5. The prompt used for\ngenerating the description is the following:\n14", "mimetype": "text/plain", "start_char_idx": 3213, "end_char_idx": 3793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "8ec081dd-8c33-499a-9d21-b9452427f753": {"__data__": {"id_": "8ec081dd-8c33-499a-9d21-b9452427f753", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}}, "text": "unteer annotators and instruct them to filter out\nqueries that do not fit the above-listed criteria. We\nalso instruct annotators to flag any documents they\ndeem to contain PII information or content not\nsuited for an academic benchmark. No flag was\nraised during the entirety of the process, validating\nour prior PDF collection strategy. 100 queries per\ntopic are collected in this manner. Annotators are\ncolleagues and collaborators of the authors who\nvolunteered to help. Each annotator spent approxi-\nmately 3 hours filtering the larger query set down\nto 100 high-quality queries per topic.\nB Implementation details\nB.1 Codebase\nThe codebase is written in PyTorch16and leverages\nHuggingFace tooling for model implementations\nand trainers17.\nB.2 Pairwise CE loss\nOur in-batch contrastive loss Lis defined as the\nsoftmaxed cross-entropy of the positive scores\ns+\nk=LI(dk, qk)w.r.t. to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, pl).\nFor numerical stability, we reformulate the loss\nwith the softplus function, leading to:\nL=1\nbbX\nk=1softplus\u0000\ns\u2212\nk\u2212s+\nk\u0001\n(2)\nB.3 Hyperparameters\nHyperparameters are tuned on a validation split\ncomposed of 2%of the training dataset. We find\nbi-encoder methods to be more sensible to learning\nrate variations than late interaction-based models\nand achieve the best performance for all models\nwith a learning rate of 5e\u22125. We experiment with\nLoRA rank and \u03b1values and do not notice partic-\nular improvements past r=\u03b1= 32 . Per-device\nbatch sizes are kept small due to long sequence\nlengths that complicate scaling past b= 4. Simu-\nlating larger batch sizes for in-batch negative sam-\npling should enable even better results. We find\nthe best results with global batch size b= 32 for 1\nepoch on our training set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c468cabe-e9b3-4838-8b3e-b64e2637a0bd": {"__data__": {"id_": "c468cabe-e9b3-4838-8b3e-b64e2637a0bd", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ec081dd-8c33-499a-9d21-b9452427f753", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a7e53d105344d8e5764eb5efed2fb0f1e41304bb81fb98f17fe99b83d3206f90", "class_name": "RelatedNodeInfo"}}, "text": "B.4 Embedding size\nMinimizing storage footprint can be essential to\nindustrial retrieval systems if databases contain mil-\n16https://pytorch.org/\n17https://huggingface.colions of documents. With this criterion in view, we\nhave compared the embedding sizes of the models\nin our study. As shown in Table 3, ColPali \u2019s em-\nbedding size is an order of magnitude larger than\nBM25 and two orders of magnitude larger than\nBGE-M3. However, this study is limited to the\nnaive method of storing ColPali \u2019s multi-vector em-\nbeddings. In practical scenarios, using cluster cen-\ntroids can reduce the size of ColPali multi-vector\nembeddings by up to an order of magnitude (San-\nthanam et al., 2022) and make it a competitive\nretrieval system.\nModel Embedding size (KB)\nBGE-M3 8.60\nBM25 (dense emb.) 3.00\nBM25 (sparse emb.) 1.56 \u00b1 0.51\nColPali (float16) 256\nTable 3: Comparison of the embedding sizes for the\nDocVQA test set from ViDoRe w.r.t. different retrieval\nmodels. The lower the size the smaller the storage\nfootprint of the model. The mean \u00b1stdsize is given for\nthe sparse embeddings.\nB.5 Latency computations\nAll latency computations are done on a NVIDIA\nL4 GPU. Queries are encoded independently (batch\nsize of 1) to simulate online querying, and pages\nare encoded with a batch size of 4 for PaliGemma\nderived models, and 8 for BGE-M3. Reported\ntimes include image and text processing time be-\nfore the model forward pass, as well as query-to-\nindex matching times. We note an interesting fea-\nture of ColPali is that all documents have the same\nsequence length, leading to prior knowledge of run-\ntime and memory consumptions. Query latency\nexperiments are averaged over 1000 queries, and\nindexing times are measured for a 100 page docu-\nment. Per page time is obtained by diving total time\nby 100, corresponding to inverse page throughput.", "mimetype": "text/plain", "start_char_idx": 1751, "end_char_idx": 3588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1ef15fb1-864c-442d-8115-38f3adfdbac5": {"__data__": {"id_": "1ef15fb1-864c-442d-8115-38f3adfdbac5", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c468cabe-e9b3-4838-8b3e-b64e2637a0bd", "node_type": "1", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "ca356ffb497f02d97009c1834fe051fd01e9c365616ea03d30616c49e95ed271", "class_name": "RelatedNodeInfo"}}, "text": "Per page time is obtained by diving total time\nby 100, corresponding to inverse page throughput.\nB.6 Captioning\nExamples of captions generated for visually rich\ndocument chunks with Claude-3 Sonnet are shown\nin Figure 6 and Figure 5. The prompt used for\ngenerating the description is the following:\n14", "mimetype": "text/plain", "start_char_idx": 3492, "end_char_idx": 3793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e": {"__data__": {"id_": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "embedding": null, "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e99a4afd-2fcf-4693-b29e-85849571a84f", "node_type": "4", "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "class_name": "RelatedNodeInfo"}}, "text": "unteer annotators and instruct them to filter out\nqueries that do not fit the above-listed criteria. We\nalso instruct annotators to flag any documents they\ndeem to contain PII information or content not\nsuited for an academic benchmark. No flag was\nraised during the entirety of the process, validating\nour prior PDF collection strategy. 100 queries per\ntopic are collected in this manner. Annotators are\ncolleagues and collaborators of the authors who\nvolunteered to help. Each annotator spent approxi-\nmately 3 hours filtering the larger query set down\nto 100 high-quality queries per topic.\nB Implementation details\nB.1 Codebase\nThe codebase is written in PyTorch16and leverages\nHuggingFace tooling for model implementations\nand trainers17.\nB.2 Pairwise CE loss\nOur in-batch contrastive loss Lis defined as the\nsoftmaxed cross-entropy of the positive scores\ns+\nk=LI(dk, qk)w.r.t. to the maximal negative\nscores s\u2212\nk= max\nl,l\u0338=kLI(qk, pl).\nFor numerical stability, we reformulate the loss\nwith the softplus function, leading to:\nL=1\nbbX\nk=1softplus\u0000\ns\u2212\nk\u2212s+\nk\u0001\n(2)\nB.3 Hyperparameters\nHyperparameters are tuned on a validation split\ncomposed of 2%of the training dataset. We find\nbi-encoder methods to be more sensible to learning\nrate variations than late interaction-based models\nand achieve the best performance for all models\nwith a learning rate of 5e\u22125. We experiment with\nLoRA rank and \u03b1values and do not notice partic-\nular improvements past r=\u03b1= 32 . Per-device\nbatch sizes are kept small due to long sequence\nlengths that complicate scaling past b= 4. Simu-\nlating larger batch sizes for in-batch negative sam-\npling should enable even better results. We find\nthe best results with global batch size b= 32 for 1\nepoch on our training set.\nB.4 Embedding size\nMinimizing storage footprint can be essential to\nindustrial retrieval systems if databases contain mil-\n16https://pytorch.org/\n17https://huggingface.colions of documents. With this criterion in view, we\nhave compared the embedding sizes of the models\nin our study. As shown in Table 3, ColPali \u2019s em-\nbedding size is an order of magnitude larger than\nBM25 and two orders of magnitude larger than\nBGE-M3. However, this study is limited to the\nnaive method of storing ColPali \u2019s multi-vector em-\nbeddings. In practical scenarios, using cluster cen-\ntroids can reduce the size of ColPali multi-vector\nembeddings by up to an order of magnitude (San-\nthanam et al., 2022) and make it a competitive\nretrieval system.\nModel Embedding size (KB)\nBGE-M3 8.60\nBM25 (dense emb.) 3.00\nBM25 (sparse emb.) 1.56 \u00b1 0.51\nColPali (float16) 256\nTable 3: Comparison of the embedding sizes for the\nDocVQA test set from ViDoRe w.r.t. different retrieval\nmodels. The lower the size the smaller the storage\nfootprint of the model. The mean \u00b1stdsize is given for\nthe sparse embeddings.\nB.5 Latency computations\nAll latency computations are done on a NVIDIA\nL4 GPU. Queries are encoded independently (batch\nsize of 1) to simulate online querying, and pages\nare encoded with a batch size of 4 for PaliGemma\nderived models, and 8 for BGE-M3. Reported\ntimes include image and text processing time be-\nfore the model forward pass, as well as query-to-\nindex matching times. We note an interesting fea-\nture of ColPali is that all documents have the same\nsequence length, leading to prior knowledge of run-\ntime and memory consumptions. Query latency\nexperiments are averaged over 1000 queries, and\nindexing times are measured for a 100 page docu-\nment. Per page time is obtained by diving total time\nby 100, corresponding to inverse page throughput.\nB.6 Captioning\nExamples of captions generated for visually rich\ndocument chunks with Claude-3 Sonnet are shown\nin Figure 6 and Figure 5. The prompt used for\ngenerating the description is the following:\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "c544a717-cf0d-42ed-9e1a-f8e27d0fc5d5": {"__data__": {"id_": "c544a717-cf0d-42ed-9e1a-f8e27d0fc5d5", "embedding": null, "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b38c1ca7790a19f425297936bd2348778fc5a0f8523aa6fd7c8168f1e5d9194a", "class_name": "RelatedNodeInfo"}}, "text": "You are an assistant specialized in document analy-\nsis. Given a table or a figure, you have to provide a\ndetailed summary of the content in maximum 3000\ncharacters. Your summary should be qualitative and\nnot quantitative. Here is the table/figure to analyze:\n{image}. Answer ONLY with the caption of the ta-\nble/figure.\nFigure 5: Example from the \"Energy\" test set.\nCaption: The image depicts the hourly energy gener-\nation profile, illustrating the contributions of various\nenergy sources over 24 hours. The data is presented\nas a stacked bar chart, with the x-axis representing the\nhours of the day from 1 to 2, and the y-axis showing\nthe average hourly generation in MW. The bars are seg-\nmented into different colors, each representing a distinct\nenergy source: nuclear, bio, geothermal, solar, wind, hy-\ndro, natural gas, and other imports.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6d5ea79e-b226-41ff-a5cb-4e3d61c990e4": {"__data__": {"id_": "6d5ea79e-b226-41ff-a5cb-4e3d61c990e4", "embedding": null, "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b38c1ca7790a19f425297936bd2348778fc5a0f8523aa6fd7c8168f1e5d9194a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c544a717-cf0d-42ed-9e1a-f8e27d0fc5d5", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "2cf0ec50dc3f3851e1b2652c61c9a19328d0ebf0a74799db35bd202e67a6ac84", "class_name": "RelatedNodeInfo"}}, "text": "The chart provides\ninsights into the temporal variations in energy genera-\ntion across different sources, highlighting the interplay\nbetween baseload and intermittent sources throughout\nthe day.\nFigure 6: Example from the \"Government Reports\" test\nset.\nCaption: The image shows a table titled \"System of\nRecord\" which outlines the different types of documents\nor records maintained across various systems or depart-\nments within an organization related to project man-\nagement and construction. The rows list documents\nlike project plans, budgets, schedules, contracts, pur-\nchase orders, invoices, change requests, bid submissions,\ndrawings, manuals, meeting minutes, and reports. The\ncolumns indicate the system or department responsible\nfor maintaining each record, such as County Servers,\nProject View, OnBase, CGI Advantage Financial Sys-\ntem, and Purchasing Department. The table uses \"W\"\nand \"T\" markers to denote which system or department\nserves as the primary source (writer) or storage loca-\ntion (trailer) for each type of document.", "mimetype": "text/plain", "start_char_idx": 847, "end_char_idx": 1891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "e4e0a8cb-0b74-44a6-9646-ace0b17d7fc9": {"__data__": {"id_": "e4e0a8cb-0b74-44a6-9646-ace0b17d7fc9", "embedding": null, "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b38c1ca7790a19f425297936bd2348778fc5a0f8523aa6fd7c8168f1e5d9194a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d5ea79e-b226-41ff-a5cb-4e3d61c990e4", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "958f385bca6c144a15eccb288adcafe801332c0d907a34f0695a572d1cf2cf47", "class_name": "RelatedNodeInfo"}}, "text": "C More similarity maps\nIn Figure 7, ColPali assigns a high similarity to all\npatches with the word \"Kazakhstan\" when given\nthe token <_Kazakhstan> . Moreover, our model\nseems to exhibit world knowledge capabilities as\nthe patch around the word \"Kashagan\" - an off-\nshore oil field in Kazakhstan - also shows a high\nsimilarity score. On the other hand, in Figure 8,\nwe observe that ColPali is also capable of complex\nimage understanding. Not only are the patches con-\ntaining the word \"formulations\" highly similar to\nthe query token _formula , but so is the upper-left\nmolecule structure.\nIt is also interesting to highlight that both similar-\nity maps showcase a few white patches with high\nsimilarity scores. This behavior might first seem\nsurprising as the white patches should not carry a\nmeaningful signal from the original images.", "mimetype": "text/plain", "start_char_idx": 1892, "end_char_idx": 2728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "852b8105-35fc-4b37-84a8-07d97ade6e11": {"__data__": {"id_": "852b8105-35fc-4b37-84a8-07d97ade6e11", "embedding": null, "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b38c1ca7790a19f425297936bd2348778fc5a0f8523aa6fd7c8168f1e5d9194a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4e0a8cb-0b74-44a6-9646-ace0b17d7fc9", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "8ed8f25525b306520665b8176d01367364b387d2d76d66a416144171ff436569", "class_name": "RelatedNodeInfo"}}, "text": "We\nbelieve the vectors associated with these patches\nshare a similar role with the ViT registers (Darcet\net al., 2023), i.e. these patches were repurposed for\ninternal computations and stored the global infor-\nmation from the whole image.\n15", "mimetype": "text/plain", "start_char_idx": 2729, "end_char_idx": 2970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "40f3da90-4f47-42c5-8a4b-af0f5f6de6d6": {"__data__": {"id_": "40f3da90-4f47-42c5-8a4b-af0f5f6de6d6", "embedding": null, "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b38c1ca7790a19f425297936bd2348778fc5a0f8523aa6fd7c8168f1e5d9194a", "class_name": "RelatedNodeInfo"}}, "text": "You are an assistant specialized in document analy-\nsis. Given a table or a figure, you have to provide a\ndetailed summary of the content in maximum 3000\ncharacters. Your summary should be qualitative and\nnot quantitative. Here is the table/figure to analyze:\n{image}. Answer ONLY with the caption of the ta-\nble/figure.\nFigure 5: Example from the \"Energy\" test set.\nCaption: The image depicts the hourly energy gener-\nation profile, illustrating the contributions of various\nenergy sources over 24 hours. The data is presented\nas a stacked bar chart, with the x-axis representing the\nhours of the day from 1 to 2, and the y-axis showing\nthe average hourly generation in MW. The bars are seg-\nmented into different colors, each representing a distinct\nenergy source: nuclear, bio, geothermal, solar, wind, hy-\ndro, natural gas, and other imports. The chart provides\ninsights into the temporal variations in energy genera-\ntion across different sources, highlighting the interplay\nbetween baseload and intermittent sources throughout\nthe day.\nFigure 6: Example from the \"Government Reports\" test\nset.\nCaption: The image shows a table titled \"System of\nRecord\" which outlines the different types of documents\nor records maintained across various systems or depart-\nments within an organization related to project man-\nagement and construction. The rows list documents\nlike project plans, budgets, schedules, contracts, pur-\nchase orders, invoices, change requests, bid submissions,\ndrawings, manuals, meeting minutes, and reports. The\ncolumns indicate the system or department responsible\nfor maintaining each record, such as County Servers,\nProject View, OnBase, CGI Advantage Financial Sys-\ntem, and Purchasing Department. The table uses \"W\"\nand \"T\" markers to denote which system or department\nserves as the primary source (writer) or storage loca-\ntion (trailer) for each type of document.\nC More similarity maps\nIn Figure 7, ColPali assigns a high similarity to all\npatches with the word \"Kazakhstan\" when given\nthe token <_Kazakhstan> .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "641771b9-5abe-49c6-905f-d8b33cf929b3": {"__data__": {"id_": "641771b9-5abe-49c6-905f-d8b33cf929b3", "embedding": null, "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b38c1ca7790a19f425297936bd2348778fc5a0f8523aa6fd7c8168f1e5d9194a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40f3da90-4f47-42c5-8a4b-af0f5f6de6d6", "node_type": "1", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "0c64fa2fd38073d2f0627ba784f4ec628d4a8c85dff55e2482f6b78a1a33735b", "class_name": "RelatedNodeInfo"}}, "text": "Moreover, our model\nseems to exhibit world knowledge capabilities as\nthe patch around the word \"Kashagan\" - an off-\nshore oil field in Kazakhstan - also shows a high\nsimilarity score. On the other hand, in Figure 8,\nwe observe that ColPali is also capable of complex\nimage understanding. Not only are the patches con-\ntaining the word \"formulations\" highly similar to\nthe query token _formula , but so is the upper-left\nmolecule structure.\nIt is also interesting to highlight that both similar-\nity maps showcase a few white patches with high\nsimilarity scores. This behavior might first seem\nsurprising as the white patches should not carry a\nmeaningful signal from the original images. We\nbelieve the vectors associated with these patches\nshare a similar role with the ViT registers (Darcet\net al., 2023), i.e. these patches were repurposed for\ninternal computations and stored the global infor-\nmation from the whole image.\n15", "mimetype": "text/plain", "start_char_idx": 2041, "end_char_idx": 2970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945": {"__data__": {"id_": "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945", "embedding": null, "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb", "node_type": "4", "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "b38c1ca7790a19f425297936bd2348778fc5a0f8523aa6fd7c8168f1e5d9194a", "class_name": "RelatedNodeInfo"}}, "text": "You are an assistant specialized in document analy-\nsis. Given a table or a figure, you have to provide a\ndetailed summary of the content in maximum 3000\ncharacters. Your summary should be qualitative and\nnot quantitative. Here is the table/figure to analyze:\n{image}. Answer ONLY with the caption of the ta-\nble/figure.\nFigure 5: Example from the \"Energy\" test set.\nCaption: The image depicts the hourly energy gener-\nation profile, illustrating the contributions of various\nenergy sources over 24 hours. The data is presented\nas a stacked bar chart, with the x-axis representing the\nhours of the day from 1 to 2, and the y-axis showing\nthe average hourly generation in MW. The bars are seg-\nmented into different colors, each representing a distinct\nenergy source: nuclear, bio, geothermal, solar, wind, hy-\ndro, natural gas, and other imports. The chart provides\ninsights into the temporal variations in energy genera-\ntion across different sources, highlighting the interplay\nbetween baseload and intermittent sources throughout\nthe day.\nFigure 6: Example from the \"Government Reports\" test\nset.\nCaption: The image shows a table titled \"System of\nRecord\" which outlines the different types of documents\nor records maintained across various systems or depart-\nments within an organization related to project man-\nagement and construction. The rows list documents\nlike project plans, budgets, schedules, contracts, pur-\nchase orders, invoices, change requests, bid submissions,\ndrawings, manuals, meeting minutes, and reports. The\ncolumns indicate the system or department responsible\nfor maintaining each record, such as County Servers,\nProject View, OnBase, CGI Advantage Financial Sys-\ntem, and Purchasing Department. The table uses \"W\"\nand \"T\" markers to denote which system or department\nserves as the primary source (writer) or storage loca-\ntion (trailer) for each type of document.\nC More similarity maps\nIn Figure 7, ColPali assigns a high similarity to all\npatches with the word \"Kazakhstan\" when given\nthe token <_Kazakhstan> . Moreover, our model\nseems to exhibit world knowledge capabilities as\nthe patch around the word \"Kashagan\" - an off-\nshore oil field in Kazakhstan - also shows a high\nsimilarity score. On the other hand, in Figure 8,\nwe observe that ColPali is also capable of complex\nimage understanding. Not only are the patches con-\ntaining the word \"formulations\" highly similar to\nthe query token _formula , but so is the upper-left\nmolecule structure.\nIt is also interesting to highlight that both similar-\nity maps showcase a few white patches with high\nsimilarity scores. This behavior might first seem\nsurprising as the white patches should not carry a\nmeaningful signal from the original images. We\nbelieve the vectors associated with these patches\nshare a similar role with the ViT registers (Darcet\net al., 2023), i.e. these patches were repurposed for\ninternal computations and stored the global infor-\nmation from the whole image.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a9c52cf6-8d73-451a-8057-a2193466b478": {"__data__": {"id_": "a9c52cf6-8d73-451a-8057-a2193466b478", "embedding": null, "metadata": {"page_label": "16", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdc0e7c7-5d52-4bca-b2a4-29565c8f2ce4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a6414dc815c41834bcd052767441b0bc6a58c3f409e8ddcb57d32cf9f15f058c", "class_name": "RelatedNodeInfo"}}, "text": "Figure 7: Similarity of the image patches w.r.t. the\nunderlined token in the user query. This example is\nfrom the Shift test set.\nFigure 8: Similarity of the image patches w.r.t. the\nunderlined token in the user query. This example is\nfrom the Healthcare Industry test set.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9583a3d8-fa88-470d-bdfd-50b7387e9d67", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "76797836-1114-4276-9b42-edaad528ac64": {"__data__": {"id_": "76797836-1114-4276-9b42-edaad528ac64", "embedding": null, "metadata": {"page_label": "16", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdc0e7c7-5d52-4bca-b2a4-29565c8f2ce4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a6414dc815c41834bcd052767441b0bc6a58c3f409e8ddcb57d32cf9f15f058c", "class_name": "RelatedNodeInfo"}}, "text": "Figure 7: Similarity of the image patches w.r.t. the\nunderlined token in the user query. This example is\nfrom the Shift test set.\nFigure 8: Similarity of the image patches w.r.t. the\nunderlined token in the user query. This example is\nfrom the Healthcare Industry test set.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9583a3d8-fa88-470d-bdfd-50b7387e9d67", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "9583a3d8-fa88-470d-bdfd-50b7387e9d67": {"__data__": {"id_": "9583a3d8-fa88-470d-bdfd-50b7387e9d67", "embedding": null, "metadata": {"page_label": "16", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdc0e7c7-5d52-4bca-b2a4-29565c8f2ce4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a6414dc815c41834bcd052767441b0bc6a58c3f409e8ddcb57d32cf9f15f058c", "class_name": "RelatedNodeInfo"}}, "text": "Figure 7: Similarity of the image patches w.r.t. the\nunderlined token in the user query. This example is\nfrom the Shift test set.\nFigure 8: Similarity of the image patches w.r.t. the\nunderlined token in the user query. This example is\nfrom the Healthcare Industry test set.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "9583a3d8-fa88-470d-bdfd-50b7387e9d67", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "69ef3de6-6b7e-4742-94f7-60babba03207": {"__data__": {"id_": "69ef3de6-6b7e-4742-94f7-60babba03207", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}}, "text": "D Additional results\nD.1 Other Metrics\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.\nUnstructured Text only\nBM25 - 26.6 - - 34.6 45.0 86.0 70.0 68.0 74.0 -\nBGE-M3 - 22.8 \u21933.8- - 26.1 \u21938.551.0 \u21916.081.0 \u21935.072.0 \u21912.067.0 \u21931.077.0 \u21913.0 -\nUnstructured + OCR\nBM25 26.7 28.9 54.0 30.4 50.0 52.0 86.0 77.0 74.0 80.0 55.9\nBGE-M3 28.1 \u21911.422.9 \u21936.053.8 \u21930.255.7 \u219125.338.6 \u219311.456.0 \u21914.082.0 \u21934.079.0 \u21912.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "297e8654-d4af-4953-93f6-113e68f13be2": {"__data__": {"id_": "297e8654-d4af-4953-93f6-113e68f13be2", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69ef3de6-6b7e-4742-94f7-60babba03207", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3646b31260b5a72210318b85c17840c3daa16b7b994268cd273ae0ae2c7cdb11", "class_name": "RelatedNodeInfo"}}, "text": "456.0 \u21914.082.0 \u21934.079.0 \u21912.076.0 \u21912.083.0 \u21913.0 57.5 \u21911.6\nUnstructured + Captioning\nBM25 35.5 30.2 61.5 24.3 49.0 47.0 79.0 76.0 75.0 81.0 55.9\nBGE-M3 29.3 \u21936.226.0 \u21934.262.1 \u21910.658.6 \u219134.330.6 \u219318.455.0 \u21918.080.0 \u21911.078.0 \u21912.069.0 \u21936.083.0 \u21912.0 57.2 \u21911.3\nContrastive VLMs\nJina-CLIP 19.4 7.3 26.7 12.5 1.6 2.0 11.0 13.0 15.0 17.0 12.6\nNomic-vision 10.", "mimetype": "text/plain", "start_char_idx": 383, "end_char_idx": 731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "6272b04c-3104-401d-8688-6e860b899445": {"__data__": {"id_": "6272b04c-3104-401d-8688-6e860b899445", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "297e8654-d4af-4953-93f6-113e68f13be2", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "a7f8594a1f4376ae2140c537882300c935c0ce8e1ff49dab8961b579fe9ef8db", "class_name": "RelatedNodeInfo"}}, "text": "0 17.0 12.6\nNomic-vision 10.4 6.7 22.1 9.6 1.6 0.0 9.0 9.0 7.0 13.0 8.8\nSigLIP (Vanilla) 34.2 21.3 51.8 46.1 17.9 13.0 50.0 51.0 47.0 65.0 39.7\nOurs\n(Copied) SigLIP (Vanilla) 34.2 21.3 51.8 46.1 17.9 13.0 50.0 51.0 47.0 65.0 39.7\nBiSigLIP (+fine-tuning) 49.2 \u219115.023.8 \u21912.559.0 \u21917.252.1 \u21916.020.7 \u21912.816.0 \u21913.062.0 \u219112.061.0 \u219110.055.", "mimetype": "text/plain", "start_char_idx": 703, "end_char_idx": 1035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "a15afcee-cb0c-421c-9ba9-aae3a07314e4": {"__data__": {"id_": "a15afcee-cb0c-421c-9ba9-aae3a07314e4", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6272b04c-3104-401d-8688-6e860b899445", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "50f01d074daa22cad2f48be03db558775fad0e7d6e083b8da558d9825f9d4880", "class_name": "RelatedNodeInfo"}}, "text": "816.0 \u21913.062.0 \u219112.061.0 \u219110.055.0 \u21918.072.0 \u21917.0 47.1 \u21917.4\nBiPali (+LLM) 46.4 \u2193-2.820.0 \u2193-3.854.6 \u2193-4.463.2 \u219111.120.4 \u2193-0.434.0 \u219118.059.0 \u2193-3.045.0 \u2193-16.057.0 \u21912.056.0 \u2193-16.0 45.6 \u2193-1.5\nColPali (+Late Inter.)", "mimetype": "text/plain", "start_char_idx": 1002, "end_char_idx": 1210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "56e1fef3-d519-4d6d-a658-9614f4a650c7": {"__data__": {"id_": "56e1fef3-d519-4d6d-a658-9614f4a650c7", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a15afcee-cb0c-421c-9ba9-aae3a07314e4", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "86a15df08dd6b1d9ec24bbeb4b4931918242c20e1ca8e0f46813099ce50a2923", "class_name": "RelatedNodeInfo"}}, "text": "0 45.6 \u2193-1.5\nColPali (+Late Inter.) 72.4 \u219126.045.6 \u219125.674.6 \u219120.075.4 \u219112.153.1 \u219132.755.0 \u219121.093.0 \u219134.085.0 \u219140.085.0 \u219128.088.0 \u219132.0 72.7 \u219127.1\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\nD.2 Model Variants\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.", "mimetype": "text/plain", "start_char_idx": 1175, "end_char_idx": 1614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "7b7f14fe-7dc6-4f00-a54d-61ff21090099": {"__data__": {"id_": "7b7f14fe-7dc6-4f00-a54d-61ff21090099", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}}, "text": "D Additional results\nD.1 Other Metrics\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.\nUnstructured Text only\nBM25 - 26.6 - - 34.6 45.0 86.0 70.0 68.0 74.0 -\nBGE-M3 - 22.8 \u21933.8- - 26.1 \u21938.551.0 \u21916.081.0 \u21935.072.0 \u21912.067.0 \u21931.077.0 \u21913.0 -\nUnstructured + OCR\nBM25 26.7 28.9 54.0 30.4 50.0 52.0 86.0 77.0 74.0 80.0 55.9\nBGE-M3 28.1 \u21911.422.9 \u21936.053.8 \u21930.255.7 \u219125.338.6 \u219311.456.0 \u21914.082.0 \u21934.079.0 \u21912.076.0 \u21912.083.0 \u21913.0 57.5 \u21911.6\nUnstructured + Captioning\nBM25 35.5 30.2 61.5 24.3 49.0 47.0 79.0 76.0 75.0 81.0 55.9\nBGE-M3 29.3 \u21936.226.0 \u21934.262.1 \u21910.658.6 \u219134.330.6 \u219318.455.0 \u21918.080.0 \u21911.078.0 \u21912.069.0 \u21936.083.0 \u21912.0 57.2 \u21911.3\nContrastive VLMs\nJina-CLIP 19.4 7.3 26.7 12.5 1.6 2.0 11.0 13.0 15.0 17.0 12.6\nNomic-vision 10.4 6.7 22.1 9.6 1.6 0.0 9.0 9.0 7.0 13.0 8.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "99d19f7a-a851-48f9-bab7-59d5c2e01a86": {"__data__": {"id_": "99d19f7a-a851-48f9-bab7-59d5c2e01a86", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b7f14fe-7dc6-4f00-a54d-61ff21090099", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "59f90dbc83d48dbf7a01c79a39b72136e67e441c2cbad2f19a4e97c81b19a6a4", "class_name": "RelatedNodeInfo"}}, "text": "0 9.0 9.0 7.0 13.0 8.8\nSigLIP (Vanilla) 34.2 21.3 51.8 46.1 17.9 13.0 50.0 51.0 47.0 65.0 39.7\nOurs\n(Copied) SigLIP (Vanilla) 34.2 21.3 51.8 46.1 17.9 13.0 50.0 51.0 47.0 65.0 39.7\nBiSigLIP (+fine-tuning) 49.2 \u219115.023.8 \u21912.559.0 \u21917.252.1 \u21916.020.7 \u21912.816.0 \u21913.062.0 \u219112.061.0 \u219110.055.0 \u21918.072.0 \u21917.0 47.1 \u21917.4\nBiPali (+LLM) 46.4 \u2193-2.820.0 \u2193-3.854.6 \u2193-4.463.2 \u219111.120.4 \u2193-0.434.0 \u219118.059.0 \u2193-3.045.0 \u2193-16.057.0 \u21912.056.0 \u2193-16.0 45.6 \u2193-1.5\nColPali (+Late Inter.) 72.4 \u219126.045.6 \u219125.674.6 \u219120.075.4 \u219112.153.1 \u219132.755.0 \u219121.093.0 \u219134.085.0 \u219140.085.0 \u219128.088.0 \u219132.0 72.7 \u219127.1\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\nD.2 Model Variants\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.", "mimetype": "text/plain", "start_char_idx": 752, "end_char_idx": 1614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "0c7c02db-1a66-47f9-b394-4cce0dc7f87b": {"__data__": {"id_": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4916c6b9-ac86-4312-9c75-72461297d832", "node_type": "1", "metadata": {}, "hash": "9335de9eceb10fe7e5e89b7f9322d41afe1bf2107346e1bc956df51440538b48", "class_name": "RelatedNodeInfo"}}, "text": "D Additional results\nD.1 Other Metrics\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.\nUnstructured Text only\nBM25 - 26.6 - - 34.6 45.0 86.0 70.0 68.0 74.0 -\nBGE-M3 - 22.8 \u21933.8- - 26.1 \u21938.551.0 \u21916.081.0 \u21935.072.0 \u21912.067.0 \u21931.077.0 \u21913.0 -\nUnstructured + OCR\nBM25 26.7 28.9 54.0 30.4 50.0 52.0 86.0 77.0 74.0 80.0 55.9\nBGE-M3 28.1 \u21911.422.9 \u21936.053.8 \u21930.255.7 \u219125.338.6 \u219311.456.0 \u21914.082.0 \u21934.079.0 \u21912.076.0 \u21912.083.0 \u21913.0 57.5 \u21911.6\nUnstructured + Captioning\nBM25 35.5 30.2 61.5 24.3 49.0 47.0 79.0 76.0 75.0 81.0 55.9\nBGE-M3 29.3 \u21936.226.0 \u21934.262.1 \u21910.658.6 \u219134.330.6 \u219318.455.0 \u21918.080.0 \u21911.078.0 \u21912.069.0 \u21936.083.0 \u21912.0 57.2 \u21911.3\nContrastive VLMs\nJina-CLIP 19.4 7.3 26.7 12.5 1.6 2.0 11.0 13.0 15.0 17.0 12.6\nNomic-vision 10.4 6.7 22.1 9.6 1.6 0.0 9.0 9.0 7.0 13.0 8.8\nSigLIP (Vanilla) 34.2 21.3 51.8 46.1 17.9 13.0 50.0 51.0 47.0 65.0 39.7\nOurs\n(Copied) SigLIP (Vanilla) 34.2 21.3 51.8 46.1 17.9 13.0 50.0 51.0 47.0 65.0 39.7\nBiSigLIP (+fine-tuning) 49.2 \u219115.023.8 \u21912.559.0 \u21917.252.1 \u21916.020.7 \u21912.816.0 \u21913.062.0 \u219112.061.0 \u219110.055.0 \u21918.072.0 \u21917.0 47.1 \u21917.4\nBiPali (+LLM) 46.4 \u2193-2.820.0 \u2193-3.854.6 \u2193-4.463.2 \u219111.120.4 \u2193-0.434.0 \u219118.059.0 \u2193-3.045.0 \u2193-16.057.0 \u21912.056.0 \u2193-16.0 45.6 \u2193-1.5\nColPali (+Late Inter.) 72.4 \u219126.045.6 \u219125.674.6 \u219120.075.4 \u219112.153.1 \u219132.755.0 \u219121.093.0 \u219134.085.0 \u219140.085.0 \u219128.088.0 \u219132.0 72.7 \u219127.1\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\nD.2 Model Variants\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "cf69cb07-f436-444d-9cf8-f739ae111f77": {"__data__": {"id_": "cf69cb07-f436-444d-9cf8-f739ae111f77", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}}, "text": "72.4 \u219126.045.6 \u219125.674.6 \u219120.075.4 \u219112.153.1 \u219132.755.0 \u219121.093.0 \u219134.085.0 \u219140.085.0 \u219128.088.0 \u219132.0 72.7 \u219127.1\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\nD.2 Model Variants\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4916c6b9-ac86-4312-9c75-72461297d832", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d6cfe3c2-94d9-4b37-8078-f5e29ac8caea": {"__data__": {"id_": "d6cfe3c2-94d9-4b37-8078-f5e29ac8caea", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf69cb07-f436-444d-9cf8-f739ae111f77", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "088f991294e7eff447ac977b12894b879e0ad38659c30de23402b28516e280cd", "class_name": "RelatedNodeInfo"}}, "text": "Health. Avg.\nColSigLIP (PaliGemma) 3.1 3.0 5.1 6.2 2.5 1.0 3.4 3.4 2.3 2.2 3.2\nBiSigLIP (PaliGemma) 18.5 14.6 33.4 39.5 16.1 5.2 27.6 32.6 36.6 35.7 26.0\nColSigLIP (Original) 2.6 2.2 2.3 5.7 1.8 1.0 2.6 4.1 1.4 1.5 2.5\nColPali (No Mem.", "mimetype": "text/plain", "start_char_idx": 391, "end_char_idx": 626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4916c6b9-ac86-4312-9c75-72461297d832", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "67842a45-8fac-4e05-906a-8f7a344ed3d6": {"__data__": {"id_": "67842a45-8fac-4e05-906a-8f7a344ed3d6", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6cfe3c2-94d9-4b37-8078-f5e29ac8caea", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "3665159fd339ba16366de821320aa389ea1a60e2c7c6c7bc10d8877a57d82ce9", "class_name": "RelatedNodeInfo"}}, "text": "Tokens) 80.4 53.2 82.4 77.4 65.7 63.4 97.0 89.9 93.6 92.4 79.6\nColPali (Best) 79.1 54.4 81.8 83.9 65.8 73.2 96.2 91.0 92.7 94.4 81.3\nTable 5: Evaluation of some \"negative results\" and ablations on ViDoRe ;ColPali for reference. Results are\npresented using NDCG@5 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\n17", "mimetype": "text/plain", "start_char_idx": 627, "end_char_idx": 978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4916c6b9-ac86-4312-9c75-72461297d832", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "3142e38e-36c7-4529-bd56-ec6120174492": {"__data__": {"id_": "3142e38e-36c7-4529-bd56-ec6120174492", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}}, "text": "72.4 \u219126.045.6 \u219125.674.6 \u219120.075.4 \u219112.153.1 \u219132.755.0 \u219121.093.0 \u219134.085.0 \u219140.085.0 \u219128.088.0 \u219132.0 72.7 \u219127.1\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\nD.2 Model Variants\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.\nColSigLIP (PaliGemma) 3.1 3.0 5.1 6.2 2.5 1.0 3.4 3.4 2.3 2.2 3.2\nBiSigLIP (PaliGemma) 18.5 14.6 33.4 39.5 16.1 5.2 27.6 32.6 36.6 35.7 26.0\nColSigLIP (Original) 2.6 2.2 2.3 5.7 1.8 1.0 2.6 4.1 1.4 1.5 2.5\nColPali (No Mem. Tokens) 80.4 53.2 82.4 77.4 65.7 63.4 97.0 89.9 93.6 92.4 79.6\nColPali (Best) 79.1 54.4 81.8 83.9 65.8 73.2 96.2 91.0 92.7 94.4 81.3\nTable 5: Evaluation of some \"negative results\" and ablations on ViDoRe ;ColPali for reference. Results are\npresented using NDCG@5 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4916c6b9-ac86-4312-9c75-72461297d832", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "4916c6b9-ac86-4312-9c75-72461297d832": {"__data__": {"id_": "4916c6b9-ac86-4312-9c75-72461297d832", "embedding": null, "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23", "node_type": "4", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "265a5e64a35ebd063e4509f711485f043fe8fea9706efcb5d6c05bb986a12ee9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "node_type": "1", "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "267b10800f2abfd1c4b98170263d9ac2a266c88d74abf030a1f491cf14583d0e", "class_name": "RelatedNodeInfo"}}, "text": "72.4 \u219126.045.6 \u219125.674.6 \u219120.075.4 \u219112.153.1 \u219132.755.0 \u219121.093.0 \u219134.085.0 \u219140.085.0 \u219128.088.0 \u219132.0 72.7 \u219127.1\nTable 4: Comprehensive evaluation of baseline models and our proposed method on ViDoRe .Results are\npresented using Recall@1 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\nD.2 Model Variants\nArxivQ DocQ InfoQ TabF TATQ Shift AI Energy Gov. Health. Avg.\nColSigLIP (PaliGemma) 3.1 3.0 5.1 6.2 2.5 1.0 3.4 3.4 2.3 2.2 3.2\nBiSigLIP (PaliGemma) 18.5 14.6 33.4 39.5 16.1 5.2 27.6 32.6 36.6 35.7 26.0\nColSigLIP (Original) 2.6 2.2 2.3 5.7 1.8 1.0 2.6 4.1 1.4 1.5 2.5\nColPali (No Mem. Tokens) 80.4 53.2 82.4 77.4 65.7 63.4 97.0 89.9 93.6 92.4 79.6\nColPali (Best) 79.1 54.4 81.8 83.9 65.8 73.2 96.2 91.0 92.7 94.4 81.3\nTable 5: Evaluation of some \"negative results\" and ablations on ViDoRe ;ColPali for reference. Results are\npresented using NDCG@5 metrics. Text-only metrics are not computed for benchmarks with only visual elements.\n17", "mimetype": "text/plain", "start_char_idx": 1211, "end_char_idx": 2189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "4916c6b9-ac86-4312-9c75-72461297d832", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "85cd3106-1457-4da8-95fe-bfda6b641292": {"__data__": {"id_": "85cd3106-1457-4da8-95fe-bfda6b641292", "embedding": null, "metadata": {"page_label": "18", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bc3c38f-1ec7-46eb-933c-7198e45be35a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "51a211486bec982eef2fc6e23b227983343fc7967fccdfdf3589f349744f5e7d", "class_name": "RelatedNodeInfo"}}, "text": "EViDoRe examples\nEnergy\nQuery : What types of accounts or\nproducts allow investors to defer pay-\ning taxes?\nQuery : What is the estimated total sav-\nings for a PV system in Durham under\nthe net metering (flat rate) billing op-\ntion over the system\u2019s useful life of 25\nyears?\nQuery : What is the projected peak\nelectricity demand in California for the\nyear 2030?\nArtificial Intelligence\nQuery : What are some common out-\ncome areas targeted by TAII for differ-\nent age groups?\nQuery : What did the robot monitor to\ndetermine when to activate or deacti-\nvate the blower motor and blinker?\nQuery : What is the key approach used\nin the PDP architecture?\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "b1ba75bc-458b-45f7-9060-5615b53b9a8d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f393cbd7-fb20-477c-b7cb-8fc3b1d0da83": {"__data__": {"id_": "f393cbd7-fb20-477c-b7cb-8fc3b1d0da83", "embedding": null, "metadata": {"page_label": "18", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bc3c38f-1ec7-46eb-933c-7198e45be35a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "51a211486bec982eef2fc6e23b227983343fc7967fccdfdf3589f349744f5e7d", "class_name": "RelatedNodeInfo"}}, "text": "EViDoRe examples\nEnergy\nQuery : What types of accounts or\nproducts allow investors to defer pay-\ning taxes?\nQuery : What is the estimated total sav-\nings for a PV system in Durham under\nthe net metering (flat rate) billing op-\ntion over the system\u2019s useful life of 25\nyears?\nQuery : What is the projected peak\nelectricity demand in California for the\nyear 2030?\nArtificial Intelligence\nQuery : What are some common out-\ncome areas targeted by TAII for differ-\nent age groups?\nQuery : What did the robot monitor to\ndetermine when to activate or deacti-\nvate the blower motor and blinker?\nQuery : What is the key approach used\nin the PDP architecture?\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "b1ba75bc-458b-45f7-9060-5615b53b9a8d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "b1ba75bc-458b-45f7-9060-5615b53b9a8d": {"__data__": {"id_": "b1ba75bc-458b-45f7-9060-5615b53b9a8d", "embedding": null, "metadata": {"page_label": "18", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bc3c38f-1ec7-46eb-933c-7198e45be35a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "51a211486bec982eef2fc6e23b227983343fc7967fccdfdf3589f349744f5e7d", "class_name": "RelatedNodeInfo"}}, "text": "EViDoRe examples\nEnergy\nQuery : What types of accounts or\nproducts allow investors to defer pay-\ning taxes?\nQuery : What is the estimated total sav-\nings for a PV system in Durham under\nthe net metering (flat rate) billing op-\ntion over the system\u2019s useful life of 25\nyears?\nQuery : What is the projected peak\nelectricity demand in California for the\nyear 2030?\nArtificial Intelligence\nQuery : What are some common out-\ncome areas targeted by TAII for differ-\nent age groups?\nQuery : What did the robot monitor to\ndetermine when to activate or deacti-\nvate the blower motor and blinker?\nQuery : What is the key approach used\nin the PDP architecture?\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "b1ba75bc-458b-45f7-9060-5615b53b9a8d", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "62c27968-04c5-4fad-9298-b0236b4cd4ce": {"__data__": {"id_": "62c27968-04c5-4fad-9298-b0236b4cd4ce", "embedding": null, "metadata": {"page_label": "19", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cf3b0b0-0931-4e79-bb17-3a77ea4e9ad3", "node_type": "4", "metadata": {"page_label": "19", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "d6474bdc70470cae55ffe27123c898135368708ad83aefbdb3b420e1b632bb6e", "class_name": "RelatedNodeInfo"}}, "text": "Healthcare Industry\nQuery : What is the chemical formula\nfor the ferroelectric material Lead Zir-\nconium Titanate (PZT)?\nQuery : What government entities are\ninvolved in public financing for health-\ncare in the US?\nQuery : What does the A VPU scale\nstand for in assessing the level of con-\nsciousness of a seriously ill child?\nGovernment Reports\nQuery : What are some mandates for\nthe EPA under the Pollution Prevention\nAct?\nQuery : What is the strategy of KPMG\nHazem Hassan?\nQuery : What is the trust signal score\nfor the consumer industry best-in-class\narchetype?\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3452640-e8cf-4e2c-8de9-06d639fd2f8f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "1ae0425d-221b-4726-8437-68ba916b0471": {"__data__": {"id_": "1ae0425d-221b-4726-8437-68ba916b0471", "embedding": null, "metadata": {"page_label": "19", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cf3b0b0-0931-4e79-bb17-3a77ea4e9ad3", "node_type": "4", "metadata": {"page_label": "19", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "d6474bdc70470cae55ffe27123c898135368708ad83aefbdb3b420e1b632bb6e", "class_name": "RelatedNodeInfo"}}, "text": "Healthcare Industry\nQuery : What is the chemical formula\nfor the ferroelectric material Lead Zir-\nconium Titanate (PZT)?\nQuery : What government entities are\ninvolved in public financing for health-\ncare in the US?\nQuery : What does the A VPU scale\nstand for in assessing the level of con-\nsciousness of a seriously ill child?\nGovernment Reports\nQuery : What are some mandates for\nthe EPA under the Pollution Prevention\nAct?\nQuery : What is the strategy of KPMG\nHazem Hassan?\nQuery : What is the trust signal score\nfor the consumer industry best-in-class\narchetype?\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3452640-e8cf-4e2c-8de9-06d639fd2f8f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "f3452640-e8cf-4e2c-8de9-06d639fd2f8f": {"__data__": {"id_": "f3452640-e8cf-4e2c-8de9-06d639fd2f8f", "embedding": null, "metadata": {"page_label": "19", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cf3b0b0-0931-4e79-bb17-3a77ea4e9ad3", "node_type": "4", "metadata": {"page_label": "19", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "d6474bdc70470cae55ffe27123c898135368708ad83aefbdb3b420e1b632bb6e", "class_name": "RelatedNodeInfo"}}, "text": "Healthcare Industry\nQuery : What is the chemical formula\nfor the ferroelectric material Lead Zir-\nconium Titanate (PZT)?\nQuery : What government entities are\ninvolved in public financing for health-\ncare in the US?\nQuery : What does the A VPU scale\nstand for in assessing the level of con-\nsciousness of a seriously ill child?\nGovernment Reports\nQuery : What are some mandates for\nthe EPA under the Pollution Prevention\nAct?\nQuery : What is the strategy of KPMG\nHazem Hassan?\nQuery : What is the trust signal score\nfor the consumer industry best-in-class\narchetype?\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "f3452640-e8cf-4e2c-8de9-06d639fd2f8f", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "ea135e51-f890-4440-a2d3-11ee1284be76": {"__data__": {"id_": "ea135e51-f890-4440-a2d3-11ee1284be76", "embedding": null, "metadata": {"page_label": "20", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d3bf4a-2890-4bb2-b3fc-9f8735b2f4af", "node_type": "4", "metadata": {"page_label": "20", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7894eb67ec7697a68c2c2402c7ee682e96bac761111621344d677f36b1ec1aff", "class_name": "RelatedNodeInfo"}}, "text": "Shift\nQuery : Selon le graphique, quelle\nest la capacit\u00e9 d\u2019import et la consom-\nmation r\u00e9elle de carburants SAF (bio-\ncarburants durables pour l\u2019aviation)\npr\u00e9vues en 2050 ?\nQuery : Quelle partie de la production\np\u00e9troli\u00e8re du Kazakhstan provient de\nchamps en mer ?\nQuery : Quels sont les pays ayant la\nplus grande part des d\u00e9couvertes cu-\nmul\u00e9es de p\u00e9trole brut en 2020 (en\nmilliers de barils, hors d\u00e9couvertes cu-\nmul\u00e9es) ?\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "da5aa105-0ca3-4a65-a43f-23b180cddc74", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "d79ce0e6-15c0-4850-86b5-2adc09ee0875": {"__data__": {"id_": "d79ce0e6-15c0-4850-86b5-2adc09ee0875", "embedding": null, "metadata": {"page_label": "20", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d3bf4a-2890-4bb2-b3fc-9f8735b2f4af", "node_type": "4", "metadata": {"page_label": "20", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7894eb67ec7697a68c2c2402c7ee682e96bac761111621344d677f36b1ec1aff", "class_name": "RelatedNodeInfo"}}, "text": "Shift\nQuery : Selon le graphique, quelle\nest la capacit\u00e9 d\u2019import et la consom-\nmation r\u00e9elle de carburants SAF (bio-\ncarburants durables pour l\u2019aviation)\npr\u00e9vues en 2050 ?\nQuery : Quelle partie de la production\np\u00e9troli\u00e8re du Kazakhstan provient de\nchamps en mer ?\nQuery : Quels sont les pays ayant la\nplus grande part des d\u00e9couvertes cu-\nmul\u00e9es de p\u00e9trole brut en 2020 (en\nmilliers de barils, hors d\u00e9couvertes cu-\nmul\u00e9es) ?\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "da5aa105-0ca3-4a65-a43f-23b180cddc74", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}, "da5aa105-0ca3-4a65-a43f-23b180cddc74": {"__data__": {"id_": "da5aa105-0ca3-4a65-a43f-23b180cddc74", "embedding": null, "metadata": {"page_label": "20", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40d3bf4a-2890-4bb2-b3fc-9f8735b2f4af", "node_type": "4", "metadata": {"page_label": "20", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}, "hash": "7894eb67ec7697a68c2c2402c7ee682e96bac761111621344d677f36b1ec1aff", "class_name": "RelatedNodeInfo"}}, "text": "Shift\nQuery : Selon le graphique, quelle\nest la capacit\u00e9 d\u2019import et la consom-\nmation r\u00e9elle de carburants SAF (bio-\ncarburants durables pour l\u2019aviation)\npr\u00e9vues en 2050 ?\nQuery : Quelle partie de la production\np\u00e9troli\u00e8re du Kazakhstan provient de\nchamps en mer ?\nQuery : Quels sont les pays ayant la\nplus grande part des d\u00e9couvertes cu-\nmul\u00e9es de p\u00e9trole brut en 2020 (en\nmilliers de barils, hors d\u00e9couvertes cu-\nmul\u00e9es) ?\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "index_id": "da5aa105-0ca3-4a65-a43f-23b180cddc74", "obj": null, "class_name": "IndexNode"}, "__type__": "3"}}, "docstore/metadata": {"936122c6-bbe4-47a9-b179-1c52d61b8ab8": {"doc_hash": "324f77e33d7a69a10c1d94e96aa9aa2462213bb9dc4d67a18af8a2b5bd6ac505", "ref_doc_id": "5845b888-160d-42c4-b417-c8b5383922d1"}, "9ae6faaf-3449-4b0f-8674-c3e403c758db": {"doc_hash": "3bb2a08ce240ca18ae71da22b61a32a09dc8b928ac507d5b410eb84a446b8cda", "ref_doc_id": "5845b888-160d-42c4-b417-c8b5383922d1"}, "c9bc29a3-4e6d-48a3-bf2a-c12ab7dad273": {"doc_hash": "1672500e62ad5e58ccaa3b6112be2765c4485f733904b108715dd261ae7b78cf", "ref_doc_id": "5845b888-160d-42c4-b417-c8b5383922d1"}, "144ee7b8-5274-4ecf-ad41-3bf4690cdaa6": {"doc_hash": "bda639a74719f15b83aff253b8b22e4c5336c313862f3eb30f091be89663bb1e", "ref_doc_id": "5845b888-160d-42c4-b417-c8b5383922d1"}, "ca1986e3-4326-4915-8fb0-dd26cf3e400e": {"doc_hash": "33602be3d70da8b68d93db4a90ae6d89018245f6e24e9915a52033d8b8e5dc2a", "ref_doc_id": "5845b888-160d-42c4-b417-c8b5383922d1"}, "b27da0fb-cf4e-4487-affa-b9e4e8fe6e96": {"doc_hash": "df33dbb2e2d44e1fb04b0dd884e2901682e86ec8b238b322e047df321673e3a7", "ref_doc_id": "5845b888-160d-42c4-b417-c8b5383922d1"}, "e481ea7a-9638-479c-9958-7cc07d1465cb": {"doc_hash": "ef6afb73363bec2b7545f549f87f65a6f0b896bfbda58fac205da6f6ed18e115", "ref_doc_id": "5845b888-160d-42c4-b417-c8b5383922d1"}, "fb4e7299-13d2-4f24-93cc-8bceb1cb2f4d": {"doc_hash": "16d8c32cb8738e1ea29d433720a10e255fa0e2b3b308f1006064f697972627da", "ref_doc_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8"}, "060b3ef8-9dfd-4716-87d1-c172674d9d98": {"doc_hash": "b9e7299f8fcd94057f6ff71bd304a9f4e7fc7db8b053686fba50bb0325aa7bae", "ref_doc_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8"}, "589a3a9d-81fa-4608-8cb1-9ccf67b460e1": {"doc_hash": "c8a429d713cbd2f384dd9548ae04091d9f6675017b54d0661c4ceadfd9e28721", "ref_doc_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8"}, "487962b6-0855-408a-9602-8a5d26045ef2": {"doc_hash": "fbddf94ad021d25829e7dbe1e7e20ef3fbef9eec62f3c8b0d77b3ab19a75e612", "ref_doc_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8"}, "d094814c-2629-494e-8db3-fe3948f4b2e3": {"doc_hash": "7ed786d5fa01df72ebc23cc4ac88cc4179b475f90411aa747a938a29b12db95c", "ref_doc_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8"}, "aea9a68f-94a7-4088-952e-d4db1a7ca183": {"doc_hash": "9d8a5ed445f42beb1af7a184b187bcf0d331bd68273e9e5fe3981e97fc967a2e", "ref_doc_id": "62f785d6-9fd9-4df3-a8da-ad1772252ee8"}, "1da1bf8a-e192-436f-bc1a-16232efd2be8": {"doc_hash": "6c854c8f570252c3f764a5153462142bc298d1aaa9d6d7cc48761a0046090431", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "cf898f94-26f0-4099-b27b-ceebef62dc0f": {"doc_hash": "a2b800759e11e06d0db621935393173289554e0fe32c9bbf90572619729254aa", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "eb75fe4d-4e03-45f3-aa0f-38f4ac61ca1c": {"doc_hash": "2920767117000799af226fbc24c96f404de46a28e534c3c8d045d3a4415d7442", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "7f0828e9-b3b9-4c37-9d45-4821dac1db97": {"doc_hash": "7daa6aa98dd0dcdd990d76ac3355a6b0fee55348540ea8342a6a258f7013297f", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "d612872a-3784-44ec-aeda-36f337d6e67f": {"doc_hash": "1f09f0f8b378008913ffb66036fa4e9b2e58db9b2448d0015e588086380beec7", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "441bb3f0-852d-4f47-b772-c9f867f9f398": {"doc_hash": "ae7c5da7858944f69123e9862c8ae299f3b80dec84ad3e53462429bc164f47de", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "7c753d2e-50df-4577-8267-c678d33d2fdf": {"doc_hash": "c48529f2aae760a841ffeb5c179d0186da87627da093fbc324bbdf3f77902cc5", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "9b2a794c-758d-412d-8c7b-9ea1f4e6a2a1": {"doc_hash": "d195a5070b9214d62f31951fa59109274dd28dece88cb463707d059747affd80", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "2b97283b-f755-4510-a6b7-d30d99f25e5a": {"doc_hash": "a18605c1406d5e408f4c441589ec29dcac966989ccd8b651d9f5e47111ae4f90", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "843197ad-92c9-4210-9fa1-fd11a9d95e73": {"doc_hash": "f468f531e8ffe22007d8523f88807278c5508ccebe5d948bfeb7b845d31a0fcf", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "06273651-20a8-4a8d-b3a9-3cb80c1f6fb3": {"doc_hash": "f66e9c113de32eec8f80ee0e5d8014d776654f369a8c66b534cd6c77ff5e673f", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "6a0d5d51-b833-4a60-a353-5891e9338c8d": {"doc_hash": "d8d05d84a21677ea5739403d4b22b4f1cdafc182ee264658cb8de110c7fe1668", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "5dd41d34-dc98-48de-95a7-d3a4c4254f9d": {"doc_hash": "d8d05d84a21677ea5739403d4b22b4f1cdafc182ee264658cb8de110c7fe1668", "ref_doc_id": "7b2336a7-7692-4d11-a9b4-52bd6dc4b606"}, "17b5ce96-f5a4-4ff3-b554-afc8e7270647": {"doc_hash": "d776b04de43d67c6514ebfa9ea3111575ac75c25790e1f2d6a74500325f89df2", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "7d091e6d-d673-4d1a-ba1f-e77eb2a832db": {"doc_hash": "5d0a6d172664180f4a35a5dacfb641f20851224328840195488724a825925ad4", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "234c2977-6b8b-4bc7-8c77-6422aea915ce": {"doc_hash": "a556d36d1ede60666b4a2f4997f733ba20598a9062ff0b9cda8374d698c276b4", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "89c39124-b4ea-4069-97ad-d515dfc2cb04": {"doc_hash": "0bafbac94372ae80e6d86cd8941666226c565e487870aff2da42378911d42794", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "c6697468-8579-4b30-bb2c-b8fd75849702": {"doc_hash": "4fbabb5f4c8f9ee73c51d594f1a7e22425193cb6a4413ae0be57e52cc2526db8", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "b5e38a5b-ea8e-4432-931f-ee69ea9cdc5e": {"doc_hash": "35b01f065974e25fbe5f9314ffdca794eb4d3847dce35741a905338f6faae132", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "39aebc4c-9037-4c1c-8911-bd8e62a7510f": {"doc_hash": "08adb99ba2b8c8e332a32da0f8b6abf1c2d094f8c1bd9d39bfd87dd001959851", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "87bd1f7c-4867-42b2-ae7e-ad27b0fb797f": {"doc_hash": "84864b50475fdc9aafed91e9b0330250828b96f36e0e56dca812b16c35bdd492", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "ac3936c0-10e5-45bc-b8d7-f92f84922877": {"doc_hash": "184aae3446c2bce37930c8292eb9f35227fb6cf6978f9230c8395d7bf76a525f", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "5cae6908-9120-4146-b113-d8392842347a": {"doc_hash": "b4881b05ad1015ab1145e9b9d1c72a0fafbdf798b78d8818c886f9e34aaab701", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "fd5a3dcb-9471-43a3-8dc6-bed5e39313f4": {"doc_hash": "e0b3388f21725acc1036aea69173b47b3c14d8b2470ffa736f1a69f51c369d31", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "e6865d62-7274-438a-9a93-5d0d50860367": {"doc_hash": "6035881c1bb4ecf87e9a5d6c2cff194612dd8003077fe721634792525acfdc65", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "5fb27f0c-84d9-4f15-9949-f545f20ff00c": {"doc_hash": "6035881c1bb4ecf87e9a5d6c2cff194612dd8003077fe721634792525acfdc65", "ref_doc_id": "40d44803-4226-4b4b-8df3-8b7a5e1aa88e"}, "0df44817-51ec-470c-bf74-d99aee327379": {"doc_hash": "8d886283763396b7bb46a986c6a88ea9be58dbb39de742818dcaccb4454dc1e4", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "11226ad6-062b-49e0-81a9-0b2426d72f50": {"doc_hash": "3c24c90f029ef2b335658e343ec2e566800e4817f4b8b9abbf687459364c69e3", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "134969f8-1eab-44cf-b9c9-d1fac50adc56": {"doc_hash": "09db0dc796cfe2cc2bfe6cda7a49e2a74f16318bafe924088e24293fa3542232", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "85acb49a-24c0-4c43-86c0-f336620033da": {"doc_hash": "d0e82a25d74e97ff5b333667ebc43813416a4ba14c1be6cdb301bfe2ab4b6f5f", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "b640e831-6431-4554-986a-e0b8d144dccc": {"doc_hash": "192f519965feb3f1885f95dbc7c56ccb4c30427dcb2695d9b608268b2655f495", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "6415abb1-dc4b-435b-afc7-3ab62b243533": {"doc_hash": "bc1eae24f9b981e65e8ab2e1a82dbbc0d5683180240b061bf4351cee6aa35663", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "c470b584-a285-4064-8ea1-68a82d8ae5bf": {"doc_hash": "5da286d5b2c896ac5293bb6f5119e67356db0ea9e400cf6eee5223d4c1e6b6d1", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "22647baa-2f22-448d-9739-711281ea1fd3": {"doc_hash": "5f27f0b6566bb66c48c6bc8a46ecb1147b5b79205500b121edaf84524b4518aa", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "0536d29a-907c-41e2-91df-2e0c62768193": {"doc_hash": "482814bcf3e47a334a28e53f9a3268490cb97578b99f8610991e4e1a2239265e", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "6260c3fd-ce52-4908-92ea-f04f6d96b982": {"doc_hash": "09fcd7457314a90557d4dd7953933f77f6ca29e3710d510cf2ed29c9fad71e4b", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "4e070bd5-9c75-4e71-ba69-aff6236ffa26": {"doc_hash": "2b218a4d2407fa354ba60ce12b1fc45140cb4072b6bcf44a3bb362e0c7919605", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "77a5d525-80de-4b81-8e7c-bf4aaf73e2c8": {"doc_hash": "bb6ad6ff1677fa867039f504300f500dfebe6e44a48ebd90d8788ac1c04a3ea2", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "30b8747c-37f1-4b65-99ba-ca8566d3c53e": {"doc_hash": "bb6ad6ff1677fa867039f504300f500dfebe6e44a48ebd90d8788ac1c04a3ea2", "ref_doc_id": "27e1e1c3-aacc-435f-a624-cfa1d11878c3"}, "75fa0034-cc52-41b2-9102-80626bdcb919": {"doc_hash": "3d42a65e74a60905c20dd4fa23e7e8d14f56e776be0ab81bed3b3c5f1e561f90", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "a44dd696-5827-4690-a506-5b6031630fd2": {"doc_hash": "fad45eab38129535665307dcc542d49d1467f45a55d06824c337f562515c9c1a", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "fd8db8b0-3746-4518-adae-20691f8d96b9": {"doc_hash": "3d2a517074fa8e268d780c6b085147cf0aa63165ce26a6de99b37110c15272d2", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "a02de361-6cad-42e5-84a3-5063865cdd01": {"doc_hash": "b78d5ec3c4ccb3cf20b580261511ddd500a50928bdfed410d4679e8f5d4a8ef4", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "6b68f57e-d476-4b0a-8ada-68ecf5c9941e": {"doc_hash": "e0de53062f63a5d5f9cf87b9bbe6a7833c4c76e81f9bf1fc5a81849503e5627d", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "757ce21e-c414-4021-8ddd-9149246a3b5d": {"doc_hash": "08a3a649a9a6f74b08997013fd846c9d06e2566b83a612b56681ce681610f7d9", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "1e8ee17c-4a31-4ffb-99c4-c99ea22cf7e4": {"doc_hash": "770b970e6e44af53d56993e390a8a02d49232eaaa366bb93e72766902d692559", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "90878eb0-7697-4008-a301-0bda7a07eb2c": {"doc_hash": "5732330d4b7e70fae600dcdb73c12c0fae53fdca5879eea59a1d0d5874c1b82a", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "09180f96-566d-43af-a849-1b147f4366e2": {"doc_hash": "4f07761ab0441877405c5e45535385a8e9d1285c57009258828fc366c8fcde58", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "48a393a9-9b2b-4f67-ba95-f8eac87f88f7": {"doc_hash": "c88312a568102e31fe37086dc3c53f7602c0d120bae8c65f3b1ad8d0444b1a85", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "c832fb63-660f-47b9-9c72-4a01c208e943": {"doc_hash": "bebd373db739f9d75e136770cbb83b14008950831c0ff777678b16db85f46fb2", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "174f5784-520e-4f35-b397-a2b05a9071eb": {"doc_hash": "28be306d3ca0a1b633c66bbdb2b36ea20c7b8a57cf874c2856f6db174fedc523", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "2a2120bb-a06b-4e42-9581-faf88bb253df": {"doc_hash": "28be306d3ca0a1b633c66bbdb2b36ea20c7b8a57cf874c2856f6db174fedc523", "ref_doc_id": "e12af6ec-82d3-4605-adb8-b3fea4678f66"}, "78bcb5a3-6a5f-4c83-b488-099980ee9594": {"doc_hash": "d43659bb1b8f786a183f0205ad77e5b3aea4fe7db7a2ea7240721824eeb99fa3", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "c140d60b-4a57-42ba-83c9-3b2347df533f": {"doc_hash": "b2ca5683137461044aad201bb753c47f7cecc76314dcb337abd5e04b2caf7801", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "d4b16900-3cf4-4abc-acdf-1d910caec986": {"doc_hash": "ee0f974ac046b06f17c68ca681a8fd4ed5633d4728722b52a53f987d805fdb63", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "d49357f0-0cd5-462e-86d7-5ec81469804d": {"doc_hash": "15eb025e26b5daea795f48d68458477108928ac8f3617a2ca09c46f212b91d16", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "6fccdadb-6bc1-40b1-8a3d-f5bcc5c33d31": {"doc_hash": "54f395985e94533172eb82b71d8d8b17f5fa2ffcc07cf7549dc5100a0955af5c", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "c633fd5a-bd69-4692-bc8c-07e17a64defd": {"doc_hash": "23dfcd8f2f63dd9ca5456445469aaeb06c4d38df72a10ffb733aed380a356f16", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "d4413dab-3ea2-4bac-99b6-457f6271e095": {"doc_hash": "7f4aad742860c7102def7552672bcbb0d22d88761680fc10dcb3d03f2ec78e9b", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "2d2bd23d-b78f-4e66-9d24-a545c04387bf": {"doc_hash": "299c85747d9efcd1dae55c241242fd737b96d2f0e8b217fdb1470019609dac65", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3": {"doc_hash": "2f6f3e7c2d2d6da036ef60705d2b58bdd553eed479c49c9139059abe9b4ce3c9", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "c916a4d5-a0e2-48fb-8518-ce1362d0f304": {"doc_hash": "cc0bf3bd54d0008d6e5b314e82b5d9e9c661a7f68a25efe44b7fa074c3d31dff", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "6c73cd9a-0437-468f-b198-d7ec0465a627": {"doc_hash": "7f4595bfc9f9748db854c05b73abd1b66fcf6a80dc1a05778925a4879a4d7969", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "b43998cd-43e5-4912-ad92-947875b41cc1": {"doc_hash": "75e91aa49aeb2b1f68437b1779f7449ad7100d4e0529f11dde4013e8ae9de77e", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "ca0449d5-8085-47f2-881b-cf4766fef965": {"doc_hash": "9314b3b0ff560e26136d0126ec168f026a1aa36fc2be27d8243dbefdd5d47cba", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "0fe00784-2507-44e6-87c4-a0898b696002": {"doc_hash": "c5b5f97a2751ec12bf70cb652129779ac00fb840f924c5e6ea02a7a5b42fd6ee", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc": {"doc_hash": "8c443c39451e7b6434d0720289c12e2dc9bcd49129244df064be942b85e54f82", "ref_doc_id": "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de"}, "ad5c09a9-1935-4099-bd8c-3b462f24725a": {"doc_hash": "fc271c5fdeeda2da35dc1df2cf940ac4aec008cf1fafcd3521dab30fec5a4c82", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "3b1bc7e9-78e2-47e9-9f66-27a2d67dc01c": {"doc_hash": "e3c36e999130ae615166dc2e2f275d6114ddb00284040e4ba8f7f1a1e12c4dc7", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "ccad4441-f467-4b83-9023-75896ff3e1e4": {"doc_hash": "7e1e79cb4eb5fa29caada6f172a9cb32b38cde2078239da06ebfd3b215b7134a", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "6c3d7274-f03e-406a-b9cf-4378af7bb951": {"doc_hash": "66e24776c75a38587832ef405c929d6ba6517c3026ecc5f81cfd42182b0c20ff", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "411a254f-99a9-4a62-a8f2-f712331cd2fc": {"doc_hash": "8fa75479d8128a203abe716b327ef7bc39a8edc89ee794ce2e6e48e323b952a4", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "36818b3b-16c2-4667-864d-1f784cee487b": {"doc_hash": "8234e71935298efe59eb653f56683998bb7e34b425cc7389e59259ac2f714bf4", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "850fda2d-5eca-4223-9d74-8ee3af816c76": {"doc_hash": "9a1855d369a8dcfb401d76071e53994d206c3b5be667b4734128ac8619e4d721", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "f1d07fbf-39c5-4a94-9ddf-af13035a45fe": {"doc_hash": "188285195f49eacfc876c31fc4ecefa4108af8904210e0cdaa8104a55c2e4422", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "f5084595-6d0c-43a9-81e7-b82144e7cf7b": {"doc_hash": "fe6a0a17f2506f7863ec152d48e09ea662126dfbafd48de20dec7c40cad47bc8", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "56903c5b-d594-4b5c-8dbc-74d0f7d4da96": {"doc_hash": "c203e8aceaa1eaa5669ae4175c21123bd042ef3f291e502ae7e6fb4fa1001035", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "a4a94e42-ed52-4bbc-8f36-78f41cb5b77a": {"doc_hash": "db565b2a40b4f86c24524f0c237a9daac0beecb322b06bebcd8a0e20a1f9cc39", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "ea8275fd-a8ec-4ac7-89cb-4bdb6a400ba3": {"doc_hash": "8d04eb37f511856d68b78f29d4d9532b2278aef3564a5f2c93565dd2d5651f38", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "4de359e4-aaeb-463b-a39f-11cd4cf4d946": {"doc_hash": "8d04eb37f511856d68b78f29d4d9532b2278aef3564a5f2c93565dd2d5651f38", "ref_doc_id": "913208b8-c3af-41c1-b72a-d253855c7f49"}, "453e08b8-aa17-4d16-ab58-d4ca14d5fbf4": {"doc_hash": "9975600fab920eebe046e70c6d7f646cd9044369bc21d176c6841bece6ef1e57", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "2ae9d5fa-f827-4f7f-890c-9a08677dbedf": {"doc_hash": "ce87014f1b40ff8959e30f61706a21d17d7d16fa337c0bf6066b461033daa13a", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "3642a03f-5ddc-4987-b350-382775aabfae": {"doc_hash": "38aee263c1da034ac702f3be1d026fd0215b0e0c10a729904dc39c852098da00", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "63588345-229c-4833-b023-dcac6247f513": {"doc_hash": "8e63ba76b1607040f42f68e07e537145761f9ac2d356f0d1f0c7a32c32f7583e", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "28dfb054-3092-4867-9fa2-4e944bf59627": {"doc_hash": "1d61a7845c3b30598b656592a2262621ad63ffea88652b37620a71dba1288138", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "64146070-c41e-40ea-a87a-29f7c1bdd873": {"doc_hash": "d2576ba2c15ecfb4ddd39fc48ae8583a7645978241a5d59a67ae8461043feff0", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "4acee7f5-5d6b-40a3-991a-9605b5d34a7f": {"doc_hash": "8daa5435bd5d2fecd4bd1767209bfbaf873269f57d462e49248a333a24f0a721", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "380e57ef-28bb-4e56-ada0-bdfd232bca73": {"doc_hash": "df1fdbbca19439299aab943e73dbbd861b106b7434ea16effc188f54f2f57be5", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "3d1df480-1647-49a1-9aac-924a3a2eea6c": {"doc_hash": "c7221db9be5fcdcd9431af4014a2e93c4ac66eb0c5e193a402db6a5b2774e5b9", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "dfbea04f-757c-4b09-bbe8-37af0467422b": {"doc_hash": "f538dbec85efb8760ad740f05c3faee61a9f2e09370ac866e04c9e5c5e122dc6", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "48ffee18-dc4d-492d-8205-d51e54be752f": {"doc_hash": "7af439937ad93e44b05f1987c80a0944cf980a945eb32ebd7e6799b7e239c6e6", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "21a0bd8e-2c7e-462b-878e-509b252673c7": {"doc_hash": "500a81a7bbc1dded8c86d6ca2eb4fd59688ffdd728ed2f993ef76ced0b1771bd", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "ecad222b-3691-41bf-a6e2-76e7945e12eb": {"doc_hash": "500a81a7bbc1dded8c86d6ca2eb4fd59688ffdd728ed2f993ef76ced0b1771bd", "ref_doc_id": "401b9843-52d5-4e2c-9415-9dfa51c0cd4f"}, "d0a5531a-a42d-4bdc-a89a-74fe521fdc1f": {"doc_hash": "377ab028614fea3837d2e686f568a03d9e080b4ae2d4ebd1333a1b221c64a3e5", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "73255c79-8fc4-4d78-ae92-d22941b7f88d": {"doc_hash": "48f83f3e3cae8ec39f2b8a9d9d8b369be13dbd3a7bd416d3309aefcd03ac1ef3", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "b4977b46-bfbe-40aa-be97-d38421b4fbe0": {"doc_hash": "f8c7d6f9ef6ad559f23fe58576c8a7c81d1d9609b98f5a742c2369770e1cad51", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "ef5d432b-e547-4104-a380-961c55fc2445": {"doc_hash": "7a8895adf4321bebb6e1f736be3e0e4dadc7fb8586bd82369e31e94efb49b26f", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "ce5b57ca-1106-4307-ae6e-75da4d7f8d7e": {"doc_hash": "aff418c88d73f1a6e4dfef88d101bb94fde294ad0b0fd0b579cc6e0f74be7f4c", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "1c5cc9e1-69b7-41b2-aced-b2308cab16aa": {"doc_hash": "c0fccb03c32e15f55438f3e7d273c3057d39df5611d7269d646e963ecc8b799f", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "b465b4ce-4a25-4a5d-bc59-2da1ee31b6db": {"doc_hash": "c85fb5fb425fbeb8aaf70560eb72f8ace1f5018743b1047cdaabcbce9d20e416", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "279b3702-571a-42a2-b3d9-413ec8ca72da": {"doc_hash": "2a1b5db0430fa7839aea7815a6c0fe50b9dae64012231c0e9cfa51e057812a8c", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "9a267d95-0a4a-4968-b5d4-a8df4675fd7c": {"doc_hash": "7c51a72a7c6cbabf6a2b3cfae09a6915c65fb7ce8607ac4a6a4b974f64730562", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "77f521b5-368a-447b-90bc-864fc4e4f31c": {"doc_hash": "7152f94e0f1d28ee19cff6b4fd0b4fccdab5becf5d5d4e4a054556bc84d4785e", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "522c8145-c711-4502-bd3c-f8af6be87aad": {"doc_hash": "4b47553b8edc218759209532da88c79cd6b2d48c14bc5f69c2943bea208b67e3", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "a6b927fa-cc71-499d-948d-d51fc165f3b5": {"doc_hash": "c9ab83f7c231ac901ffc7bab0f29ea4e0f9bda275a03bad10b9d574bf4ee3384", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "4442909d-8978-4334-a45f-05989a1fa8a8": {"doc_hash": "8e84dccd154939ac74eead0a72aa597796a625e0981988f14b954faba40bc772", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "50f0e63c-0b5c-4b1d-ac7c-30b84d84d9f0": {"doc_hash": "ef62e3250564c62b66ba326dd898a597b3b122254915f842e6f96d37e18008c5", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "8d9bf0f8-daf6-4e6d-adbd-d2fea4411f4d": {"doc_hash": "92e66dd01cf6778ed2cb703a05a8bc03e5f8afac7563e5a48393dfc3f625500c", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "70c7a548-cac3-4e1c-ad39-5c0a7cc46ca4": {"doc_hash": "aa2e29a253fe02b7f47deb722220d84f57d1e6c6cf444e9648dae8b4e3f8f3c8", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "1303e699-89ff-4ff0-bb47-0cfeb7019408": {"doc_hash": "0282aa8f15b219ddc898a7fecf87f37851acb6fe98aed6be4b4dc95f8f202f46", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "17613d62-f18f-4308-a7d9-725dd9a25ac1": {"doc_hash": "26f7cd56c3971389e7dd92ba9bbc44d8966385b8ba0fcfda4301606a4e6a4694", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "b97b614d-9aa8-4a34-80c4-257d7519da09": {"doc_hash": "b3e32e0ae97d8db6830a60a816bfe86756cb57c35b0aa355c33e09d6a686e32f", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "404f6ffc-bfc5-432e-af58-2b1965b50c95": {"doc_hash": "b3e32e0ae97d8db6830a60a816bfe86756cb57c35b0aa355c33e09d6a686e32f", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "341906a7-7b81-4e34-b04c-aff139b399d9": {"doc_hash": "b3e32e0ae97d8db6830a60a816bfe86756cb57c35b0aa355c33e09d6a686e32f", "ref_doc_id": "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26"}, "6f4f8de3-6858-45df-995c-66d1f7b22aff": {"doc_hash": "a15ef67c9f731cc60c48fdb46edfc735aa41815712ac9404090b4348fabeaea5", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "bb809562-7d23-4f7c-a220-c03efbe1727e": {"doc_hash": "247658fb845fa354239d9b24394da3ae5683da52669bbe074cb0ff8a19bef3b6", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "0ffc365b-1710-4571-ac52-8d59489b0b9a": {"doc_hash": "e74dc4f10ec00bf9aaba9bb4414fc57b553a9af7a0037dcea2bc6e67b8d5e42c", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "d6edc881-f11e-4a5f-93ca-9015782f0478": {"doc_hash": "371572b28f65058a8c1e3fd072a0f98c24e9417648c7e751fd7cdaebadab5e72", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "59ecdf1f-910e-432c-94a9-7f20ddde9645": {"doc_hash": "0e930ae25bb44890b65016b1f6d9002d1a9168bac877df6f3bd40251d4359853", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "bdea7c05-e6b0-48c5-b573-2e6c54f44b69": {"doc_hash": "b5e891d926f4cbc571de3b420f5d618e29e5f1b5f1c587fd9ace0341c18130e4", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "08d29566-3d35-4713-9a87-50f5d5cb4ac6": {"doc_hash": "32bdc7ebb48d2e758a110c366308e13a619529797d44d210cc61be7512f526bd", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "91ee45e9-591e-4100-9e56-cef5fcf3d88b": {"doc_hash": "9b285417eaf762d6cf2c88b77d854a5b0d4305e114a9aeceda91b35798428e41", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "a1dea19a-b472-4445-a1cb-f972803ddaa5": {"doc_hash": "25e84841628412439c6fa47709a30bfd5834aa5bf92ad042e9899acd407e356a", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "bd409718-af93-42eb-ab96-73f34ed2934f": {"doc_hash": "e15c7b18b4407028df227df10fcbc8722abaea2ab55df0a36818a93b0462bf5e", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "0042aebc-0ac7-4408-9572-333d93d86b9d": {"doc_hash": "09567df3ea211561f40da0187cf2469d75fa69d4a5d135db3731fbdbd5259052", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "f0b22b9c-7538-4564-a7d5-d4999c69c4e8": {"doc_hash": "7e435bfb28fa155b2531697e33493cea1f1deb781aacc0bb787d074fef36bf35", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "d25799d6-a0b5-4496-ad34-c37b368f9bba": {"doc_hash": "2d95f2893635bb4b4625b9f6c45bb171bdd9382c5d2511007d99a9d6c11d2291", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "da2ad865-a1b8-494a-b9a5-b70724cfdabe": {"doc_hash": "14b46c8dbb626d073af8f0d5823cb639f8a2ff79da7cf5cee5a549776c6bc687", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "2a779f7d-157c-451f-bec8-3a9f3a952901": {"doc_hash": "30ae7852b551852ff970bb32c3f8082c68440704415e88c50e00ceca2aeb987c", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "5bce4097-a720-4346-9ef2-ee9216f9f290": {"doc_hash": "9f9b65c998143a9953c0e307213536c1d84af0e9b64309efab849cac8e9da1f2", "ref_doc_id": "e9f632b9-6938-49f3-90c6-27b726540ac6"}, "a4010cca-a0f4-4c03-a2af-1cf80549465f": {"doc_hash": "887e3b5e3b48b284128f5654c0ac03339e31ebe25b59acd596878159abbd8d73", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "cc205667-9e0a-400d-8055-d241287e6349": {"doc_hash": "298a9e693c72baf3a24448ca7ff7ada2cee48062c897ae8a87d19ff9265af9a4", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "38ee5820-2509-46f2-b725-cb55f3e88168": {"doc_hash": "5d7408a1604f1888ffc736c62d6b06b2ed19ff904998776ebcdb94fd4fd7d382", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "439e17dc-9d7a-4928-ab30-c0bcbeffb9d3": {"doc_hash": "c772de3f0e7adc9ca2f823cdd35363556a04612e2dee6580033a7cb3b72a710e", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "56397c8d-a735-4e92-98aa-7280c4d91845": {"doc_hash": "6e812d6baf09921201bc5c648a33c70d62606a88a1ab8ab4bc3f751ad2cccafc", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "6a0836e9-5cef-4080-aee6-8245acb184fe": {"doc_hash": "42e97b07c07c7adf8fdd5228bfa5c7185da17e35136e15d4cf2928522ee9aafa", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "12eed830-25b2-4743-b818-531a7567923e": {"doc_hash": "282bf34c79de172b71853686af289e7207a8eeeecd2dec799d6cf3c2759d099d", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "e6bba099-272d-4193-8426-e836167810be": {"doc_hash": "ba11330b4e0f245cd12098918a4b80f66cb34f8308fb2be4b822d77b0cc796f5", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "b4ce308a-7cbb-41cd-b975-3c4907187238": {"doc_hash": "c74df9a53b563760651c6fa0a45fddfcc1726836305145ee0e02a7fe85bead5b", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "abee7795-8c88-40b1-b005-024a6605c0ed": {"doc_hash": "36ce3a46aa7cfba6021877eb4c02fa9959db3a2c515ecbe3f0ad9c4b119c285d", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "53db8acf-dacc-43d1-9b71-da0733f1b590": {"doc_hash": "d48145082af5a218bddd782838909bcbbc772c1d3c1e4909f52e7d96d87b5199", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "87a07630-fd48-4cc8-a4fb-87b61d2c651b": {"doc_hash": "dbc0c256ac81167b7ee9e91690d888228888542aa99db804e452875123e58f01", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "e1a7dc18-5fe0-44bd-96ed-3b4443ea7a1c": {"doc_hash": "a69b33f947a1ade1064f009cf7267a7b5ed3a367b18b74d80d22f35366bec5be", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "44a21df2-4dd0-47a9-96d3-07dc7b22ea17": {"doc_hash": "b147e62109f748dcd9c8b52fc8e80c6daa68ebedff46487e8a8afc100dc8557c", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "a2c2e9d6-4091-45dd-9754-0cb9cac0ec89": {"doc_hash": "fcb901af5f29280152a30de5d995fdcb5d81453111deda2141bc1e5d37cdb249", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "1e0e1ccc-96c7-482c-a028-0f47c5f704d6": {"doc_hash": "3731ccdcc8896ef01e1cf33be2d84751da3333d72a9f8f2934fc598c217c5a68", "ref_doc_id": "4b3c9195-49ef-46a1-bc04-da586869ecc0"}, "67b293de-51ac-4be7-b1d7-309f3bcf2f57": {"doc_hash": "0ec688d9aede8a32ec322e95b2efd6d8a78d9bcf2aaefb8b97fbb73566a1285d", "ref_doc_id": "c855f4f0-3f40-42b9-ab44-bc338badf868"}, "b27f9de5-bbe4-421f-b677-fa36200836bd": {"doc_hash": "9f58fd46ee349331b7a0b018e32a2f52487c0c2b95cc84eec7078c2b5258e6fe", "ref_doc_id": "c855f4f0-3f40-42b9-ab44-bc338badf868"}, "173cb8fb-4926-4e7c-b254-af74e55813dc": {"doc_hash": "a750e54154357743e60d3d66ad0e8d84fa571f47bb76ce71d50c85e38c8af30c", "ref_doc_id": "c855f4f0-3f40-42b9-ab44-bc338badf868"}, "3431851a-bda9-4cfa-a555-95089afb452e": {"doc_hash": "9b305e2121eadf343363b322dac3174c6ea6000958b5640f26b2bd953b829a9a", "ref_doc_id": "c855f4f0-3f40-42b9-ab44-bc338badf868"}, "d12797e7-db32-46f3-a145-dec8296d1752": {"doc_hash": "43e816d44350deb77aae96e79087341affe7f276e6f2a626aae2e5df62f65c7e", "ref_doc_id": "c855f4f0-3f40-42b9-ab44-bc338badf868"}, "04cbb945-790d-4e6b-bd07-e017beaba9ce": {"doc_hash": "5827fceea34100e08848dff81acc1f46a357d29b461842bd87496814011d28e5", "ref_doc_id": "c855f4f0-3f40-42b9-ab44-bc338badf868"}, "c84ce928-12c8-4d14-af6f-bac7a49c486d": {"doc_hash": "8d6f5a75d9e1c7045e162e8ea5aba9976c691d978f42b6d0b25db1680db1bed8", "ref_doc_id": "c855f4f0-3f40-42b9-ab44-bc338badf868"}, "5545bc25-a275-4b36-94e5-b71f10fc17d8": {"doc_hash": "e8197de589638fabec44bd8f5af3bfa92b60df979bb7079314374622383c4b21", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "d72fc83f-82fe-478e-b567-405349b260b7": {"doc_hash": "18a0b866f5f4b84c12c048cb10d4aa05ae1e65e46cd630158e0ff57d49891523", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "7ceabaf6-f4c3-4d42-b228-30dad5a402e4": {"doc_hash": "07c18e7c2fa0ef57975d54288511fc2d48d4c28340cebb3b26a342e78fd3fdbb", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "569cde60-c250-4f05-8f87-d003e6feff30": {"doc_hash": "1d93fa475b299744f02fb69445c100d45bdbc071f993fe36664f23437735d2cd", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "2abba5b6-8fa5-4edf-87c5-c1690fe2a72b": {"doc_hash": "a550817491ff2f926c0c189d7201698b31466daf8a421b1503e6f8f9a212b7f1", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "8ec081dd-8c33-499a-9d21-b9452427f753": {"doc_hash": "a7e53d105344d8e5764eb5efed2fb0f1e41304bb81fb98f17fe99b83d3206f90", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "c468cabe-e9b3-4838-8b3e-b64e2637a0bd": {"doc_hash": "ca356ffb497f02d97009c1834fe051fd01e9c365616ea03d30616c49e95ed271", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "1ef15fb1-864c-442d-8115-38f3adfdbac5": {"doc_hash": "41cdae24f1647509fff92755fd8b007be8830c948935828854d4dd097b81cc22", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e": {"doc_hash": "6499346709e53f58545be595d06dc048469365681a0e017955f98c9d3da1fee1", "ref_doc_id": "e99a4afd-2fcf-4693-b29e-85849571a84f"}, "c544a717-cf0d-42ed-9e1a-f8e27d0fc5d5": {"doc_hash": "2cf0ec50dc3f3851e1b2652c61c9a19328d0ebf0a74799db35bd202e67a6ac84", "ref_doc_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb"}, "6d5ea79e-b226-41ff-a5cb-4e3d61c990e4": {"doc_hash": "958f385bca6c144a15eccb288adcafe801332c0d907a34f0695a572d1cf2cf47", "ref_doc_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb"}, "e4e0a8cb-0b74-44a6-9646-ace0b17d7fc9": {"doc_hash": "8ed8f25525b306520665b8176d01367364b387d2d76d66a416144171ff436569", "ref_doc_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb"}, "852b8105-35fc-4b37-84a8-07d97ade6e11": {"doc_hash": "daf4c3d938099368db4f7614ad17fe9ef60fbb959bd67cf7eef3403fcff0adc5", "ref_doc_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb"}, "40f3da90-4f47-42c5-8a4b-af0f5f6de6d6": {"doc_hash": "0c64fa2fd38073d2f0627ba784f4ec628d4a8c85dff55e2482f6b78a1a33735b", "ref_doc_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb"}, "641771b9-5abe-49c6-905f-d8b33cf929b3": {"doc_hash": "f877b136418f283d3903a9ce5db5e1f36cbb639a734f0681852c70f6be338ba6", "ref_doc_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb"}, "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945": {"doc_hash": "b38c1ca7790a19f425297936bd2348778fc5a0f8523aa6fd7c8168f1e5d9194a", "ref_doc_id": "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb"}, "a9c52cf6-8d73-451a-8057-a2193466b478": {"doc_hash": "a6414dc815c41834bcd052767441b0bc6a58c3f409e8ddcb57d32cf9f15f058c", "ref_doc_id": "bdc0e7c7-5d52-4bca-b2a4-29565c8f2ce4"}, "76797836-1114-4276-9b42-edaad528ac64": {"doc_hash": "a6414dc815c41834bcd052767441b0bc6a58c3f409e8ddcb57d32cf9f15f058c", "ref_doc_id": "bdc0e7c7-5d52-4bca-b2a4-29565c8f2ce4"}, "9583a3d8-fa88-470d-bdfd-50b7387e9d67": {"doc_hash": "a6414dc815c41834bcd052767441b0bc6a58c3f409e8ddcb57d32cf9f15f058c", "ref_doc_id": "bdc0e7c7-5d52-4bca-b2a4-29565c8f2ce4"}, "69ef3de6-6b7e-4742-94f7-60babba03207": {"doc_hash": "3646b31260b5a72210318b85c17840c3daa16b7b994268cd273ae0ae2c7cdb11", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "297e8654-d4af-4953-93f6-113e68f13be2": {"doc_hash": "a7f8594a1f4376ae2140c537882300c935c0ce8e1ff49dab8961b579fe9ef8db", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "6272b04c-3104-401d-8688-6e860b899445": {"doc_hash": "50f01d074daa22cad2f48be03db558775fad0e7d6e083b8da558d9825f9d4880", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "a15afcee-cb0c-421c-9ba9-aae3a07314e4": {"doc_hash": "86a15df08dd6b1d9ec24bbeb4b4931918242c20e1ca8e0f46813099ce50a2923", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "56e1fef3-d519-4d6d-a658-9614f4a650c7": {"doc_hash": "34deed3650d253fece03683c775b06639e80c32405eb3a189b6dd6db9cb13fc8", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "7b7f14fe-7dc6-4f00-a54d-61ff21090099": {"doc_hash": "59f90dbc83d48dbf7a01c79a39b72136e67e441c2cbad2f19a4e97c81b19a6a4", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "99d19f7a-a851-48f9-bab7-59d5c2e01a86": {"doc_hash": "904efb2ca183ae53558ddffe30bf3cd4cd73623cf69683cf0b6bdec560b4f7c2", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "0c7c02db-1a66-47f9-b394-4cce0dc7f87b": {"doc_hash": "267b10800f2abfd1c4b98170263d9ac2a266c88d74abf030a1f491cf14583d0e", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "cf69cb07-f436-444d-9cf8-f739ae111f77": {"doc_hash": "088f991294e7eff447ac977b12894b879e0ad38659c30de23402b28516e280cd", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "d6cfe3c2-94d9-4b37-8078-f5e29ac8caea": {"doc_hash": "3665159fd339ba16366de821320aa389ea1a60e2c7c6c7bc10d8877a57d82ce9", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "67842a45-8fac-4e05-906a-8f7a344ed3d6": {"doc_hash": "f2bafe1abc308b76cd673caba111294a1dc6a6a72505adc51636bb2f4f7502cc", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "3142e38e-36c7-4529-bd56-ec6120174492": {"doc_hash": "eef9189990293350866e7d2a8a5d4296e9beffdcfb9df426438b1b8332b04ae9", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "4916c6b9-ac86-4312-9c75-72461297d832": {"doc_hash": "eef9189990293350866e7d2a8a5d4296e9beffdcfb9df426438b1b8332b04ae9", "ref_doc_id": "26629a4a-3c25-46ff-9bbc-81830c2e4e23"}, "85cd3106-1457-4da8-95fe-bfda6b641292": {"doc_hash": "51a211486bec982eef2fc6e23b227983343fc7967fccdfdf3589f349744f5e7d", "ref_doc_id": "4bc3c38f-1ec7-46eb-933c-7198e45be35a"}, "f393cbd7-fb20-477c-b7cb-8fc3b1d0da83": {"doc_hash": "51a211486bec982eef2fc6e23b227983343fc7967fccdfdf3589f349744f5e7d", "ref_doc_id": "4bc3c38f-1ec7-46eb-933c-7198e45be35a"}, "b1ba75bc-458b-45f7-9060-5615b53b9a8d": {"doc_hash": "51a211486bec982eef2fc6e23b227983343fc7967fccdfdf3589f349744f5e7d", "ref_doc_id": "4bc3c38f-1ec7-46eb-933c-7198e45be35a"}, "62c27968-04c5-4fad-9298-b0236b4cd4ce": {"doc_hash": "d6474bdc70470cae55ffe27123c898135368708ad83aefbdb3b420e1b632bb6e", "ref_doc_id": "2cf3b0b0-0931-4e79-bb17-3a77ea4e9ad3"}, "1ae0425d-221b-4726-8437-68ba916b0471": {"doc_hash": "d6474bdc70470cae55ffe27123c898135368708ad83aefbdb3b420e1b632bb6e", "ref_doc_id": "2cf3b0b0-0931-4e79-bb17-3a77ea4e9ad3"}, "f3452640-e8cf-4e2c-8de9-06d639fd2f8f": {"doc_hash": "d6474bdc70470cae55ffe27123c898135368708ad83aefbdb3b420e1b632bb6e", "ref_doc_id": "2cf3b0b0-0931-4e79-bb17-3a77ea4e9ad3"}, "ea135e51-f890-4440-a2d3-11ee1284be76": {"doc_hash": "7894eb67ec7697a68c2c2402c7ee682e96bac761111621344d677f36b1ec1aff", "ref_doc_id": "40d3bf4a-2890-4bb2-b3fc-9f8735b2f4af"}, "d79ce0e6-15c0-4850-86b5-2adc09ee0875": {"doc_hash": "7894eb67ec7697a68c2c2402c7ee682e96bac761111621344d677f36b1ec1aff", "ref_doc_id": "40d3bf4a-2890-4bb2-b3fc-9f8735b2f4af"}, "da5aa105-0ca3-4a65-a43f-23b180cddc74": {"doc_hash": "7894eb67ec7697a68c2c2402c7ee682e96bac761111621344d677f36b1ec1aff", "ref_doc_id": "40d3bf4a-2890-4bb2-b3fc-9f8735b2f4af"}}, "docstore/ref_doc_info": {"5845b888-160d-42c4-b417-c8b5383922d1": {"node_ids": ["936122c6-bbe4-47a9-b179-1c52d61b8ab8", "9ae6faaf-3449-4b0f-8674-c3e403c758db", "c9bc29a3-4e6d-48a3-bf2a-c12ab7dad273", "144ee7b8-5274-4ecf-ad41-3bf4690cdaa6", "ca1986e3-4326-4915-8fb0-dd26cf3e400e", "b27da0fb-cf4e-4487-affa-b9e4e8fe6e96", "e481ea7a-9638-479c-9958-7cc07d1465cb"], "metadata": {"page_label": "1", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "62f785d6-9fd9-4df3-a8da-ad1772252ee8": {"node_ids": ["fb4e7299-13d2-4f24-93cc-8bceb1cb2f4d", "060b3ef8-9dfd-4716-87d1-c172674d9d98", "589a3a9d-81fa-4608-8cb1-9ccf67b460e1", "487962b6-0855-408a-9602-8a5d26045ef2", "d094814c-2629-494e-8db3-fe3948f4b2e3", "aea9a68f-94a7-4088-952e-d4db1a7ca183"], "metadata": {"page_label": "2", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "7b2336a7-7692-4d11-a9b4-52bd6dc4b606": {"node_ids": ["1da1bf8a-e192-436f-bc1a-16232efd2be8", "cf898f94-26f0-4099-b27b-ceebef62dc0f", "eb75fe4d-4e03-45f3-aa0f-38f4ac61ca1c", "7f0828e9-b3b9-4c37-9d45-4821dac1db97", "d612872a-3784-44ec-aeda-36f337d6e67f", "441bb3f0-852d-4f47-b772-c9f867f9f398", "7c753d2e-50df-4577-8267-c678d33d2fdf", "9b2a794c-758d-412d-8c7b-9ea1f4e6a2a1", "2b97283b-f755-4510-a6b7-d30d99f25e5a", "843197ad-92c9-4210-9fa1-fd11a9d95e73", "06273651-20a8-4a8d-b3a9-3cb80c1f6fb3", "6a0d5d51-b833-4a60-a353-5891e9338c8d", "5dd41d34-dc98-48de-95a7-d3a4c4254f9d"], "metadata": {"page_label": "3", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "40d44803-4226-4b4b-8df3-8b7a5e1aa88e": {"node_ids": ["17b5ce96-f5a4-4ff3-b554-afc8e7270647", "7d091e6d-d673-4d1a-ba1f-e77eb2a832db", "234c2977-6b8b-4bc7-8c77-6422aea915ce", "89c39124-b4ea-4069-97ad-d515dfc2cb04", "c6697468-8579-4b30-bb2c-b8fd75849702", "b5e38a5b-ea8e-4432-931f-ee69ea9cdc5e", "39aebc4c-9037-4c1c-8911-bd8e62a7510f", "87bd1f7c-4867-42b2-ae7e-ad27b0fb797f", "ac3936c0-10e5-45bc-b8d7-f92f84922877", "5cae6908-9120-4146-b113-d8392842347a", "fd5a3dcb-9471-43a3-8dc6-bed5e39313f4", "e6865d62-7274-438a-9a93-5d0d50860367", "5fb27f0c-84d9-4f15-9949-f545f20ff00c"], "metadata": {"page_label": "4", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "27e1e1c3-aacc-435f-a624-cfa1d11878c3": {"node_ids": ["0df44817-51ec-470c-bf74-d99aee327379", "11226ad6-062b-49e0-81a9-0b2426d72f50", "134969f8-1eab-44cf-b9c9-d1fac50adc56", "85acb49a-24c0-4c43-86c0-f336620033da", "b640e831-6431-4554-986a-e0b8d144dccc", "6415abb1-dc4b-435b-afc7-3ab62b243533", "c470b584-a285-4064-8ea1-68a82d8ae5bf", "22647baa-2f22-448d-9739-711281ea1fd3", "0536d29a-907c-41e2-91df-2e0c62768193", "6260c3fd-ce52-4908-92ea-f04f6d96b982", "4e070bd5-9c75-4e71-ba69-aff6236ffa26", "77a5d525-80de-4b81-8e7c-bf4aaf73e2c8", "30b8747c-37f1-4b65-99ba-ca8566d3c53e"], "metadata": {"page_label": "5", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "e12af6ec-82d3-4605-adb8-b3fea4678f66": {"node_ids": ["75fa0034-cc52-41b2-9102-80626bdcb919", "a44dd696-5827-4690-a506-5b6031630fd2", "fd8db8b0-3746-4518-adae-20691f8d96b9", "a02de361-6cad-42e5-84a3-5063865cdd01", "6b68f57e-d476-4b0a-8ada-68ecf5c9941e", "757ce21e-c414-4021-8ddd-9149246a3b5d", "1e8ee17c-4a31-4ffb-99c4-c99ea22cf7e4", "90878eb0-7697-4008-a301-0bda7a07eb2c", "09180f96-566d-43af-a849-1b147f4366e2", "48a393a9-9b2b-4f67-ba95-f8eac87f88f7", "c832fb63-660f-47b9-9c72-4a01c208e943", "174f5784-520e-4f35-b397-a2b05a9071eb", "2a2120bb-a06b-4e42-9581-faf88bb253df"], "metadata": {"page_label": "6", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "cccd7932-5bd3-4f38-a5d8-6ee5de5ab5de": {"node_ids": ["78bcb5a3-6a5f-4c83-b488-099980ee9594", "c140d60b-4a57-42ba-83c9-3b2347df533f", "d4b16900-3cf4-4abc-acdf-1d910caec986", "d49357f0-0cd5-462e-86d7-5ec81469804d", "6fccdadb-6bc1-40b1-8a3d-f5bcc5c33d31", "c633fd5a-bd69-4692-bc8c-07e17a64defd", "d4413dab-3ea2-4bac-99b6-457f6271e095", "2d2bd23d-b78f-4e66-9d24-a545c04387bf", "bc3337ad-a0c9-4e17-af1d-a61b169a4ee3", "c916a4d5-a0e2-48fb-8518-ce1362d0f304", "6c73cd9a-0437-468f-b198-d7ec0465a627", "b43998cd-43e5-4912-ad92-947875b41cc1", "ca0449d5-8085-47f2-881b-cf4766fef965", "0fe00784-2507-44e6-87c4-a0898b696002", "e7ec0fda-0f95-4ac9-9348-390f3f39f6cc"], "metadata": {"page_label": "7", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "913208b8-c3af-41c1-b72a-d253855c7f49": {"node_ids": ["ad5c09a9-1935-4099-bd8c-3b462f24725a", "3b1bc7e9-78e2-47e9-9f66-27a2d67dc01c", "ccad4441-f467-4b83-9023-75896ff3e1e4", "6c3d7274-f03e-406a-b9cf-4378af7bb951", "411a254f-99a9-4a62-a8f2-f712331cd2fc", "36818b3b-16c2-4667-864d-1f784cee487b", "850fda2d-5eca-4223-9d74-8ee3af816c76", "f1d07fbf-39c5-4a94-9ddf-af13035a45fe", "f5084595-6d0c-43a9-81e7-b82144e7cf7b", "56903c5b-d594-4b5c-8dbc-74d0f7d4da96", "a4a94e42-ed52-4bbc-8f36-78f41cb5b77a", "ea8275fd-a8ec-4ac7-89cb-4bdb6a400ba3", "4de359e4-aaeb-463b-a39f-11cd4cf4d946"], "metadata": {"page_label": "8", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "401b9843-52d5-4e2c-9415-9dfa51c0cd4f": {"node_ids": ["453e08b8-aa17-4d16-ab58-d4ca14d5fbf4", "2ae9d5fa-f827-4f7f-890c-9a08677dbedf", "3642a03f-5ddc-4987-b350-382775aabfae", "63588345-229c-4833-b023-dcac6247f513", "28dfb054-3092-4867-9fa2-4e944bf59627", "64146070-c41e-40ea-a87a-29f7c1bdd873", "4acee7f5-5d6b-40a3-991a-9605b5d34a7f", "380e57ef-28bb-4e56-ada0-bdfd232bca73", "3d1df480-1647-49a1-9aac-924a3a2eea6c", "dfbea04f-757c-4b09-bbe8-37af0467422b", "48ffee18-dc4d-492d-8205-d51e54be752f", "21a0bd8e-2c7e-462b-878e-509b252673c7", "ecad222b-3691-41bf-a6e2-76e7945e12eb"], "metadata": {"page_label": "9", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "5a25520a-b4d4-4cd6-9b38-764f1b8a5c26": {"node_ids": ["d0a5531a-a42d-4bdc-a89a-74fe521fdc1f", "73255c79-8fc4-4d78-ae92-d22941b7f88d", "b4977b46-bfbe-40aa-be97-d38421b4fbe0", "ef5d432b-e547-4104-a380-961c55fc2445", "ce5b57ca-1106-4307-ae6e-75da4d7f8d7e", "1c5cc9e1-69b7-41b2-aced-b2308cab16aa", "b465b4ce-4a25-4a5d-bc59-2da1ee31b6db", "279b3702-571a-42a2-b3d9-413ec8ca72da", "9a267d95-0a4a-4968-b5d4-a8df4675fd7c", "77f521b5-368a-447b-90bc-864fc4e4f31c", "522c8145-c711-4502-bd3c-f8af6be87aad", "a6b927fa-cc71-499d-948d-d51fc165f3b5", "4442909d-8978-4334-a45f-05989a1fa8a8", "50f0e63c-0b5c-4b1d-ac7c-30b84d84d9f0", "8d9bf0f8-daf6-4e6d-adbd-d2fea4411f4d", "70c7a548-cac3-4e1c-ad39-5c0a7cc46ca4", "1303e699-89ff-4ff0-bb47-0cfeb7019408", "17613d62-f18f-4308-a7d9-725dd9a25ac1", "b97b614d-9aa8-4a34-80c4-257d7519da09", "404f6ffc-bfc5-432e-af58-2b1965b50c95", "341906a7-7b81-4e34-b04c-aff139b399d9"], "metadata": {"page_label": "10", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "e9f632b9-6938-49f3-90c6-27b726540ac6": {"node_ids": ["6f4f8de3-6858-45df-995c-66d1f7b22aff", "bb809562-7d23-4f7c-a220-c03efbe1727e", "0ffc365b-1710-4571-ac52-8d59489b0b9a", "d6edc881-f11e-4a5f-93ca-9015782f0478", "59ecdf1f-910e-432c-94a9-7f20ddde9645", "bdea7c05-e6b0-48c5-b573-2e6c54f44b69", "08d29566-3d35-4713-9a87-50f5d5cb4ac6", "91ee45e9-591e-4100-9e56-cef5fcf3d88b", "a1dea19a-b472-4445-a1cb-f972803ddaa5", "bd409718-af93-42eb-ab96-73f34ed2934f", "0042aebc-0ac7-4408-9572-333d93d86b9d", "f0b22b9c-7538-4564-a7d5-d4999c69c4e8", "d25799d6-a0b5-4496-ad34-c37b368f9bba", "da2ad865-a1b8-494a-b9a5-b70724cfdabe", "2a779f7d-157c-451f-bec8-3a9f3a952901", "5bce4097-a720-4346-9ef2-ee9216f9f290"], "metadata": {"page_label": "11", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "4b3c9195-49ef-46a1-bc04-da586869ecc0": {"node_ids": ["a4010cca-a0f4-4c03-a2af-1cf80549465f", "cc205667-9e0a-400d-8055-d241287e6349", "38ee5820-2509-46f2-b725-cb55f3e88168", "439e17dc-9d7a-4928-ab30-c0bcbeffb9d3", "56397c8d-a735-4e92-98aa-7280c4d91845", "6a0836e9-5cef-4080-aee6-8245acb184fe", "12eed830-25b2-4743-b818-531a7567923e", "e6bba099-272d-4193-8426-e836167810be", "b4ce308a-7cbb-41cd-b975-3c4907187238", "abee7795-8c88-40b1-b005-024a6605c0ed", "53db8acf-dacc-43d1-9b71-da0733f1b590", "87a07630-fd48-4cc8-a4fb-87b61d2c651b", "e1a7dc18-5fe0-44bd-96ed-3b4443ea7a1c", "44a21df2-4dd0-47a9-96d3-07dc7b22ea17", "a2c2e9d6-4091-45dd-9754-0cb9cac0ec89", "1e0e1ccc-96c7-482c-a028-0f47c5f704d6"], "metadata": {"page_label": "12", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "c855f4f0-3f40-42b9-ab44-bc338badf868": {"node_ids": ["67b293de-51ac-4be7-b1d7-309f3bcf2f57", "b27f9de5-bbe4-421f-b677-fa36200836bd", "173cb8fb-4926-4e7c-b254-af74e55813dc", "3431851a-bda9-4cfa-a555-95089afb452e", "d12797e7-db32-46f3-a145-dec8296d1752", "04cbb945-790d-4e6b-bd07-e017beaba9ce", "c84ce928-12c8-4d14-af6f-bac7a49c486d"], "metadata": {"page_label": "13", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "e99a4afd-2fcf-4693-b29e-85849571a84f": {"node_ids": ["5545bc25-a275-4b36-94e5-b71f10fc17d8", "d72fc83f-82fe-478e-b567-405349b260b7", "7ceabaf6-f4c3-4d42-b228-30dad5a402e4", "569cde60-c250-4f05-8f87-d003e6feff30", "2abba5b6-8fa5-4edf-87c5-c1690fe2a72b", "8ec081dd-8c33-499a-9d21-b9452427f753", "c468cabe-e9b3-4838-8b3e-b64e2637a0bd", "1ef15fb1-864c-442d-8115-38f3adfdbac5", "866c4aa6-3d9e-49ca-a88e-ecc0f76eb14e"], "metadata": {"page_label": "14", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "1c062b2a-4e48-4b73-9f7b-18d95bab2ecb": {"node_ids": ["c544a717-cf0d-42ed-9e1a-f8e27d0fc5d5", "6d5ea79e-b226-41ff-a5cb-4e3d61c990e4", "e4e0a8cb-0b74-44a6-9646-ace0b17d7fc9", "852b8105-35fc-4b37-84a8-07d97ade6e11", "40f3da90-4f47-42c5-8a4b-af0f5f6de6d6", "641771b9-5abe-49c6-905f-d8b33cf929b3", "4d5dc2fa-8a8c-445e-8ff4-192d6cd76945"], "metadata": {"page_label": "15", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "bdc0e7c7-5d52-4bca-b2a4-29565c8f2ce4": {"node_ids": ["a9c52cf6-8d73-451a-8057-a2193466b478", "76797836-1114-4276-9b42-edaad528ac64", "9583a3d8-fa88-470d-bdfd-50b7387e9d67"], "metadata": {"page_label": "16", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "26629a4a-3c25-46ff-9bbc-81830c2e4e23": {"node_ids": ["69ef3de6-6b7e-4742-94f7-60babba03207", "297e8654-d4af-4953-93f6-113e68f13be2", "6272b04c-3104-401d-8688-6e860b899445", "a15afcee-cb0c-421c-9ba9-aae3a07314e4", "56e1fef3-d519-4d6d-a658-9614f4a650c7", "7b7f14fe-7dc6-4f00-a54d-61ff21090099", "99d19f7a-a851-48f9-bab7-59d5c2e01a86", "0c7c02db-1a66-47f9-b394-4cce0dc7f87b", "cf69cb07-f436-444d-9cf8-f739ae111f77", "d6cfe3c2-94d9-4b37-8078-f5e29ac8caea", "67842a45-8fac-4e05-906a-8f7a344ed3d6", "3142e38e-36c7-4529-bd56-ec6120174492", "4916c6b9-ac86-4312-9c75-72461297d832"], "metadata": {"page_label": "17", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "4bc3c38f-1ec7-46eb-933c-7198e45be35a": {"node_ids": ["85cd3106-1457-4da8-95fe-bfda6b641292", "f393cbd7-fb20-477c-b7cb-8fc3b1d0da83", "b1ba75bc-458b-45f7-9060-5615b53b9a8d"], "metadata": {"page_label": "18", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "2cf3b0b0-0931-4e79-bb17-3a77ea4e9ad3": {"node_ids": ["62c27968-04c5-4fad-9298-b0236b4cd4ce", "1ae0425d-221b-4726-8437-68ba916b0471", "f3452640-e8cf-4e2c-8de9-06d639fd2f8f"], "metadata": {"page_label": "19", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}, "40d3bf4a-2890-4bb2-b3fc-9f8735b2f4af": {"node_ids": ["ea135e51-f890-4440-a2d3-11ee1284be76", "d79ce0e6-15c0-4850-86b5-2adc09ee0875", "da5aa105-0ca3-4a65-a43f-23b180cddc74"], "metadata": {"page_label": "20", "file_name": "ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_path": "data/pdf/ColPali Efficient Document Retrieval with Vision Language Models.pdf", "file_type": "application/pdf", "file_size": 9380119, "creation_date": "2024-10-10", "last_modified_date": "2024-10-10"}}}}